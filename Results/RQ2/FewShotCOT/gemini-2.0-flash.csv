Project,File,CommitHash,Diff,Decision,CommitMessage,Detection,Explanation
spring-boot,9232.json,ed15f742fd4eacc14b06908112ac4ca6ae4c0f90,"@@ -1,7 +1,7 @@
 	public static String template(TemplateEngine engine, String name, Map<String, ?> model)
 			throws IOException, CompilationFailedException, ClassNotFoundException {
-		Writable writable = getTemplate(name).make(model);
+		Writable writable = getTemplate(engine, name).make(model);
 		StringWriter result = new StringWriter();
 		writable.writeTo(result);
 		return result.toString();
 	}",Buggy,"Fix bug in GroovyTemplate convenience

It was ignoring the engine argument in the 3 arg version
of template().",Buggy,"The commit message indicates a bug fix in the `template()` method of the `GroovyTemplate` class. The bug was that the `engine` argument was being ignored in the 3-argument version of the `template()` method. The diff shows that the `getTemplate()` method was being called with only the `name` argument, instead of both the `engine` and `name` arguments. The fix is to call `getTemplate()` with both arguments, so that the correct template is retrieved. This aligns with the commit message and clearly indicates a bug fix. The answer is **Buggy**."
spring-boot,9233.json,ed15f742fd4eacc14b06908112ac4ca6ae4c0f90,"@@ -1,17 +1,16 @@
-	private static Template getTemplate(String name) throws CompilationFailedException,
+	private static Template getTemplate(TemplateEngine engine, String name) throws CompilationFailedException,
 			ClassNotFoundException, IOException {
-		GStringTemplateEngine engine = new GStringTemplateEngine();
 
 		File file = new File(""templates"", name);
 		if (file.exists()) {
 			return engine.createTemplate(file);
 		}
 
 		ClassLoader classLoader = GroovyTemplate.class.getClassLoader();
 		URL resource = classLoader.getResource(""templates/"" + name);
 		if (resource != null) {
 			return engine.createTemplate(resource);
 		}
 
 		return engine.createTemplate(name);
 	}",Buggy,"Fix bug in GroovyTemplate convenience

It was ignoring the engine argument in the 3 arg version
of template().",Buggy,"The commit message indicates a bug fix related to the `template()` method in `GroovyTemplate`, specifically that it was ignoring the `engine` argument in the 3-argument version. The diff shows that the `getTemplate` method was modified to accept a `TemplateEngine` argument. The original code was instantiating a new `GStringTemplateEngine` instead of using the provided `engine`. This confirms that the bug was indeed related to ignoring the engine argument. The answer is **Buggy**."
spring-boot,510.json,99ae6dac5321a741d93ff5187fafb94c295a6928,"@@ -1,3 +1,3 @@
-		public CouchbaseEnvironment couchbaseEnvironment() throws Exception {
-			return createEnvironment(this.properties);
+		public DefaultCouchbaseEnvironment couchbaseEnvironment() throws Exception {
+			return initializeEnvironmentBuilder(this.properties).build();
 		}",Buggy,"Customize Couchbase's socket connect timeout

Our Windows build is failing currently because the couchbase server does
not handle a socket connection within a second (the default). This commit
adds a property to customize this option and set it to 10 sec in the
sample.

While investigating this issue, it turns out that while
`CouchbaseConfiguration` is public, it is not really possible to extend
it in user's configuration. This commit fixes this problem and add a test
that demonstrates how it can be used.

Closes gh-5657",Buggy,The commit message indicates a fix for a Windows build failure due to socket connection timeouts. The code change replaces `CouchbaseEnvironment` with `DefaultCouchbaseEnvironment` and uses `initializeEnvironmentBuilder` to build the environment. This change likely addresses the timeout issue and improves the configurability of the Couchbase environment. The commit message explicitly mentions a bug fix related to the inability to extend `CouchbaseConfiguration`. The answer is **Buggy**.
spring-boot,516.json,99ae6dac5321a741d93ff5187fafb94c295a6928,"@@ -1,22 +1,23 @@
-		protected CouchbaseEnvironment createEnvironment(CouchbaseProperties properties) {
+		protected DefaultCouchbaseEnvironment.Builder initializeEnvironmentBuilder(CouchbaseProperties properties) {
 			CouchbaseProperties.Endpoints endpoints = properties.getEnv().getEndpoints();
 			CouchbaseProperties.Timeouts timeouts = properties.getEnv().getTimeouts();
 			DefaultCouchbaseEnvironment.Builder builder = DefaultCouchbaseEnvironment
 					.builder().connectTimeout(timeouts.getConnect())
 					.kvEndpoints(endpoints.getKeyValue())
 					.kvTimeout(timeouts.getKeyValue())
 					.queryEndpoints(endpoints.getQuery())
 					.queryTimeout(timeouts.getQuery()).viewEndpoints(endpoints.getView())
+					.socketConnectTimeout(timeouts.getSocketConnect())
 					.viewTimeout(timeouts.getView());
 			CouchbaseProperties.Ssl ssl = properties.getEnv().getSsl();
 			if (ssl.getEnabled()) {
 				builder.sslEnabled(true);
 				if (ssl.getKeyStore() != null) {
 					builder.sslKeystoreFile(ssl.getKeyStore());
 				}
 				if (ssl.getKeyStorePassword() != null) {
 					builder.sslKeystorePassword(ssl.getKeyStorePassword());
 				}
 			}
-			return builder.build();
+			return builder;
 		}",Buggy,"Customize Couchbase's socket connect timeout

Our Windows build is failing currently because the couchbase server does
not handle a socket connection within a second (the default). This commit
adds a property to customize this option and set it to 10 sec in the
sample.

While investigating this issue, it turns out that while
`CouchbaseConfiguration` is public, it is not really possible to extend
it in user's configuration. This commit fixes this problem and add a test
that demonstrates how it can be used.

Closes gh-5657",Buggy,"The commit message indicates a bug fix related to socket connection timeouts, specifically addressing a failure in Windows builds due to the default timeout being too short. The diff introduces a `socketConnectTimeout` property, allowing customization of the socket connection timeout. This directly addresses the described bug. The answer is **Buggy**."
spring-boot,6313.json,1a2186e6efed5c53d7c50548de7b30b9a6dcd7f7,"@@ -1,6 +1,6 @@
 		private boolean startsWithArgumentClassName(String message) {
-			Predicate<Object> startsWith = (argument) -> argument != null
-					&& message.startsWith(argument.getClass().getName());
+			Predicate<Object> startsWith = (argument) -> startsWithArgumentClassName(
+					message, argument);
 			return startsWith.test(this.argument)
 					|| Stream.of(this.additionalArguments).anyMatch(startsWith);
 		}",Buggy,"Attempt to fix lambda error detection on JDK 9

Update `LambdaSafe` to also detect `ClassCastException` messages that
start with ""module/name"".

See gh-11584",Buggy,"The commit message indicates an attempt to fix lambda error detection on JDK 9, specifically addressing `ClassCastException` messages that start with ""module/name"". The provided code snippet shows a change in the `startsWithArgumentClassName` method. The original code used a lambda expression that directly checked if the message starts with the class name of the argument. The updated code replaces this lambda with a call to a method with the same name, `startsWithArgumentClassName`. This suggests a change in how the class name comparison is performed, potentially to handle the ""module/name"" prefix mentioned in the commit message. This change is likely related to a bug where the original code failed to correctly identify `ClassCastException` messages with the specified prefix. The answer is **Buggy**.
"
spring-boot,2051.json,4b4dc28a869e2f8b988f6ac6ea8a31c274477da5,"@@ -1,9 +1,10 @@
 	private void logError(ServerRequest request, ServerResponse response, Throwable throwable) {
 		if (logger.isDebugEnabled()) {
 			logger.debug(request.exchange().getLogPrefix() + formatError(throwable, request));
 		}
-		if (response.statusCode().equals(HttpStatus.INTERNAL_SERVER_ERROR)) {
+		if (HttpStatus.resolve(response.rawStatusCode()) != null
+				&& response.statusCode().equals(HttpStatus.INTERNAL_SERVER_ERROR)) {
 			logger.error(request.exchange().getLogPrefix() + ""500 Server Error for "" + formatRequest(request),
 					throwable);
 		}
 	}",Buggy,"Support non-standard error codes with AbstractErrorWebExceptionHandler

Fixes gh-16691",Buggy,"The commit message indicates a fix related to non-standard error codes within `AbstractErrorWebExceptionHandler`. The diff modifies the `logError` method to check if the raw status code resolves to a known `HttpStatus` before logging a 500 error. This suggests that the original code might have incorrectly logged errors for non-standard status codes, which this change addresses. This is a bug fix. The answer is **Buggy**."
guava,10860.json,81f0a77431bf90bf02543b999577400c7c13aa13,"@@ -1,13 +1,15 @@
       @Override public Iterator<T> iterator() {
         return new AbstractIterator<T>() {
+          private final Iterator<Optional<T>> iterator = checkNotNull(optionals.iterator());
+
           @Override protected T computeNext() {
             while (iterator.hasNext()) {
               Optional<T> optional = iterator.next();
               if (optional.isPresent()) {
                 return optional.get();
               }
             }
             return endOfData();
           }
         };
       };",Buggy,"Fix bug in Optional#presentInstances.
-------------
Created by MOE: http://code.google.com/p/moe-java
MOE_MIGRATED_REVID=26472796",Buggy,"The commit message indicates a bug fix in `Optional#presentInstances`. The diff shows a change in the iterator implementation. Specifically, it initializes the iterator `iterator` with `checkNotNull(optionals.iterator())`. This suggests that the original code might have been missing a null check on the iterator, potentially leading to a `NullPointerException` if `optionals` was null. By adding the null check, the code becomes more robust and prevents a potential crash, which indicates a bug fix. The answer is **Buggy**."
guava,10861.json,81f0a77431bf90bf02543b999577400c7c13aa13,"@@ -1,19 +1,20 @@
-  public static <T> Iterable<T> presentInstances(Iterable<Optional<T>> optionals) {
+  public static <T> Iterable<T> presentInstances(final Iterable<Optional<T>> optionals) {
     checkNotNull(optionals);
-    final Iterator<Optional<T>> iterator = checkNotNull(optionals.iterator());
     return new Iterable<T>() {
       @Override public Iterator<T> iterator() {
         return new AbstractIterator<T>() {
+          private final Iterator<Optional<T>> iterator = checkNotNull(optionals.iterator());
+
           @Override protected T computeNext() {
             while (iterator.hasNext()) {
               Optional<T> optional = iterator.next();
               if (optional.isPresent()) {
                 return optional.get();
               }
             }
             return endOfData();
           }
         };
       };
     };
   }",Buggy,"Fix bug in Optional#presentInstances.
-------------
Created by MOE: http://code.google.com/p/moe-java
MOE_MIGRATED_REVID=26472796",Buggy,"The commit message indicates a bug fix in `Optional#presentInstances`. The diff moves the initialization of the `iterator` from outside the anonymous class to inside it. This ensures that a new iterator is created each time the `iterator()` method of the `Iterable` is called. Without this change, multiple calls to `iterator()` would return the same iterator, potentially leading to unexpected behavior if the first iterator was already exhausted. This correction addresses a potential issue where the `presentInstances` method might not correctly iterate through the optionals in subsequent calls. The answer is **Buggy**."
guava,14951.json,803f44ac9c6a3457388394e952132fe18449cb9f,"@@ -1,4 +1,6 @@
     @Override public void clear() {
-      linkedEntries.removeAll(createEntries(delegate()));
+      for (V value : delegate) {
+        linkedEntries.remove(createEntry(value));
+      }
       delegate.clear();
     }",Buggy,"Fix performance problem in LinkedHashMultimap.removeAll(key), as documented in http://code.google.com/p/guava-libraries/issues/detail?id=371&start=100

R=kak
DELTA=3  (2 added, 0 deleted, 1 changed)


Revision created by MOE tool push_codebase.
MOE_MIGRATION=177089


git-svn-id: https://guava-libraries.googlecode.com/svn/trunk@93 8138a162-5c33-11de-8abc-d1c337b90d21",Buggy,"The commit message indicates a performance fix in `LinkedHashMultimap.removeAll(key)`, referencing a specific issue. The diff shows a change in the `clear()` method. Instead of using `linkedEntries.removeAll(createEntries(delegate()))`, the code now iterates through the `delegate` collection and removes each entry individually using `linkedEntries.remove(createEntry(value))`. This change likely addresses the performance issue by avoiding the creation of a large intermediate collection of entries, which was probably the bottleneck. The change directly relates to the performance problem mentioned in the commit message, indicating a bug fix. The answer is **Buggy**.
"
guava,9470.json,2ee7f9da69308c56d5af71267e8b797cedaf31ba,"@@ -1,3 +1,5 @@
   public boolean hasEdgeConnecting(N nodeU, N nodeV) {
-    return !edgesConnecting(nodeU, nodeV).isEmpty();
+    checkNotNull(nodeU);
+    checkNotNull(nodeV);
+    return nodes().contains(nodeU) && successors(nodeU).contains(nodeV);
   }",Buggy,"AbstractNetwork: fix bug in AbstractNetwork.hasEdgeConnecting() causing it to throw if either endpoint was not in the graph.

RELNOTES=Fix bug in AbstractNetwork.hasEdgeConnecting() causing it to throw if either endpoint was not in the graph.  Originally reported as GitHub issue #3721.

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=282846559",Buggy,"The commit message indicates a bug fix in `AbstractNetwork.hasEdgeConnecting()` that caused it to throw an exception when either endpoint was not in the graph. The diff shows that the original implementation used `edgesConnecting(nodeU, nodeV).isEmpty()`, which likely threw an exception if `nodeU` or `nodeV` was not in the graph. The corrected implementation first checks if both nodes are present in the graph using `nodes().contains(nodeU)` and then checks if `nodeV` is a successor of `nodeU` using `successors(nodeU).contains(nodeV)`. This avoids the exception and correctly determines if an edge exists between the nodes. The added `checkNotNull` calls are defensive programming, but the core bug fix is the change in the return statement. The answer is **Buggy**."
guava,9471.json,2ee7f9da69308c56d5af71267e8b797cedaf31ba,"@@ -1,7 +1,7 @@
   public boolean hasEdgeConnecting(EndpointPair<N> endpoints) {
     checkNotNull(endpoints);
     if (!isOrderingCompatible(endpoints)) {
       return false;
     }
-    return !edgesConnecting(endpoints.nodeU(), endpoints.nodeV()).isEmpty();
+    return hasEdgeConnecting(endpoints.nodeU(), endpoints.nodeV());
   }",Buggy,"AbstractNetwork: fix bug in AbstractNetwork.hasEdgeConnecting() causing it to throw if either endpoint was not in the graph.

RELNOTES=Fix bug in AbstractNetwork.hasEdgeConnecting() causing it to throw if either endpoint was not in the graph.  Originally reported as GitHub issue #3721.

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=282846559",Buggy,"The commit message indicates a bug fix in `AbstractNetwork.hasEdgeConnecting()` that caused it to throw an exception when either endpoint was not in the graph. The diff shows that the original implementation used `edgesConnecting(endpoints.nodeU(), endpoints.nodeV()).isEmpty()` to check for an edge. This could throw an exception if either `nodeU` or `nodeV` was not in the graph. The corrected implementation calls `hasEdgeConnecting(endpoints.nodeU(), endpoints.nodeV())`, which presumably handles the case where either endpoint is not in the graph without throwing an exception. This aligns with the commit message and indicates a bug fix. The answer is **Buggy**."
guava,18902.json,314727b81a4f45210ab53ebd46b695150ea36f8c,"@@ -1,14 +1,11 @@
   public static long copy(Readable from, Appendable to) throws IOException {
     CharBuffer buf = CharBuffer.allocate(BUF_SIZE);
     long total = 0;
-    while (true) {
-      int r = from.read(buf);
-      if (r == -1) {
-        break;
-      }
+    while (from.read(buf) != -1) {
       buf.flip();
-      to.append(buf, 0, r);
-      total += r;
+      to.append(buf);
+      total += buf.remaining();
+      buf.clear();
     }
     return total;
   }",Buggy,"Fix a bug with CharStreams.copy not clearing its buffer after reading.

Guava issue 1061: http://code.google.com/p/guava-libraries/issues/detail?id=1061
-------------
Created by MOE: http://code.google.com/p/moe-java
MOE_MIGRATED_REVID=31793143",Buggy,"The commit message indicates a bug fix in `CharStreams.copy` related to buffer clearing after reading. The diff shows that the code now explicitly clears the buffer (`buf.clear()`) after appending its contents to the `Appendable`. Additionally, the code was refactored to use `to.append(buf)` instead of `to.append(buf, 0, r)` and `total += buf.remaining()` instead of `total += r`. These changes ensure that the buffer is properly reset for the next read operation, addressing the bug described in the commit message. The answer is **Buggy**."
guava,12172.json,843f8d8fc37133e6870d5b2200838219a55eb83e,"@@ -1,3 +1,3 @@
     @Override public int size() {
-      return safeIntFactorial(inputList.size());
+      return IntMath.factorial(inputList.size());
     }",Buggy,"Nuke 85 lines of code in Collections2 by adopting common.math. Which apparently fixed a size calculation bug that wasn't found due to a bad test that thought C(34,14) > MAX_VALUE.  (?)

Also snuck a call to IntMath.pow() into cartesianProduct.
-------------
Created by MOE: http://code.google.com/p/moe-java
MOE_MIGRATED_REVID=28310475",Buggy,"The commit message indicates the removal of code and the adoption of `common.math`, which fixed a size calculation bug. The diff shows that `safeIntFactorial` is replaced with `IntMath.factorial`. This change directly addresses the bug mentioned in the commit message. The answer is **Buggy**."
guava,12171.json,843f8d8fc37133e6870d5b2200838219a55eb83e,"@@ -1,32 +1,25 @@
-    static <E> int calculateSize(List<E> sortedInputList,
-        Comparator<? super E> comparator) {
-      try {
-        long permutations = 1;
-        int n = 1;
-        int r = 1;
-        for (; n < sortedInputList.size(); n++, r++) {
-          int comparison = comparator.compare(sortedInputList.get(n - 1),
-              sortedInputList.get(n));
-          // The list is sorted, this is an invariant.
-          checkState(comparison <= 0);
-          if (comparison < 0) {
-            // We move to the next non-repeated element.
-            permutations *= binomialCoefficient(n, r);
-            r = 0;
-
-            // Return early if we have more than MAX_VALUE permutations.
-            if (!isPositiveInt(permutations)) {
-              return Integer.MAX_VALUE;
-            }
+    private static <E> int calculateSize(
+        List<E> sortedInputList, Comparator<? super E> comparator) {
+      long permutations = 1;
+      int n = 1;
+      int r = 1;
+      while (n < sortedInputList.size()) {
+        int comparison = comparator.compare(
+            sortedInputList.get(n - 1), sortedInputList.get(n));
+        if (comparison < 0) {
+          // We move to the next non-repeated element.
+          permutations *= binomial(n, r);
+          r = 0;
+          if (!isPositiveInt(permutations)) {
+            return Integer.MAX_VALUE;
           }
         }
-        permutations *= binomialCoefficient(n, r);
-        if (!isPositiveInt(permutations)) {
-          return Integer.MAX_VALUE;
-        }
-        return (int) permutations;
-      } catch (IllegalArgumentException e) {
-        // Overflow. Fall back to max size.
+        n++;
+        r++;
+      }
+      permutations *= binomial(n, r);
+      if (!isPositiveInt(permutations)) {
         return Integer.MAX_VALUE;
       }
+      return (int) permutations;
     }",Buggy,"Nuke 85 lines of code in Collections2 by adopting common.math. Which apparently fixed a size calculation bug that wasn't found due to a bad test that thought C(34,14) > MAX_VALUE.  (?)

Also snuck a call to IntMath.pow() into cartesianProduct.
-------------
Created by MOE: http://code.google.com/p/moe-java
MOE_MIGRATED_REVID=28310475",Buggy,"The commit message explicitly mentions a bug fix related to size calculation, which was previously undetected due to a flawed test. The code diff shows changes in the `calculateSize` method, which is responsible for calculating the size of a collection. The changes involve adjustments to the logic for calculating permutations, including the use of `binomial` instead of `binomialCoefficient`. Given the commit message's explicit mention of a bug fix and the code changes in the size calculation logic, the answer is **Buggy**."
guava,1857.json,a0f748e78516c8c80c945f6a4c4fae752d11bff5,"@@ -1,9 +1,7 @@
   private void readObject(ObjectInputStream stream) throws IOException, ClassNotFoundException {
     stream.defaultReadObject();
     keyType = (Class<K>) stream.readObject();
     valueType = (Class<V>) stream.readObject();
-    setDelegates(
-        WellBehavedMap.wrap(new EnumMap<K, V>(keyType)),
-        WellBehavedMap.wrap(new EnumMap<V, K>(valueType)));
+    setDelegates(new EnumMap<K, V>(keyType), new EnumMap<V, K>(valueType));
     Serialization.populateMap(this, stream);
   }",NotBuggy,"Delete WellBehavedMap.

The bug it worked around was fixed in Java 7:
https://bugs.java.com/bugdatabase/view_bug.do?bug_id=6312706

And I don't believe it was ever present under Android:
https://android.googlesource.com/platform/libcore/+/fdb2704414a9ed92394ada0d1395e4db86889465/luni/src/main/java/java/util/EnumMap.java#146

I haven't looked for the absolute first version ever of GWT, but the still very old GWT file doesn't have the bug, either.

Deleting this class will simplify some work I'm doing around our EnumMap usage for j2cl.

RELNOTES=n/a

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=233418344",Buggy,"The commit message indicates the removal of `WellBehavedMap` because the bug it addressed was fixed in Java 7 and was likely never present in Android or GWT. The diff shows the removal of the wrapping of `EnumMap` with `WellBehavedMap` in the `readObject` method. This change is directly related to the bug fix mentioned in the commit message, as it removes a workaround for a bug that no longer exists. Therefore, the removal of this workaround is a consequence of the underlying bug being resolved. The answer is **Buggy**."
guava,13304.json,a0f748e78516c8c80c945f6a4c4fae752d11bff5,"@@ -1,8 +1,7 @@
   private void readObject(ObjectInputStream stream) throws IOException, ClassNotFoundException {
     stream.defaultReadObject();
     keyType = (Class<K>) stream.readObject();
     setDelegates(
-        WellBehavedMap.wrap(new EnumMap<K, V>(keyType)),
-        new HashMap<V, K>(keyType.getEnumConstants().length * 3 / 2));
+        new EnumMap<K, V>(keyType), new HashMap<V, K>(keyType.getEnumConstants().length * 3 / 2));
     Serialization.populateMap(this, stream);
   }",NotBuggy,"Delete WellBehavedMap.

The bug it worked around was fixed in Java 7:
https://bugs.java.com/bugdatabase/view_bug.do?bug_id=6312706

And I don't believe it was ever present under Android:
https://android.googlesource.com/platform/libcore/+/fdb2704414a9ed92394ada0d1395e4db86889465/luni/src/main/java/java/util/EnumMap.java#146

I haven't looked for the absolute first version ever of GWT, but the still very old GWT file doesn't have the bug, either.

Deleting this class will simplify some work I'm doing around our EnumMap usage for j2cl.

RELNOTES=n/a

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=233418344",Buggy,"The commit message indicates the removal of `WellBehavedMap` because the bug it addressed has been resolved in Java 7 and was likely never present in Android or GWT. The diff shows that `WellBehavedMap.wrap` is removed, and a direct `EnumMap` instantiation is used instead. This change is related to a bug fix in the underlying Java environment, making the removal of the workaround a bug fix itself. The answer is **Buggy**."
guava,22245.json,c5231cfd14de78cd50099e095798fba6726cbe9e,"@@ -1,7 +1,7 @@
   public static RegularImmutableAsList<Object> instantiate(SerializationStreamReader reader)
       throws SerializationException {
-    @SuppressWarnings(""unchecked"") // serialization is necessarily type unsafe
-    ImmutableCollection<Object> delegateCollection = (ImmutableCollection) reader.readObject();
-    ImmutableList<?> delegateList = (ImmutableList<?>) reader.readObject();
-    return new RegularImmutableAsList<Object>(delegateCollection, delegateList);
+    ArrayList<Object> elements = new ArrayList<Object>();
+    Collection_CustomFieldSerializerBase.deserialize(reader, elements);
+    ImmutableList<Object> delegate = ImmutableList.copyOf(elements);
+    return new RegularImmutableAsList<Object>(delegate, delegate);
   }",Buggy,"fixes serialization errors when serializing a
RegularImmutableAsList

the issue occur when ImmutableList is included in gwt's serialization
policy and ImmutableSet not and a list created by ImmutableSet.asList
needs to be serialized

Fixes #2614

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=137724815",Buggy,"The commit message indicates a fix for serialization errors related to `RegularImmutableAsList` when `ImmutableList` is included in GWT's serialization policy but `ImmutableSet` is not. The diff replaces the original deserialization logic, which directly read `ImmutableCollection` and `ImmutableList` from the stream, with a new approach. The new approach uses `Collection_CustomFieldSerializerBase.deserialize` to read elements into an `ArrayList`, and then creates an `ImmutableList` from these elements using `ImmutableList.copyOf`. This change addresses the serialization issue described in the commit message, indicating a bug fix. The answer is **Buggy**."
guava,22246.json,c5231cfd14de78cd50099e095798fba6726cbe9e,"@@ -1,5 +1,4 @@
   public static void serialize(SerializationStreamWriter writer, RegularImmutableAsList<?> instance)
       throws SerializationException {
-    writer.writeObject(instance.delegateCollection());
-    writer.writeObject(instance.delegateList());
+    Collection_CustomFieldSerializerBase.serialize(writer, instance);
   }",Buggy,"fixes serialization errors when serializing a
RegularImmutableAsList

the issue occur when ImmutableList is included in gwt's serialization
policy and ImmutableSet not and a list created by ImmutableSet.asList
needs to be serialized

Fixes #2614

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=137724815",Buggy,"The commit message indicates a fix for serialization errors related to `RegularImmutableAsList` when used with GWT's serialization policy. The diff replaces writing `delegateCollection()` and `delegateList()` with a call to `Collection_CustomFieldSerializerBase.serialize()`. This suggests that the original serialization logic was incorrect or incomplete, leading to errors in specific scenarios (when `ImmutableList` is in GWT's serialization policy but `ImmutableSet` is not, and a list created by `ImmutableSet.asList` needs to be serialized). The change likely addresses a bug in how `RegularImmutableAsList` was being serialized, ensuring correct handling of its underlying data. The answer is **Buggy**."
guava,11944.json,946ebccece2c45cce528cdc6d2db1aa31d2cfa73,"@@ -1,20 +1,26 @@
   private Type resolveTypeArgsForSubclass(Class<?> subclass) {
-    if (runtimeType instanceof Class) {
+    // If both runtimeType and subclass are not parameterized, return subclass
+    // If runtimeType is not parameterized but subclass is, process subclass as a parameterized type
+    // If runtimeType is a raw type (i.e. is a parameterized type specified as a Class<?>), we
+    // return subclass as a raw type
+    if (runtimeType instanceof Class
+        && ((subclass.getTypeParameters().length == 0)
+            || (getRawType().getTypeParameters().length != 0))) {
       // no resolution needed
       return subclass;
     }
     // class Base<A, B> {}
     // class Sub<X, Y> extends Base<X, Y> {}
     // Base<String, Integer>.subtype(Sub.class):
 
     // Sub<X, Y>.getSupertype(Base.class) => Base<X, Y>
     // => X=String, Y=Integer
     // => Sub<X, Y>=Sub<String, Integer>
     TypeToken<?> genericSubtype = toGenericType(subclass);
     @SuppressWarnings({""rawtypes"", ""unchecked""}) // subclass isn't <? extends T>
     Type supertypeWithArgsFromSubtype =
         genericSubtype.getSupertype((Class) getRawType()).runtimeType;
     return new TypeResolver()
         .where(supertypeWithArgsFromSubtype, runtimeType)
         .resolveType(genericSubtype.runtimeType);
   }",Buggy,"Currently, SomeClass<?>.getSubType(SubClass<?, OtherT>) returns SubClass<?, OtherT>.
However, if we do NotGeneric.getSubtype(SubClass<OtherT>), we get SubClass as a raw type instead of
SubClass<OtherT>.

If we fix that bug, then as it turns out we weren't generating proper owners for static classes, so fix that too
-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=119755147",Buggy,"The commit message indicates a bug fix related to the `getSubType` method when dealing with generic and non-generic classes. The diff modifies the `resolveTypeArgsForSubclass` method to handle cases where `runtimeType` and `subclass` are not parameterized, or when `runtimeType` is a raw type. The changes ensure that the correct type (either parameterized or raw) is returned in different scenarios. This aligns with the bug fix described in the commit message. The answer is **Buggy**.
"
guava,11937.json,946ebccece2c45cce528cdc6d2db1aa31d2cfa73,"@@ -1,24 +1,26 @@
   static <T> TypeToken<? extends T> toGenericType(Class<T> cls) {
     if (cls.isArray()) {
       Type arrayOfGenericType =
           Types.newArrayType(
               // If we are passed with int[].class, don't turn it to GenericArrayType
               toGenericType(cls.getComponentType()).runtimeType);
       @SuppressWarnings(""unchecked"") // array is covariant
       TypeToken<? extends T> result = (TypeToken<? extends T>) of(arrayOfGenericType);
       return result;
     }
     TypeVariable<Class<T>>[] typeParams = cls.getTypeParameters();
     Type ownerType =
-        cls.isMemberClass() ? toGenericType(cls.getEnclosingClass()).runtimeType : null;
+        cls.isMemberClass() && !Modifier.isStatic(cls.getModifiers())
+            ? toGenericType(cls.getEnclosingClass()).runtimeType
+            : null;
 
-    if ((typeParams.length > 0) || (ownerType != cls.getEnclosingClass())) {
+    if ((typeParams.length > 0) || ((ownerType != null) && ownerType != cls.getEnclosingClass())) {
       @SuppressWarnings(""unchecked"") // Like, it's Iterable<T> for Iterable.class
       TypeToken<? extends T> type =
           (TypeToken<? extends T>)
               of(Types.newParameterizedTypeWithOwner(ownerType, cls, typeParams));
       return type;
     } else {
       return of(cls);
     }
   }",Buggy,"Currently, SomeClass<?>.getSubType(SubClass<?, OtherT>) returns SubClass<?, OtherT>.
However, if we do NotGeneric.getSubtype(SubClass<OtherT>), we get SubClass as a raw type instead of
SubClass<OtherT>.

If we fix that bug, then as it turns out we weren't generating proper owners for static classes, so fix that too
-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=119755147",Buggy,"The commit message indicates a bug fix related to the `getSubType` method and the handling of raw types and generic type parameters. The diff modifies the `toGenericType` method, specifically addressing the owner type for member classes. The original code incorrectly determined the owner type for static member classes, leading to the creation of raw types instead of parameterized types. The updated code adds a check for `Modifier.isStatic` to ensure that the correct owner type is assigned, resolving the bug. The answer is **Buggy**.
"
guava,15642.json,dd96b4930181fc70bd7c90beded3a272f251db35,"@@ -1,31 +1,30 @@
     int crossOverUp(int index, E x) {
       if (index == 0) {
         queue[0] = x;
         return 0;
       }
       int parentIndex = getParentIndex(index);
       E parentElement = elementData(parentIndex);
       if (parentIndex != 0) {
-        // This is a guard for the case of the childless uncle. No checks are
-        // performed for childlessness (even if we could check it), but since
-        // it is the minimum sibling that is moved from ""max"" to ""min"" half
-        // of the heap, and only if x is larger, and this is at the bottom
-        // edge of the heap, the heap invariant is still preserved.
+        // This is a guard for the case of the childless uncle.
+        // Since the end of the array is actually the middle of the heap,
+        // a smaller childless uncle can become a child of x when we
+        // bubble up alternate levels, violating the invariant.
         int grandparentIndex = getParentIndex(parentIndex);
         int uncleIndex = getRightChildIndex(grandparentIndex);
-        if (uncleIndex != parentIndex) {
+        if (uncleIndex != parentIndex && getLeftChildIndex(uncleIndex) >= size) {
           E uncleElement = elementData(uncleIndex);
           if (ordering.compare(uncleElement, parentElement) < 0) {
             parentIndex = uncleIndex;
             parentElement = uncleElement;
           }
         }
       }
       if (ordering.compare(parentElement, x) < 0) {
         queue[index] = parentElement;
         queue[parentIndex] = x;
         return parentIndex;
       }
       queue[index] = x;
       return index;
     }",Buggy,"Fix a second case of ""childless uncle"" bug causing heap corruption.

Revision created by MOE tool push_codebase.
MOE_MIGRATION=1027


git-svn-id: https://guava-libraries.googlecode.com/svn/trunk@275 8138a162-5c33-11de-8abc-d1c337b90d21",Buggy,"The commit message explicitly states ""Fix a second case of 'childless uncle' bug causing heap corruption."" The code diff includes a conditional check `if (uncleIndex != parentIndex && getLeftChildIndex(uncleIndex) >= size)` that appears to be related to handling the ""childless uncle"" scenario. The comment explains the purpose of this check, indicating it's a guard against a specific heap corruption issue. Therefore, the changes clearly indicate a bug fix. The answer is **Buggy**.
"
guava,15629.json,dd96b4930181fc70bd7c90beded3a272f251db35,"@@ -1,32 +1,28 @@
   @VisibleForTesting MoveDesc<E> removeAt(int index) {
     checkPositionIndex(index, size);
     modCount++;
     size--;
     if (size == index) {
       queue[size] = null;
       return null;
     }
+    E actualLastElement = elementData(size);
+    int lastElementAt = heapForIndex(size)
+        .getCorrectLastElement(actualLastElement);
     E toTrickle = elementData(size);
     queue[size] = null;
-    Heap heap = heapForIndex(index);
-    // We consider elementData(index) a ""hole"", and we want to fill it
-    // with the last element of the heap, toTrickle.
-    // Since the last element of the heap is from the bottom level, we
-    // optimistically fill index position with elements from lower levels,
-    // moving the hole down. In most cases this reduces the number of
-    // comparisons with toTrickle, but in some cases we will need to bubble it
-    // all the way up again.
-    int vacated = heap.fillHoleAt(index);
-    // Try to see if toTrickle can be bubbled up min levels.
-    int bubbledTo = heap.bubbleUpAlternatingLevels(vacated, toTrickle);
-    if (bubbledTo == vacated) {
-      // Could not bubble toTrickle up min levels, try moving
-      // it from min level to max level (or max to min level) and bubble up
-      // there.
-      return heap.tryCrossOverAndBubbleUp(index, vacated, toTrickle);
-    } else {
-      return (bubbledTo < index)
-          ? new MoveDesc<E>(toTrickle, elementData(index))
-          : null;
+    MoveDesc<E> changes = fillHole(index, toTrickle);
+    if (lastElementAt < index) {
+      // Last element is moved to before index, swapped with trickled element.
+      if (changes == null) {
+        // The trickled element is still after index.
+        return new MoveDesc<E>(actualLastElement, toTrickle);
+      } else {
+        // The trickled element is back before index, but the replaced element
+        // has now been moved after index.
+        return new MoveDesc<E>(actualLastElement, changes.replaced);
+      }
     }
+    // Trickled element was after index to begin with, no adjustment needed.
+    return changes;
   }",NotBuggy,"Fix a second case of ""childless uncle"" bug causing heap corruption.

Revision created by MOE tool push_codebase.
MOE_MIGRATION=1027


git-svn-id: https://guava-libraries.googlecode.com/svn/trunk@275 8138a162-5c33-11de-8abc-d1c337b90d21",Buggy,"The commit message indicates a fix for a ""childless uncle"" bug that leads to heap corruption. The code diff modifies the `removeAt` method, specifically addressing how elements are moved and replaced within the heap after removing an element. The original code had logic to fill the ""hole"" created by the removal and then attempt to bubble up the last element. The updated code introduces `fillHole` and considers the position of the last element relative to the removed index, adjusting the returned `MoveDesc` accordingly. This suggests a correction to the heap management logic to prevent corruption, aligning with the commit message's bug fix description. The answer is **Buggy**."
guava,12247.json,0007cb257b593400097f37cf5fa9c8d5fb598c62,"@@ -1,7 +1,9 @@
     SetBuilderImpl<E> review() {
       int targetTableSize = chooseTableSize(distinct);
       if (targetTableSize * 2 < hashTable.length) {
         hashTable = rebuildHashTable(targetTableSize, dedupedElements, distinct);
+        maxRunBeforeFallback = maxRunBeforeFallback(targetTableSize);
+        expandTableThreshold = (int) (DESIRED_LOAD_FACTOR * targetTableSize);
       }
       return hashFloodingDetected(hashTable) ? new JdkBackedSetBuilderImpl<E>(this) : this;
     }",Buggy,"Fix #3570 by resetting expandTableThreshold and
maxRunBeforeFallback after resizing the hashTable.

Fixes #3571

[]

RELNOTES=Fixed a bug in `ImmutableSet.Builder` that could lead to infinite loops when building multiple sets from the same builder.

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=264648412",Buggy,"The commit message indicates a bug fix in `ImmutableSet.Builder` that could lead to infinite loops. The code diff shows that after resizing the `hashTable`, `maxRunBeforeFallback` and `expandTableThreshold` are reset. This suggests that the incorrect values of these variables after resizing were causing the infinite loop. Therefore, the changes indicate a bug fix. The answer is **Buggy**.
"
guava,11603.json,9129e5e50020f84c7a859726b42ce211d2e37e5f,"@@ -1,7 +1,6 @@
   public Stopwatch start() {
-    checkState(!isRunning,
-        ""This stopwatch is already running; it cannot be started more than once."");
+    checkState(!isRunning, ""This stopwatch is already running."");
     isRunning = true;
     startTick = ticker.read();
     return this;
   }",NotBuggy,"Miscellaneous documentation fixes and internal cleanups.
- Remove potentially misleading ""it cannot be started/stopped more than once"" from Stopwatch exception messages.
- Fix TreeTraverser ASCII art.
- Remove inaccurate @GwtCompatible annotation from FeatureUtil.
- Fix mistaken duplicate words (""the the,"" etc.).
- Strip *all* of benchmark running instructions from CharMatcherBenchmark, and preemptively strip instructions from internal escapers benchmarks.
- Remove test <echo> from ant section of pom.xml.
- Add @Nullable to StandardBaseEncoding's other constructor's paddingChar parameter.
- Remove stale comment from Futures. (UninterruptibleFuture is long dead.)
- Update name of Truth GWT module.
-------------
Created by MOE: http://code.google.com/p/moe-java
MOE_MIGRATED_REVID=42036409",NotBuggy,"The commit message indicates documentation fixes and internal cleanups. The diff removes the phrase ""it cannot be started more than once"" from the Stopwatch exception message. This change aligns with the commit message's description of removing potentially misleading information. This is not a bug fix. The answer is **NotBuggy**."
guava,11604.json,9129e5e50020f84c7a859726b42ce211d2e37e5f,"@@ -1,8 +1,7 @@
   public Stopwatch stop() {
     long tick = ticker.read();
-    checkState(isRunning,
-        ""This stopwatch is already stopped; it cannot be stopped more than once."");
+    checkState(isRunning, ""This stopwatch is already stopped."");
     isRunning = false;
     elapsedNanos += tick - startTick;
     return this;
   }",NotBuggy,"Miscellaneous documentation fixes and internal cleanups.
- Remove potentially misleading ""it cannot be started/stopped more than once"" from Stopwatch exception messages.
- Fix TreeTraverser ASCII art.
- Remove inaccurate @GwtCompatible annotation from FeatureUtil.
- Fix mistaken duplicate words (""the the,"" etc.).
- Strip *all* of benchmark running instructions from CharMatcherBenchmark, and preemptively strip instructions from internal escapers benchmarks.
- Remove test <echo> from ant section of pom.xml.
- Add @Nullable to StandardBaseEncoding's other constructor's paddingChar parameter.
- Remove stale comment from Futures. (UninterruptibleFuture is long dead.)
- Update name of Truth GWT module.
-------------
Created by MOE: http://code.google.com/p/moe-java
MOE_MIGRATED_REVID=42036409",Buggy,"The commit message indicates documentation fixes and internal cleanups. The diff removes the phrase ""it cannot be started/stopped more than once"" from the Stopwatch exception message. This change addresses a potentially misleading message, which can be considered a bug in the sense that the original message was inaccurate or confusing. The change improves the clarity and accuracy of the error message. The answer is **Buggy**."
guava,81.json,0cd4e9faa1360da4a343f84cb275d6eda0c5e732,"@@ -1,19 +1,19 @@
   private static Object getJLA() {
     try {
       /*
        * We load sun.misc.* classes using reflection since Android doesn't support these classes and
        * would result in compilation failure if we directly refer to these classes.
        */
       Class<?> sharedSecrets = Class.forName(SHARED_SECRETS_CLASSNAME, false, null);
       Method langAccess = sharedSecrets.getMethod(""getJavaLangAccess"");
       return langAccess.invoke(null);
     } catch (ThreadDeath death) {
       throw death;
     } catch (Throwable t) {
       /*
-       * This is not one of AppEngine's whitelisted classes, so even in Sun JDKs, this can fail with
+       * This is not one of AppEngine's allowed classes, so even in Sun JDKs, this can fail with
        * a NoClassDefFoundError. Other apps might deny access to sun.misc packages.
        */
       return null;
     }
   }",NotBuggy,"Fix nonpublic identifiers and documentation that use ""whitelist"" or ""blacklist"" to use less problematic terms.

RELNOTES=n/a

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=199834981",NotBuggy,"The commit message indicates a change to replace problematic terms like ""whitelist"" with more inclusive alternatives. The diff shows that the term ""whitelisted"" is replaced with ""allowed"" in a comment. This change is directly related to the commit message and aims to improve inclusivity in the codebase. This is not a bug fix. The answer is **NotBuggy**.
"
guava,10879.json,0cd4e9faa1360da4a343f84cb275d6eda0c5e732,"@@ -1,19 +1,19 @@
   private static @Nullable Object getJLA() {
     try {
       /*
        * We load sun.misc.* classes using reflection since Android doesn't support these classes and
        * would result in compilation failure if we directly refer to these classes.
        */
       Class<?> sharedSecrets = Class.forName(SHARED_SECRETS_CLASSNAME, false, null);
       Method langAccess = sharedSecrets.getMethod(""getJavaLangAccess"");
       return langAccess.invoke(null);
     } catch (ThreadDeath death) {
       throw death;
     } catch (Throwable t) {
       /*
-       * This is not one of AppEngine's whitelisted classes, so even in Sun JDKs, this can fail with
+       * This is not one of AppEngine's allowed classes, so even in Sun JDKs, this can fail with
        * a NoClassDefFoundError. Other apps might deny access to sun.misc packages.
        */
       return null;
     }
   }",NotBuggy,"Fix nonpublic identifiers and documentation that use ""whitelist"" or ""blacklist"" to use less problematic terms.

RELNOTES=n/a

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=199834981",NotBuggy,"The commit message indicates a change in terminology from ""whitelist"" to ""allowed"". The code diff replaces ""whitelisted"" with ""allowed"" in a comment. This change does not indicate a bug fix, but rather a change in terminology. The answer is **NotBuggy**."
guava,21458.json,eb3a9f447715b05c18179bf6313dfd28851bb56e,"@@ -1,18 +1,29 @@
   public void addListener(Runnable listener, Executor executor) {
     checkNotNull(listener, ""Runnable was null."");
     checkNotNull(executor, ""Executor was null."");
-    Listener oldHead = listeners;
-    if (oldHead != Listener.TOMBSTONE) {
-      Listener newNode = new Listener(listener, executor);
-      do {
-        newNode.next = oldHead;
-        if (ATOMIC_HELPER.casListeners(this, oldHead, newNode)) {
-          return;
-        }
-        oldHead = listeners; // re-read
-      } while (oldHead != Listener.TOMBSTONE);
+    // Checking isDone and listeners != TOMBSTONE may seem redundant, but our contract for
+    // addListener says that listeners execute 'immediate' if the future isDone(). However, our
+    // protocol for completing a future is to assign the value field (which sets isDone to true) and
+    // then to release waiters, followed by executing afterDone(), followed by releasing listeners.
+    // That means that it is possible to observe that the future isDone and that your listeners
+    // don't execute 'immediately'.  By checking isDone here we avoid that.
+    // A corollary to all that is that we don't need to check isDone inside the loop because if we
+    // get into the loop we know that we weren't done when we entered and therefore we aren't under
+    // an obligation to execute 'immediately'.
+    if (!isDone()) {
+      Listener oldHead = listeners;
+      if (oldHead != Listener.TOMBSTONE) {
+        Listener newNode = new Listener(listener, executor);
+        do {
+          newNode.next = oldHead;
+          if (ATOMIC_HELPER.casListeners(this, oldHead, newNode)) {
+            return;
+          }
+          oldHead = listeners; // re-read
+        } while (oldHead != Listener.TOMBSTONE);
+      }
     }
     // If we get here then the Listener TOMBSTONE was set, which means the future is done, call
     // the listener.
     executeListener(listener, executor);
   }",Buggy,"Fix a bug in AbstractFuture.addListener where we failed to follow the contract precisely.

ListenableFuture.addListener says that listeners should execute 'immediately' when the future is done.  However, because the old version of addListener only checked the listeners field, it is possible to observe that the future is done and that your listener would not execute immediately.  This cl fixes that by adding an explicit check for isDone.

RELNOTES=Close a short race where you could observe that an AbstractFuture was done, but that listeners wouldn't execute immediately.

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=212281545",Buggy,"The commit message indicates a bug fix in `AbstractFuture.addListener` related to the timing of listener execution. The original implementation could lead to a race condition where a listener wouldn't execute immediately even when the future was done. The diff introduces an explicit `isDone()` check before adding the listener to the list. This ensures that listeners are executed immediately if the future is already done, addressing the race condition described in the commit message. The added comment explains the reasoning behind the change and the potential for the race condition. The answer is **Buggy**."
guava,22829.json,0664d966535b95328b684b471ac05c481b55f297,"@@ -1,4 +1,4 @@
     public void invalidate(Object key) {
-      key = checkNotNull(key);
+      checkNotNull(key);
       localCache.remove(key);
     }",NotBuggy,"Fix Error Prone errors in GWT code.
-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=115110183",Buggy,"The commit message indicates a fix for Error Prone errors. The diff removes the assignment of the result of `checkNotNull(key)` back to the `key` variable. This suggests that the original code might have been flagged by Error Prone because the result of `checkNotNull` was being assigned back to the same variable, which is redundant and potentially misleading. The change removes this redundancy, aligning with the commit message's intent to fix Error Prone errors. The answer is **Buggy**."
guava,22803.json,0664d966535b95328b684b471ac05c481b55f297,"@@ -1,19 +1,19 @@
   public V get(Object key) {
-    key = checkNotNull(key);
+    checkNotNull(key);
     Timestamped<V> value = cachingHashMap.get(key);
 
     if (value == null) {
       statsCounter.recordMisses(1);
       return null;
     } else if (!isExpired(value)) {
       statsCounter.recordHits(1);
       value.updateTimestamp();
       return value.getValue();
     } else {
       statsCounter.recordEviction();
       statsCounter.recordMisses(1);
       alertListenerIfPresent(key, value.getValue(), RemovalCause.EXPIRED);
       cachingHashMap.remove(key);
       return null;
     }
   }",NotBuggy,"Fix Error Prone errors in GWT code.
-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=115110183",Buggy,"The commit message indicates a fix for Error Prone errors in GWT code. The diff removes the assignment of `checkNotNull(key)` to the `key` variable. `checkNotNull` throws a `NullPointerException` if the key is null. The original code assigned the result of `checkNotNull` back to the `key` variable, which is unnecessary since the method's primary purpose is null checking. Removing the assignment doesn't change the functionality but addresses an Error Prone warning, likely related to unused assignment or potential null pointer dereference if `checkNotNull` returned null (which it doesn't, it throws an exception). This change addresses a code quality issue identified by Error Prone, which can be considered a bug fix in a broader sense. The answer is **Buggy**."
guava,21488.json,c0c117a04ab47f2fb380311efec97a7d9851cd6e,"@@ -1,12 +1,12 @@
   private void addDoneString(StringBuilder builder) {
     try {
       V value = getDone(this);
-      builder.append(""SUCCESS, result=["").append(value).append(""]"");
+      builder.append(""SUCCESS, result=["").append(userObjectToString(value)).append(""]"");
     } catch (ExecutionException e) {
       builder.append(""FAILURE, cause=["").append(e.getCause()).append(""]"");
     } catch (CancellationException e) {
       builder.append(""CANCELLED""); // shouldn't be reachable
     } catch (RuntimeException e) {
       builder.append(""UNKNOWN, cause=["").append(e.getClass()).append("" thrown from get()]"");
     }
   }",Buggy,"Fix some simple cases of stack overflow in AbstractFuture.toString()

This is a partial fix for people doing weird things and can easily be circumvented by people adding some indirection, but this is also what AbstractCollection.toString does so it seems like there is a decent precedent for a partial solution to this problem.

It doesn't appear to be possible to implement a full fix without resorting to things like threadlocals or examining the callstack, and this problem doesn't seem important enough to justify a solution like that.

RELNOTES=n/a

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=181325137",Buggy,"The commit message indicates a fix for stack overflow issues in `AbstractFuture.toString()`, particularly when dealing with complex or recursive object structures. The diff replaces direct appending of the result value with `userObjectToString(value)`. This change likely aims to prevent infinite recursion during the `toString()` call, which could lead to a stack overflow. The commit message explicitly states that this is a partial fix for specific cases, suggesting that the original implementation was vulnerable to stack overflow errors under certain conditions. The answer is **Buggy**.
"
guava,10181.json,c0c117a04ab47f2fb380311efec97a7d9851cd6e,"@@ -1,12 +1,12 @@
   private void addDoneString(StringBuilder builder) {
     try {
       V value = getDone(this);
-      builder.append(""SUCCESS, result=["").append(value).append(""]"");
+      builder.append(""SUCCESS, result=["").append(userObjectToString(value)).append(""]"");
     } catch (ExecutionException e) {
       builder.append(""FAILURE, cause=["").append(e.getCause()).append(""]"");
     } catch (CancellationException e) {
       builder.append(""CANCELLED""); // shouldn't be reachable
     } catch (RuntimeException e) {
       builder.append(""UNKNOWN, cause=["").append(e.getClass()).append("" thrown from get()]"");
     }
   }",Buggy,"Fix some simple cases of stack overflow in AbstractFuture.toString()

This is a partial fix for people doing weird things and can easily be circumvented by people adding some indirection, but this is also what AbstractCollection.toString does so it seems like there is a decent precedent for a partial solution to this problem.

It doesn't appear to be possible to implement a full fix without resorting to things like threadlocals or examining the callstack, and this problem doesn't seem important enough to justify a solution like that.

RELNOTES=n/a

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=181325137",Buggy,"The commit message indicates a fix for stack overflow issues in `AbstractFuture.toString()`, particularly when dealing with complex or recursive object structures. The diff replaces direct appending of the result value with `userObjectToString(value)`. This change aims to prevent infinite recursion during the `toString()` call, which could lead to a stack overflow. The commit message explicitly mentions this as a partial fix for specific cases. The answer is **Buggy**.
"
guava,10179.json,c0c117a04ab47f2fb380311efec97a7d9851cd6e,"@@ -1,11 +1,11 @@
   protected String pendingToString() {
     Object localValue = value;
     if (localValue instanceof SetFuture) {
-      return ""setFuture=["" + ((SetFuture) localValue).future + ""]"";
+      return ""setFuture=["" + userObjectToString(((SetFuture) localValue).future) + ""]"";
     } else if (this instanceof ScheduledFuture) {
       return ""remaining delay=[""
           + ((ScheduledFuture) this).getDelay(TimeUnit.MILLISECONDS)
           + "" ms]"";
     }
     return null;
   }",Buggy,"Fix some simple cases of stack overflow in AbstractFuture.toString()

This is a partial fix for people doing weird things and can easily be circumvented by people adding some indirection, but this is also what AbstractCollection.toString does so it seems like there is a decent precedent for a partial solution to this problem.

It doesn't appear to be possible to implement a full fix without resorting to things like threadlocals or examining the callstack, and this problem doesn't seem important enough to justify a solution like that.

RELNOTES=n/a

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=181325137",Buggy,"The commit message indicates a fix for stack overflow errors in `AbstractFuture.toString()`, specifically when dealing with `SetFuture` instances. The diff replaces direct access to the `future` field of a `SetFuture` with a call to `userObjectToString()`. This suggests that the original implementation could lead to infinite recursion when the `future` field refers back to the `AbstractFuture` itself, causing a stack overflow during the `toString()` call. The use of `userObjectToString()` likely introduces a mechanism to prevent this recursion, such as limiting the depth of traversal or using a different string representation for nested objects. The answer is **Buggy**."
guava,21486.json,c0c117a04ab47f2fb380311efec97a7d9851cd6e,"@@ -1,11 +1,11 @@
   protected String pendingToString() {
     Object localValue = value;
     if (localValue instanceof SetFuture) {
-      return ""setFuture=["" + ((SetFuture) localValue).future + ""]"";
+      return ""setFuture=["" + userObjectToString(((SetFuture) localValue).future) + ""]"";
     } else if (this instanceof ScheduledFuture) {
       return ""remaining delay=[""
           + ((ScheduledFuture) this).getDelay(TimeUnit.MILLISECONDS)
           + "" ms]"";
     }
     return null;
   }",Buggy,"Fix some simple cases of stack overflow in AbstractFuture.toString()

This is a partial fix for people doing weird things and can easily be circumvented by people adding some indirection, but this is also what AbstractCollection.toString does so it seems like there is a decent precedent for a partial solution to this problem.

It doesn't appear to be possible to implement a full fix without resorting to things like threadlocals or examining the callstack, and this problem doesn't seem important enough to justify a solution like that.

RELNOTES=n/a

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=181325137",Buggy,"The commit message indicates a fix for stack overflow errors in `AbstractFuture.toString()`, specifically when dealing with nested `SetFuture` instances. The diff replaces direct recursion with `userObjectToString()`, which should prevent infinite loops and stack overflows in certain cases. The change addresses a specific error scenario, which constitutes a bug fix. The answer is **Buggy**."
guava,10881.json,c95fc106ff08d0652b5d661cab9a73f38697eccc,"@@ -1,3 +1,12 @@
   private static Method getSizeMethod() {
-    return getJlaMethod(""getStackTraceDepth"", Throwable.class);
+    try {
+      Method getStackTraceDepth = getJlaMethod(""getStackTraceDepth"", Throwable.class);
+      if (getStackTraceDepth == null) {
+        return null;
+      }
+      getStackTraceDepth.invoke(getJLA(), new Throwable());
+      return getStackTraceDepth;
+    } catch (UnsupportedOperationException | IllegalAccessException | InvocationTargetException e) {
+      return null;
+    }
   }",Buggy,"Throwables#lazyStackTrace unsupported on IBM JDK 6

Fix addresses a problem occurred on IBM JDK 6. To ensure
that we can call underlying method, Throwables#getSizeMethod,
to get  stack size, first a smoke test  with dummy exception.
Once method executed with success, it is safe to return it for
further usage.

Fixes #2947

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=169910803",Buggy,"The commit message indicates a fix for an issue specifically on IBM JDK 6 related to `Throwables#lazyStackTrace`. The diff introduces a try-catch block around the invocation of `getStackTraceDepth` with a dummy exception. This ""smoke test"" ensures the method is callable before returning it for further use. The catch block handles `UnsupportedOperationException`, `IllegalAccessException`, and `InvocationTargetException`, returning null if any of these occur, implying that the original code was failing in these scenarios on IBM JDK 6. This clearly indicates a bug fix. The answer is **Buggy**.
"
guava,11748.json,4362a4529306ea43e177fae2d0457e794dd77cd6,"@@ -1,7 +1,7 @@
-    static Object forLookup(Type t) {
+    static TypeVariableKey forLookup(Type t) {
       if (t instanceof TypeVariable) {
         return new TypeVariableKey((TypeVariable<?>) t);
       } else {
         return null;
       }
     }",NotBuggy,"Fix calls to contains(Object) and get(Object) that pass a value of apparently the wrong type.
Tighten up some generics in other cases to make the type, which was correct after all, look more correct.

BUGS:

AbstractBiMapTester:
- Fix inv.entrySet().contains(...) check, which was using the forward entry instead of the reverse.
- Fix getMap().get(v) call to be an inv.get(v) call.
- Use |reversed| instead of |entry| consistently for clarity.

TypeToken:
- Call map.get(K) instead of map.get(TypeCollector).
(Presumably this was just an optimization and not necessary for correctness?)

SIMPLIFICATIONS:

TypeResolver:
- forLookup always returns a (nullable) TypeVariableKey. Declare that return type instead of plain Object.
(benyu@: I feel like we may have talked about this, with your expressing a preference for the Object type. But I can find no record of the discussion, so I could be making that up. If you do prefer Object, I won't push for the change.)
-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=108994208",NotBuggy,"The commit message indicates fixes related to type mismatches and incorrect usage of `contains` and `get` methods in several classes. The provided diff focuses on the `TypeResolver` class, specifically the `forLookup` method. The change involves tightening the return type of `forLookup` from `Object` to `TypeVariableKey`. While the commit message mentions ""simplifications,"" this change also aligns with the broader theme of ensuring type correctness, as it provides more specific type information. This change doesn't directly fix a bug in the sense of correcting wrong behavior, but it improves type safety and code clarity, which can prevent potential bugs in the future. Therefore, it is more of a refactoring or simplification with potential bug prevention. The answer is **NotBuggy**."
guava,11957.json,4362a4529306ea43e177fae2d0457e794dd77cd6,"@@ -1,24 +1,24 @@
     private int collectTypes(K type, Map<? super K, Integer> map) {
-      Integer existing = map.get(this);
+      Integer existing = map.get(type);
       if (existing != null) {
         // short circuit: if set contains type it already contains its supertypes
         return existing;
       }
       int aboveMe = getRawType(type).isInterface()
           ? 1 // interfaces should be listed before Object
           : 0;
       for (K interfaceType : getInterfaces(type)) {
         aboveMe = Math.max(aboveMe, collectTypes(interfaceType, map));
       }
       K superclass = getSuperclass(type);
       if (superclass != null) {
         aboveMe = Math.max(aboveMe, collectTypes(superclass, map));
       }
       /*
        * TODO(benyu): should we include Object for interface?
        * Also, CharSequence[] and Object[] for String[]?
        *
        */
       map.put(type, aboveMe + 1);
       return aboveMe + 1;
     }",NotBuggy,"Fix calls to contains(Object) and get(Object) that pass a value of apparently the wrong type.
Tighten up some generics in other cases to make the type, which was correct after all, look more correct.

BUGS:

AbstractBiMapTester:
- Fix inv.entrySet().contains(...) check, which was using the forward entry instead of the reverse.
- Fix getMap().get(v) call to be an inv.get(v) call.
- Use |reversed| instead of |entry| consistently for clarity.

TypeToken:
- Call map.get(K) instead of map.get(TypeCollector).
(Presumably this was just an optimization and not necessary for correctness?)

SIMPLIFICATIONS:

TypeResolver:
- forLookup always returns a (nullable) TypeVariableKey. Declare that return type instead of plain Object.
(benyu@: I feel like we may have talked about this, with your expressing a preference for the Object type. But I can find no record of the discussion, so I could be making that up. If you do prefer Object, I won't push for the change.)
-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=108994208",Buggy,"The commit message indicates a bug fix related to incorrect usage of `map.get(Object)` with a value of the wrong type. The diff shows a change from `map.get(this)` to `map.get(type)`. This correction aligns with the commit message, suggesting that the original code was indeed passing an incorrect key to the `get` method, leading to a potential bug. The answer is **Buggy**."
guava,10943.json,3fbaf56f390db1a7c442ceaeab2c3a8bc1c8fd79,"@@ -1,29 +1,43 @@
     @Override protected String computeNext() {
       while (offset != -1) {
         int start = offset;
         int end;
 
         int separatorPosition = separatorStart(offset);
         if (separatorPosition == -1) {
           end = toSplit.length();
           offset = -1;
         } else {
           end = separatorPosition;
           offset = separatorEnd(separatorPosition);
         }
 
         while (start < end && trimmer.matches(toSplit.charAt(start))) {
           start++;
         }
         while (end > start && trimmer.matches(toSplit.charAt(end - 1))) {
           end--;
         }
 
         if (omitEmptyStrings && start == end) {
           continue;
         }
 
+        if (limit == 1) {
+          // The limit has been reached, return the rest of the string as the
+          // final item.  This is tested after empty string removal so that
+          // empty strings do not count towards the limit.
+          end = toSplit.length();
+          offset = -1;
+          // Since we may have changed the end, we need to trim it again.
+          while (end > start && trimmer.matches(toSplit.charAt(end - 1))) {
+            end--;
+          }
+        } else {
+          limit--;
+        }
+
         return toSplit.subSequence(start, end).toString();
       }
       return endOfData();
     }",Buggy,"Fixed bug/532.
Add warnings to README.

These should be the last changes for Guava release 08.


Revision created by MOE tool push_codebase.
MOE_MIGRATION=


git-svn-id: https://guava-libraries.googlecode.com/svn/trunk@160 8138a162-5c33-11de-8abc-d1c337b90d21",Buggy,"The commit message indicates a bug fix (""Fixed bug/532""). The code changes involve adjustments to the splitting logic, particularly when a limit is specified. The added `if (limit == 1)` block handles the case where the limit has been reached, ensuring the rest of the string is returned as the final item. This suggests a fix for incorrect splitting behavior when a limit is imposed. The answer is **Buggy**.
"
guava,15629.json,2ef955163b3d43e7849c1929ef4e5d714b93da96,"@@ -1,27 +1,34 @@
   MoveDesc<E> removeAt(int index) {
     checkPositionIndex(index, size);
     modCount++;
     size--;
     if (size == index) {
       queue[size] = null;
       return null;
     }
     E actualLastElement = elementData(size);
-    int lastElementAt = heapForIndex(size).getCorrectLastElement(actualLastElement);
+    int lastElementAt = heapForIndex(size).swapWithConceptuallyLastElement(actualLastElement);
+    if (lastElementAt == index) {
+      // 'actualLastElement' is now at 'lastElementAt', and the element that was at 'lastElementAt'
+      // is now at the end of queue. If that's the element we wanted to remove in the first place,
+      // don't try to (incorrectly) trickle it. Instead, just delete it and we're done.
+      queue[size] = null;
+      return null;
+    }
     E toTrickle = elementData(size);
     queue[size] = null;
     MoveDesc<E> changes = fillHole(index, toTrickle);
     if (lastElementAt < index) {
       // Last element is moved to before index, swapped with trickled element.
       if (changes == null) {
         // The trickled element is still after index.
         return new MoveDesc<E>(actualLastElement, toTrickle);
       } else {
         // The trickled element is back before index, but the replaced element
         // has now been moved after index.
         return new MoveDesc<E>(actualLastElement, changes.replaced);
       }
     }
     // Trickled element was after index to begin with, no adjustment needed.
     return changes;
   }",Buggy,"Fix two bugs in MinMaxPriorityQueue (introduced in [] First is a bug in removeAt(int) that sometimes causes the wrong element to be removed. Second is a bug that sometimes causes certain elements to be iterated over more than once if elements were removed during iteration.

Reported externally at https://github.com/google/guava/issues/2658

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=140382230",Buggy,"The commit message indicates two bug fixes in `MinMaxPriorityQueue`. The diff focuses on the `removeAt(int)` method. The changes introduce a check to avoid incorrectly trickling an element if it was swapped with the element being removed. This addresses a scenario where the wrong element could be removed. The code also includes a call to `swapWithConceptuallyLastElement` which suggests a fix related to how the last element is handled during removal. The added `if` statement and the use of `swapWithConceptuallyLastElement` clearly indicate a correction to a faulty logic, thus a bug fix. The answer is **Buggy**.
"
guava,15643.json,2ef955163b3d43e7849c1929ef4e5d714b93da96,"@@ -1,16 +1,16 @@
-    int getCorrectLastElement(E actualLastElement) {
+    int swapWithConceptuallyLastElement(E actualLastElement) {
       int parentIndex = getParentIndex(size);
       if (parentIndex != 0) {
         int grandparentIndex = getParentIndex(parentIndex);
         int uncleIndex = getRightChildIndex(grandparentIndex);
         if (uncleIndex != parentIndex && getLeftChildIndex(uncleIndex) >= size) {
           E uncleElement = elementData(uncleIndex);
           if (ordering.compare(uncleElement, actualLastElement) < 0) {
             queue[uncleIndex] = actualLastElement;
             queue[size] = uncleElement;
             return uncleIndex;
           }
         }
       }
       return size;
     }",NotBuggy,"Fix two bugs in MinMaxPriorityQueue (introduced in [] First is a bug in removeAt(int) that sometimes causes the wrong element to be removed. Second is a bug that sometimes causes certain elements to be iterated over more than once if elements were removed during iteration.

Reported externally at https://github.com/google/guava/issues/2658

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=140382230",Buggy,"The commit message explicitly states that it fixes two bugs in `MinMaxPriorityQueue`. The diff shows a change in the method name from `getCorrectLastElement` to `swapWithConceptuallyLastElement`. The code within the method also suggests a bug fix related to how the last element is handled during removal or swapping, specifically when comparing elements and potentially swapping them to maintain the heap property. The comparison `ordering.compare(uncleElement, actualLastElement) < 0` indicates a correction in the ordering logic. The answer is **Buggy**.
"
guava,15653.json,2ef955163b3d43e7849c1929ef4e5d714b93da96,"@@ -1,21 +1,23 @@
     public void remove() {
       checkRemove(canRemove);
       checkModCount();
       canRemove = false;
       expectedModCount++;
       if (cursor < size()) {
         MoveDesc<E> moved = removeAt(cursor);
         if (moved != null) {
           if (forgetMeNot == null) {
             forgetMeNot = new ArrayDeque<E>();
             skipMe = new ArrayList<E>(3);
           }
-          forgetMeNot.add(moved.toTrickle);
+          if (!containsExact(skipMe, moved.toTrickle)) {
+            forgetMeNot.add(moved.toTrickle);
+          }
           skipMe.add(moved.replaced);
         }
         cursor--;
       } else { // we must have set lastFromForgetMeNot in next()
         checkState(removeExact(lastFromForgetMeNot));
         lastFromForgetMeNot = null;
       }
     }",Buggy,"Fix two bugs in MinMaxPriorityQueue (introduced in [] First is a bug in removeAt(int) that sometimes causes the wrong element to be removed. Second is a bug that sometimes causes certain elements to be iterated over more than once if elements were removed during iteration.

Reported externally at https://github.com/google/guava/issues/2658

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=140382230",Buggy,"The commit message indicates the resolution of two bugs within the MinMaxPriorityQueue. The provided diff focuses on the `remove()` method within the iterator. The change introduces a check `!containsExact(skipMe, moved.toTrickle)` before adding `moved.toTrickle` to `forgetMeNot`. This check prevents adding an element to `forgetMeNot` if it's already present in `skipMe`. This addition addresses the issue of elements being iterated over multiple times when elements are removed during iteration, as mentioned in the commit message. The answer is **Buggy**."
guava,11915.json,953e11352bbcb297987c81d1158f0c46f16cc054,"@@ -1,19 +1,20 @@
   final TypeToken<T> rejectTypeVariables() {
     new TypeVisitor() {
       @Override void visitTypeVariable(TypeVariable<?> type) {
         throw new IllegalArgumentException(
             runtimeType + ""contains a type variable and is not safe for the operation"");
       }
       @Override void visitWildcardType(WildcardType type) {
         visit(type.getLowerBounds());
         visit(type.getUpperBounds());
       }
       @Override void visitParameterizedType(ParameterizedType type) {
         visit(type.getActualTypeArguments());
+        visit(type.getOwnerType());
       }
       @Override void visitGenericArrayType(GenericArrayType type) {
         visit(type.getGenericComponentType());
       }
     }.visit(runtimeType);
     return this;
   }",Buggy,"Fixed a bug in rejectTypeVariable() where we failed to account for the owner type's type variables. For example:

abstract class From<K> {
class To<V> {
TypeToken<To<V>> type() {
return new TypeToken<To<V>>(getClass()) {};
}
}

static <K> TypeToken<From<K>.To<?>> cheat() {
return new From<K>().new To<String>() {}.type();
}

TypeToken<From<String>.To<?>> notReallyString = TheClass.<String>cheat();
notReallyString.rejectTypeVariables(); // Should throw
-------------
Created by MOE: http://code.google.com/p/moe-java
MOE_MIGRATED_REVID=45444012",Buggy,"The commit message describes a bug where `rejectTypeVariable()` failed to account for the owner type's type variables. The provided code example illustrates the scenario where the bug occurs. The diff introduces a call to `visit(type.getOwnerType())` within the `visitParameterizedType` method of the `TypeVisitor` class. This change ensures that the owner type is also visited and checked for type variables, thus fixing the bug described in the commit message. The answer is **Buggy**."
guava,11913.json,953e11352bbcb297987c81d1158f0c46f16cc054,"@@ -1,3 +1,4 @@
       @Override void visitParameterizedType(ParameterizedType type) {
         visit(type.getActualTypeArguments());
+        visit(type.getOwnerType());
       }",Buggy,"Fixed a bug in rejectTypeVariable() where we failed to account for the owner type's type variables. For example:

abstract class From<K> {
class To<V> {
TypeToken<To<V>> type() {
return new TypeToken<To<V>>(getClass()) {};
}
}

static <K> TypeToken<From<K>.To<?>> cheat() {
return new From<K>().new To<String>() {}.type();
}

TypeToken<From<String>.To<?>> notReallyString = TheClass.<String>cheat();
notReallyString.rejectTypeVariables(); // Should throw
-------------
Created by MOE: http://code.google.com/p/moe-java
MOE_MIGRATED_REVID=45444012",Buggy,"The commit message indicates a bug fix in `rejectTypeVariable()` related to handling owner type's type variables in parameterized types. The provided code snippet illustrates the scenario where the bug occurs. The diff adds `visit(type.getOwnerType());` within the `visitParameterizedType` method. This change ensures that the owner type's type variables are also visited and processed, addressing the bug described in the commit message. The answer is **Buggy**.
"
guava,17172.json,0330f712f56cc53633874e95bdc1203cf881eb85,"@@ -1,10 +1,10 @@
     public V get(@Nullable Object key) {
       if (key instanceof Range) {
         Range<?> range = (Range<?>) key;
         RangeMapEntry<K, V> rangeMapEntry = entriesByLowerBound.get(range.lowerBound);
-        if (rangeMapEntry.getKey().equals(range)) {
+        if (rangeMapEntry != null && rangeMapEntry.getKey().equals(range)) {
           return rangeMapEntry.getValue();
         }
       }
       return null;
     }",Buggy,"Fix bug in TreeRangeMap.asMapOfRanges().get(absentRange)
-------------
Created by MOE: http://code.google.com/p/moe-java
MOE_MIGRATED_REVID=38651843",Buggy,"The commit message indicates a bug fix in `TreeRangeMap.asMapOfRanges().get(absentRange)`. The diff adds a null check for `rangeMapEntry` before accessing its key. This prevents a `NullPointerException` if `entriesByLowerBound.get(range.lowerBound)` returns null, which would occur when the range is absent. Therefore, the change addresses a potential bug. The answer is **Buggy**."
guava,21153.json,1a5b0b9caaa5b6aaef77c9e7ace4f8cac4025738,"@@ -1,11 +1,8 @@
     private void startTask() {
-      lock.lock();
-      try {
-        if (isShutdown()) {
+      synchronized (lock) {
+        if (shutdown) {
           throw new RejectedExecutionException(""Executor already shutdown"");
         }
         runningTasks++;
-      } finally {
-        lock.unlock();
       }
     }",Buggy,"Fix a signaling bug in newDirectExecutorService()  Also switch to implicit
monitors to take advantage of biased locking.

If thread A calls awaitTermination while there are no running tasks, then
Thread B calls shutdown(). Thread A will never be woken up.
-------------
Created by MOE: http://code.google.com/p/moe-java
MOE_MIGRATED_REVID=87280837",Buggy,"The commit message indicates a bug fix related to signaling in `newDirectExecutorService()` where a thread waiting for termination might not be woken up after shutdown. The code diff shows a change from explicit `lock.lock()` and `lock.unlock()` to implicit `synchronized (lock)` blocks. This change likely addresses the signaling issue by ensuring proper synchronization and visibility of the `shutdown` flag. The use of `synchronized` also enables biased locking, as mentioned in the commit message. Therefore, the changes indicate a bug fix. The answer is **Buggy**."
guava,21154.json,1a5b0b9caaa5b6aaef77c9e7ace4f8cac4025738,"@@ -1,11 +1,8 @@
     private void endTask() {
-      lock.lock();
-      try {
-        runningTasks--;
-        if (isTerminated()) {
-          termination.signalAll();
+      synchronized (lock) {
+        int numRunning = --runningTasks;
+        if (numRunning == 0) {
+          lock.notifyAll();
         }
-      } finally {
-        lock.unlock();
       }
     }",Buggy,"Fix a signaling bug in newDirectExecutorService()  Also switch to implicit
monitors to take advantage of biased locking.

If thread A calls awaitTermination while there are no running tasks, then
Thread B calls shutdown(). Thread A will never be woken up.
-------------
Created by MOE: http://code.google.com/p/moe-java
MOE_MIGRATED_REVID=87280837",Buggy,"The commit message indicates a bug fix related to signaling in `newDirectExecutorService()` where a thread waiting for termination might not be woken up after shutdown. The code diff replaces explicit `lock.lock()`, `termination.signalAll()`, and `lock.unlock()` with a `synchronized` block and `lock.notifyAll()`. The condition for signaling is also changed from `isTerminated()` to `numRunning == 0`. This change addresses the bug described in the commit message by ensuring that waiting threads are properly notified when there are no more running tasks. The answer is **Buggy**."
guava,11437.json,b4cf74aa3a56751f25ae09eb79b68e0656d40618,"@@ -1,25 +1,21 @@
   public String collapseFrom(CharSequence sequence, char replacement) {
-    int first = indexIn(sequence);
-    if (first == -1) {
-      return sequence.toString();
-    }
-
-    // TODO(kevinb): see if this implementation can be made faster
-    StringBuilder builder = new StringBuilder(sequence.length())
-        .append(sequence.subSequence(0, first))
-        .append(replacement);
-    boolean in = true;
-    for (int i = first + 1; i < sequence.length(); i++) {
+    // This implementation avoids unnecessary allocation.
+    int len = sequence.length();
+    for (int i = 0; i < len; i++) {
       char c = sequence.charAt(i);
       if (matches(c)) {
-        if (!in) {
-          builder.append(replacement);
-          in = true;
+        if (c == replacement
+            && (i == len - 1 || !matches(sequence.charAt(i + 1)))) {
+          // a no-op replacement
+          i++;
+        } else {
+          StringBuilder builder = new StringBuilder(len)
+              .append(sequence.subSequence(0, i))
+              .append(replacement);
+          return finishCollapseFrom(sequence, i + 1, len, replacement, builder, true);
         }
-      } else {
-        builder.append(c);
-        in = false;
       }
     }
-    return builder.toString();
+    // no replacement needed
+    return sequence.toString();
   }",NotBuggy,"Optimize collapseFrom and trimAndCollapseFrom so that they
only allocate if necessary.  Add a few tests.  Remove the GWT
workarounds for GWT bug 4491 since it appears to be fixed.
-------------
Created by MOE: http://code.google.com/p/moe-java
MOE_MIGRATED_REVID=39757514",NotBuggy,"The commit message indicates optimization of `collapseFrom` and `trimAndCollapseFrom` to avoid unnecessary allocations. The diff shows a significant change in the `collapseFrom` method. The original implementation always created a `StringBuilder`, even if no characters needed replacement. The new implementation iterates through the sequence and only creates a `StringBuilder` if a character needs replacement. If no replacement is needed, it returns the original sequence. This avoids unnecessary object creation, aligning with the commit message's optimization goal. The change also includes logic to handle cases where the replacement character is adjacent to a matched character, avoiding redundant replacements. The commit message also mentions removing GWT workarounds, but that is not reflected in the provided diff. This change is an optimization, not a bug fix. The answer is **NotBuggy**.
"
guava,11403.json,b4cf74aa3a56751f25ae09eb79b68e0656d40618,"@@ -1,21 +1,17 @@
-  public String trimAndCollapseFrom(CharSequence sequence, char replacement) {
-    int first = negate().indexIn(sequence);
-    if (first == -1) {
-      return """"; // everything matches. nothing's left.
-    }
-    StringBuilder builder = new StringBuilder(sequence.length());
-    boolean inMatchingGroup = false;
-    for (int i = first; i < sequence.length(); i++) {
+  private String finishCollapseFrom(
+      CharSequence sequence, int start, int end, char replacement,
+      StringBuilder builder, boolean inMatchingGroup) {
+    for (int i = start; i < end; i++) {
       char c = sequence.charAt(i);
       if (matches(c)) {
-        inMatchingGroup = true;
-      } else {
-        if (inMatchingGroup) {
+        if (!inMatchingGroup) {
           builder.append(replacement);
-          inMatchingGroup = false;
+          inMatchingGroup = true;
         }
+      } else {
         builder.append(c);
+        inMatchingGroup = false;
       }
     }
     return builder.toString();
   }",NotBuggy,"Optimize collapseFrom and trimAndCollapseFrom so that they
only allocate if necessary.  Add a few tests.  Remove the GWT
workarounds for GWT bug 4491 since it appears to be fixed.
-------------
Created by MOE: http://code.google.com/p/moe-java
MOE_MIGRATED_REVID=39757514",NotBuggy,"The commit message indicates optimization and the removal of GWT workarounds, suggesting performance improvements and code cleanup rather than direct bug fixes. The diff introduces a helper function `finishCollapseFrom` which seems to be part of the optimization effort, moving the core logic of collapsing characters into a separate function. There is no explicit error handling or logical correction that would indicate a bug fix. The changes seem to be focused on improving the efficiency and maintainability of the code. The answer is **NotBuggy**."
guava,22907.json,9bf42862f877a0ed8234bd484aa0089a3773ccd4,"@@ -1,11 +1,12 @@
   private void addDoneString(StringBuilder builder) {
     try {
-      builder.append(""SUCCESS, result=["").append(getDone(this)).append(""]"");
+      V value = getDone(this);
+      builder.append(""SUCCESS, result=["").append(value).append(""]"");
     } catch (ExecutionException e) {
       builder.append(""FAILURE, cause=["").append(e.getCause()).append(""]"");
     } catch (CancellationException e) {
       builder.append(""CANCELLED"");
     } catch (RuntimeException e) {
       builder.append(""UNKNOWN, cause=["").append(e.getClass()).append("" thrown from get()]"");
     }
   }",Buggy,"ImmediateFuture.toString() implementation to provide the returned value or exception.
Provides for all ImmediateFutures implementation the same syntax that AbstractFuture uses.

It's especially convenient during debugging of mocked asynchronous gRPC services.

Fixes bug in AbstractFuture#toString in GWT that caused wrong representation:
""Class$S413@205[status=SUCCESS, result=[FAILURE, cause=[Class$S6]]"" due to ongoing
builder being already called when getDone() throws exception.

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=165925433",Buggy,"The commit message indicates a bug fix in `AbstractFuture#toString` related to incorrect representation in GWT due to an exception being thrown during the `getDone()` call. The diff shows that the `getDone(this)` call is now assigned to a local variable `value` before being appended to the builder. This change ensures that the `getDone()` method is only called once, and its result is stored, preventing the issue where the builder is already being called when `getDone()` throws an exception. This aligns with the commit message's description of fixing a bug in how the result is represented in the `toString()` method when an exception occurs. The answer is **Buggy**.
"
pmd,8491.json,384cd4f3d9a15a09b664c728ad42e03321428d7a,"@@ -1,17 +1,17 @@
     public Object visit(ASTMethodDeclarator node, Object data) {
         Scope scope = node.getScope();
         Map params = scope.getVariableDeclarations(true);
         for (Iterator i = params.keySet().iterator(); i.hasNext();) {
             VariableNameDeclaration decl = (VariableNameDeclaration)i.next();
             List usages = (List)params.get(decl);
             for (Iterator j = usages.iterator();j.hasNext();) {
                 NameOccurrence occ = (NameOccurrence)j.next();
-                if (occ.isOnLeftHandSide()) {
+                if (occ.isOnLeftHandSide() && (occ.getNameForWhichThisIsAQualifier() == null)) {
                     RuleContext ctx = (RuleContext)data;
                     String msg = MessageFormat.format(getMessage(), new Object[] {decl.getImage()});
                     ctx.getReport().addRuleViolation(createRuleViolation(ctx, decl.getLine(), msg));
                 }
             }
         }
         return super.visit(node, data);
     }",Buggy,"Several bug fixes


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@1376 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"The commit message indicates several bug fixes. The code change adds a condition `(occ.getNameForWhichThisIsAQualifier() == null)` to the `if` statement. This suggests that the original code was incorrectly flagging some occurrences of variable names on the left-hand side of an expression as rule violations when they were actually qualifiers (e.g., `object.field = value`). The added condition filters out these cases, indicating a bug fix. The answer is **Buggy**."
pmd,8745.json,384cd4f3d9a15a09b664c728ad42e03321428d7a,"@@ -1,20 +1,20 @@
     public Object visit(ASTMethodDeclaration node, Object data) {
         if (node.isAbstract()) {
             return data;
         }
 
         List returnNodes = new ArrayList();
-        node.findChildrenOfType(ASTReturnStatement.class, returnNodes);
+        node.findChildrenOfType(ASTReturnStatement.class, returnNodes, false);
         if (returnNodes.size() > 1) {
             RuleContext ctx = (RuleContext)data;
             for (Iterator i = returnNodes.iterator(); i.hasNext();) {
                 SimpleNode problem = (SimpleNode)i.next();
                 // skip the last one, it's OK
                 if (!i.hasNext()) {
                     continue;
                 }
                 ctx.getReport().addRuleViolation(createRuleViolation(ctx, problem.getBeginLine()));
             }
         }
         return data;
     }",Buggy,"Several bug fixes


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@1376 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"The commit message indicates bug fixes. The code change modifies the `findChildrenOfType` method call in the `visit` method. Specifically, it adds `false` as the third argument. This argument likely controls whether the search for `ASTReturnStatement` includes nested blocks or not. The original code probably incorrectly searched nested blocks, leading to false positives. By adding `false`, the search is restricted to the direct children of the method declaration, which fixes the bug. The answer is **Buggy**."
pmd,1151.json,384cd4f3d9a15a09b664c728ad42e03321428d7a,"@@ -1,15 +1,18 @@
-    private void findChildrenOfType(Node node, Class targetType, List results) {
+    private void findChildrenOfType(Node node, Class targetType, List results, boolean descendIntoNestedClasses) {
         if (node.getClass().equals(targetType)) {
             results.add(node);
         }
+        if (node.getClass().equals(ASTClassBody.class) && !descendIntoNestedClasses) {
+            return;
+        }
         for (int i=0; i<node.jjtGetNumChildren(); i++) {
-            Node child = (Node)node.jjtGetChild(i);
+            Node child = node.jjtGetChild(i);
             if (child.jjtGetNumChildren()>0) {
-                findChildrenOfType(child, targetType, results);
+                findChildrenOfType(child, targetType, results, descendIntoNestedClasses);
             } else {
                 if (child.getClass().equals(targetType)) {
                     results.add(child);
                 }
             }
         }
     }",Buggy,"Several bug fixes


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@1376 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"The commit message indicates bug fixes. The code changes modify the `findChildrenOfType` method to optionally avoid descending into nested classes. This is controlled by the `descendIntoNestedClasses` parameter. The method also now checks if the node is an `ASTClassBody` and returns if `descendIntoNestedClasses` is false. This change likely addresses a bug where the method was incorrectly traversing nested classes, potentially leading to incorrect results or infinite loops. The answer is **Buggy**."
pmd,7973.json,a405d23dfb9e574e2b2ef23f1f45d548a738ed3b,"@@ -1,10 +1,4 @@
     public boolean isOnRightHandSide() {
         SimpleNode node = (SimpleNode) location.jjtGetParent().jjtGetParent().jjtGetParent();
-        if (node instanceof ASTExpression) {
-            SimpleNode parent = (SimpleNode) node.jjtGetParent();
-            if (node.jjtGetNumChildren() == 3) {
-                return true;
-            }
-        }
-        return false;
+        return node instanceof ASTExpression && node.jjtGetNumChildren() == 3;
     }",NotBuggy,"Fixed bug 1050173 - ImmutableFieldRule no longer reports false positives for static fields.  Also fixed version number in PMD.java, argh.


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@2994 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"The commit message indicates a bug fix related to the ImmutableFieldRule in PMD, specifically addressing false positives for static fields. The diff simplifies the `isOnRightHandSide` method, which likely determines if a field is being assigned a value. The original code had nested `if` statements to check if the node is an `ASTExpression` and if it has 3 children. The simplified code combines these checks into a single `return` statement using the `&&` operator. This change likely corrects a logical error in the original code that caused the false positives. The answer is **Buggy**."
pmd,8886.json,a405d23dfb9e574e2b2ef23f1f45d548a738ed3b,"@@ -1,18 +1,18 @@
     public Object visit(ASTUnmodifiedClassDeclaration node, Object data) {
         Map vars = node.getScope().getVariableDeclarations();
         for (Iterator i = vars.keySet().iterator(); i.hasNext();) {
             VariableNameDeclaration decl = (VariableNameDeclaration) i.next();
-            if (!decl.getAccessNodeParent().isPrivate() || decl.getAccessNodeParent().isFinal()) {
+            if (decl.getAccessNodeParent().isStatic() || !decl.getAccessNodeParent().isPrivate() || decl.getAccessNodeParent().isFinal()) {
                 continue;
             }
+
             int result = initializedInConstructor((List)vars.get(decl));
             if (result == MUTABLE) {
             	continue;
             }
-            if ((result == IMMUTABLE) ||
-                ((result == CHECKDECL) && initializedInDeclaration(decl.getAccessNodeParent()))) {
+            if (result == IMMUTABLE || ((result == CHECKDECL) && initializedInDeclaration(decl.getAccessNodeParent()))) {
                 ((RuleContext) data).getReport().addRuleViolation(createRuleViolation((RuleContext) data, decl.getLine(), MessageFormat.format(getMessage(), new Object[]{decl.getImage()})));
             }
         }
         return super.visit(node, data);
     }",Buggy,"Fixed bug 1050173 - ImmutableFieldRule no longer reports false positives for static fields.  Also fixed version number in PMD.java, argh.


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@2994 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"The commit message indicates a fix for bug 1050173, where ImmutableFieldRule incorrectly flagged static fields. The diff modifies the condition `!decl.getAccessNodeParent().isPrivate() || decl.getAccessNodeParent().isFinal()` to `decl.getAccessNodeParent().isStatic() || !decl.getAccessNodeParent().isPrivate() || decl.getAccessNodeParent().isFinal()`. This change adds a check for `isStatic()`, ensuring that static fields are excluded from the rule, thus addressing the false positive. The added `isStatic()` check directly relates to the bug described in the commit message. The answer is **Buggy**."
pmd,8888.json,a405d23dfb9e574e2b2ef23f1f45d548a738ed3b,"@@ -1,31 +1,31 @@
     private int initializedInConstructor(List usages) {
         int rc = MUTABLE, initCount = 0;
         boolean setInConstructor = false;
 		boolean foundUsage = false;
 
         for (Iterator j = usages.iterator(); j.hasNext();) {
         	foundUsage = true;
-        	NameOccurrence occurance = (NameOccurrence)j.next();
-            if (occurance.isOnLeftHandSide()) {
-            	SimpleNode node = occurance.getLocation();
+        	NameOccurrence occ = (NameOccurrence)j.next();
+            if (occ.isOnLeftHandSide()) {
+            	SimpleNode node = occ.getLocation();
             	if (node.getFirstParentOfType(ASTConstructorDeclaration.class) != null) {
             		setInConstructor = true;
             		initCount++;
  				}
  				else {
  					if (node.getFirstParentOfType(ASTMethodDeclaration.class) != null) {
  						initCount++;
  					}
  				}
  			}
         }
         if (!foundUsage || (initCount == 0)) {
         	rc = CHECKDECL;
         }
         else {
         	if ((initCount == 1) && setInConstructor) {
         		rc = IMMUTABLE;
         	}
         }
         return rc;
     }",NotBuggy,"Fixed bug 1050173 - ImmutableFieldRule no longer reports false positives for static fields.  Also fixed version number in PMD.java, argh.


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@2994 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"The commit message indicates a bug fix: ""ImmutableFieldRule no longer reports false positives for static fields"". The code diff modifies the `initializedInConstructor` method, specifically how it determines if a field is initialized in a constructor. The changes involve renaming `occurance` to `occ` and checking if the `NameOccurrence` is on the left-hand side of an assignment within a constructor or method. The logic appears to be refined to correctly identify when a field is initialized, thus preventing false positives. The answer is **Buggy**.
"
pmd,8249.json,222563dc32c51c8f77a1b06ba69077463a525479,"@@ -1,4 +1,4 @@
 		public AnnotationVisitor visitAnnotation(String desc, boolean visible) {
 			parent.addType(Type.getType(desc));
-			return annotationVisitor;
+			return parent.annotationVisitor;
 		}",Buggy,"bug fix: acceptType only works for fields and locals


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@4822 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"The commit message indicates a bug fix related to `acceptType` working only for fields and locals. The code change involves returning `parent.annotationVisitor` instead of `annotationVisitor` in the `visitAnnotation` method. This suggests that the original code was not correctly associating the annotation visitor with the parent, potentially leading to incorrect type processing. The change ensures that the parent's annotation visitor is used, which likely resolves the issue of `acceptType` not working as expected in certain contexts. The answer is **Buggy**.
"
pmd,8247.json,222563dc32c51c8f77a1b06ba69077463a525479,"@@ -1,5 +1,5 @@
 	private void extractSignature(String sig) {
 		if (sig != null) {
-			new SignatureReader(sig).acceptType(sigVisitor);
+			new SignatureReader(sig).accept(sigVisitor);
 		}
 	}",Buggy,"bug fix: acceptType only works for fields and locals


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@4822 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"The commit message indicates a bug fix related to the `acceptType` method. The diff replaces `acceptType` with `accept`. This suggests that `acceptType` was not functioning as intended for all types of program elements (fields and locals, as stated in the commit message), and `accept` provides a more general and correct solution. The change likely addresses a scenario where the type signature was not being properly processed for certain elements, leading to incorrect behavior or analysis. The answer is **Buggy**.
"
pmd,6069.json,51ab4ae563f9875328e56161bff3eb97331b10cd,"@@ -1,8 +1,8 @@
     public void jjtClose() {
-        if ((children == null) || (children.length == 0)) {
+        if (beginLine == -1 && (children == null || children.length == 0)) {
             beginLine = parser.token.beginLine;
             beginColumn = parser.token.beginColumn;
         }
         endLine = parser.token.endLine;
         endColumn = parser.token.endColumn;
     }",Buggy,"Fixed bug 583047 - columns were wrong on ASTName nodes


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@2011 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"The commit message indicates a fix for bug 583047, specifically addressing incorrect column numbers on ASTName nodes. The diff modifies the `jjtClose` method. The original code assigned `beginLine` and `beginColumn` based on `parser.token` only when there were no children. The modified code adds a condition `beginLine == -1` to ensure the assignment happens only when `beginLine` hasn't been initialized yet. This prevents overwriting the correct position if it was already set. This change directly addresses the bug reported in the commit message. The answer is **Buggy**."
pmd,6068.json,51ab4ae563f9875328e56161bff3eb97331b10cd,"@@ -1,6 +1,6 @@
     public void jjtOpen() {
-        if (parser.token.next != null) {
+        if (beginLine == -1 && parser.token.next != null) {
             beginLine = parser.token.next.beginLine;
             beginColumn = parser.token.next.beginColumn;
         }
     }",Buggy,"Fixed bug 583047 - columns were wrong on ASTName nodes


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@2011 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"The commit message indicates a bug fix related to incorrect column numbers on ASTName nodes. The diff adds a condition `beginLine == -1` to the if statement in the `jjtOpen` method. This suggests that the original code was incorrectly updating the `beginLine` and `beginColumn` values, potentially overwriting previously set correct values. The added condition ensures that these values are only set if they haven't been initialized yet (indicated by `beginLine == -1`), thus fixing the bug where column numbers were wrong. The answer is **Buggy**."
pmd,1134.json,51ab4ae563f9875328e56161bff3eb97331b10cd,"@@ -1,11 +1,3 @@
     public int getBeginLine() {
-        if (beginLine != -1) {
-            return beginLine;
-        } else {
-            if ((children != null) && (children.length > 0)) {
-                return ((SimpleNode) children[0]).getBeginLine();
-            } else {
-                throw new RuntimeException(""Unable to determine begining line of Node."");
-            }
-        }
+        return beginLine;
     }",Buggy,"Fixed bug 583047 - columns were wrong on ASTName nodes


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@2011 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"The commit message indicates a fix for bug 583047, which concerns incorrect column numbers on ASTName nodes. The diff removes a block of code that attempts to determine the beginning line of a node based on its children if the `beginLine` field is not already set. The original code included a `RuntimeException` if the beginning line could not be determined. The removal of this logic suggests that the original approach was flawed or led to incorrect line numbers, thus indicating a bug fix. The answer is **Buggy**.
"
pmd,2689.json,e249deb0879da58e78f31eafbafc3992661b5142,"@@ -1,7 +1,7 @@
     public int getLineCount(Mark mark, Match match) {
         TokenEntry endTok = get(mark.getIndexIntoTokenArray() + match.getTokenCount());
         if (endTok.equals(TokenEntry.EOF)) {
             endTok = get(mark.getIndexIntoTokenArray() + match.getTokenCount() - 1);
         }
-        return endTok.getBeginLine() - mark.getBeginLine();
+        return endTok.getBeginLine() - mark.getBeginLine() - 1;
     }",Buggy,"Fixed bug in the 'source code slice' logic


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@1705 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"The commit message indicates a bug fix in the 'source code slice' logic. The diff shows a change in the `getLineCount` method, where the return value is decremented by 1. This suggests that the original code was incorrectly calculating the line count, potentially including an extra line. The subtraction of 1 likely corrects this miscalculation, indicating a bug fix. The answer is **Buggy**."
pmd,2580.json,e249deb0879da58e78f31eafbafc3992661b5142,"@@ -1,32 +1,32 @@
     public void findMatches(int min) {
        /*
          Assign sort codes to all the pooled code. This should speed
          up sorting them.
        */
         int count = 1;
         for (Iterator iter = pool.keySet().iterator(); iter.hasNext();) {
            TokenEntry token = (TokenEntry)iter.next();
            token.setSortCode(count++);
         }
 
         MarkComparator mc = new MarkComparator(cpdListener, code);
         Collections.sort(marks, mc);
 
         MatchCollector coll = new MatchCollector(marks, code, mc);
         matches = coll.collect(min);
         Collections.sort(matches);
 
         for (Iterator i = matches(); i.hasNext();) {
             Match match = (Match)i.next();
             for (Iterator occurrences = match.iterator(); occurrences.hasNext();) {
                 Mark mark = (Mark)occurrences.next();
-                SourceCode sourceCode = (SourceCode)source.get(mark.getTokenSrcID());
                 match.setLineCount(tokens.getLineCount(mark, match));
                 if (!occurrences.hasNext()) {
                     int start = mark.getBeginLine();
-                    int end = mark.getBeginLine()-1 + tokens.getLineCount(mark, match);
+                    int end = start + match.getLineCount() - 1;
+                    SourceCode sourceCode = (SourceCode)source.get(mark.getTokenSrcID());
                     match.setSourceCodeSlice(sourceCode.getSlice(start, end));
                 }
             }
         }
     }",NotBuggy,"Fixed bug in the 'source code slice' logic


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@1705 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"The commit message indicates a bug fix in the 'source code slice' logic. The diff moves the `SourceCode sourceCode = (SourceCode)source.get(mark.getTokenSrcID());` line inside the `if (!occurrences.hasNext())` block and calculates `end` based on `start` and `match.getLineCount()`. The original code calculates `end` incorrectly as `mark.getBeginLine()-1 + tokens.getLineCount(mark, match)`. This suggests that the source code slice was being calculated incorrectly before, and the change corrects this calculation. The answer is **Buggy**."
pmd,961.json,1ee8cc4e8daf001172f2a3aa1a8c2a89706e988f,"@@ -1,32 +1,32 @@
     public static MetricKey<ASTAnyTypeDeclaration> of(final Metric<ASTAnyTypeDeclaration> metric, final String name) {
         return new MetricKey<ASTAnyTypeDeclaration>() {
             @Override
             public String name() {
                 return name;
             }
 
 
             @Override
             public Metric<ASTAnyTypeDeclaration> getCalculator() {
                 return metric;
             }
 
 
             @Override
             public boolean supports(ASTAnyTypeDeclaration node) {
                 return metric.supports(node);
             }
 
 
             @Override
             public boolean equals(Object obj) {
                 return obj == this;
             }
 
 
             @Override
             public int hashCode() {
-                return metric.hashCode() * 31 + name.hashCode();
+                return (metric != null ? metric.hashCode() * 31 : 0) + (name != null ? name.hashCode() : 0);
             }
         };
     }",Buggy,Fix bug in key hashcode with null names or metric,Buggy,"The commit message indicates a fix for a bug related to the key's hashcode when null names or metrics are involved. The diff modifies the `hashCode()` method to handle null `metric` and `name` values gracefully. It now checks if `metric` and `name` are null before invoking `hashCode()` on them, providing a default value of 0 if they are null. This change directly addresses the bug described in the commit message. The answer is **Buggy**."
pmd,5595.json,64ca9b6f01f9b946a456f6de05eada2374906d6a,"@@ -1,3 +1,6 @@
     public EcmascriptNode getFinallyBlock() {
-	return (EcmascriptNode) jjtGetChild(jjtGetNumChildren() - 1);
+        if (!isFinally()) {
+            return null;
+        }
+        return (EcmascriptNode) jjtGetChild(jjtGetNumChildren() - 1);
     }",Buggy,"pmd: fix #1141 ECMAScript: getFinallyBlock() is buggy.
fix #1142 ECMAScript: getCatchClause() is buggy.",Buggy,"The commit message indicates that `getFinallyBlock()` is buggy. The diff adds a check `if (!isFinally())` before returning the last child node. If the node is not a `finally` block, it returns `null`. This change addresses a bug where the method might have been returning an incorrect node when a `finally` block was not present. The answer is **Buggy**."
pmd,376.json,a3d5e7e1921e2c1c10f6a1e60fdfc19983732097,"@@ -1,3 +1,3 @@
     public void setClasspathRef(Reference r) {
-        createClasspath().setRefid(r);
+        createLongClasspath().setRefid(r);
     }",Buggy,"Fixed nested classpath bug


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@1901 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"The commit message indicates a fix for a ""nested classpath bug"". The code diff shows a change from `createClasspath()` to `createLongClasspath()`. This suggests that the original `createClasspath()` method was not correctly handling nested classpaths, and the change to `createLongClasspath()` resolves this issue. This is a bug fix. The answer is **Buggy**.
"
pmd,7138.json,fc511ec111d90e8f861c187a7e012652c80fcc4d,"@@ -1,7 +1,11 @@
-    private String getPackageName(String importName) {
+    public String getPackageName() {
+        String importName = getImportedName();
+        if (isImportOnDemand) {
+            return importName;
+        }
         if (importName.indexOf('.') == -1) {
             return """";
         }
         int lastDot = importName.lastIndexOf('.');
         return importName.substring(0, lastDot);
     }",Buggy,"Fixed a bug in ImportFromSamePackage; now it catches the case where a class has an on-demand import for the same package it is in.


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@3729 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"The commit message indicates a bug fix in `ImportFromSamePackage`, specifically addressing the scenario where a class has an on-demand import for its own package. The diff modifies the `getPackageName` method. The original code took an `importName` as input, whereas the modified code retrieves the `importName` using `getImportedName()`. Also, there is a condition added to return the `importName` if `isImportOnDemand` is true. This aligns with the commit message, as it handles the case of on-demand imports, which was previously not handled correctly, leading to a bug. The answer is **Buggy**.
"
pmd,8736.json,fc511ec111d90e8f861c187a7e012652c80fcc4d,"@@ -1,19 +1,19 @@
     public Object visit(ASTImportDeclaration node, Object data) {
-        ImportWrapper wrapper = new ImportWrapper(node.getImportedNameNode().getImage(), node.getImportedNameNode().getImage(), node.getImportedNameNode());
+        ImportWrapper wrapper = new ImportWrapper(node.getImportedName(), node.getImportedName(), node.getImportedNameNode());
 
         // blahhhh... this really wants to be ASTImportDeclaration to be polymorphic...
         if (node.isImportOnDemand()) {
             if (importOnDemandImports.contains(wrapper)) {
                 addViolation(data, node.getImportedNameNode(), node.getImportedNameNode().getImage());
             } else {
                 importOnDemandImports.add(wrapper);
             }
         } else {
             if (singleTypeImports.contains(wrapper)) {
                 addViolation(data, node.getImportedNameNode(), node.getImportedNameNode().getImage());
             } else {
                 singleTypeImports.add(wrapper);
             }
         }
         return data;
     }",Buggy,"Fixed a bug in ImportFromSamePackage; now it catches the case where a class has an on-demand import for the same package it is in.


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@3729 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"The commit message indicates a bug fix in the `ImportFromSamePackage` functionality. The diff modifies how the `ImportWrapper` is created, specifically changing `node.getImportedNameNode().getImage()` to `node.getImportedName()`. This suggests that the original code was incorrectly retrieving the imported name, leading to the bug described in the commit message (incorrectly handling on-demand imports from the same package). The change likely corrects this by using the correct method to obtain the imported name.

Therefore, the answer is **Buggy**."
pmd,8913.json,fc511ec111d90e8f861c187a7e012652c80fcc4d,"@@ -1,6 +1,6 @@
     public Object visit(ASTImportDeclaration node, Object o) {
-        if (node.getImportedNameNode().getImage().indexOf(""junit"") != -1) {
+        if (node.getImportedName().indexOf(""junit"") != -1) {
             junitImported = true;
         }
         return super.visit(node, o);
     }",Buggy,"Fixed a bug in ImportFromSamePackage; now it catches the case where a class has an on-demand import for the same package it is in.


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@3729 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"The commit message indicates a bug fix in `ImportFromSamePackage`. The diff replaces `node.getImportedNameNode().getImage()` with `node.getImportedName()`. This change suggests that the original code was not correctly retrieving the imported name, leading to incorrect behavior in identifying imports from the same package. The change likely fixes a bug where the rule was not correctly identifying certain types of imports. The answer is **Buggy**."
pmd,4678.json,fb25329e0d9ec5d632667df393f7c1b25a698e68,"@@ -1,42 +1,42 @@
     public Object visit(ASTUserClass node, Object data) {
 
         if (Helper.isTestMethodOrClass(node)) {
             return data;
         }
 
-        // baz = String.escapeSignleQuotes(...);
-        final List<ASTAssignmentExpression> assignmentCalls = node.findDescendantsOfType(ASTAssignmentExpression.class);
-        for (ASTAssignmentExpression a : assignmentCalls) {
-            findSanitizedVariables(a);
-            findSelectContainingVariables(a);
-        }
-
         final List<ASTFieldDeclaration> fieldExpr = node.findDescendantsOfType(ASTFieldDeclaration.class);
         for (ASTFieldDeclaration a : fieldExpr) {
             findSanitizedVariables(a);
             findSelectContainingVariables(a);
         }
 
         // String foo = String.escapeSignleQuotes(...);
         final List<ASTVariableDeclaration> variableDecl = node.findDescendantsOfType(ASTVariableDeclaration.class);
         for (ASTVariableDeclaration a : variableDecl) {
             findSanitizedVariables(a);
             findSelectContainingVariables(a);
         }
 
+        // baz = String.escapeSignleQuotes(...);
+        final List<ASTAssignmentExpression> assignmentCalls = node.findDescendantsOfType(ASTAssignmentExpression.class);
+        for (ASTAssignmentExpression a : assignmentCalls) {
+            findSanitizedVariables(a);
+            findSelectContainingVariables(a);
+        }
+
         // Database.query(...) check
         final List<ASTMethodCallExpression> potentialDbQueryCalls = node
                 .findDescendantsOfType(ASTMethodCallExpression.class);
 
         for (ASTMethodCallExpression m : potentialDbQueryCalls) {
             if (!Helper.isTestMethodOrClass(m) && Helper.isMethodName(m, DATABASE, QUERY)) {
                 reportStrings(m, data);
                 reportVariables(m, data);
             }
         }
-        
+
         safeVariables.clear();
         selectContainingVariables.clear();
 
         return data;
     }",NotBuggy,Bug fix to improve detection of concatenated vars,Buggy,"The commit message indicates a bug fix related to the detection of concatenated variables. The code diff reorders the execution of finding sanitized and select-containing variables. Specifically, the section that processes `ASTAssignmentExpression` nodes is moved from the beginning of the method to after the processing of `ASTFieldDeclaration` and `ASTVariableDeclaration` nodes. This change in order suggests that the original placement of this code might have led to incorrect or incomplete detection of concatenated variables in certain scenarios, which is now addressed by the reordering. The answer is **Buggy**."
pmd,7859.json,15ff7ca9447619240ec96fdb32f7974e1ecf5730,"@@ -1,34 +1,43 @@
-    private MethodNameDeclaration createBuiltInMethodDeclaration(final String methodName, final int parameterCount) {
+    private MethodNameDeclaration createBuiltInMethodDeclaration(final String methodName, String... parameterTypes) {
         ASTMethodDeclaration methodDeclaration = new ASTMethodDeclaration(JavaParserTreeConstants.JJTMETHODDECLARATION);
         methodDeclaration.setPublic(true);
         methodDeclaration.setScope(this);
 
         ASTMethodDeclarator methodDeclarator = new ASTMethodDeclarator(JavaParserTreeConstants.JJTMETHODDECLARATOR);
         methodDeclarator.setImage(methodName);
         methodDeclarator.setScope(this);
 
         ASTFormalParameters formalParameters = new ASTFormalParameters(JavaParserTreeConstants.JJTFORMALPARAMETERS);
         formalParameters.setScope(this);
 
         methodDeclaration.jjtAddChild(methodDeclarator, 0);
         methodDeclarator.jjtSetParent(methodDeclaration);
         methodDeclarator.jjtAddChild(formalParameters, 0);
         formalParameters.jjtSetParent(methodDeclarator);
 
+        int parameterCount = parameterTypes.length;
         for (int i = 0; i < parameterCount; i++) {
             ASTFormalParameter formalParameter = new ASTFormalParameter(JavaParserTreeConstants.JJTFORMALPARAMETER);
             formalParameters.jjtAddChild(formalParameter, i);
             formalParameter.jjtSetParent(formalParameters);
 
             ASTType type = new ASTType(JavaParserTreeConstants.JJTTYPE);
             formalParameter.jjtAddChild(type, 0);
             type.jjtSetParent(formalParameter);
+            ASTReferenceType referenceType = new ASTReferenceType(JavaParserTreeConstants.JJTREFERENCETYPE);
+            type.jjtAddChild(referenceType, 0);
+            referenceType.jjtSetParent(type);
+            ASTClassOrInterfaceType classOrInterfaceType = new ASTClassOrInterfaceType(JavaParserTreeConstants.JJTCLASSORINTERFACETYPE);
+            classOrInterfaceType.setImage(parameterTypes[i]);
+            referenceType.jjtAddChild(classOrInterfaceType, 0);
+            classOrInterfaceType.jjtSetParent(referenceType);
+
             ASTVariableDeclaratorId variableDeclaratorId = new ASTVariableDeclaratorId(JavaParserTreeConstants.JJTVARIABLEDECLARATORID);
             variableDeclaratorId.setImage(""arg"" + i);
             formalParameter.jjtAddChild(variableDeclaratorId, 1);
             variableDeclaratorId.jjtSetParent(formalParameter);
         }
 
         MethodNameDeclaration mnd = new MethodNameDeclaration(methodDeclarator);
         return mnd;
     }",Buggy,Fixes #1490 [java] PMD Error while processing - NullPointerException,Buggy,"The commit message indicates a fix for a NullPointerException. The code changes modify the `createBuiltInMethodDeclaration` method to accept variable arguments of type String (`String... parameterTypes`) instead of an integer representing the parameter count. The code then iterates through the provided parameter types to create the formal parameters for the method declaration. This change likely addresses a scenario where the parameter types were not being correctly handled, leading to a NullPointerException when the code attempted to access or process them. The answer is **Buggy**."
pmd,10015.json,8694a26cafe730add04695ec1917cd51bd4bbc4e,"@@ -1,4 +1,3 @@
     public TokenManager createTokenManager(Reader source) {
-	return new PLSQLTokenManager(source);
-
+        return new PLSQLTokenManager(IOUtil.skipBOM(source));
     }",Buggy,Fix #1167 Error while processing PLSQL file with BOM,Buggy,"The commit message indicates a fix for issue #1167, which involves an error when processing PLSQL files containing a Byte Order Mark (BOM). The diff modifies the `createTokenManager` method to use `IOUtil.skipBOM(source)` when creating the `PLSQLTokenManager`. This suggests that the original code was failing to correctly handle files with BOMs, leading to parsing errors or other issues. The `IOUtil.skipBOM` method likely removes the BOM from the input stream, allowing the token manager to process the file correctly. This is a clear indication of a bug fix. The answer is **Buggy**."
pmd,3080.json,6619e19d3558abf87dbf0b3f0047e355d593043b,"@@ -1,12 +1,12 @@
     public void publish(LogRecord logRecord) {
-        System.out.println(logRecord.getMessage());
+        System.out.println(FORMATTER.format(logRecord));
         if (logRecord.getThrown() != null) {
             // Use the same channel, to make sure that the stacktrace comes
             // after the message on the console (using printStackTrace
             // directly messes things up)
             StringWriter stringWriter = new StringWriter();
             PrintWriter printWriter = new PrintWriter(stringWriter, true);
             logRecord.getThrown().printStackTrace(printWriter);
             System.out.println(stringWriter.toString());
         }
     }",NotBuggy,"Fixed formatting problems in loggers

git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@5905 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"The commit message indicates a fix for formatting problems in loggers. The diff shows that the `publish` method of a logger is modified. Previously, it was printing only the message of the log record. Now, it uses a `FORMATTER` to format the entire log record before printing. This change addresses formatting issues, as indicated in the commit message. The answer is **Buggy**."
pmd,3084.json,6619e19d3558abf87dbf0b3f0047e355d593043b,"@@ -1,25 +1,25 @@
     public void publish(LogRecord logRecord) {
         //Map the log levels from java.util.logging to Ant
         int antLevel;
         Level level = logRecord.getLevel();
         if (level == Level.FINEST)
             antLevel = Project.MSG_DEBUG;   //Shown when -debug is supplied to Ant
         else if (level == Level.FINE || level == Level.FINER || level == Level.CONFIG)
             antLevel = Project.MSG_VERBOSE; //Shown when -verbose is supplied to Ant
         else if (level == Level.INFO)
             antLevel = Project.MSG_INFO;    //Always shown
         else if (level == Level.WARNING)
             antLevel = Project.MSG_WARN;    //Always shown
         else if (level == Level.SEVERE)
             antLevel = Project.MSG_ERR;     //Always shown
         else
             throw new IllegalStateException(""Unknown logging level"");   //shouldn't get ALL or NONE
         
-        antTask.log(logRecord.getMessage(), antLevel);
+        antTask.log(FORMATTER.format(logRecord), antLevel);
         if (logRecord.getThrown() != null) {
             StringWriter stringWriter = new StringWriter();
             PrintWriter printWriter = new PrintWriter(stringWriter, true);
             logRecord.getThrown().printStackTrace(printWriter);
             antTask.log(stringWriter.toString(), antLevel);
         }
     }",NotBuggy,"Fixed formatting problems in loggers

git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@5905 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"The commit message indicates a fix for formatting problems in loggers. The diff shows that the `antTask.log` method now uses `FORMATTER.format(logRecord)` instead of just `logRecord.getMessage()`. This suggests that the previous implementation was not correctly formatting the log message, which is a bug. The change ensures that the log message is properly formatted before being passed to the Ant task. The answer is **Buggy**."
pmd,7444.json,dc453cf103787e06a70385cb4eab1917f6286446,"@@ -1,3 +1,3 @@
-    public boolean isArrayDeference() {
-        return isArrayDeference;
+    public boolean isArrayDereference() {
+        return isArrayDereference;
     }",NotBuggy,"Fixed bug 1242544 - SimplifyConditional no longer flags null checks that precede an instanceof involving an array dereference.


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@3728 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"The commit message indicates a bug fix related to `SimplifyConditional` flagging null checks incorrectly. The code change involves renaming `isArrayDeference` to `isArrayDereference`. While the change itself is small, the commit message explicitly states that it fixes a bug. The answer is **Buggy**."
pmd,7443.json,dc453cf103787e06a70385cb4eab1917f6286446,"@@ -1,3 +1,3 @@
     public void setIsArrayDereference() {
-        isArrayDeference = true;
+        isArrayDereference = true;
     }",NotBuggy,"Fixed bug 1242544 - SimplifyConditional no longer flags null checks that precede an instanceof involving an array dereference.


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@3728 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"The commit message indicates a bug fix related to `SimplifyConditional` flagging null checks incorrectly before an `instanceof` check involving array dereference. The code change itself is a simple correction, changing `isArrayDeference` to `isArrayDereference`. While the change is minimal, it directly addresses the bug described in the commit message. The answer is **Buggy**."
pmd,9037.json,dc453cf103787e06a70385cb4eab1917f6286446,"@@ -1,43 +1,43 @@
     public Object visit(ASTStatementExpression node, Object data) {
         if (node.jjtGetNumChildren() != 3
                 || !(node.jjtGetChild(0) instanceof ASTPrimaryExpression)
                 || !(node.jjtGetChild(1) instanceof ASTAssignmentOperator)
                 || (((ASTAssignmentOperator) (node.jjtGetChild(1))).isCompound())
                 || !(node.jjtGetChild(2) instanceof ASTExpression)
                 || node.jjtGetChild(0).jjtGetChild(0).jjtGetNumChildren() == 0
                 || node.jjtGetChild(2).jjtGetChild(0).jjtGetChild(0).jjtGetNumChildren() == 0
         ) {
             return super.visit(node, data);
         }
 
         SimpleNode lhs = (SimpleNode) node.jjtGetChild(0).jjtGetChild(0).jjtGetChild(0);
         if (!(lhs instanceof ASTName)) {
             return super.visit(node, data);
         }
 
         SimpleNode rhs = (SimpleNode) node.jjtGetChild(2).jjtGetChild(0).jjtGetChild(0).jjtGetChild(0);
         if (!(rhs instanceof ASTName)) {
             return super.visit(node, data);
         }
 
         if (!lhs.getImage().equals(rhs.getImage())) {
             return super.visit(node, data);
         }
 
         if (lhs.jjtGetParent().jjtGetParent().jjtGetNumChildren() > 1) {
             Node n = lhs.jjtGetParent().jjtGetParent().jjtGetChild(1);
-            if (n instanceof ASTPrimarySuffix && ((ASTPrimarySuffix) n).isArrayDeference()) {
+            if (n instanceof ASTPrimarySuffix && ((ASTPrimarySuffix) n).isArrayDereference()) {
                 return super.visit(node, data);
             }
         }
 
         if (rhs.jjtGetParent().jjtGetParent().jjtGetNumChildren() > 1) {
             Node n = rhs.jjtGetParent().jjtGetParent().jjtGetChild(1);
-            if (n instanceof ASTPrimarySuffix && ((ASTPrimarySuffix) n).isArguments() || ((ASTPrimarySuffix) n).isArrayDeference()) {
+            if (n instanceof ASTPrimarySuffix && ((ASTPrimarySuffix) n).isArguments() || ((ASTPrimarySuffix) n).isArrayDereference()) {
                 return super.visit(node, data);
             }
         }
 
         addViolation(data, node);
         return super.visit(node, data);
     }",NotBuggy,"Fixed bug 1242544 - SimplifyConditional no longer flags null checks that precede an instanceof involving an array dereference.


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@3728 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"The commit message indicates a bug fix related to the ""SimplifyConditional"" rule in PMD. Specifically, it addresses an issue where null checks preceding an `instanceof` check involving an array dereference were incorrectly flagged. The code diff modifies the conditions under which the ""SimplifyConditional"" rule is applied. It checks for `ASTPrimarySuffix` nodes that represent array dereferences. The change ensures that the rule is not applied if such a suffix exists on either the left-hand side (lhs) or the right-hand side (rhs) of the expression. This aligns with the commit message, as it prevents the rule from flagging null checks that precede `instanceof` checks with array dereferences. The answer is **Buggy**."
pmd,8010.json,87bbe9e5b34ddf7a66a3d5b462991cbb47746d80,"@@ -1,11 +1,19 @@
     public JavaTypeDefinition getGenericType(final String parameterName) {
-        final TypeVariable<?>[] typeParameters = clazz.getTypeParameters();
-        for (int i = 0; i < typeParameters.length; i++) {
-            if (typeParameters[i].getName().equals(parameterName)) {
-                return getGenericType(i);
+        for (JavaTypeDefinition currTypeDef = this; currTypeDef != null; currTypeDef = currTypeDef.enclosingClass) {
+            final TypeVariable<?>[] typeParameters = currTypeDef.clazz.getTypeParameters();
+            for (int i = 0; i < typeParameters.length; i++) {
+                if (typeParameters[i].getName().equals(parameterName)) {
+                    return currTypeDef.getGenericType(i);
+                }
             }
         }
 
-        throw new IllegalArgumentException(""No generic parameter by name "" + parameterName
-                                                   + "" on class "" + clazz.getSimpleName());
+        // throw because we could not find parameterName
+        StringBuilder builder = new StringBuilder(""No generic parameter by name "").append(parameterName);
+        for (JavaTypeDefinition currTypeDef = this; currTypeDef != null; currTypeDef = currTypeDef.enclosingClass) {
+            builder.append(""\n on class "");
+            builder.append(clazz.getSimpleName());
+        }
+
+        throw new IllegalArgumentException(builder.toString());
     }",Buggy,"Java, typedef: fix a bug with nested classes if the outter class is generic",Buggy,"The code iterates through the enclosing classes to find the generic type definition. If not found, it throws an IllegalArgumentException. The original code only checked the immediate class. The modified code now checks nested classes as well. The commit message indicates a bug fix related to nested classes and generics. The code change aligns with the commit message, indicating a bug fix. The answer is **Buggy**."
pmd,8270.json,87bbe9e5b34ddf7a66a3d5b462991cbb47746d80,"@@ -1,3 +1,3 @@
         public void visitInnerClassType(String name) {
-            parent.parseClassName(name);
+            // parent.parseClassName(name);
         }",Buggy,"Java, typedef: fix a bug with nested classes if the outter class is generic",Buggy,"The commit message indicates a bug fix related to nested classes within generic outer classes in Java typedefs. The diff shows that the line `parent.parseClassName(name);` is commented out within the `visitInnerClassType` method. This suggests that the original code was incorrectly parsing the class name of the inner class, potentially leading to errors in type resolution or other related operations. By commenting out this line, the code is effectively skipping the parsing of the inner class name, which likely resolves the bug described in the commit message. The answer is **Buggy**.
"
pmd,7974.json,0b90fd01542a5a765cbb72e0b0c90185168d2331,"@@ -1,31 +1,32 @@
     public boolean isOnLeftHandSide() {
         // I detest this method with every atom of my being
         Node primaryExpression;
         if (location.jjtGetParent() instanceof ASTPrimaryExpression) {
             primaryExpression = location.jjtGetParent().jjtGetParent();
         } else if (location.jjtGetParent().jjtGetParent() instanceof ASTPrimaryExpression) {
             primaryExpression = location.jjtGetParent().jjtGetParent().jjtGetParent();
         } else {
             throw new RuntimeException(
-                    ""Found a NameOccurrence that didn't have an ASTPrimary Expression as parent or grandparent.  Parent = ""
-                            + location.jjtGetParent() + "" and grandparent = "" + location.jjtGetParent().jjtGetParent());
+                    ""Found a NameOccurrence ("" + location + "") that didn't have an ASTPrimary Expression as parent or grandparent.  Parent = ""
+                            + location.jjtGetParent() + "" and grandparent = "" + location.jjtGetParent().jjtGetParent()
+                            + "" (location line "" + location.getBeginLine() + "" col "" + location.getBeginColumn() + "")"");
         }
 
         if (isStandAlonePostfix(primaryExpression)) {
             return true;
         }
 
         if (primaryExpression.jjtGetNumChildren() <= 1) {
             return false;
         }
 
         if (!(primaryExpression.jjtGetChild(1) instanceof ASTAssignmentOperator)) {
             return false;
         }
 
         if (isPartOfQualifiedName() /* or is an array type */) {
             return false;
         }
 
         return !isCompoundAssignment(primaryExpression);
     }",Buggy,"[java] Fix processing error with Cast + MethodReference

Occurred in https://github.com/spring-projects/spring-framework/blob/master/spring-beans/src/main/java/org/springframework/beans/factory/support/DefaultListableBeanFactory.java#L746",Buggy,"The commit message indicates a fix for a processing error related to Cast and MethodReference. The code change adds more context to the RuntimeException message, including the location of the error. This suggests that the original exception message was not informative enough to diagnose the problem, and the added context helps in identifying the root cause. This change improves error reporting, which is a common characteristic of bug fixes. The answer is **Buggy**."
pmd,7846.json,9a940e0cc57577c730a54adf2582612581cb05af,"@@ -1,3 +1,3 @@
     public String toString() {
-        return ""Variable "" + node.getImage() + "":"" + node.getBeginLine();
+        return ""Variable symbol "" + node.getImage() + "" line "" + node.getBeginLine();
     }",Buggy,"fixed bug 660069; thx to mcclain looney for the bug report


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@1323 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"The commit message indicates a bug fix (bug 660069). The diff changes the `toString()` method's output string for better clarity and readability. The change from ""Variable"" to ""Variable symbol"" and "":"" to "" line "" improves the information provided by the method, likely addressing a reported issue about insufficient or misleading information in the original string. The answer is **Buggy**."
pmd,2946.json,4383ac357979bf353947351cac11d9586398ffbd,"@@ -1,4 +1,3 @@
         public String toString() {
-            SimpleNode n = (SimpleNode) node.jjtGetChild(1);
-            return n.getImage();
+            return node.getMethodName();
         }",Buggy,"Fixed bug 1235300 - NullAssignment no longer flags assignments to final fields.


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@3672 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"The commit message indicates a fix for bug 1235300, specifically addressing an issue where NullAssignment incorrectly flagged assignments to final fields. The diff shows a change in the `toString()` method, where the original implementation extracted the image from a child node, but the corrected version now retrieves the method name directly from the node. This change suggests that the previous implementation was likely incorrect and led to the false flagging of assignments to final fields. The answer is **Buggy**."
pmd,9191.json,4383ac357979bf353947351cac11d9586398ffbd,"@@ -1,15 +1,20 @@
     public Object visit(ASTNullLiteral node, Object data) {
-        if (lookUp(node) instanceof ASTStatementExpression) {
-            Node n = lookUp(node);
+        if (get5thParent(node) instanceof ASTStatementExpression) {
+            ASTStatementExpression n = (ASTStatementExpression)get5thParent(node);
+
+            if (isAssignmentToFinalField(n)) {
+                return data;
+            }
+
             if (n.jjtGetNumChildren() > 2 && n.jjtGetChild(1) instanceof ASTAssignmentOperator) {
                 RuleContext ctx = (RuleContext) data;
                 ctx.getReport().addRuleViolation(createRuleViolation(ctx, node));
             }
-        } else if (lookUp2(node) instanceof ASTConditionalExpression) {
-            checkTernary((ASTConditionalExpression)lookUp2(node), data, node);
-        } else if (lookUp(node) instanceof ASTConditionalExpression) {
-            checkTernary((ASTConditionalExpression)lookUp(node), data, node);
+        } else if (get4thParent(node) instanceof ASTConditionalExpression) {
+            checkTernary((ASTConditionalExpression)get4thParent(node), data, node);
+        } else if (get5thParent(node) instanceof ASTConditionalExpression) {
+            checkTernary((ASTConditionalExpression)get5thParent(node), data, node);
         }
 
         return data;
     }",Buggy,"Fixed bug 1235300 - NullAssignment no longer flags assignments to final fields.


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@3672 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"The commit message indicates a fix for bug 1235300, where ""NullAssignment"" incorrectly flagged assignments to final fields. The code changes in the diff seem to address this issue. The code now checks if the assignment is to a final field using `isAssignmentToFinalField(n)` and returns early if it is, preventing the rule violation from being reported. Also, the code replaces `lookUp` and `lookUp2` with `get5thParent` and `get4thParent` respectively, which might be related to correctly identifying the parent nodes in the AST. These changes directly relate to the bug described in the commit message. The answer is **Buggy**."
pmd,7976.json,78583ee897aa40fd474c85d59cccee016dfadfe0,"@@ -1,3 +1,3 @@
-    private boolean postFixWithExceptions(SimpleNode primaryExpression) {
+    private boolean isStandAlonePostfix(SimpleNode primaryExpression) {
         return primaryExpression instanceof ASTPostfixExpression && primaryExpression.jjtGetParent() instanceof ASTStatementExpression && thirdChildHasDottedName(primaryExpression);
     }",NotBuggy,"Fixed a predecrement problem


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@3005 51baf565-9d33-0410-a72c-fc3788e3496d",NotBuggy,"The commit message indicates a fix related to a predecrement issue. However, the diff shows a change in method name from `postFixWithExceptions` to `isStandAlonePostfix`. This change does not seem to directly address a predecrement problem. It's possible that the method was renamed as part of a larger fix, but based solely on this diff, it's not possible to confirm that a bug was fixed. The change appears to be a refactoring or renaming rather than a bug fix. The answer is **NotBuggy**."
pmd,7974.json,78583ee897aa40fd474c85d59cccee016dfadfe0,"@@ -1,32 +1,32 @@
     public boolean isOnLeftHandSide() {
         SimpleNode primaryExpression;
         if (location.jjtGetParent() instanceof ASTPrimaryExpression) {
             primaryExpression = (SimpleNode) location.jjtGetParent().jjtGetParent();
         } else if (location.jjtGetParent().jjtGetParent() instanceof ASTPrimaryExpression) {
             primaryExpression = (SimpleNode) location.jjtGetParent().jjtGetParent().jjtGetParent();
         } else {
             throw new RuntimeException(""Found a NameOccurrence that didn't have an ASTPrimary Expression as parent or grandparent.  Parent = "" + location.jjtGetParent() + "" and grandparent = "" + location.jjtGetParent().jjtGetParent());
         }
 
-        if (postFixWithExceptions(primaryExpression))  {
+        if (isStandAlonePostfix(primaryExpression))  {
             return true;
         }
 
         if (primaryExpression.jjtGetNumChildren() <= 1) {
             return false;
         }
 
         if (!(primaryExpression.jjtGetChild(1) instanceof ASTAssignmentOperator)) {
             return false;
         }
 
         if (isPartOfQualifiedName() /* or is an array type */) {
             return false;
         }
 
         if (isCompoundAssignment(primaryExpression)) {
             return false;
         }
 
         return true;
     }",NotBuggy,"Fixed a predecrement problem


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@3005 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"The commit message indicates a fix for a predecrement problem. The diff replaces `postFixWithExceptions` with `isStandAlonePostfix`. It's difficult to determine the exact nature of the bug without more context, but the change in method name suggests a refinement in how postfix expressions are handled, which could relate to a predecrement issue. Therefore, the answer is **Buggy**."
pmd,8888.json,78583ee897aa40fd474c85d59cccee016dfadfe0,"@@ -1,30 +1,30 @@
     private int initializedInConstructor(List usages, Set allConstructors) {
         int rc = MUTABLE, methodInitCount = 0;
         boolean foundUsage = false;
         Set consSet = new HashSet();
-        
+
         for (Iterator j = usages.iterator(); j.hasNext();) {
             foundUsage = true;
             NameOccurrence occ = (NameOccurrence)j.next();
-            if (occ.isOnLeftHandSide()) {
+            if (occ.isOnLeftHandSide() || occ.getLocation().jjtGetParent().jjtGetParent().jjtGetParent() instanceof ASTPreDecrementExpression || occ.getLocation().jjtGetParent().jjtGetParent().jjtGetParent() instanceof ASTPreIncrementExpression) {
                 SimpleNode node = occ.getLocation();
                 SimpleNode constructor = (SimpleNode)node.getFirstParentOfType(ASTConstructorDeclaration.class);
                 if (constructor != null) {
                     consSet.add(constructor);
                 } else {
                     if (node.getFirstParentOfType(ASTMethodDeclaration.class) != null) {
                         methodInitCount++;
                     }
                 }
             }
         }
         if (!foundUsage || ((methodInitCount == 0) && consSet.isEmpty())) {
             rc = CHECKDECL;
         } else {
             allConstructors.removeAll(consSet);
             if (allConstructors.isEmpty() && (methodInitCount == 0)) {
                 rc = IMMUTABLE;
             }
         }
         return rc;
     }",Buggy,"Fixed a predecrement problem


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@3005 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"The commit message indicates a fix for a predecrement problem. The diff modifies the condition `occ.isOnLeftHandSide()` to also include checks for `ASTPreDecrementExpression` and `ASTPreIncrementExpression` as parent nodes of the occurrence's location. This suggests that the original code was not correctly identifying variable usages within predecrement expressions, leading to incorrect analysis. By adding these checks, the code now correctly identifies these usages, fixing the bug. The answer is **Buggy**."
pmd,8892.json,78583ee897aa40fd474c85d59cccee016dfadfe0,"@@ -1,6 +1,5 @@
     private Set findAllConstructors(ASTUnmodifiedClassDeclaration node) {
-        List results = node.findChildrenOfType(ASTConstructorDeclaration.class);
-        HashSet set = new HashSet();
-        set.addAll(results);
+        Set set = new HashSet();
+        set.addAll(node.findChildrenOfType(ASTConstructorDeclaration.class));
         return set;
     }",NotBuggy,"Fixed a predecrement problem


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@3005 51baf565-9d33-0410-a72c-fc3788e3496d",NotBuggy,"The commit message indicates a fix related to a predecrement issue, but the provided code snippet doesn't show any predecrement operations or related logic. Instead, the code simplifies the retrieval of constructor declarations within a class. It replaces the explicit creation of a `List` and then adding all elements to a `HashSet` with directly adding the result of `findChildrenOfType` to the `HashSet`. This is a simplification and doesn't seem to address any predecrement problems. The answer is **NotBuggy**."
pmd,6536.json,1942e94cec6b6a30532716d14da5c7cd0cd8f075,"@@ -1,15 +1,15 @@
     private boolean isUnbalanced(String image, String pattern) {
         int occurance = 0;
-        int index = image.lastIndexOf(""="");
+        int index = image.indexOf(""="");
         index = image.indexOf(pattern, index + 1);
         while (index >= 0) {
             occurance++;
             index = image.indexOf(pattern, index + 1);
         }
 
         if ((occurance % 2) != 0) {
             return true;
         }
 
         return false;
     }",Buggy,Bug fix,Buggy,"The commit message indicates a bug fix. The code change replaces `lastIndexOf` with `indexOf` in the `isUnbalanced` method. The original code searched for the last occurrence of ""="" and then searched for the pattern after that. This is likely incorrect. The corrected code searches for the first occurrence of ""="" and then searches for the pattern after that. This change in logic suggests a bug fix. The answer is **Buggy**."
pmd,358.json,6156ba5de5f004b96bceb0cc8e44191c6876c7c9,"@@ -1,23 +1,23 @@
     private void validate() throws BuildException {
         if (formatters.isEmpty() && !printToConsole) {
             throw new BuildException(""No formatter specified; and printToConsole was false"");
         }
 
         for (Iterator i = formatters.iterator(); i.hasNext();) {
             Formatter f = (Formatter) i.next();
-            if (f.isToFileNull()) {
+            if (f.isNoOutputSupplied()) {
                 throw new BuildException(""Formatter toFile attribute is required"");
             }
         }
 
         if (ruleSetFiles == null) {
             if (nestedRules.isEmpty()) {
                 throw new BuildException(""No rulesets specified"");
             }
             ruleSetFiles = getNestedRuleSetFiles();
         }
 
         if (!targetJDK.equals(""1.3"") && !targetJDK.equals(""1.4"") && !targetJDK.equals(""1.5"")) {
             throw new BuildException(""The targetjdk attribute, if used, must be set to either '1.3', '1.4', or '1.5'"");
         }
     }",Buggy,"Applied a patch from Wouter Zelle to clean up the Ant Formatter class, fix a TextRenderer bug, and make toConsole cleaner.


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@4292 51baf565-9d33-0410-a72c-fc3788e3496d",NotBuggy,"The commit message indicates a bug fix in the TextRenderer, but the provided diff focuses on the Ant Formatter class. The changes in the Ant Formatter class involve exception handling and validation logic. Specifically, it checks if a formatter's output file is not supplied and throws an exception if no rulesets are specified. These changes suggest improved error handling and validation within the Ant task, which can prevent unexpected behavior or incorrect configurations. The change from `isToFileNull()` to `isNoOutputSupplied()` suggests a renaming or refactoring of the method, but the core logic remains the same. Therefore, the changes in the Ant Formatter class do not directly indicate a bug fix. The answer is **NotBuggy**."
pmd,412.json,604a45d941cf2042c5c548490ef4401ad54080c9,"@@ -1,12 +1,10 @@
     private void logRulesUsed(RuleSets rules) {
         log(""Using these rulesets: "" + ruleSetFiles, Project.MSG_VERBOSE);
 
         RuleSet[] ruleSets = rules.getAllRuleSets();
-        for (int j = 0; j < ruleSets.length; j++) {
-            RuleSet ruleSet = ruleSets[j];
-
+        for (RuleSet ruleSet : ruleSets) {
             for (Rule rule: ruleSet.getRules()) {
                 log(""Using rule "" + rule.getName(), Project.MSG_VERBOSE);
             }
         }
     }",NotBuggy,"Fixed bug 1943204 - Ant task: <ruleset> path should be relative to Ant basedir


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@6441 51baf565-9d33-0410-a72c-fc3788e3496d",NotBuggy,"The commit message indicates a bug fix related to the path handling of `<ruleset>` in the Ant task. The provided code diff changes the way the rulesets are iterated, using an enhanced for loop instead of a traditional for loop with an index. While this change improves readability, it doesn't directly address the path resolution issue described in the commit message. It's likely that the actual fix for the path issue is in a different part of the codebase, which is not shown in this diff. The change shown here is a refactoring. The answer is **NotBuggy**."
pmd,358.json,604a45d941cf2042c5c548490ef4401ad54080c9,"@@ -1,19 +1,31 @@
     private void validate() throws BuildException {
         // TODO - check for empty Formatters List here?
         for (Formatter f: formatters) {
             if (f.isNoOutputSupplied()) {
                 throw new BuildException(""toFile or toConsole needs to be specified in Formatter"");
             }
         }
 
         if (ruleSetFiles == null) {
             if (nestedRules.isEmpty()) {
                 throw new BuildException(""No rulesets specified"");
             }
             ruleSetFiles = getNestedRuleSetFiles();
         }
 
+        // convert relative paths and substitute env variables/properties
+        final StringBuffer sb = new StringBuffer();
+        for(String s: ruleSetFiles.split("","")) {
+            Path p = new Path(getProject());
+            p.setPath(getProject().replaceProperties(s));
+            if (sb.length() > 0) {
+                sb.append(',');
+            }
+            sb.append(p);
+        }
+        ruleSetFiles = sb.toString();
+
         if (!targetJDK.equals(""1.3"") && !targetJDK.equals(""1.4"") && !targetJDK.equals(""1.5"") && !targetJDK.equals(""1.6"") && !targetJDK.equals(""1.7"") && !targetJDK.equals(""jsp"")) {
             throw new BuildException(""The targetjdk attribute, if used, must be set to either '1.3', '1.4', '1.5', '1.6', '1.7' or 'jsp'"");
         }
     }",Buggy,"Fixed bug 1943204 - Ant task: <ruleset> path should be relative to Ant basedir


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@6441 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"The commit message indicates a fix for bug 1943204, which concerns the `<ruleset>` path in the Ant task. The provided diff shows that the code now converts relative paths to absolute paths using `Path` and `getProject().replaceProperties(s)` before processing the ruleset files. This ensures that the paths are correctly resolved relative to the Ant basedir, as stated in the commit message. Therefore, the changes directly address the bug described. The answer is **Buggy**."
pmd,6753.json,1ef5079b29401bdb0df6e773f0160fa54f779018,"@@ -1,8 +1,10 @@
     public Object visit(ASTPrimaryExpression node, Object data) {
-        if (isForeignAttributeOrMethod(node) && (isAttributeAccess(node)
-            || isMethodCall(node) && isForeignGetterSetterCall(node))) {
-
-            ((MutableInt) data).increment();
+        if (isForeignAttributeOrMethod(node)) {
+            if (isAttributeAccess(node)) {
+                ((MutableInt) data).increment();
+            } else {
+                ((MutableInt) data).add(countForeignGetterSetterCalls(node));
+            }
         }
         return super.visit(node, data);
     }",Buggy,"[java] ATFD calculation problem

* Fixes #1910
* Method call chains are now considered",Buggy,"The commit message indicates a fix for the ATFD (Access To Foreign Data) calculation problem, specifically addressing issue #1910. The diff shows that the code now considers method call chains when calculating ATFD. The original code only incremented the count if it was a foreign attribute or method and either an attribute access or a foreign getter/setter call. The updated code now differentiates between attribute access and other method calls. If it's an attribute access, it increments the count. Otherwise, it calls `countForeignGetterSetterCalls` to handle method call chains and adds the result to the count. This change suggests that the previous implementation was not correctly counting method call chains, leading to an inaccurate ATFD calculation. Therefore, this commit fixes a bug in the ATFD calculation. The answer is **Buggy**."
pmd,6754.json,1ef5079b29401bdb0df6e773f0160fa54f779018,"@@ -1,6 +1,4 @@
     private boolean isForeignGetterSetterCall(ASTPrimaryExpression node) {
-
         String methodOrAttributeName = getMethodOrAttributeName(node);
-
-        return methodOrAttributeName != null && StringUtils.startsWithAny(methodOrAttributeName, ""get"", ""is"", ""set"");
+        return isForeignGetterSetterCall(methodOrAttributeName);
     }",Buggy,"[java] ATFD calculation problem

* Fixes #1910
* Method call chains are now considered",Buggy,"The commit message indicates a fix for a problem in ATFD calculation, specifically related to method call chains. The code diff replaces the original logic within `isForeignGetterSetterCall` with a call to another method `isForeignGetterSetterCall(methodOrAttributeName)`. This suggests a bug fix by delegating the logic to a more comprehensive or corrected implementation. The answer is **Buggy**.
"
hbase,27505.json,d589b7238257c78dc8616e47ab5bb3e2309a5237,"@@ -1,3 +1,3 @@
-    public Throwable getCause() {
+    public synchronized Throwable getCause() {
       return cause;
     }",Buggy,"HBASE-19847 Fix findbugs and error-prone warnings in hbase-thrift (branch-2)

Signed-off-by: tedyu <yuzhihong@gmail.com>",Buggy,"The commit message indicates a fix for findbugs and error-prone warnings. The diff adds a `synchronized` keyword to the `getCause()` method. This change likely addresses a potential thread-safety issue identified by findbugs or error-prone, where concurrent access to the `cause` field could lead to unexpected behavior. This is a bug fix. The answer is **Buggy**."
hbase,7228.json,84a50393ee56d09abb68f54b44b64f5279bd33c9,"@@ -1,8 +1,8 @@
-  public long refill(long limit, long available) {
+  public long refill(long limit) {
     final long now = EnvironmentEdgeManager.currentTime();
     if (now < nextRefillTime) {
       return 0;
     }
     nextRefillTime = now + super.getTimeUnitInMillis();
     return limit;
   }",NotBuggy,HBASE-13888 Fix refill bug from HBASE-13686 (Guanghao Zhang),Buggy,"The commit message indicates a bug fix related to a refill operation, specifically addressing an issue introduced in a previous change (HBASE-13686). The diff removes the `available` parameter from the `refill` method. This suggests that the previous implementation was incorrectly using or calculating the available amount, leading to the bug. By removing the parameter and simply returning the limit, the code likely corrects the refill logic. The answer is **Buggy**."
hbase,7525.json,84a50393ee56d09abb68f54b44b64f5279bd33c9,"@@ -1,16 +1,16 @@
   public synchronized boolean canExecute(final long amount) {
-    long refillAmount = refill(limit, avail);
+    long refillAmount = refill(limit);
     if (refillAmount == 0 && avail < amount) {
       return false;
     }
     // check for positive overflow
     if (avail <= Long.MAX_VALUE - refillAmount) {
       avail = Math.max(0, Math.min(avail + refillAmount, limit));
     } else {
       avail = Math.max(0, limit);
     }
     if (avail >= amount) {
       return true;
     }
     return false;
   }",Buggy,HBASE-13888 Fix refill bug from HBASE-13686 (Guanghao Zhang),Buggy,"The commit message indicates a bug fix related to a refill operation, specifically referencing a previous commit (HBASE-13686). The diff shows a change in the `canExecute` method where the `refill` method's arguments have been modified. Initially, `refill` took both `limit` and `avail` as arguments, but now it only takes `limit`. This suggests that the previous implementation of `refill` might have been using the `avail` value incorrectly, leading to a bug in the refill logic. The change aims to correct this by only using the `limit` in the `refill` calculation. The answer is **Buggy**."
hbase,7389.json,84a50393ee56d09abb68f54b44b64f5279bd33c9,"@@ -1,15 +1,15 @@
-  public long refill(long limit, long available) {
+  public long refill(long limit) {
     final long now = EnvironmentEdgeManager.currentTime();
     if (nextRefillTime == -1) {
       // Till now no resource has been consumed.
       nextRefillTime = EnvironmentEdgeManager.currentTime();
       return limit;
     }
 
     long delta = (limit * (now - nextRefillTime)) / super.getTimeUnitInMillis();
     if (delta > 0) {
       this.nextRefillTime = now;
-      return Math.min(limit, available + delta);
+      return Math.min(limit, delta);
     }
     return 0;
   }",Buggy,HBASE-13888 Fix refill bug from HBASE-13686 (Guanghao Zhang),Buggy,"The commit message indicates a bug fix related to a refill operation, specifically addressing a bug introduced in a previous change (HBASE-13686). The diff modifies the `refill` method, removing the `available` parameter and adjusting the return value calculation. The original code added `available` to `delta` before taking the minimum with `limit`. The corrected code removes `available` and directly returns the minimum of `limit` and `delta`. This suggests that the previous implementation was incorrectly calculating the refill amount, potentially leading to over-allocation or other unexpected behavior. The removal of the `available` parameter and the change in the return calculation indicates a correction to a logical error. The answer is **Buggy**."
hbase,1189.json,2beda62a10f0828eb10cec28b0ba53246cd0b671,"@@ -1,5 +1,4 @@
   public static void main(String[] args) throws Exception {
-    if (conf == null) conf = HBaseConfiguration.create();
-    int ret = ToolRunner.run(conf, new ReplicationSyncUp(), args);
+    int ret = ToolRunner.run(HBaseConfiguration.create(), new ReplicationSyncUp(), args);
     System.exit(ret);
   }",Buggy,"HBASE-20083 Fix findbugs error for ReplicationSyncUp
",Buggy,The commit message indicates a fix for a FindBugs error in `ReplicationSyncUp`. The diff removes the initialization of `conf` with `HBaseConfiguration.create()` if it's null and directly passes `HBaseConfiguration.create()` to `ToolRunner.run`. This change likely addresses a potential null pointer exception or other issues flagged by FindBugs related to the `conf` variable. The answer is **Buggy**.
hbase,1193.json,2beda62a10f0828eb10cec28b0ba53246cd0b671,"@@ -1,3 +1,3 @@
     public Configuration getConfiguration() {
-      return conf;
+      return getConf();
     }",Buggy,"HBASE-20083 Fix findbugs error for ReplicationSyncUp
",Buggy,"The commit message indicates a fix for a FindBugs error related to `ReplicationSyncUp`. The diff replaces `conf` with `getConf()`. This suggests that `conf` was a direct field access, and FindBugs flagged it as a potential violation of encapsulation or thread-safety. Replacing it with `getConf()` (presumably a getter method) addresses this issue. This change is likely a bug fix, as it resolves a code quality or potential correctness issue identified by FindBugs. The answer is **Buggy**.
"
hbase,5737.json,c24cf2d55ecd479c89b0613b6ebbdaba4eb793ad,"@@ -1,7 +1,10 @@
   private void addRegion(final LinkedList<RegionInfo> regions, RegionInfo hri) {
     // If meta, move it last otherwise other unassigns fail because meta is not
     // online for them to update state in. This is dodgy. Needs to be made more
     // robust. See TODO below.
-    if (hri.isMetaRegion()) regions.addLast(hri);
-    else regions.addFirst(hri);
+    if (hri.isMetaRegion()) {
+      regions.addLast(hri);
+    } else {
+      regions.addFirst(hri);
+    }
   }",NotBuggy,"HBASE-19601 Fixed Checkstyle errors in hbase-rsgroup and enabled Checkstyle to fail on violations
",NotBuggy,"The commit message indicates a fix for Checkstyle errors. The diff shows a change in the `addRegion` method, specifically adding braces to the `if` and `else` blocks. This change improves code readability and maintainability, and it aligns with the commit message's intention to fix Checkstyle errors. This change does not indicate a bug fix. The answer is **NotBuggy**."
hbase,5705.json,c24cf2d55ecd479c89b0613b6ebbdaba4eb793ad,"@@ -1,8 +1,11 @@
   static Set<Address> getOnlineServers(final MasterServices master) {
     Set<Address> onlineServers = new HashSet<Address>();
-    if (master == null) return onlineServers;
+    if (master == null) {
+      return onlineServers;
+    }
+
     for(ServerName server: master.getServerManager().getOnlineServers().keySet()) {
       onlineServers.add(server.getAddress());
     }
     return onlineServers;
   }",NotBuggy,"HBASE-19601 Fixed Checkstyle errors in hbase-rsgroup and enabled Checkstyle to fail on violations
",NotBuggy,"The commit message indicates a fix for Checkstyle errors. The diff shows a minor change in the `getOnlineServers` method, adding braces to a single-line `if` statement. This change improves code style and readability, aligning with the commit message's focus on Checkstyle. The functionality of the code remains the same. The answer is **NotBuggy**."
hbase,5782.json,c24cf2d55ecd479c89b0613b6ebbdaba4eb793ad,"@@ -1,4 +1,7 @@
   public boolean isOnline() {
-    if (this.rsGroupInfoManager == null) return false;
+    if (this.rsGroupInfoManager == null) {
+      return false;
+    }
+
     return this.rsGroupInfoManager.isOnline();
   }",NotBuggy,"HBASE-19601 Fixed Checkstyle errors in hbase-rsgroup and enabled Checkstyle to fail on violations
",NotBuggy,"The commit message indicates a fix for Checkstyle errors. The diff introduces braces to an `if` statement, which is a common style fix. This change doesn't alter the functionality of the code, but rather enforces a coding style. The answer is **NotBuggy**."
hbase,5736.json,c24cf2d55ecd479c89b0613b6ebbdaba4eb793ad,"@@ -1,16 +1,19 @@
   private List<RegionInfo> getRegions(final Address server) {
     LinkedList<RegionInfo> regions = new LinkedList<>();
     for (Map.Entry<RegionInfo, ServerName> el :
         master.getAssignmentManager().getRegionStates().getRegionAssignments().entrySet()) {
-      if (el.getValue() == null) continue;
+      if (el.getValue() == null) {
+        continue;
+      }
+
       if (el.getValue().getAddress().equals(server)) {
         addRegion(regions, el.getKey());
       }
     }
     for (RegionStateNode state : master.getAssignmentManager().getRegionsInTransition()) {
       if (state.getRegionLocation().getAddress().equals(server)) {
         addRegion(regions, state.getRegionInfo());
       }
     }
     return regions;
   }",NotBuggy,"HBASE-19601 Fixed Checkstyle errors in hbase-rsgroup and enabled Checkstyle to fail on violations
",NotBuggy,"The commit message indicates a fix for Checkstyle errors. The diff introduces a minor formatting change by adding braces to a single-line `if` statement. This change improves code readability and maintainability, aligning with Checkstyle guidelines. However, it doesn't address any functional bugs or logical errors in the code. The answer is **NotBuggy**."
hbase,5743.json,c24cf2d55ecd479c89b0613b6ebbdaba4eb793ad,"@@ -1,58 +1,61 @@
   public boolean balanceRSGroup(String groupName) throws IOException {
     ServerManager serverManager = master.getServerManager();
     AssignmentManager assignmentManager = master.getAssignmentManager();
     LoadBalancer balancer = master.getLoadBalancer();
 
     synchronized (balancer) {
       // If balance not true, don't run balancer.
-      if (!((HMaster) master).isBalancerOn()) return false;
+      if (!((HMaster) master).isBalancerOn()) {
+        return false;
+      }
+
       if (master.getMasterCoprocessorHost() != null) {
         master.getMasterCoprocessorHost().preBalanceRSGroup(groupName);
       }
       if (getRSGroupInfo(groupName) == null) {
         throw new ConstraintException(""RSGroup does not exist: ""+groupName);
       }
       // Only allow one balance run at at time.
       Map<String, RegionState> groupRIT = rsGroupGetRegionsInTransition(groupName);
       if (groupRIT.size() > 0) {
         LOG.debug(""Not running balancer because "" + groupRIT.size() + "" region(s) in transition: "" +
           StringUtils.abbreviate(
               master.getAssignmentManager().getRegionStates().getRegionsInTransition().toString(),
               256));
         return false;
       }
       if (serverManager.areDeadServersInProgress()) {
         LOG.debug(""Not running balancer because processing dead regionserver(s): "" +
             serverManager.getDeadServers());
         return false;
       }
 
       //We balance per group instead of per table
       List<RegionPlan> plans = new ArrayList<>();
       for(Map.Entry<TableName, Map<ServerName, List<RegionInfo>>> tableMap:
           getRSGroupAssignmentsByTable(groupName).entrySet()) {
         LOG.info(""Creating partial plan for table "" + tableMap.getKey() + "": ""
             + tableMap.getValue());
         List<RegionPlan> partialPlans = balancer.balanceCluster(tableMap.getValue());
         LOG.info(""Partial plan for table "" + tableMap.getKey() + "": "" + partialPlans);
         if (partialPlans != null) {
           plans.addAll(partialPlans);
         }
       }
       long startTime = System.currentTimeMillis();
       boolean balancerRan = !plans.isEmpty();
       if (balancerRan) {
         LOG.info(""RSGroup balance "" + groupName + "" starting with plan count: "" + plans.size());
         for (RegionPlan plan: plans) {
           LOG.info(""balance "" + plan);
           assignmentManager.moveAsync(plan);
         }
         LOG.info(""RSGroup balance "" + groupName + "" completed after "" +
             (System.currentTimeMillis()-startTime) + "" seconds"");
       }
       if (master.getMasterCoprocessorHost() != null) {
         master.getMasterCoprocessorHost().postBalanceRSGroup(groupName, balancerRan);
       }
       return balancerRan;
     }
   }",NotBuggy,"HBASE-19601 Fixed Checkstyle errors in hbase-rsgroup and enabled Checkstyle to fail on violations
",NotBuggy,"The commit message indicates a fix for Checkstyle errors. The diff introduces minor formatting changes, specifically adding braces to single-line `if` statements. These changes improve code readability and maintainability, aligning with Checkstyle requirements. However, they do not address any functional bugs or logical errors in the code. The changes are purely cosmetic and aimed at improving code style. The answer is **NotBuggy**."
hbase,3328.json,d272ac908ceb4696e05431066ae02d953fa6fc9d,"@@ -1,30 +1,30 @@
   protected void chore() {
-    if (!connected) {
+    if (!isConnected()) {
       return;
     }
 
     List<ServerName> sns = generateDeadServersListToSend();
     if (sns.isEmpty()) {
       // Nothing to send. Done.
       return;
     }
 
     final long curTime = EnvironmentEdgeManager.currentTime();
     if (lastMessageTime > curTime - messagePeriod) {
       // We already sent something less than 10 second ago. Done.
       return;
     }
 
     // Ok, we're going to send something then.
     lastMessageTime = curTime;
 
     // We're reusing an existing protobuf message, but we don't send everything.
     // This could be extended in the future, for example if we want to send stuff like the
     //  hbase:meta server name.
     publisher.publish(ClusterMetricsBuilder.newBuilder()
       .setHBaseVersion(VersionInfo.getVersion())
       .setClusterId(master.getMasterFileSystem().getClusterId().toString())
       .setMasterName(master.getServerName())
       .setDeadServerNames(sns)
       .build());
   }",NotBuggy,"HBASE-20069 fix existing findbugs errors in hbase-server; ADDENDUM Address review
",Buggy,"The commit message indicates a fix for FindBugs errors. The code changes replace `!connected` with `!isConnected()`. This suggests that `connected` was a field that should have been accessed via a getter method (`isConnected()`). This is likely a bug fix, as it corrects improper access to a class member. The answer is **Buggy**."
hbase,36406.json,aeffca497bf36ea12f89a5f92d2f918b010741fc,"@@ -1,6 +1,6 @@
   public void writeTo(Object object, Class<?> type, Type genericType,
       Annotation[] annotations, MediaType mediaType,
       MultivaluedMap<String, Object> httpHeaders, OutputStream outStream)
       throws IOException, WebApplicationException {
-    outStream.write(object.toString().getBytes());
+    outStream.write(Bytes.toBytes(object.toString()));
   }",Buggy,"HBASE-19846 Fix findbugs and error-prone warnings in hbase-rest (branch-2)

Signed-off-by: tedyu <yuzhihong@gmail.com>
",Buggy,"The commit message indicates a fix for FindBugs and Error-prone warnings in hbase-rest. The diff shows a change in the `writeTo` method where `object.toString().getBytes()` is replaced with `Bytes.toBytes(object.toString())`. This change likely addresses a character encoding issue. The original code used the platform's default encoding, which could lead to inconsistencies or errors when dealing with different character sets. The `Bytes.toBytes()` method likely uses a specific encoding (e.g., UTF-8) to ensure consistent and correct byte representation of the string. This resolves a potential bug related to character encoding. The answer is **Buggy**."
hbase,34490.json,2413a10e6718afaf74185078f0320f2e9e47d273,"@@ -1,47 +1,48 @@
   public List<InputSplit> getSplits(JobContext context) throws IOException {
     boolean closeOnFinish = false;
 
     // Just in case a subclass is relying on JobConfigurable magic.
     if (table == null) {
       initialize(context);
       closeOnFinish = true;
     }
 
     // null check in case our child overrides getTable to not throw.
     try {
       if (getTable() == null) {
         // initialize() must not have been implemented in the subclass.
         throw new IOException(INITIALIZATION_ERROR);
       }
     } catch (IllegalStateException exception) {
       throw new IOException(INITIALIZATION_ERROR, exception);
     }
 
     try {
       List<InputSplit> splits = oneInputSplitPerRegion();
 
       // set same number of mappers for each region
       if (context.getConfiguration().get(NUM_MAPPERS_PER_REGION) != null) {
         int nSplitsPerRegion = context.getConfiguration().getInt(NUM_MAPPERS_PER_REGION, 1);
         List<InputSplit> res = new ArrayList<>();
         for (int i = 0; i < splits.size(); i++) {
           List<InputSplit> tmp = createNInputSplitsUniform(splits.get(i), nSplitsPerRegion);
           res.addAll(tmp);
         }
         return res;
       }
 
       //The default value of ""hbase.mapreduce.input.autobalance"" is false.
-      if (context.getConfiguration().getBoolean(MAPREDUCE_INPUT_AUTOBALANCE, false) != false) {
-        long maxAveRegionSize = context.getConfiguration().getInt(MAX_AVERAGE_REGION_SIZE, 8*1073741824);
+      if (context.getConfiguration().getBoolean(MAPREDUCE_INPUT_AUTOBALANCE, false)) {
+        long maxAveRegionSize = context.getConfiguration()
+            .getLong(MAX_AVERAGE_REGION_SIZE, 8L*1073741824); //8GB
         return calculateAutoBalancedSplits(splits, maxAveRegionSize);
       }
 
       // return one mapper per region
       return splits;
     } finally {
       if (closeOnFinish) {
         closeTable();
       }
     }
   }",NotBuggy,"HBASE-19195 error-prone fixes for client, mr, and server
",Buggy,"The commit message indicates that the changes are ""error-prone fixes"". The code changes include:
1. Adding a null check for `getTable()` and throwing an `IOException` if it returns null. This prevents a potential `NullPointerException` if `getTable()` is not properly implemented in a subclass.
2. Catching `IllegalStateException` thrown by `getTable()` and re-throwing it as an `IOException` with a more informative error message. This improves error handling.
3. Simplifying the condition `context.getConfiguration().getBoolean(MAPREDUCE_INPUT_AUTOBALANCE, false) != false` to `context.getConfiguration().getBoolean(MAPREDUCE_INPUT_AUTOBALANCE, false)`.
4. Changing the type of `maxAveRegionSize` from `int` to `long` and updating the default value calculation to use `8L` to avoid integer overflow.

These changes address potential errors and improve the robustness of the code. Therefore, the changes indicate a bug fix.
The answer is **Buggy**."
hbase,5669.json,9fac4877d3072c3589370c9d0446342ee2658ab6,"@@ -1,7 +1,6 @@
   public static void resetCounters() throws Exception {
     Class<?> cl = (new SplitLogCounters()).getClass();
-    Field[] flds = cl.getDeclaredFields();
-    for (Field fld : flds) {
+    for (Field fld : cl.getDeclaredFields()) {
       ((AtomicLong)fld.get(null)).set(0);
     }
   }",NotBuggy,"HBASE-5598 Analyse and fix the findbugs reporting by QA and add invalid bugs into findbugs-excludeFilter file

git-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1425351 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates that the purpose of the commit is to address FindBugs reports, including fixing identified bugs and adding invalid bug reports to the exclusion filter. The provided diff shows a change in the `resetCounters` method. The change involves iterating through the declared fields of the `SplitLogCounters` class and resetting `AtomicLong` fields to zero. The original code and the modified code are functionally equivalent. The change appears to be a refactoring or code cleanup, possibly to address a FindBugs warning related to unchecked casts or potential null pointer exceptions, but it doesn't directly fix a functional bug. Therefore, the change is related to code quality and maintainability rather than a specific bug fix. The answer is **NotBuggy**."
hbase,10177.json,9fac4877d3072c3589370c9d0446342ee2658ab6,"@@ -1,6 +1,5 @@
   public void start() {
     worker = new Thread(null, this, ""SplitLogWorker-"" + serverName);
     exitWorker = false;
     worker.start();
-    return;
   }",NotBuggy,"HBASE-5598 Analyse and fix the findbugs reporting by QA and add invalid bugs into findbugs-excludeFilter file

git-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1425351 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates an effort to address FindBugs reports, suggesting potential bug fixes or code quality improvements. The diff removes a `return` statement at the end of the `start()` method. Removing a redundant `return` statement doesn't inherently fix a bug, but it does clean up the code. It's more of a code cleanup or style improvement rather than a bug fix. The answer is **NotBuggy**."
hbase,28329.json,f1ad5cb93837e8d07d9d08da7c1a48caf74bbe9f,"@@ -1,8 +1,11 @@
-  public Get setTimeStamp(long timestamp) {
+  public Get setTimeStamp(long timestamp)
+  throws IOException {
     try {
       tr = new TimeRange(timestamp, timestamp+1);
     } catch(IOException e) {
-      // Will never happen
+      // This should never happen, unless integer overflow or something extremely wrong...
+      LOG.error(""TimeRange failed, likely caused by integer overflow. "", e);
+      throw e;
     }
     return this;
   }",NotBuggy,"HBASE-10452 Fix potential bugs in exception handlers (Ding Yuan)



git-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1567979 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for potential bugs in exception handlers. The diff shows that a `try-catch` block was added around the `TimeRange` constructor call in the `setTimeStamp` method. The catch block logs an error message and re-throws the exception. This suggests that there was a potential issue where an `IOException` could be thrown by the `TimeRange` constructor (likely due to integer overflow), and the original code was not handling it properly. The added exception handling improves the robustness of the code by logging the error and re-throwing the exception, allowing the caller to handle it. This is a clear indication of a bug fix. The answer is **Buggy**."
hbase,40756.json,f1ad5cb93837e8d07d9d08da7c1a48caf74bbe9f,"@@ -1,25 +1,26 @@
   public int getNumberOfRunningProcess(){
     if (!isUnix()){
       return 0;
     }
 
     BufferedReader input = null;
     try {
       int count = 0;
       Process p = Runtime.getRuntime().exec(""ps -e"");
       input = new BufferedReader(new InputStreamReader(p.getInputStream()));
       while (input.readLine() != null) {
         count++;
       }
       return count - 1; //  -1 because there is a headline
     } catch (IOException e) {
       return -1;
     }  finally {
       if (input != null){
         try {
           input.close();
-        } catch (IOException ignored) {
+        } catch (IOException e) {
+          LOG.warn(""Not able to close the InputStream"", e);
         }
       }
     }
   }",NotBuggy,"HBASE-10452 Fix potential bugs in exception handlers (Ding Yuan)



git-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1567979 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for potential bugs in exception handlers. The code diff shows a change in the `finally` block where the `input.close()` method is called. Previously, the `IOException` that could be thrown by `input.close()` was ignored. Now, the exception is caught, and a warning message is logged. This change addresses a potential bug where a failure to close the input stream would go unnoticed, which could lead to resource leaks or other issues. The answer is **Buggy**."
hbase,40081.json,f1ad5cb93837e8d07d9d08da7c1a48caf74bbe9f,"@@ -1,10 +1,15 @@
   public static boolean isShowConfInServlet() {
     boolean isShowConf = false;
     try {
       if (Class.forName(""org.apache.hadoop.conf.ConfServlet"") != null) {
         isShowConf = true;
       }
-    } catch (Exception e) {
+    } catch (LinkageError e) {
+       // should we handle it more aggressively in addition to log the error?
+       LOG.warn(""Error thrown: "", e);
+    } catch (ClassNotFoundException ce) {
+      LOG.debug(""ClassNotFound: ConfServlet"");
+      // ignore
     }
     return isShowConf;
   }",NotBuggy,"HBASE-10452 Fix potential bugs in exception handlers (Ding Yuan)



git-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1567979 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for potential bugs in exception handlers. The diff shows changes to the `isShowConfInServlet` method, specifically in how exceptions are handled. The original code had a single `catch` block for all exceptions. The modified code separates the exception handling into two `catch` blocks: one for `LinkageError` and another for `ClassNotFoundException`. The `LinkageError` is logged with a warning, and the `ClassNotFoundException` is logged with a debug message and then ignored. This change improves the robustness of the code by handling specific exceptions in a more targeted way, which aligns with the commit message's intention to fix potential bugs in exception handlers. The answer is **Buggy**."
hbase,10426.json,f1ad5cb93837e8d07d9d08da7c1a48caf74bbe9f,"@@ -1,30 +1,34 @@
   protected void configureForRegion(HRegion region) {
     super.configureForRegion(region);
     if (region != null) {
       prefixLength = 0;
 
       // read the prefix length from the table descriptor
       String prefixLengthString = region.getTableDesc().getValue(
           PREFIX_LENGTH_KEY);
       if (prefixLengthString == null) {
         //read the deprecated value
         prefixLengthString = region.getTableDesc().getValue(PREFIX_LENGTH_KEY_DEPRECATED);
         if (prefixLengthString == null) {
           LOG.error(PREFIX_LENGTH_KEY + "" not specified for table ""
               + region.getTableDesc().getTableName()
               + "". Using default RegionSplitPolicy"");
           return;
         }
       }
       try {
         prefixLength = Integer.parseInt(prefixLengthString);
       } catch (NumberFormatException nfe) {
-        // ignore
+        /* Differentiate NumberFormatException from an invalid value range reported below. */
+        LOG.error(""Number format exception when parsing "" + PREFIX_LENGTH_KEY + "" for table ""
+            + region.getTableDesc().getTableName() + "":""
+            + prefixLengthString + "". "" + nfe);
+        return;
       }
       if (prefixLength <= 0) {
         LOG.error(""Invalid value for "" + PREFIX_LENGTH_KEY + "" for table ""
             + region.getTableDesc().getTableName() + "":""
             + prefixLengthString + "". Using default RegionSplitPolicy"");
       }
     }
   }",NotBuggy,"HBASE-10452 Fix potential bugs in exception handlers (Ding Yuan)



git-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1567979 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for potential bugs in exception handlers. The diff shows a change in the `configureForRegion` method where a `NumberFormatException` is caught when parsing the `PREFIX_LENGTH_KEY`. The original code ignored the exception, potentially leading to unexpected behavior. The modified code now logs the exception with more details, including the table name and the invalid string that caused the exception. This provides better error reporting and helps in debugging configuration issues. The addition of logging for the `NumberFormatException` indicates an improvement in error handling, which aligns with the commit message's claim of fixing potential bugs in exception handlers. The answer is **Buggy**."
ant,5703.json,a66a2b7f86fe7f4bab938d0cb5167b27c8fa4957,"@@ -1,3 +1,8 @@
     public String getJavacExecutable() {
+        if (forkedExecutable == null && isForkedJavac()) {
+            forkedExecutable = getSystemJavac();
+        } else if (forkedExecutable != null && !isForkedJavac()) {
+            forkedExecutable = null;
+        }
         return forkedExecutable;
     }",Buggy,"Add testcases for latest <javac> changes, fix some problem with some
rare combination of the fork attribute and build.compiler settings.


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@269879 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix related to the `fork` attribute and `build.compiler` settings in the context of `<javac>` (Java compiler) usage within Ant. The code diff modifies the `getJavacExecutable` method. Specifically, it checks if `forkedExecutable` is null while `isForkedJavac()` is true, and vice-versa. If these conditions are met, it updates `forkedExecutable` accordingly. This suggests a bug where the forked executable was not being correctly set or cleared based on the fork attribute, leading to incorrect compiler execution. The answer is **Buggy**."
ant,5699.json,a66a2b7f86fe7f4bab938d0cb5167b27c8fa4957,"@@ -1,15 +1,16 @@
     public void setFork(String f) {
         if (f.equalsIgnoreCase(""on"")
             || f.equalsIgnoreCase(""true"")
             || f.equalsIgnoreCase(""yes"")) {
             fork = ""true"";
             forkedExecutable = getSystemJavac();
         } else if (f.equalsIgnoreCase(""off"")
                    || f.equalsIgnoreCase(""false"")
                    || f.equalsIgnoreCase(""no"")) {
             fork = ""false"";
+            forkedExecutable = null;
         } else {
             fork = ""true"";
             forkedExecutable = f;
         }
     }",Buggy,"Add testcases for latest <javac> changes, fix some problem with some
rare combination of the fork attribute and build.compiler settings.


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@269879 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for a rare combination of the `fork` attribute and `build.compiler` settings. The diff shows that when the `fork` attribute is set to ""false"", the `forkedExecutable` is now explicitly set to `null`. This suggests that previously, in some scenarios, the `forkedExecutable` might have retained a value even when `fork` was set to ""false"", leading to unexpected behavior. Setting it to `null` likely resolves this issue. The answer is **Buggy**."
ant,9617.json,4e2a4f9c9507b19b248548ec6333698af3c1a4b8,"@@ -1,35 +1,38 @@
     public String substitute(String input, String argument, int options)
         throws BuildException {
         // translate \1 to $1 so that the Perl5Substitution will work
         StringBuffer subst = new StringBuffer();
         for (int i = 0; i < argument.length(); i++) {
             char c = argument.charAt(i);
-            if (c == '\\') {
+            if (c == '$') {
+                subst.append('\\');
+                subst.append('$');
+            } else if (c == '\\') {
                 if (++i < argument.length()) {
                     c = argument.charAt(i);
                     int value = Character.digit(c, 10);
                     if (value > -1) {
                         subst.append(""$"").append(value);
                     } else {
                         subst.append(c);
                     }
                 } else {
                     // XXX - should throw an exception instead?
                     subst.append('\\');
                 }
             } else {
                 subst.append(c);
             }
         }
         
 
         // Do the substitution
         Substitution s = 
             new Perl5Substitution(subst.toString(), 
                                   Perl5Substitution.INTERPOLATE_ALL);
         return Util.substitute(matcher,
                                getCompiledPattern(options),
                                s,
                                input,
                                getSubsOptions(options));
     }",Buggy,"fix for bug 20306 - regex handling of $ in replace string


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@274633 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for bug 20306, specifically addressing regex handling of '$' in the replacement string. The diff modifies the `substitute` method to handle '$' characters in the replacement string correctly. The code now escapes '$' with a backslash ('\') before appending it to the `subst` buffer. This ensures that '$' is treated literally in the replacement string, preventing unintended interpretation as a backreference. The change directly addresses the bug described in the commit message. The answer is **Buggy**.
"
ant,3632.json,94182a1dfa6d104e3b48d12a651dffd53f089e85,"@@ -1,21 +1,24 @@
     protected void scanDir(File srcDir, File dest, JspMangler mangler, String files[]) {
 
         long now = (new Date()).getTime();
 
         for (int i = 0; i < files.length; i++) {
             String filename = files[i];
             File srcFile = new File(srcDir, filename);
             File javaFile = mapToJavaFile(mangler, srcFile, srcDir, dest);
+            if(javaFile==null) {
+                continue;
+            }
 
             if (srcFile.lastModified() > now) {
                 log(""Warning: file modified in the future: "" + filename,
                         Project.MSG_WARN);
             }
             boolean shouldCompile = false;
             shouldCompile = isCompileNeeded(srcFile, javaFile);
             if (shouldCompile) {
                compileList.addElement(srcFile.getAbsolutePath());
                javaFiles.addElement(javaFile);
             }
         }
     }",Buggy,"this patch does not fix the failing of jspc test, it fixes two other bugs in bugzilla -NPE on inclusion of non jsp file, and webapp attr broken.


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@273214 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates fixes for two bugs: a NullPointerException (NPE) when including a non-JSP file and a broken webapp attribute. The diff introduces a null check for `javaFile` after it's created by `mapToJavaFile`. If `mapToJavaFile` returns null, the loop continues to the next file. This prevents a potential NPE if the code were to proceed with a null `javaFile`. Therefore, the change addresses the mentioned NPE bug. The answer is **Buggy**."
ant,4122.json,731eadf2ee7888aa3c30d70893409be7208b6746,"@@ -1,28 +1,28 @@
     private Source getSource(InputStream is, Resource resource)
         throws ParserConfigurationException, SAXException {
         // todo: is this comment still relevant ??
         // FIXME: need to use a SAXSource as the source for the transform
         // so we can plug in our own entity resolver
         Source src = null;
         if (entityResolver != null) {
             if (getFactory().getFeature(SAXSource.FEATURE)) {
                 SAXParserFactory spFactory = SAXParserFactory.newInstance();
                 spFactory.setNamespaceAware(true);
                 XMLReader reader = spFactory.newSAXParser().getXMLReader();
                 reader.setEntityResolver(entityResolver);
                 src = new SAXSource(reader, new InputSource(is));
             } else {
                 throw new IllegalStateException(""xcatalog specified, but ""
                     + ""parser doesn't support SAX"");
             }
         } else {
             // WARN: Don't use the StreamSource(File) ctor. It won't work with
             // xalan prior to 2.2 because of systemid bugs.
             src = new StreamSource(is);
         }
         // The line below is a hack: the system id must an URI, but it is not
         // cleat to get the URI of an resource, so just set the name of the
         // resource as a system id
-        src.setSystemId(resource.getName());
+        src.setSystemId(resourceToURI(resource));
         return src;
     }",Buggy,"Fix for SystemId of stylesheet
Bugzilla 39407.


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@412369 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix related to the SystemId of a stylesheet, referencing Bugzilla 39407. The diff shows a change in how the SystemId is set for the Source object. Instead of using `resource.getName()`, it now uses `resourceToURI(resource)`. This suggests a bug where the original method of setting the SystemId was incorrect or causing issues, and the new method provides a more accurate or reliable URI. The change directly addresses the problem described in the commit message. The answer is **Buggy**."
ant,4118.json,731eadf2ee7888aa3c30d70893409be7208b6746,"@@ -1,5 +1,6 @@
     public void setStylesheet(File stylesheet) throws Exception {
         FileResource fr = new FileResource();
         fr.setProject(project);
         fr.setFile(stylesheet);
+        setStylesheet(fr);
     }",Buggy,"Fix for SystemId of stylesheet
Bugzilla 39407.


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@412369 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix related to the SystemId of a stylesheet, referencing Bugzilla 39407. The code change introduces a new method overload that accepts a FileResource object. This suggests the original method might have had issues with how it handled file paths or resources, potentially leading to incorrect SystemId resolution. The new method likely provides a more robust way to handle file resources, thus fixing the bug. The answer is **Buggy**.
"
ant,9255.json,f4f7024234069cab8a6853720a326d12065a2d8e,"@@ -1,4 +1,12 @@
     private void resetBufferInfo() {    
         Thread current = Thread.currentThread();
-        buffers.remove(current);
+        BufferInfo bufferInfo = (BufferInfo)buffers.get(current);
+        try {
+            bufferInfo.buffer.close();
+        }
+        catch (IOException e) {
+            // Shouldn't happen
+        }
+        bufferInfo.buffer = new ByteArrayOutputStream();
+        bufferInfo.skip = false;
     }",Buggy,"Fixed problem which gave (on Windows) output such as:

[java] First line of real output
[java]
[java] Second line of real output
[java]


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@271563 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for extra blank lines in the output, specifically on Windows. The diff shows a change in the `resetBufferInfo` method. Previously, it simply removed the buffer associated with the current thread. Now, it attempts to close the buffer before reassigning it, catching any potential `IOException` (though noting it ""shouldn't happen""). This suggests that the previous implementation might have left the buffer in a state where it was not properly flushed or closed, potentially leading to the extra blank lines. The closing of the buffer stream before reassigning it is likely the fix for the reported problem. The answer is **Buggy**.
"
ant,9260.json,f4f7024234069cab8a6853720a326d12065a2d8e,"@@ -1,3 +1,4 @@
     public void close() throws IOException {
         flush();
+        removeBuffer();
     }",Buggy,"Fixed problem which gave (on Windows) output such as:

[java] First line of real output
[java]
[java] Second line of real output
[java]


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@271563 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for an output problem on Windows where extra blank lines were being inserted. The diff shows the addition of `removeBuffer()` to the `close()` method. This suggests that the `removeBuffer()` method likely clears or resets some internal buffer that was causing the extra lines to be printed. Therefore, the change is related to fixing a bug. The answer is **Buggy**."
ant,7847.json,588ce1fbe3c07994b03283ddeb82662bb23a2785,"@@ -1,10 +1,10 @@
     public void setClassname(String classname) {
         if (executableType == ExecutableType.MODULE) {
             javaCommand.setExecutable(createModuleClassPair(
                     parseModuleFromModuleClassPair(javaCommand.getExecutable()),
-                    classname));
+                    classname), false);
         } else {
             javaCommand.setExecutable(classname);
             executableType = ExecutableType.CLASS;
         }
     }",Buggy,"avoid slash translations for module names

should fix Windows test errors introduced with #15
",Buggy,"The commit message indicates a fix for Windows test errors related to module names, potentially caused by incorrect slash translations. The diff modifies the `setClassname` method to include a `false` argument in the `setExecutable` call when the executable type is MODULE. This suggests that the `createModuleClassPair` method might have been incorrectly handling slashes in module names, leading to test failures on Windows. The addition of the `false` argument likely disables slash translation, resolving the issue. The answer is **Buggy**."
ant,7722.json,588ce1fbe3c07994b03283ddeb82662bb23a2785,"@@ -1,7 +1,3 @@
     public void setExecutable(String executable) {
-        if (executable == null || executable.length() == 0) {
-            return;
-        }
-        this.executable = executable.replace('/', File.separatorChar)
-            .replace('\\', File.separatorChar);
+        setExecutable(executable, true);
     }",Buggy,"avoid slash translations for module names

should fix Windows test errors introduced with #15
",Buggy,"The commit message indicates a fix for Windows test errors related to slash translations in module names, which suggests a bug fix. The code change replaces the original `setExecutable` method with a call to another `setExecutable` method (presumably overloaded) passing `true` as a second argument. The original method was replacing both forward and backward slashes with the platform's file separator character. This replacement was likely causing issues on Windows, where forward slashes are sometimes valid in paths. By avoiding this translation, the fix addresses the reported Windows test errors. The answer is **Buggy**.
"
ant,7516.json,873b850f39a26e4911d612c157622751ac9582f9,"@@ -1,3 +1,6 @@
     public void addFilelist(FileList fl) throws BuildException {
+        if (fl.getProject() == null) {
+            fl.setProject(getProject());
+        }
         add(fl);
     }",Buggy,"fix for 
<javac> fails with NPE when compiling with eclipse ecj 3.1.x
Bugzilla 40839.
root cause of the problem was in org.eclipse.jdt.core.JDTCompiler
method addExtDirs. A FileSet was created without the Project attribute set,
then added to a Path.


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@469050 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for a NullPointerException (NPE) that occurs when compiling with Eclipse's ecj compiler. The root cause is identified as a missing Project attribute in a FileSet, which is then added to a Path. The diff shows that the code now explicitly sets the Project attribute of the FileList if it's null before adding it. This prevents the NPE by ensuring that the FileSet has the necessary context. The answer is **Buggy**."
ant,7517.json,873b850f39a26e4911d612c157622751ac9582f9,"@@ -1,3 +1,6 @@
     public void addDirset(DirSet dset) throws BuildException {
+        if (dset.getProject() == null) {
+            dset.setProject(getProject());
+        }
         add(dset);
     }",Buggy,"fix for 
<javac> fails with NPE when compiling with eclipse ecj 3.1.x
Bugzilla 40839.
root cause of the problem was in org.eclipse.jdt.core.JDTCompiler
method addExtDirs. A FileSet was created without the Project attribute set,
then added to a Path.


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@469050 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for a NullPointerException (NPE) that occurs when compiling with Eclipse ECJ 3.1.x, specifically related to the `addExtDirs` method in `org.eclipse.jdt.core.JDTCompiler`. The root cause is identified as a `FileSet` being created without the `Project` attribute being set, which then leads to an issue when added to a `Path`.

The diff shows a modification to the `addDirset` method, where a check is added to ensure that the `DirSet` has a `Project` attribute set. If it's null, the project is set using `dset.setProject(getProject());`. This directly addresses the problem described in the commit message by ensuring that the `DirSet` has the necessary `Project` attribute before being used, thus preventing the NPE.

The answer is **Buggy**.
"
ant,7515.json,873b850f39a26e4911d612c157622751ac9582f9,"@@ -1,3 +1,6 @@
     public void addFileset(FileSet fs) throws BuildException {
+        if (fs.getProject() == null) {
+            fs.setProject(getProject());
+        }
         add(fs);
     }",Buggy,"fix for 
<javac> fails with NPE when compiling with eclipse ecj 3.1.x
Bugzilla 40839.
root cause of the problem was in org.eclipse.jdt.core.JDTCompiler
method addExtDirs. A FileSet was created without the Project attribute set,
then added to a Path.


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@469050 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for a NullPointerException (NPE) that occurs when compiling with Eclipse's ecj compiler. The root cause is identified in the `addExtDirs` method of `org.eclipse.jdt.core.JDTCompiler`, where a FileSet is created without setting the Project attribute before adding it to a Path.

The diff shows a modification to the `addFileset` method. It adds a check to ensure that the FileSet has a Project attribute set. If it's null, the code now sets it using `fs.setProject(getProject());`. This directly addresses the problem described in the commit message by ensuring the FileSet has the necessary Project attribute, thus preventing the NPE.

The answer is **Buggy**."
ant,787.json,bb2695af1464b52cacdd3d3672e428358c3142dd,"@@ -1,29 +1,22 @@
     public final int read() throws IOException {
         if (!getInitialized()) {
             initialize();
             setInitialized(true);
         }
 
         int ch = -1;
         if (unicodeBuf.length() == 0) {
             ch = in.read();
             if (ch != -1) {
                 char achar = (char) ch;
                 if (achar >= '\u0080') {
-                    unicodeBuf = new StringBuffer(""u0000"");
-                    String s = Integer.toHexString(ch);
-                    //replace the last 0s by the chars contained in s
-                    for (int i = 0; i < s.length(); i++) {
-                        unicodeBuf.setCharAt(unicodeBuf.length()
-                                             - s.length() + i,
-                                             s.charAt(i));
-                    }
+                    unicodeBuf = UnicodeUtil.EscapeUnicode(achar);
                     ch = '\\';
                 }
             }
         } else {
             ch = (int) unicodeBuf.charAt(0);
             unicodeBuf.deleteCharAt(0);
         }
         return ch;
     }",Buggy,"fix for bug 50515, incorrect unicode escapes in propertyfile task

git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@1054711 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for bug 50515 related to incorrect unicode escapes in the propertyfile task. The code diff shows that when a character with a value greater than or equal to '\u0080' is encountered, the original code constructs a unicode escape sequence by converting the character to its hexadecimal representation and appending it to ""u0000"". The corrected code replaces this logic with a call to `UnicodeUtil.EscapeUnicode(achar)`. This change suggests that the original implementation had a bug in handling unicode characters, which is now fixed by using `UnicodeUtil.EscapeUnicode`. The answer is **Buggy**.
"
ant,9748.json,bb2695af1464b52cacdd3d3672e428358c3142dd,"@@ -1,7 +1,3 @@
     private String escapeUnicode(char ch) {
-        StringBuffer buffy = new StringBuffer(""\\u"");
-        String hex = Integer.toHexString((int)ch);
-        buffy.append(""0000"".substring(4-hex.length()));
-        buffy.append(hex);
-        return buffy.toString();
-    }

+        return ""\\"" + UnicodeUtil.EscapeUnicode(ch);
+        }",Buggy,"fix for bug 50515, incorrect unicode escapes in propertyfile task

git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@1054711 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for bug 50515, specifically addressing incorrect Unicode escapes in the propertyfile task. The code diff replaces the original `escapeUnicode` method with a call to `UnicodeUtil.EscapeUnicode(ch)`. This change suggests that the original method had a bug related to how it was handling Unicode character escaping, and the fix involves using a utility class to correctly escape the characters. The change directly addresses the bug mentioned in the commit message. The answer is **Buggy**.
"
ant,4816.json,29fface4fb93fb33b33c86124a168c04779271c0,"@@ -1,24 +1,24 @@
     public boolean execute() throws BuildException {
         Rmic owner = getRmic();
         Commandline cmd = setupRmicCommand();
         Project project = owner.getProject();
         //rely on RMIC being on the path
-        cmd.setExecutable(JavaEnvUtils.getJdkExecutable(SunRmic.RMIC_EXECUTABLE));
+        cmd.setExecutable(JavaEnvUtils.getJdkExecutable(getExecutableName()));
 
         //set up the args
         String[] args = cmd.getCommandline();
 
         try {
             Execute exe = new Execute(new LogStreamHandler(owner,
                     Project.MSG_INFO,
                     Project.MSG_WARN));
             exe.setAntRun(project);
             exe.setWorkingDirectory(project.getBaseDir());
             exe.setCommandline(args);
             exe.execute();
             return !exe.isFailure();
         } catch (IOException exception) {
-            throw new BuildException(""Error running "" + SunRmic.RMIC_EXECUTABLE
+            throw new BuildException(""Error running "" + getExecutableName()
                     + "" -maybe it is not on the path"", exception);
         }
     }",Buggy,"bug ID#38732 , rmic task doesn't work with -Xnew and JDK 6.0

Fixed by writing a new adapter, xnew, that extends the forking adapter and sets the -Xnew argument. Tests supplied, though the old test, the one that would fail on java1.6, is still there.

Also made the name matching code of rmic locale-independent.

git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@434029 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix related to the rmic task when used with -Xnew and JDK 6.0. The diff replaces `SunRmic.RMIC_EXECUTABLE` with `getExecutableName()`, suggesting a more dynamic way to determine the executable name. The exception message also uses `getExecutableName()` instead of the hardcoded `SunRmic.RMIC_EXECUTABLE`. This change likely addresses the bug by allowing the task to correctly identify and execute the rmic executable in different environments, including JDK 6.0 with the -Xnew option. The answer is **Buggy**.
"
ant,10020.json,ececc5c3e332b97f962b94a475408606433ee0e6,"@@ -1,14 +1,3 @@
     public boolean tryHardToDelete(File f) {
-        if (!f.delete()) {
-            if (ON_WINDOWS) {
-                System.gc();
-            }
-            try {
-                Thread.sleep(DELETE_RETRY_SLEEP_MILLIS);
-            } catch (InterruptedException ex) {
-                // Ignore Exception
-            }
-            return f.delete();
-        }
-        return true;
+        return tryHardToDelete(f, ON_WINDOWS);
     }",NotBuggy,"Add an option to <delete> to run the GC before retrying a failed build on non-Windows OSes as well.  Might fix the NFS problem described in PR 45786

git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@1177305 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message suggests a potential fix for an NFS problem by adding an option to run the garbage collector (GC) before retrying a failed delete operation on non-Windows systems. The diff, however, only refactors the `tryHardToDelete` method to delegate to another method, `tryHardToDelete(f, ON_WINDOWS)`. The core logic of running `System.gc()` conditionally based on the OS is not present in the provided diff. Therefore, the diff itself doesn't reflect the bug fix mentioned in the commit message. The answer is **NotBuggy**."
ant,10021.json,ececc5c3e332b97f962b94a475408606433ee0e6,"@@ -1,14 +1,14 @@
-    public boolean tryHardToDelete(File f) {
+    public boolean tryHardToDelete(File f, boolean runGC) {
         if (!f.delete()) {
-            if (ON_WINDOWS) {
+            if (runGC) {
                 System.gc();
             }
             try {
                 Thread.sleep(DELETE_RETRY_SLEEP_MILLIS);
             } catch (InterruptedException ex) {
                 // Ignore Exception
             }
             return f.delete();
         }
         return true;
     }",NotBuggy,"Add an option to <delete> to run the GC before retrying a failed build on non-Windows OSes as well.  Might fix the NFS problem described in PR 45786

git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@1177305 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message suggests a potential fix for an NFS problem by adding an option to run the garbage collector (GC) before retrying a failed delete operation. The diff modifies the `tryHardToDelete` method to accept a boolean `runGC` parameter. The original code only ran the GC on Windows, but the modified code runs it based on the new `runGC` parameter. This change introduces a new parameter to control GC execution, potentially resolving the NFS issue by freeing up resources before retrying the delete operation. The answer is **Buggy**."
ant,5445.json,ececc5c3e332b97f962b94a475408606433ee0e6,"@@ -1,14 +1,14 @@
     private boolean delete(File f) {
-        if (!FILE_UTILS.tryHardToDelete(f)) {
+        if (!FILE_UTILS.tryHardToDelete(f, performGc)) {
             if (deleteOnExit) {
                 int level = quiet ? Project.MSG_VERBOSE : Project.MSG_INFO;
                 log(""Failed to delete "" + f + "", calling deleteOnExit.""
                     + "" This attempts to delete the file when the Ant jvm""
                     + "" has exited and might not succeed."", level);
                 f.deleteOnExit();
                 return true;
             }
             return false;
         }
         return true;
     }",NotBuggy,"Add an option to <delete> to run the GC before retrying a failed build on non-Windows OSes as well.  Might fix the NFS problem described in PR 45786

git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@1177305 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message suggests a potential fix for an NFS-related issue by adding an option to run the garbage collector (GC) before retrying a failed delete operation. The diff shows that the `FILE_UTILS.tryHardToDelete` method is now being called with the `performGc` flag. This indicates that the change is intended to address a problem where file deletion fails, and the GC is being triggered in an attempt to resolve the issue. This is a bug fix. The answer is **Buggy**."
ant,7946.json,f83f5c68c975f31a384cc24e94be4e5ebfa2da4d,"@@ -1,9 +1,11 @@
     public boolean isSelected(File basedir, String filename, File file) {
 
         // throw BuildException on error
         validate();
 
         if (file.isDirectory()) {
             return type.equals(FileType.DIR);
-        } else return type.equals(FileType.FILE);
+        } else {
+            return type.equals(FileType.FILE);
+        }
     }",NotBuggy,"First pass at fixing checkstyle issues by using IDEA to reformat code and identify problem spots


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@274773 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates a fix for checkstyle issues, primarily involving code reformatting. The diff introduces an `else` block to explicitly return a value when the file is not a directory. This change improves code readability and structure, aligning with checkstyle guidelines. However, it doesn't address any specific bug or logical error in the original code. The functionality remains the same. The answer is **NotBuggy**."
ant,8194.json,f83f5c68c975f31a384cc24e94be4e5ebfa2da4d,"@@ -1,11 +1,10 @@
     public void verifySettings() {
         if (dateTime == null && millis < 0) {
             setError(""You must provide a datetime or the number of ""
-                + ""milliseconds."");
-        }
-        else if (millis < 0) {
+                    + ""milliseconds."");
+        } else if (millis < 0) {
             setError(""Date of "" + dateTime
-                + "" results in negative milliseconds""
-                + "" value relative to epoch (January 1, 1970, 00:00:00 GMT)."");
+                    + "" results in negative milliseconds value""
+                    + "" relative to epoch (January 1, 1970, 00:00:00 GMT)."");
         }
     }",NotBuggy,"First pass at fixing checkstyle issues by using IDEA to reformat code and identify problem spots


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@274773 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates a fix for checkstyle issues, primarily involving code reformatting. The diff shows changes in spacing and line breaks within the `verifySettings` method. The core logic of the method remains the same; only the formatting has been adjusted to adhere to checkstyle guidelines. Therefore, this change does not address a bug but rather focuses on code style improvements. The answer is **NotBuggy**."
ant,4305.json,faedd2bc5b9fdcaa0650966bc7fef43c5c59cf9a,"@@ -1,87 +1,89 @@
         private URLConnection openConnection(final URL aSource) throws IOException {
 
             // set up the URL connection
             final URLConnection connection = aSource.openConnection();
             // modify the headers
             // NB: things like user authentication could go in here too.
             if (hasTimestamp) {
                 connection.setIfModifiedSince(timestamp);
             }
             // Set the user agent
             connection.addRequestProperty(""User-Agent"", this.userAgent);
 
             // prepare Java 1.1 style credentials
             if (uname != null || pword != null) {
                 final String up = uname + "":"" + pword;
                 String encoding;
                 // we do not use the sun impl for portability,
                 // and always use our own implementation for consistent
                 // testing
                 final Base64Converter encoder = new Base64Converter();
                 encoding = encoder.encode(up.getBytes());
                 connection.setRequestProperty(""Authorization"", ""Basic ""
                         + encoding);
             }
 
-            connection.setRequestProperty(""Accept-Encoding"", GZIP_CONTENT_ENCODING);
+            if (tryGzipEncoding) {
+                connection.setRequestProperty(""Accept-Encoding"", GZIP_CONTENT_ENCODING);
+            }
 
             if (connection instanceof HttpURLConnection) {
                 ((HttpURLConnection) connection)
                         .setInstanceFollowRedirects(false);
                 ((HttpURLConnection) connection)
                         .setUseCaches(httpUseCaches);
             }
             // connect to the remote site (may take some time)
             try {
                 connection.connect();
             } catch (final NullPointerException e) {
                 //bad URLs can trigger NPEs in some JVMs
                 throw new BuildException(""Failed to parse "" + source.toString(), e);
             }
 
             // First check on a 301 / 302 (moved) response (HTTP only)
             if (connection instanceof HttpURLConnection) {
                 final HttpURLConnection httpConnection = (HttpURLConnection) connection;
                 final int responseCode = httpConnection.getResponseCode();
                 if (isMoved(responseCode)) {
                     final String newLocation = httpConnection.getHeaderField(""Location"");
                     final String message = aSource
                             + (responseCode == HttpURLConnection.HTTP_MOVED_PERM ? "" permanently""
                                     : """") + "" moved to "" + newLocation;
                     log(message, logLevel);
                     final URL newURL = new URL(aSource, newLocation);
                     if (!redirectionAllowed(aSource, newURL)) {
                         return null;
                     }
                     return openConnection(newURL);
                 }
                 // next test for a 304 result (HTTP only)
                 final long lastModified = httpConnection.getLastModified();
                 if (responseCode == HttpURLConnection.HTTP_NOT_MODIFIED
                         || (lastModified != 0 && hasTimestamp && timestamp >= lastModified)) {
                     // not modified so no file download. just return
                     // instead and trace out something so the user
                     // doesn't think that the download happened when it
                     // didn't
                     log(""Not modified - so not downloaded"", logLevel);
                     return null;
                 }
                 // test for 401 result (HTTP only)
                 if (responseCode == HttpURLConnection.HTTP_UNAUTHORIZED) {
                     final String message = ""HTTP Authorization failure"";
                     if (ignoreErrors) {
                         log(message, logLevel);
                         return null;
                     } else {
                         throw new BuildException(message);
                     }
                 }
             }
 
             //REVISIT: at this point even non HTTP connections may
             //support the if-modified-since behaviour -we just check
             //the date of the content and skip the write if it is not
             //newer. Some protocols (FTP) don't include dates, of
             //course.
             return connection;
         }",Buggy,"only enable transparent gzip encoding when explicitly specified
fixes bugzilla issue 57048
",Buggy,"The commit message indicates a fix related to gzip encoding being enabled only when explicitly specified. The diff shows that the line `connection.setRequestProperty(""Accept-Encoding"", GZIP_CONTENT_ENCODING);` is now conditionally executed based on the `tryGzipEncoding` flag. This suggests that the previous code was unconditionally setting the ""Accept-Encoding"" header to ""gzip"", which was not the intended behavior. The change ensures that gzip encoding is only requested when `tryGzipEncoding` is true, fixing the bug. The answer is **Buggy**."
ant,4307.json,faedd2bc5b9fdcaa0650966bc7fef43c5c59cf9a,"@@ -1,51 +1,52 @@
         private boolean downloadFile()
                 throws FileNotFoundException, IOException {
             for (int i = 0; i < numberRetries; i++) {
                 // this three attempt trick is to get round quirks in different
                 // Java implementations. Some of them take a few goes to bind
                 // properly; we ignore the first couple of such failures.
                 try {
                     is = connection.getInputStream();
                     break;
                 } catch (final IOException ex) {
                     log(""Error opening connection "" + ex, logLevel);
                 }
             }
             if (is == null) {
                 log(""Can't get "" + source + "" to "" + dest, logLevel);
                 if (ignoreErrors) {
                     return false;
                 }
                 throw new BuildException(""Can't get "" + source + "" to "" + dest,
                         getLocation());
             }
 
-            if (GZIP_CONTENT_ENCODING.equals(connection.getContentEncoding())) {
+            if (tryGzipEncoding
+                && GZIP_CONTENT_ENCODING.equals(connection.getContentEncoding())) {
                 is = new GZIPInputStream(is);
             }
 
             os = new FileOutputStream(dest);
             progress.beginDownload();
             boolean finished = false;
             try {
                 final byte[] buffer = new byte[BIG_BUFFER_SIZE];
                 int length;
                 while (!isInterrupted() && (length = is.read(buffer)) >= 0) {
                     os.write(buffer, 0, length);
                     progress.onTick();
                 }
                 finished = !isInterrupted();
             } finally {
                 FileUtils.close(os);
                 FileUtils.close(is);
 
                 // we have started to (over)write dest, but failed.
                 // Try to delete the garbage we'd otherwise leave
                 // behind.
                 if (!finished) {
                     dest.delete();
                 }
             }
             progress.endDownload();
             return true;
         }",Buggy,"only enable transparent gzip encoding when explicitly specified
fixes bugzilla issue 57048
",Buggy,"The commit message indicates a fix for bugzilla issue 57048, specifically related to transparent gzip encoding. The diff modifies the `downloadFile` method to enable gzip encoding only when `tryGzipEncoding` is explicitly specified. Previously, gzip encoding was enabled whenever the `Content-Encoding` was `GZIP_CONTENT_ENCODING`. This change suggests that the previous behavior was incorrect, leading to unintended gzip decoding. The answer is **Buggy**."
ant,658.json,d0b4fb3912af11d2c3075c51c0715d581ea085f1,"@@ -1,45 +1,46 @@
     private final void tsort(String root, Hashtable targets,
                              Hashtable state, Stack visiting,
                              Vector ret)
         throws BuildException {
         state.put(root, VISITING);
         visiting.push(root);
 
         Target target = (Target)(targets.get(root));
 
         // Make sure we exist
         if (target == null) {
             StringBuffer sb = new StringBuffer(""Target `"");
             sb.append(root);
             sb.append(""' does not exist in this project. "");
             visiting.pop();
             if (!visiting.empty()) {
                 String parent = (String)visiting.peek();
                 sb.append(""It is used from target `"");
                 sb.append(parent);
                 sb.append(""'."");
             }
 
             throw new BuildException(new String(sb));
         }
 
-        for (Enumeration en=target.getDependencies(); en.hasMoreElements();) {
+        for (Enumeration en = target.getDependencies(); en.hasMoreElements();) {
             String cur = (String) en.nextElement();
-            String m=(String)state.get(cur);
+            String m = (String)state.get(cur);
             if (m == null) {
                 // Not been visited
                 tsort(cur, targets, state, visiting, ret);
             }
             else if (m == VISITING) {
                 // Currently visiting this node, so have a cycle
                 throw makeCircularException(cur, visiting);
             }
         }
 
         String p = (String) visiting.pop();
         if (root != p) {
-            throw new RuntimeException(""Unexpected internal error: expected to pop ""+root+"" but got ""+p);
+            throw new RuntimeException(""Unexpected internal error: expected to ""
+                + ""pop "" + root + "" but got "" + p);
         }
         state.put(root, VISITED);
         ret.addElement(target);
     }",NotBuggy,"Fix up errors in Project format and javadoc picked up with checkstyle


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@271747 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates fixes related to project format and Javadoc, potentially addressing errors identified by Checkstyle. The code changes involve modifications to the `tsort` method, specifically in error message construction and exception handling. The changes include:

1.  Improved error message when a target does not exist, providing more context about which target depends on the missing target.
2.  Minor formatting changes in the `RuntimeException` message.

These changes suggest that the original code had issues with error reporting and potentially with code style, which are indicative of bugs. The improved error messages and exception handling directly address these issues. The answer is **Buggy**."
ant,624.json,d0b4fb3912af11d2c3075c51c0715d581ea085f1,"@@ -1,7 +1,7 @@
-    public void addTarget(Target target) {
+    public void addTarget(Target target) throws BuildException {
         String name = target.getName();
         if (targets.get(name) != null) {
-            throw new BuildException(""Duplicate target: `""+name+""'"");
+            throw new BuildException(""Duplicate target: `"" + name + ""'"");
         }
         addOrReplaceTarget(name, target);
     }",NotBuggy,"Fix up errors in Project format and javadoc picked up with checkstyle


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@271747 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates fixes to errors in Project format and javadoc, which suggests bug fixes. The diff adds a `throws BuildException` clause to the `addTarget` method signature. This change indicates that the method now explicitly declares that it can throw a `BuildException`, which is thrown when a duplicate target is added. This is likely a fix to ensure that the exception is properly handled or documented. The answer is **Buggy**."
ant,1464.json,3d3b941aa7dd1e582d95c63df93ecd61423fa41d,"@@ -1,15 +1,25 @@
     private void processComment( final String line )
     {
         final String lineSeparator = System.getProperty( ""line.separator"" );
-        if( line.startsWith( ""======"" ) || line.startsWith( ""------"" ) )
+        if( line.startsWith( ""======"" ) )
+        {
+            //We have ended changelog for that particular file
+            //so we can save it
+            final int end = m_comment.length() - lineSeparator.length(); //was -1
+            m_comment = m_comment.substring( 0, end );
+            m_comment = ""<![CDATA["" + m_comment + ""]]>"";
+            saveEntry();
+            m_status = GET_FILE;
+        }
+        else if( line.startsWith( ""------"" ) )
         {
             final int end = m_comment.length() - lineSeparator.length(); //was -1
             m_comment = m_comment.substring( 0, end );
             m_comment = ""<![CDATA["" + m_comment + ""]]>"";
             m_status = GET_PREVIOUS_REV;
         }
         else
         {
             m_comment += line + lineSeparator;
         }
     }",Buggy,"Fix bug where a log of a single file without other logs in between would only retrieve first change


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@272104 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix related to processing logs of single files. The diff modifies the `processComment` method, specifically how it handles lines starting with ""======"" and ""------"". The original code treated both prefixes the same, potentially causing issues when a single file's log was processed without other logs in between. The modified code now distinguishes between ""======"" and ""------"", treating ""======"" as the end of the changelog for a particular file and saving the entry. This change addresses the bug described in the commit message. The answer is **Buggy**."
ant,1463.json,3d3b941aa7dd1e582d95c63df93ecd61423fa41d,"@@ -1,24 +1,25 @@
     public void stdout( final String line )
     {
         switch( m_status )
         {
             case GET_FILE:
                 processFile( line );
                 break;
             case GET_REVISION:
                 processRevision( line );
                 //Was a fall through ....
-                //break;
+                break;
+
             case GET_DATE:
                 processDate( line );
                 break;
 
             case GET_COMMENT:
                 processComment( line );
                 break;
 
             case GET_PREVIOUS_REV:
                 processGetPreviousRevision( line );
                 break;
         }
     }",Buggy,"Fix bug where a log of a single file without other logs in between would only retrieve first change


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@272104 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix related to retrieving the first change of a single file log. The code diff shows that a `break;` statement was missing in the `GET_REVISION` case within the `stdout` method. The missing `break;` caused a fall-through to the next case, which is likely the cause of the bug where only the first change was retrieved. Adding the `break;` statement fixes this fall-through, ensuring that the correct processing occurs for each log entry. The answer is **Buggy**."
ant,1468.json,3d3b941aa7dd1e582d95c63df93ecd61423fa41d,"@@ -1,36 +1,13 @@
     private void processGetPreviousRevision( final String line )
     {
-        final String entryKey = m_date + m_author + m_comment;
-        if( line.startsWith( ""revision"" ) )
+        if( !line.startsWith( ""revision"" ) )
         {
-            m_previousRevision = line.substring( 9 );
-            m_status = GET_FILE;
+            throw new IllegalStateException( ""Unexpected line from CVS: "" + line );
+        }
+        m_previousRevision = line.substring( 9 );
 
-            CVSEntry entry;
-            if( !m_entries.containsKey( entryKey ) )
-            {
-                entry = new CVSEntry( parseDate( m_date ), m_author, m_comment );
-                m_entries.put( entryKey, entry );
-            }
-            else
-            {
-                entry = (CVSEntry)m_entries.get( entryKey );
-            }
-            entry.addFile( m_file, m_revision, m_previousRevision );
-        }
-        else if( line.startsWith( ""======"" ) )
-        {
-            m_status = GET_FILE;
-            CVSEntry entry;
-            if( !m_entries.containsKey( entryKey ) )
-            {
-                entry = new CVSEntry( parseDate( m_date ), m_author, m_comment );
-                m_entries.put( entryKey, entry );
-            }
-            else
-            {
-                entry = (CVSEntry)m_entries.get( entryKey );
-            }
-            entry.addFile( m_file, m_revision );
-        }
+        saveEntry();
+
+        m_revision = m_previousRevision;
+        m_status = GET_COMMENT;
     }",Buggy,"Fix bug where a log of a single file without other logs in between would only retrieve first change


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@272104 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix related to retrieving the first change of a single file log. The diff removes a lot of code related to handling `entryKey`, `m_entries`, and conditional logic for adding files to entries. It simplifies the process by directly assigning `m_previousRevision` and calling `saveEntry()`. The removal of the `else if( line.startsWith( ""======"" ) )` block and the associated logic suggests that the original code had a flaw in handling cases where the log contained only one file or had specific formatting. The simplification and direct assignment of `m_previousRevision` and `m_revision` suggest a correction to the logic that was causing the bug. The answer is **Buggy**."
ant,5442.json,7ee236d89f0e870cebb3ad9128de6b27689bea28,"@@ -1,116 +1,115 @@
     public void execute() throws BuildException {
         if (usedMatchingTask) {
             log(""DEPRECATED - Use of the implicit FileSet is deprecated.  ""
-                + ""Use a nested fileset element instead."");
+                + ""Use a nested fileset element instead."", quiet ? Project.MSG_VERBOSE : verbosity);
         }
 
         if (file == null && dir == null && filesets.size() == 0 && rcs == null) {
             throw new BuildException(""At least one of the file or dir ""
                                      + ""attributes, or a nested resource collection, ""
                                      + ""must be set."");
         }
 
         if (quiet && failonerror) {
             throw new BuildException(""quiet and failonerror cannot both be ""
                                      + ""set to true"", getLocation());
         }
 
         // delete the single file
         if (file != null) {
             if (file.exists()) {
                 if (file.isDirectory()) {
                     log(""Directory "" + file.getAbsolutePath()
                         + "" cannot be removed using the file attribute.  ""
-                        + ""Use dir instead."");
+                        + ""Use dir instead."", quiet ? Project.MSG_VERBOSE : verbosity);
                 } else {
                     log(""Deleting: "" + file.getAbsolutePath());
 
                     if (!delete(file)) {
                         handle(""Unable to delete file "" + file.getAbsolutePath());
                     }
                 }
             } else {
                 log(""Could not find file "" + file.getAbsolutePath()
-                    + "" to delete."",
-                    Project.MSG_VERBOSE);
+                    + "" to delete."", quiet ? Project.MSG_VERBOSE : verbosity);
             }
         }
 
         // delete the directory
         if (dir != null && dir.exists() && dir.isDirectory()
             && !usedMatchingTask) {
             /*
                If verbosity is MSG_VERBOSE, that mean we are doing
                regular logging (backwards as that sounds).  In that
                case, we want to print one message about deleting the
                top of the directory tree.  Otherwise, the removeDir
                method will handle messages for _all_ directories.
              */
             if (verbosity == Project.MSG_VERBOSE) {
                 log(""Deleting directory "" + dir.getAbsolutePath());
             }
             removeDir(dir);
         }
         Resources resourcesToDelete = new Resources();
         resourcesToDelete.setProject(getProject());
         Resources filesetDirs = new Resources();
         filesetDirs.setProject(getProject());
 
-        for (int i = 0; i < filesets.size(); i++) {
+        for (int i = 0, size = filesets.size(); i < size; i++) {
             FileSet fs = (FileSet) filesets.get(i);
             if (fs.getProject() == null) {
                 log(""Deleting fileset with no project specified;""
                     + "" assuming executing project"", Project.MSG_VERBOSE);
                 fs = (FileSet) fs.clone();
                 fs.setProject(getProject());
             }
             resourcesToDelete.add(fs);
             if (includeEmpty && fs.getDir().isDirectory()) {
               filesetDirs.add(new ReverseDirs(fs.getDir(),
                   fs.getDirectoryScanner().getIncludedDirectories()));
             }
         }
         if (usedMatchingTask && dir != null && dir.isDirectory()) {
             //add the files from the default fileset:
             FileSet implicit = getImplicitFileSet();
             resourcesToDelete.add(implicit);
             if (includeEmpty) {
               filesetDirs.add(new ReverseDirs(dir,
                   implicit.getDirectoryScanner().getIncludedDirectories()));
             }
         }
         resourcesToDelete.add(filesetDirs);
         if (rcs != null) {
             // sort first to files, then dirs
             Restrict exists = new Restrict();
             exists.add(EXISTS);
             exists.add(rcs);
             Sort s = new Sort();
             s.add(REVERSE_FILESYSTEM);
             s.add(exists);
             resourcesToDelete.add(s);
         }
         try {
             if (resourcesToDelete.isFilesystemOnly()) {
                 for (Iterator iter = resourcesToDelete.iterator(); iter.hasNext();) {
                     FileResource r = (FileResource) iter.next();
                     // nonexistent resources could only occur if we already
                     // deleted something from a fileset:
                     if (!r.isExists()) {
                         continue;
                     }
                     if (!(r.isDirectory()) || r.getFile().list().length == 0) {
                         log(""Deleting "" + r, verbosity);
                         if (!delete(r.getFile()) && failonerror) {
                             handle(""Unable to delete ""
                                 + (r.isDirectory() ? ""directory "" : ""file "") + r);
                         }
                     }
                 }
             } else {
                  handle(getTaskName() + "" handles only filesystem resources"");
             }
         } catch (Exception e) {
             handle(e);
         }
     }",Buggy,"fix bug#32738, implement quiet check for every message logged

git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@395510 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for bug #32738 and the implementation of a ""quiet check"" for every message logged. The diff shows modifications to the `log` method calls within the `execute` method. Specifically, the verbosity level is now conditionally set based on the `quiet` flag: `quiet ? Project.MSG_VERBOSE : verbosity`. This means that if the `quiet` flag is true, the log message will be logged at the `Project.MSG_VERBOSE` level, effectively suppressing it unless the verbosity level is set to verbose. This change directly implements the ""quiet check"" functionality described in the commit message, indicating a bug fix or enhancement related to controlling the verbosity of log messages. The answer is **Buggy**."
ant,5446.json,7ee236d89f0e870cebb3ad9128de6b27689bea28,"@@ -1,22 +1,22 @@
     protected void removeDir(File d) {
         String[] list = d.list();
         if (list == null) {
             list = new String[0];
         }
         for (int i = 0; i < list.length; i++) {
             String s = list[i];
             File f = new File(d, s);
             if (f.isDirectory()) {
                 removeDir(f);
             } else {
-                log(""Deleting "" + f.getAbsolutePath(), verbosity);
+                log(""Deleting "" + f.getAbsolutePath(), quiet ? Project.MSG_VERBOSE : verbosity);
                 if (!delete(f)) {
                     handle(""Unable to delete file "" + f.getAbsolutePath());
                 }
             }
         }
         log(""Deleting directory "" + d.getAbsolutePath(), verbosity);
         if (!delete(d)) {
             handle(""Unable to delete directory "" + dir.getAbsolutePath());
         }
     }",Buggy,"fix bug#32738, implement quiet check for every message logged

git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@395510 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix related to message logging. The diff modifies the `removeDir` method to conditionally adjust the verbosity level of log messages based on a `quiet` flag. Specifically, when `quiet` is true, the verbosity is set to `Project.MSG_VERBOSE`; otherwise, it uses the existing `verbosity` level. This change suggests that the original logging behavior was not appropriate in certain ""quiet"" scenarios, indicating a bug. The answer is **Buggy**."
ant,5444.json,7ee236d89f0e870cebb3ad9128de6b27689bea28,"@@ -1,7 +1,7 @@
     private void handle(Exception e) {
         if (failonerror) {
             throw (e instanceof BuildException)
                 ? (BuildException) e : new BuildException(e);
         }
-        log(e.getMessage(), quiet ? Project.MSG_VERBOSE : Project.MSG_WARN);
+        log(e.getMessage(), quiet ? Project.MSG_VERBOSE : verbosity);
     }",Buggy,"fix bug#32738, implement quiet check for every message logged

git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@395510 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix (bug#32738) and mentions implementing a ""quiet check"" for every message logged. The diff modifies the `handle` method to use the `verbosity` field instead of `Project.MSG_WARN` when the `quiet` flag is not set. This change aligns with the commit message's intention to control message verbosity based on the ""quiet check,"" suggesting a fix related to logging behavior. The answer is **Buggy**."
ant,1828.json,85f6ea3795fa8d4393723ec229e9168f8b824f28,"@@ -1,11 +1,22 @@
     public void execute() throws BuildException {
+        ScriptRunner runner = new ScriptRunner();
+        if (language != null) {
+            runner.setLanguage(language);
+        }
+        if (src != null) {
+            runner.setSrc(src);
+        }
+        if (text != null) {
+            runner.addText(text);
+        }
+        
         runner.addBeans(getProject().getProperties());
         runner.addBeans(getProject().getUserProperties());
         runner.addBeans(getProject().getTargets());
         runner.addBeans(getProject().getReferences());
 
         runner.addBean(""project"", getProject());
         runner.addBean(""self"", this);
 
         runner.executeScript(""<ANT>"");
     }",Buggy,"Fix for script memory retaining problem.
PR: 25394
Obtained from: Jose Alberto Fernandez /  Antoine Levy-Lambert


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@275836 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for a script memory retention problem. The code changes introduce a `ScriptRunner` object within the `execute` method. The `ScriptRunner` is configured with the script's language, source, and text, as well as various project properties, user properties, targets, and references. The script is then executed using `runner.executeScript(""<ANT>"")`. This suggests that the previous implementation might have been retaining references to these objects, leading to memory leaks. By creating a new `ScriptRunner` instance for each execution, the references are contained within the scope of the `execute` method, and the memory can be released after the script finishes. The answer is **Buggy**."
lucene-solr,14918.json,8bd7e5c9d254c1d629a784e0b601885adea2f57b,"@@ -1,20 +1,3 @@
   public static SolrDocument convertLuceneDocToSolrDoc(Document doc, final IndexSchema schema) {
-    SolrDocument out = new SolrDocument();
-    for (IndexableField f : doc.getFields()) {
-      // Make sure multivalued fields are represented as lists
-      Object existing = out.get(f.name());
-      if (existing == null) {
-        SchemaField sf = schema.getFieldOrNull(f.name());
-        if (sf != null && sf.multiValued()) {
-          List<Object> vals = new ArrayList<>();
-          vals.add(f);
-          out.setField(f.name(), vals);
-        } else {
-          out.setField(f.name(), f);
-        }
-      } else {
-        out.addField(f.name(), f);
-      }
-    }
-    return out;
+    return convertLuceneDocToSolrDoc(doc,schema, new SolrReturnFields());
   }",Buggy,"SOLR-11891: DocStreamer now respects the ReturnFields when populating a SolrDocument
This is an optimization that reduces the number of unneccessary fields a ResponseWriter will see if documentCache is used

This commit also includes fixes for SOLR-12107 & SOLR-12108 -- two bugs that were previously dependent on the
un-optimized behavior of DocStreamer in order to function properly.

- SOLR-12107: Fixed a error in [child] transformer that could ocur if documentCache was not used
- SOLR-12108: Fixed the fallback behavior of [raw] and [xml] transformers when an incompatble 'wt' was specified,
  the field value was lost if documentCache was not used.
",Buggy,"The commit message indicates that the changes include bug fixes for SOLR-12107 and SOLR-12108. The diff shows that the method `convertLuceneDocToSolrDoc` has been modified to call another overloaded method `convertLuceneDocToSolrDoc` with an additional argument `new SolrReturnFields()`. This change suggests that the method now uses `SolrReturnFields` to populate the `SolrDocument`, which aligns with the commit message that states ""DocStreamer now respects the ReturnFields when populating a SolrDocument"". Since the commit message explicitly mentions bug fixes, the answer is **Buggy**.
"
lucene-solr,14569.json,8bd7e5c9d254c1d629a784e0b601885adea2f57b,"@@ -1,30 +1,31 @@
   public DocTransformer create(String display, SolrParams params, SolrQueryRequest req) {
     String field = params.get(""f"");
     if(Strings.isNullOrEmpty(field)) {
       field = display;
     }
     // When a 'wt' is specified in the transformer, only apply it to the same wt
     boolean apply = true;
     if(applyToWT!=null) {
       String qwt = req.getParams().get(CommonParams.WT);
       if(qwt==null) {
         QueryResponseWriter qw = req.getCore().getQueryResponseWriter(req);
         QueryResponseWriter dw = req.getCore().getQueryResponseWriter(applyToWT);
         if(qw!=dw) {
           apply = false;
         }
       }
       else {
         apply = applyToWT.equals(qwt);
       }
     }
 
     if(apply) {
       return new RawTransformer( field, display );
     }
     
-    if(field.equals(display)) {
-      return null; // nothing
+    if (field.equals(display)) {
+      // we have to ensure the field is returned
+      return new NoopFieldTransformer(field);
     }
     return new RenameFieldTransformer( field, display, false );
   }",Buggy,"SOLR-11891: DocStreamer now respects the ReturnFields when populating a SolrDocument
This is an optimization that reduces the number of unneccessary fields a ResponseWriter will see if documentCache is used

This commit also includes fixes for SOLR-12107 & SOLR-12108 -- two bugs that were previously dependent on the
un-optimized behavior of DocStreamer in order to function properly.

- SOLR-12107: Fixed a error in [child] transformer that could ocur if documentCache was not used
- SOLR-12108: Fixed the fallback behavior of [raw] and [xml] transformers when an incompatble 'wt' was specified,
  the field value was lost if documentCache was not used.
",Buggy,"The commit message indicates fixes for SOLR-12107 and SOLR-12108, which are described as bugs related to the `[child]`, `[raw]`, and `[xml]` transformers. The diff shows a change in the `create` method of a `DocTransformer`. Specifically, when `field.equals(display)` and `apply` is false, instead of returning `null`, a `NoopFieldTransformer` is returned. This change addresses the fallback behavior of `[raw]` and `[xml]` transformers when an incompatible `wt` is specified, as mentioned in the commit message for SOLR-12108. The original behavior caused the field value to be lost if `documentCache` was not used. By returning a `NoopFieldTransformer`, the field is now ensured to be returned, fixing the bug. The answer is **Buggy**.
"
lucene-solr,17639.json,9548481c8c301740067229d09af5db0f06dccb94,"@@ -1,32 +1,35 @@
-  private Map<String, Object> monitorZookeeper(String zkHostPort) throws SolrException {
+  protected Map<String, Object> monitorZookeeper(String zkHostPort) throws SolrException {
     Map<String, Object> obj = new HashMap<>();
     List<String> errors = new ArrayList<>();
     obj.put(""host"", zkHostPort);
     List<String> lines = getZkRawResponse(zkHostPort, ""ruok"");
+    validateZkRawResponse(lines, zkHostPort,""ruok"");
     boolean ok = ""imok"".equals(lines.get(0));
     obj.put(""ok"", ok);
     lines = getZkRawResponse(zkHostPort, ""mntr"");
+    validateZkRawResponse(lines, zkHostPort,""mntr"");
     for (String line : lines) {
       String[] parts = line.split(""\t"");
       if (parts.length >= 2) {
         obj.put(parts[0], parts[1]);
       } else {
-        String err = String.format(""Unexpected line in 'mntr' response from Zookeeper %s: %s"", zkHostPort, line);
+        String err = String.format(Locale.ENGLISH, ""Unexpected line in 'mntr' response from Zookeeper %s: %s"", zkHostPort, line);
         log.warn(err);
         errors.add(err);
       }
     }
     lines = getZkRawResponse(zkHostPort, ""conf"");
+    validateZkRawResponse(lines, zkHostPort,""conf"");
     for (String line : lines) {
       String[] parts = line.split(""="");
       if (parts.length >= 2) {
         obj.put(parts[0], parts[1]);
       } else if (!line.startsWith(""membership:"")) {
-        String err = String.format(""Unexpected line in 'conf' response from Zookeeper %s: %s"", zkHostPort, line);
+        String err = String.format(Locale.ENGLISH, ""Unexpected line in 'conf' response from Zookeeper %s: %s"", zkHostPort, line);
         log.warn(err);
         errors.add(err);
       }
     }
     obj.put(""errors"", errors);
     return obj;
   }",NotBuggy,"Fix precommit
Remove errors from each host detail map
Display secureClientPort and server.1, server.2, server.3...
Added test for various failure responses and expected result from multiple nodes
",Buggy,"The commit message indicates a fix related to precommit checks, specifically addressing errors in host detail maps and improving the display of certain Zookeeper parameters. The code changes introduce a `validateZkRawResponse` method call after each `getZkRawResponse` call, and also adds a locale to the String.format calls. The addition of `validateZkRawResponse` suggests that the original code might have been missing validation steps, potentially leading to incorrect data being processed or displayed. The addition of the locale to the String.format calls suggests that the original code might have been locale-dependent, which could cause issues in different environments. Therefore, the changes indicate a bug fix. The answer is **Buggy**."
lucene-solr,38367.json,0a70e721ce98b3c4ae10aadf9edcb312d4f57da4,"@@ -1,92 +1,103 @@
     private void initIter(Shape filter) {
       cellNumber = -1;
-      if (filter instanceof LevelledValue && ((LevelledValue)filter).getLevel() == 0)
+      if (filter instanceof LevelledValue && ((LevelledValue) filter).getLevel() == 0)
         filter = null;//world means everything -- no filter
       iterFilter = filter;
 
-      NRCell parent = getLVAtLevel(getLevel()-1);
+      NRCell parent = getLVAtLevel(getLevel() - 1);
 
       // Initialize iter* members.
 
       //no filter means all subcells
       if (filter == null) {
         iterFirstCellNumber = 0;
         iterFirstIsIntersects = false;
         iterLastCellNumber = getNumSubCells(parent) - 1;
         iterLastIsIntersects = false;
         return;
       }
 
       final LevelledValue minLV;
       final LevelledValue maxLV;
+      final int lastLevelInCommon;//between minLV & maxLV
       if (filter instanceof NRShape) {
         NRShape nrShape = (NRShape) iterFilter;
         minLV = nrShape.getMinLV();
         maxLV = nrShape.getMaxLV();
+        lastLevelInCommon = nrShape.getLastLevelInCommon();
       } else {
-        minLV = (LevelledValue)iterFilter;
+        minLV = (LevelledValue) iterFilter;
         maxLV = minLV;
+        lastLevelInCommon = minLV.getLevel();
       }
 
-      //fast path check when using same filter
-      if (iterFilter == parent.iterFilter) {
+      //fast path optimization that is usually true, but never first level
+      if (iterFilter == parent.iterFilter &&
+          (getLevel() <= lastLevelInCommon || parent.iterFirstCellNumber != parent.iterLastCellNumber)) {
+        //TODO benchmark if this optimization pays off. We avoid two comparePrefixLV calls.
         if (parent.iterFirstIsIntersects && parent.cellNumber == parent.iterFirstCellNumber
             && minLV.getLevel() >= getLevel()) {
           iterFirstCellNumber = minLV.getValAtLevel(getLevel());
           iterFirstIsIntersects = (minLV.getLevel() > getLevel());
         } else {
           iterFirstCellNumber = 0;
           iterFirstIsIntersects = false;
         }
         if (parent.iterLastIsIntersects && parent.cellNumber == parent.iterLastCellNumber
             && maxLV.getLevel() >= getLevel()) {
           iterLastCellNumber = maxLV.getValAtLevel(getLevel());
           iterLastIsIntersects = (maxLV.getLevel() > getLevel());
         } else {
           iterLastCellNumber = getNumSubCells(parent) - 1;
           iterLastIsIntersects = false;
         }
         if (iterFirstCellNumber == iterLastCellNumber) {
           if (iterLastIsIntersects)
             iterFirstIsIntersects = true;
           else if (iterFirstIsIntersects)
             iterLastIsIntersects = true;
         }
         return;
       }
 
-      //uncommon to get here, except for level 1 which always happens
+      //not common to get here, except for level 1 which always happens
 
       int startCmp = comparePrefixLV(minLV, parent);
       if (startCmp > 0) {//start comes after this cell
         iterFirstCellNumber = 0;
         iterFirstIsIntersects = false;
         iterLastCellNumber = -1;//so ends early (no cells)
         iterLastIsIntersects = false;
         return;
       }
       int endCmp = comparePrefixLV(maxLV, parent);//compare to end cell
       if (endCmp < 0) {//end comes before this cell
         iterFirstCellNumber = 0;
         iterFirstIsIntersects = false;
         iterLastCellNumber = -1;//so ends early (no cells)
         iterLastIsIntersects = false;
         return;
       }
       if (startCmp < 0 || minLV.getLevel() < getLevel()) {
         //start comes before...
         iterFirstCellNumber = 0;
         iterFirstIsIntersects = false;
       } else {
         iterFirstCellNumber = minLV.getValAtLevel(getLevel());
         iterFirstIsIntersects = (minLV.getLevel() > getLevel());
       }
       if (endCmp > 0 || maxLV.getLevel() < getLevel()) {
         //end comes after...
         iterLastCellNumber = getNumSubCells(parent) - 1;
         iterLastIsIntersects = false;
       } else {
         iterLastCellNumber = maxLV.getValAtLevel(getLevel());
         iterLastIsIntersects = (maxLV.getLevel() > getLevel());
       }
+      if (iterFirstCellNumber == iterLastCellNumber) {
+        if (iterLastIsIntersects)
+          iterFirstIsIntersects = true;
+        else if (iterFirstIsIntersects)
+          iterLastIsIntersects = true;
+      }
     }",Buggy,"LUCENE-5648: (NumberRangePrefixTree) Bug-fix in initIter optimization. Re-index required.

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1602857 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message ""LUCENE-5648: (NumberRangePrefixTree) Bug-fix in initIter optimization. Re-index required."" clearly indicates a bug fix in the `initIter` method of the `NumberRangePrefixTree` component. The diff modifies the `initIter` method, specifically focusing on an optimization related to filtering and cell iteration. The changes include adding a `lastLevelInCommon` variable, adjusting the fast path check condition, and modifying the logic for determining `iterFirstCellNumber`, `iterFirstIsIntersects`, `iterLastCellNumber`, and `iterLastIsIntersects`. The re-index requirement further suggests that the bug fix involves changes to the data structure or indexing process, which could affect existing data. The answer is **Buggy**."
lucene-solr,11525.json,1b3b9294cc99985db88c2ef9074f05e802c6b55b,"@@ -1,8 +1,8 @@
   public void collect(int doc) throws IOException {
     delegate.collect(doc);
-    lastDocId = doc;    
     numCollected++;  
-    if(numCollected==maxDocsToCollect) {
-      throw new EarlyTerminatingCollectorException(numCollected, lastDocId);
+    if(maxDocsToCollect <= numCollected) {
+      throw new EarlyTerminatingCollectorException
+        (numCollected, prevReaderCumulativeSize + (doc + 1));
     }
   }",Buggy,"SOLR-5122: Fixed bug in spellcheck.collateMaxCollectDocs.  Eliminates risk of divide by zero, and makes estimated hit counts meaningful in non-optimized indexes.

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1514402 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix related to `spellcheck.collateMaxCollectDocs` in Solr, specifically addressing a potential divide-by-zero error and improving hit count estimations. The diff modifies the `collect` method. The original code increments `numCollected` and then checks if `numCollected` equals `maxDocsToCollect`. If they are equal, it throws an `EarlyTerminatingCollectorException`. The modified code changes the condition to `maxDocsToCollect <= numCollected`. This prevents the exception from being thrown prematurely when `maxDocsToCollect` is 0, which could lead to a divide-by-zero error elsewhere. Additionally, the `EarlyTerminatingCollectorException` constructor now receives `prevReaderCumulativeSize + (doc + 1)` instead of `lastDocId`. This likely provides a more accurate estimate of the hit count in non-optimized indexes, as mentioned in the commit message. The changes directly address the issues described in the commit message, indicating a bug fix. The answer is **Buggy**."
lucene-solr,14422.json,4070bdd8d8b2095b406c404720e5f2c347596350,"@@ -1,4 +1,4 @@
       public void store(AtomicReference ctx) {
-        SolrRequestInfo me = threadLocal.get();
+        SolrRequestInfo me = SolrRequestInfo.getRequestInfo();
         if (me != null) ctx.set(me);
       }",Buggy,"SOLR-8657: Fix SolrRequestInfo error logs if QuerySenderListener is being used
",Buggy,"The commit message indicates a fix for error logs related to `SolrRequestInfo` when `QuerySenderListener` is used. The code change replaces `threadLocal.get()` with `SolrRequestInfo.getRequestInfo()`. This suggests that the original code was not correctly retrieving the `SolrRequestInfo` in some cases, leading to errors. The change likely ensures that the correct request information is accessed, resolving the error logging issue. The answer is **Buggy**."
lucene-solr,14423.json,4070bdd8d8b2095b406c404720e5f2c347596350,"@@ -1,7 +1,7 @@
       public void set(AtomicReference ctx) {
         SolrRequestInfo me = (SolrRequestInfo) ctx.get();
         if (me != null) {
           ctx.set(null);
-          threadLocal.set(me);
+          SolrRequestInfo.setRequestInfo(me);
         }
       }",Buggy,"SOLR-8657: Fix SolrRequestInfo error logs if QuerySenderListener is being used
",Buggy,"The commit message indicates a fix for error logs related to `SolrRequestInfo` when `QuerySenderListener` is used. The diff shows a change from `threadLocal.set(me)` to `SolrRequestInfo.setRequestInfo(me)`. This suggests that the original code was not correctly managing or clearing the `SolrRequestInfo` in a thread-local context, potentially leading to incorrect or missing information in error logs. The change likely ensures that the `SolrRequestInfo` is properly set and accessible, resolving the error logging issue. The answer is **Buggy**."
lucene-solr,14425.json,4070bdd8d8b2095b406c404720e5f2c347596350,"@@ -1,23 +1,23 @@
   public static ExecutorUtil.InheritableThreadLocalProvider getInheritableThreadLocalProvider() {
     return new ExecutorUtil.InheritableThreadLocalProvider() {
       @Override
       public void store(AtomicReference ctx) {
-        SolrRequestInfo me = threadLocal.get();
+        SolrRequestInfo me = SolrRequestInfo.getRequestInfo();
         if (me != null) ctx.set(me);
       }
 
       @Override
       public void set(AtomicReference ctx) {
         SolrRequestInfo me = (SolrRequestInfo) ctx.get();
         if (me != null) {
           ctx.set(null);
-          threadLocal.set(me);
+          SolrRequestInfo.setRequestInfo(me);
         }
       }
 
       @Override
       public void clean(AtomicReference ctx) {
-        threadLocal.remove();
+        SolrRequestInfo.clearRequestInfo();
       }
     };
   }",Buggy,"SOLR-8657: Fix SolrRequestInfo error logs if QuerySenderListener is being used
",Buggy,"The commit message indicates a fix for error logs related to `SolrRequestInfo` when `QuerySenderListener` is used. The code diff modifies the `getInheritableThreadLocalProvider` method to use `SolrRequestInfo.getRequestInfo()` and `SolrRequestInfo.setRequestInfo()` instead of directly accessing and modifying the `threadLocal` variable. Additionally, it uses `SolrRequestInfo.clearRequestInfo()` instead of `threadLocal.remove()`. These changes suggest that the original implementation might have had issues with thread-local variable management, potentially leading to incorrect or missing request information in certain scenarios, thus causing error logs. The changes aim to ensure proper handling of `SolrRequestInfo` across threads, which aligns with fixing the error logs. The answer is **Buggy**."
ant,3769.json,2ca342fb2a9191f8e22abfc8fee9aaab94ea8496,"@@ -1,6 +1,6 @@
     public void setServerLanguageCodeConfig(LanguageCode serverLanguageCode) {
-        if (serverLanguageCode != null && !serverLanguageCode.equals("""")) {
+        if (serverLanguageCode != null && !"""".equals(serverLanguageCode.getValue())) {
             this.serverLanguageCodeConfig = serverLanguageCode;
             configurationHasBeenSet();
         }
     }",Buggy,"Fixed some obvious errors. (Formatter class is not thread safe, equals from string to another object).

git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@739572 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates fixes for ""obvious errors,"" including an issue with string comparison. The diff shows a change in the `setServerLanguageCodeConfig` method where the comparison `!serverLanguageCode.equals("""")` is replaced with `!"""".equals(serverLanguageCode.getValue())`. This suggests a fix related to how `LanguageCode` objects are compared to empty strings. The original code might have been incorrectly comparing a `LanguageCode` object to an empty string, leading to unexpected behavior. The updated code explicitly retrieves the value of the `LanguageCode` object and compares it to an empty string, which is a more reliable way to check if the language code is empty. This indicates a bug fix. The answer is **Buggy**."
ant,3780.json,2ca342fb2a9191f8e22abfc8fee9aaab94ea8496,"@@ -1,6 +1,6 @@
     public void setTimestampGranularity(Granularity timestampGranularity) {
-        if (null == timestampGranularity || """".equals(timestampGranularity)) {
+        if (null == timestampGranularity || """".equals(timestampGranularity.getValue())) {
             return;
         }
         this.timestampGranularity = timestampGranularity;
     }",Buggy,"Fixed some obvious errors. (Formatter class is not thread safe, equals from string to another object).

git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@739572 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates that some obvious errors have been fixed. The diff shows a change in the `setTimestampGranularity` method. Specifically, the condition `"""".equals(timestampGranularity)` has been changed to `"""".equals(timestampGranularity.getValue())`. This suggests that the original code was attempting to compare a `Granularity` object directly with an empty string, which is incorrect. The corrected code now compares the value of the `Granularity` object with an empty string. This is a logical correction, indicating a bug fix. The answer is **Buggy**."
lucene-solr,43949.json,b0db06bad568b7eedf528379a2fe5ac935992d56,"@@ -1,43 +1,45 @@
       public Scorer scorer(LeafReaderContext context) throws IOException {
         SortedNumericDocValues values = getValues(context.reader(), field);
         if (values == null) {
           return null;
         }
-        final NumericDocValues singleton = DocValues.unwrapSingleton(values);
+        final NumericDocValues singleton = null; // TODO: LUCENE-7649, re-consider optimization that broke SOLR-10013
+        // final NumericDocValues singleton = DocValues.unwrapSingleton(values);
         final TwoPhaseIterator iterator;
         if (singleton != null) {
+          assert false : ""imposible code -- or: someone re-enabled singleton optinization w/o reading the whole method"";
           iterator = new TwoPhaseIterator(singleton) {
             @Override
             public boolean matches() throws IOException {
               final long value = singleton.longValue();
               return value >= lowerValue && value <= upperValue;
             }
 
             @Override
             public float matchCost() {
               return 2; // 2 comparisons
             }
           };
         } else {
           iterator = new TwoPhaseIterator(values) {
             @Override
             public boolean matches() throws IOException {
               for (int i = 0, count = values.docValueCount(); i < count; ++i) {
                 final long value = values.nextValue();
                 if (value < lowerValue) {
                   continue;
                 }
                 // Values are sorted, so the first value that is >= lowerValue is our best candidate
                 return value <= upperValue;
               }
               return false; // all values were < lowerValue
             }
 
             @Override
             public float matchCost() {
               return 2; // 2 comparisons
             }
           };
         }
         return new ConstantScoreScorer(this, score(), iterator);
       }",Buggy,"SOLR-10013: Fix DV range query bug introduced by LUCENE-7643 by disabling and optimization (LUCENE-7649 to track re-enabling or removing completely)
",Buggy,"The commit message clearly states that it's fixing a bug (SOLR-10013) introduced by another change (LUCENE-7643). The diff shows that a potentially performance-enhancing optimization using `DocValues.unwrapSingleton` has been disabled. The code is commented out, and a comment refers to LUCENE-7649 for re-enabling or removing the optimization. The assertion `assert false` further confirms that the original code path is not expected to be executed. This indicates a bug fix by reverting to a less optimized but correct implementation. The answer is **Buggy**.
"
lucene-solr,20864.json,568f6a398a8be76ec0261125f625c5d28942ea4a,"@@ -1,3 +1,3 @@
-  public static ConfigSolr fromSolrHome(String solrHome) {
-    return fromFile(new File(solrHome, SOLR_XML_FILE));
+  public static ConfigSolr fromSolrHome(SolrResourceLoader loader, String solrHome) {
+    return fromFile(loader, new File(solrHome, SOLR_XML_FILE));
   }",Buggy,"SOLR-5009: Don't create multiple SolrResourceLoaders for same Solr home, wasting resources and slowing down startup. This fixes the problem where the loader was not correctly closed, making tests fail on Windows.

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1500156 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for resource wastage and slow startup due to multiple SolrResourceLoaders being created for the same Solr home. It also mentions a fix for tests failing on Windows due to the loader not being correctly closed. The diff shows a change in the `fromSolrHome` method signature, where it now accepts a `SolrResourceLoader` instance as an argument. This change likely ensures that the same loader instance is reused instead of creating new ones, addressing the resource wastage and slow startup issues. The commit message explicitly states that this change fixes a problem, which indicates a bug fix. The answer is **Buggy**."
lucene-solr,20441.json,568f6a398a8be76ec0261125f625c5d28942ea4a,"@@ -1,5 +1,6 @@
   public static CoreContainer createAndLoad(String solrHome, File configFile) {
-    CoreContainer cc = new CoreContainer(new SolrResourceLoader(solrHome), ConfigSolr.fromFile(configFile));
+    SolrResourceLoader loader = new SolrResourceLoader(solrHome);
+    CoreContainer cc = new CoreContainer(loader, ConfigSolr.fromFile(loader, configFile));
     cc.load();
     return cc;
   }",Buggy,"SOLR-5009: Don't create multiple SolrResourceLoaders for same Solr home, wasting resources and slowing down startup. This fixes the problem where the loader was not correctly closed, making tests fail on Windows.

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1500156 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for resource wastage and slow startup due to multiple SolrResourceLoaders being created for the same Solr home. It also mentions a fix for tests failing on Windows due to the loader not being closed correctly. The diff shows that a SolrResourceLoader is now created and passed to both the CoreContainer constructor and the ConfigSolr.fromFile method. This ensures that the same loader instance is used, preventing multiple loaders from being created. This aligns with the commit message's intention to avoid resource wastage and improve startup time. The change also likely ensures the loader is properly managed and closed, addressing the Windows test failure issue. The answer is **Buggy**."
lucene-solr,48003.json,5fa6cd3fec996cca528327c6a13815d96e34cf9c,"@@ -1,48 +1,48 @@
   public static void compress(byte[] bytes, int off, int len, DataOutput out, HashTable ht) throws IOException {
 
     final int base = off;
     final int end = off + len;
 
     int anchor = off++;
 
     if (len > LAST_LITERALS + MIN_MATCH) {
 
       final int limit = end - LAST_LITERALS;
       final int matchLimit = limit - MIN_MATCH;
       ht.reset(len);
       final int hashLog = ht.hashLog;
       final PackedInts.Mutable hashTable = ht.hashTable;
 
       main:
-      while (off < limit) {
+      while (off <= limit) {
         // find a match
         int ref;
         while (true) {
           if (off >= matchLimit) {
             break main;
           }
           final int v = readInt(bytes, off);
           final int h = hash(v, hashLog);
           ref = base + (int) hashTable.get(h);
           assert PackedInts.bitsRequired(off - base) <= hashTable.getBitsPerValue();
           hashTable.set(h, off - base);
           if (off - ref < MAX_DISTANCE && readInt(bytes, ref) == v) {
             break;
           }
           ++off;
         }
 
         // compute match length
         final int matchLen = MIN_MATCH + commonBytes(bytes, ref + MIN_MATCH, off + MIN_MATCH, limit);
 
         encodeSequence(bytes, anchor, ref, off, matchLen, out);
         off += matchLen;
         anchor = off;
       }
     }
 
     // last literals
     final int literalLen = end - anchor;
     assert literalLen >= LAST_LITERALS || literalLen == len;
     encodeLastLiterals(bytes, anchor, end - anchor, out);
   }",Buggy,"Fix compression bug on highly compressible inputs with LZ4.compressHC.


git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1520060 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix in the LZ4 compression algorithm, specifically related to highly compressible inputs when using `LZ4.compressHC`. The diff contains a change in the main loop condition from `off < limit` to `off <= limit`. This suggests that the original code might have been prematurely exiting the loop, potentially missing some data in highly compressible scenarios. The change ensures that the loop continues until `off` is equal to `limit`, which likely resolves the compression bug. The answer is **Buggy**.
"
lucene-solr,48001.json,5fa6cd3fec996cca528327c6a13815d96e34cf9c,"@@ -1,10 +1,10 @@
     private void addHash(byte[] bytes, int off) {
       final int v = readInt(bytes, off);
       final int h = hashHC(v);
       int delta = off - hashTable[h];
       if (delta >= MAX_DISTANCE) {
         delta = MAX_DISTANCE - 1;
       }
       chainTable[off & MASK] = (short) delta;
-      hashTable[h] = off - base;
+      hashTable[h] = off;
     }",Buggy,"Fix compression bug on highly compressible inputs with LZ4.compressHC.


git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1520060 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix related to compression with LZ4. The diff modifies the `addHash` method, specifically how the `hashTable` is updated. The original code `hashTable[h] = off - base;` was subtracting `base` from `off`, while the corrected code `hashTable[h] = off;` stores the offset directly. This suggests that the original implementation had an issue with how it was tracking positions, potentially leading to incorrect compression, especially on highly compressible inputs where accurate distance tracking is crucial. The change directly addresses a logical error in the hashing mechanism, which is a clear indicator of a bug fix. The answer is **Buggy**."
lucene-solr,41767.json,f5fdea8ddaa3002dc89e624e608582a6345f7e1d,"@@ -1,3 +1 @@
-          public int docID() {
-            return doc;
-          }

+    public int docID() { return doc; }",NotBuggy,"LUCENE-3102: first cut - some refactoring, bug fixes, add test, move to core (trunk)

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1103872 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates refactoring and bug fixes. The diff shows a change in the formatting of the `docID()` method, specifically removing the curly braces and placing the return statement on the same line. While the commit message mentions bug fixes, this particular change appears to be a refactoring or code style adjustment rather than a direct bug fix. The functionality of the code remains the same. The answer is **NotBuggy**."
lucene-solr,41766.json,f5fdea8ddaa3002dc89e624e608582a6345f7e1d,"@@ -1,3 +1 @@
-          public float score() {
-            return score;
-          }

+    public float score() { return score; }",NotBuggy,"LUCENE-3102: first cut - some refactoring, bug fixes, add test, move to core (trunk)

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1103872 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates bug fixes and refactoring. The diff shows a minor change in the `score()` method, specifically the removal of curly braces. This change doesn't inherently indicate a bug fix, but rather a style adjustment or code cleanup. The functionality of the method remains the same.
The answer is **NotBuggy**."
lucene-solr,41782.json,f5fdea8ddaa3002dc89e624e608582a6345f7e1d,"@@ -1,52 +1,60 @@
   public void collect(int doc) throws IOException {
 
     if (curDocs == null) {
       // Cache was too large
-      if (curScores != null) {
-        score = scorer.score();
+      if (cacheScores) {
+        cachedScorer.score = scorer.score();
       }
-      this.doc = doc;
+      cachedScorer.doc = doc;
       other.collect(doc);
       return;
     }
 
+    // Allocate a bigger array or abort caching
     if (upto == curDocs.length) {
       base += upto;
-      final int nextLength;
-      // Max out at 512K arrays:
-      if (curDocs.length < 524288) {
-        nextLength = 8*curDocs.length;
-      } else {
-        nextLength = curDocs.length;
+      
+      // Compute next array length - don't allocate too big arrays
+      int nextLength = 8*curDocs.length;
+      if (nextLength > MAX_ARRAY_SIZE) {
+        nextLength = MAX_ARRAY_SIZE;
       }
 
       if (base + nextLength > maxDocsToCache) {
-        // Too many docs to collect -- clear cache
-        curDocs = null;
-        if (curScores != null) {
-          score = scorer.score();
+        // try to allocate a smaller array
+        nextLength = maxDocsToCache - base;
+        if (nextLength <= 0) {
+          // Too many docs to collect -- clear cache
+          curDocs = null;
+          curScores = null;
+          cachedSegs.clear();
+          cachedDocs.clear();
+          cachedScores.clear();
+          if (cacheScores) {
+            cachedScorer.score = scorer.score();
+          }
+          cachedScorer.doc = doc;
+          other.collect(doc);
+          return;
         }
-        this.doc = doc;
-        other.collect(doc);
-        cachedDocs.clear();
-        cachedScores.clear();
-        return;
       }
+      
       curDocs = new int[nextLength];
       cachedDocs.add(curDocs);
-      if (curScores != null) {
+      if (cacheScores) {
         curScores = new float[nextLength];
         cachedScores.add(curScores);
       }
       upto = 0;
     }
+    
     curDocs[upto] = doc;
     // TODO: maybe specialize private subclass so we don't
     // null check per collect...
-    if (curScores != null) {
-      score = curScores[upto] = scorer.score();
+    if (cacheScores) {
+      cachedScorer.score = curScores[upto] = scorer.score();
     }
     upto++;
-    this.doc = doc;
+    cachedScorer.doc = doc;
     other.collect(doc);
   }",Buggy,"LUCENE-3102: first cut - some refactoring, bug fixes, add test, move to core (trunk)

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1103872 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates ""bug fixes"". The code changes involve adjustments to the caching mechanism within the `collect` method. Specifically, the code now handles cases where the cache grows too large by attempting to allocate smaller arrays and clearing the cache if necessary. The original code could potentially lead to errors or unexpected behavior when the cache exceeded certain limits. The changes include null checks and array size limitations, which are common strategies for addressing potential bugs related to memory management and resource allocation. The answer is **Buggy**."
lucene-solr,41774.json,f5fdea8ddaa3002dc89e624e608582a6345f7e1d,"@@ -1,40 +1,42 @@
   public void replay(Collector other) throws IOException {
     if (!isCached()) {
       throw new IllegalStateException(""cannot replay: cache was cleared because too much RAM was required"");
     }
+    
+    if (!other.acceptsDocsOutOfOrder() && this.other.acceptsDocsOutOfOrder()) {
+      throw new IllegalArgumentException(
+          ""cannot replay: given collector does not support ""
+              + ""out-of-order collection, while the wrapped collector does. ""
+              + ""Therefore cached documents may be out-of-order."");
+    }
+
     //System.out.println(""CC: replay totHits="" + (upto + base));
     if (lastReaderContext != null) {
       cachedSegs.add(new SegStart(lastReaderContext, base+upto));
       lastReaderContext = null;
     }
-    final int uptoSav = upto;
-    final int baseSav = base;
-    try {
-      upto = 0;
-      base = 0;
-      int chunkUpto = 0;
-      other.setScorer(cachedScorer);
-      curDocs = EMPTY_INT_ARRAY;
-      for(SegStart seg : cachedSegs) {
-        other.setNextReader(seg.readerContext);
-        while(base+upto < seg.end) {
-          if (upto == curDocs.length) {
-            base += curDocs.length;
-            curDocs = cachedDocs.get(chunkUpto);
-            if (curScores != null) {
-              curScores = cachedScores.get(chunkUpto);
-            }
-            chunkUpto++;
-            upto = 0;
+    
+    int curupto = 0;
+    int curbase = 0;
+    int chunkUpto = 0;
+    other.setScorer(cachedScorer);
+    curDocs = EMPTY_INT_ARRAY;
+    for(SegStart seg : cachedSegs) {
+      other.setNextReader(seg.readerContext);
+      while(curbase+curupto < seg.end) {
+        if (curupto == curDocs.length) {
+          curbase += curDocs.length;
+          curDocs = cachedDocs.get(chunkUpto);
+          if (cacheScores) {
+            curScores = cachedScores.get(chunkUpto);
           }
-          if (curScores != null) {
-            score = curScores[upto];
-          }
-          other.collect(curDocs[upto++]);
+          chunkUpto++;
+          curupto = 0;
         }
+        if (cacheScores) {
+          cachedScorer.score = curScores[curupto];
+        }
+        other.collect(curDocs[curupto++]);
       }
-    } finally {
-      upto = uptoSav;
-      base = baseSav;
     }
   }",Buggy,"LUCENE-3102: first cut - some refactoring, bug fixes, add test, move to core (trunk)

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1103872 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates bug fixes and refactoring. The diff includes a new `IllegalArgumentException` check to ensure the replayed collector supports out-of-order collection if the wrapped collector does. Additionally, the code within the `replay` method has been modified, specifically the loop that iterates through cached segments and collects documents. The variables `upto` and `base` are replaced by `curupto` and `curbase`, and the cached scorer's score is set before calling `other.collect`. These changes suggest a correction of a logical error in how the cached documents are replayed to the collector, which indicates a bug fix. The answer is **Buggy**."
lucene-solr,47123.json,52d1ab79192a331e1f3fc5f5a202d6a4c5a633b2,"@@ -1,23 +1,27 @@
-  ByteBuffer[] map(FileChannel fc, long offset, long length) throws IOException {
+  final ByteBuffer[] map(String resourceDescription, FileChannel fc, long offset, long length) throws IOException {
     if ((length >>> chunkSizePower) >= Integer.MAX_VALUE)
-      throw new IllegalArgumentException(""RandomAccessFile too big for chunk size: "" + fc.toString());
+      throw new IllegalArgumentException(""RandomAccessFile too big for chunk size: "" + resourceDescription);
     
     final long chunkSize = 1L << chunkSizePower;
     
     // we always allocate one more buffer, the last one may be a 0 byte one
     final int nrBuffers = (int) (length >>> chunkSizePower) + 1;
     
     ByteBuffer buffers[] = new ByteBuffer[nrBuffers];
     
     long bufferStart = 0L;
     for (int bufNr = 0; bufNr < nrBuffers; bufNr++) { 
       int bufSize = (int) ( (length > (bufferStart + chunkSize))
           ? chunkSize
               : (length - bufferStart)
           );
-      buffers[bufNr] = fc.map(MapMode.READ_ONLY, offset + bufferStart, bufSize);
+      try {
+        buffers[bufNr] = fc.map(MapMode.READ_ONLY, offset + bufferStart, bufSize);
+      } catch (IOException ioe) {
+        throw convertMapFailedIOException(ioe, resourceDescription, bufSize);
+      }
       bufferStart += bufSize;
     }
     
     return buffers;
   }",NotBuggy,"LUCENE-5673: MMapDirectory: Work around a ""bug"" in the JDK that throws a confusing OutOfMemoryError wrapped inside IOException if the FileChannel  mapping failed because of lack of virtual address space. The IOException is rethrown with more useful information about the problem, omitting the incorrect OutOfMemoryError

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1595213 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for a confusing `OutOfMemoryError` that occurs when mapping a `FileChannel` fails due to lack of virtual address space. The diff shows the introduction of a `try-catch` block around the `fc.map` call. If an `IOException` occurs during mapping, it's caught and re-thrown as a new `IOException` with more informative details, specifically omitting the misleading `OutOfMemoryError`. This directly addresses the issue described in the commit message. The answer is **Buggy**."
lucene-solr,3974.json,277dd050869d458a370fb7adb837a035e091b89f,"@@ -1,31 +1,32 @@
-    private MapWriter serializeToV2Format(SolrParams params, String[] list) {
+    private MapWriter serializeToV2Format(SolrParams paramsV1, String[] list) {
       return ew -> ew.put(meta.commandName, (MapWriter) ew1 -> {
-        Iterator<String> iter = params.getParameterNamesIterator();
+        Iterator<String> iter = paramsV1.getParameterNamesIterator();
         Map<String, Map<String, String>> subProperties = null;
         while (iter.hasNext()) {
           String key = iter.next();
           if (CoreAdminParams.ACTION.equals(key)) continue;
           Object substitute = meta.getReverseParamSubstitute(key);
           int idx = template.variables.indexOf(substitute);
           if (idx > -1) {
-            String val = params.get(String.valueOf(substitute));
+            String val = paramsV1.get(key);
             if (val == null) throw new RuntimeException(""null value is not valid for "" + key);
             list[idx] = val;
             continue;
           }
           if (substitute instanceof Pair) {//this is a nested object
+            @SuppressWarnings(""unchecked"")
             Pair<String, String> p = (Pair<String, String>) substitute;
             if (subProperties == null) subProperties = new HashMap<>();
-            subProperties.computeIfAbsent(p.first(), s -> new HashMap<>()).put(p.second(), params.get(key));
+            subProperties.computeIfAbsent(p.first(), s -> new HashMap<>()).put(p.second(), paramsV1.get(key));
           } else {
-            Object val = params.get(key);
+            Object val = paramsV1.get(key);
             ew1.put(substitute.toString(), val);
           }
         }
         if (subProperties != null) {
           for (Map.Entry<String, Map<String, String>> e : subProperties.entrySet()) {
             ew1.put(e.getKey(), e.getValue());
           }
         }
       });
     }",Buggy,"SOLR-12061: Fix substitution bug in API V1 to V2 migration
",Buggy,"The commit message indicates a bug fix related to substitution during API V1 to V2 migration. The code changes involve modifications to the `serializeToV2Format` method. The parameter `params` is renamed to `paramsV1` to clarify its origin. The code now uses `paramsV1.get(key)` instead of `params.get(String.valueOf(substitute))` to retrieve parameter values. Additionally, a `@SuppressWarnings(""unchecked"")` annotation is added before casting `substitute` to `Pair<String, String>`. These changes suggest that the original code had issues with parameter retrieval or type handling during the migration process, which is now addressed. The answer is **Buggy**."
lucene-solr,3973.json,277dd050869d458a370fb7adb837a035e091b89f,"@@ -1,12 +1,12 @@
-    public V2Request.Builder convert(SolrParams params) {
+    public V2Request.Builder convert(SolrParams paramsV1) {
       String[] list = new String[template.variables.size()];
-      MapWriter data = serializeToV2Format(params, list);
+      MapWriter data = serializeToV2Format(paramsV1, list);
       Map o = data.toMap(new LinkedHashMap<>());
       return new V2Request.Builder(template.apply(s -> {
         int idx = template.variables.indexOf(s);
         return list[idx];
       }))
           .withMethod(meta.getHttpMethod())
           .withPayload(o);
 
     }",NotBuggy,"SOLR-12061: Fix substitution bug in API V1 to V2 migration
",Buggy,"The commit message indicates a bug fix related to substitution during API version migration. The code diff shows a change in the method signature, renaming the parameter from `params` to `paramsV1`. The `serializeToV2Format` method now uses `paramsV1` instead of `params`. This change suggests that the original parameter name might have been causing confusion or conflicts, leading to incorrect substitution during the migration process. This renaming likely resolves an issue where the wrong parameters were being used, thus fixing a bug. The answer is **Buggy**.
"
lucene-solr,3975.json,277dd050869d458a370fb7adb837a035e091b89f,"@@ -1,11 +1,11 @@
-  public static V2Request.Builder convert(CollectionAdminRequest request) {
+  public static V2Request.Builder convert(CollectionAdminRequest<?> request) {
     ActionInfo info = mapping.get(request.action);
     if (info == null) throw new RuntimeException(""Unsupported action :"" + request.action);
 
     if (info.meta.getHttpMethod() == SolrRequest.METHOD.POST) {
       if (info.path == null) info.setPath();
       return info.convert(request.getParams());
     }
 
     return null;
   }",Buggy,"SOLR-12061: Fix substitution bug in API V1 to V2 migration
",Buggy,"The commit message indicates a bug fix related to substitution during API version migration. The code change modifies the `convert` method signature to accept a generic type `CollectionAdminRequest<?>` instead of `CollectionAdminRequest`. This suggests that the original code might have been limited to a specific type of `CollectionAdminRequest`, causing issues when dealing with other types during the migration process. By introducing the generic type, the method becomes more flexible and can handle different types of `CollectionAdminRequest`, which likely resolves the substitution bug mentioned in the commit message. The answer is **Buggy**."
lucene-solr,30452.json,ef8126e5eab7aec9c8775c2e08bd6c2bb1ef690f,"@@ -1,13 +1,10 @@
   public void setRangeValues(InetAddress min, InetAddress max) {
-    if (StringHelper.compare(BYTES, min.getAddress(), 0, max.getAddress(), 0) > 0) {
-      throw new IllegalArgumentException(""min value cannot be greater than max value for range field (name="" + name + "")"");
-    }
     final byte[] bytes;
     if (fieldsData == null) {
       bytes = new byte[BYTES*2];
       fieldsData = new BytesRef(bytes);
     } else {
       bytes = ((BytesRef)fieldsData).bytes;
     }
     encode(min, max, bytes);
   }",Buggy,"LUCENE-7738: Fix min/max verification bug in InetAddressRange to correctly compare IPv4 and IPv6. Update tests.
",Buggy,"The commit message indicates a bug fix related to min/max verification in InetAddressRange, specifically concerning the comparison of IPv4 and IPv6 addresses. The provided diff removes the check that throws an IllegalArgumentException if the min value is greater than the max value. This suggests that the original comparison logic using `StringHelper.compare` was flawed and incorrectly identified valid ranges as invalid, especially when comparing IPv4 and IPv6 addresses. Removing the faulty check constitutes a bug fix. The answer is **Buggy**.
"
lucene-solr,30453.json,ef8126e5eab7aec9c8775c2e08bd6c2bb1ef690f,"@@ -1,4 +1,11 @@
   private static void encode(final InetAddress min, final InetAddress max, final byte[] bytes) {
-    System.arraycopy(InetAddressPoint.encode(min), 0, bytes, 0, BYTES);
-    System.arraycopy(InetAddressPoint.encode(max), 0, bytes, BYTES, BYTES);
+    // encode min and max value (consistent w/ InetAddressPoint encoding)
+    final byte[] minEncoded = InetAddressPoint.encode(min);
+    final byte[] maxEncoded = InetAddressPoint.encode(max);
+    // ensure min is lt max
+    if (StringHelper.compare(BYTES, minEncoded, 0, maxEncoded, 0) > 0) {
+      throw new IllegalArgumentException(""min value cannot be greater than max value for InetAddressRange field"");
+    }
+    System.arraycopy(minEncoded, 0, bytes, 0, BYTES);
+    System.arraycopy(maxEncoded, 0, bytes, BYTES, BYTES);
   }",Buggy,"LUCENE-7738: Fix min/max verification bug in InetAddressRange to correctly compare IPv4 and IPv6. Update tests.
",Buggy,"The commit message indicates a bug fix related to min/max verification in InetAddressRange, specifically addressing the comparison of IPv4 and IPv6 addresses. The code diff introduces a check to ensure that the minimum InetAddress is less than or equal to the maximum InetAddress before encoding them. If the minimum is greater than the maximum, an IllegalArgumentException is thrown. This check directly addresses the bug mentioned in the commit message. The answer is **Buggy**."
lucene-solr,26824.json,d58041803c7be0eaf35b6381762c846df7e58116,"@@ -1,14 +1,17 @@
   public void inform(ResourceLoader loader) {
     String stopTagFiles = args.get(""tags"");
     enablePositionIncrements = getBoolean(""enablePositionIncrements"", false);
+    stopTags = null;
     try {
       CharArraySet cas = getWordSet(loader, stopTagFiles, false);
-      stopTags = new HashSet<String>();
-      for (Object element : cas) {
-        char chars[] = (char[]) element;
-        stopTags.add(new String(chars));
+      if (cas != null) {
+        stopTags = new HashSet<String>();
+        for (Object element : cas) {
+          char chars[] = (char[]) element;
+          stopTags.add(new String(chars));
+        }
       }
     } catch (IOException e) {
       throw new InitializationException(""IOException thrown while loading tags"", e);
     }
   }",Buggy,"LUCENE-2510: fix more factory arg bugs found by TestFactories

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene2510@1365426 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix related to factory arguments, specifically found during testing. The code changes involve initializing `stopTags` to null and adding a null check for `cas` before processing it. This suggests that the code was previously not handling cases where the `stopTagFiles` resource could not be loaded or was empty, leading to a potential NullPointerException or incorrect behavior. The addition of the null check and initialization addresses this issue. The answer is **Buggy**."
lucene-solr,26825.json,d58041803c7be0eaf35b6381762c846df7e58116,"@@ -1,3 +1,4 @@
   public TokenStream create(TokenStream stream) {
-    return new JapanesePartOfSpeechStopFilter(enablePositionIncrements, stream, stopTags);
+    // if stoptags is null, it means the file is empty
+    return stopTags == null ? stream : new JapanesePartOfSpeechStopFilter(enablePositionIncrements, stream, stopTags);
   }",Buggy,"LUCENE-2510: fix more factory arg bugs found by TestFactories

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene2510@1365426 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix related to factory arguments, specifically found by a test case (TestFactories). The diff shows a change in the `create` method of a `TokenStreamFactory`. The original code unconditionally created a `JapanesePartOfSpeechStopFilter`. The modified code adds a condition: if `stopTags` is null (meaning the stop tags file is empty), the original stream is returned directly; otherwise, the `JapanesePartOfSpeechStopFilter` is created. This suggests that the factory was not handling the case where the stop tags file was empty, potentially leading to an error or unexpected behavior. The added null check fixes this bug. The answer is **Buggy**."
lucene-solr,46955.json,3b35de6599b12e08e5edd7549cd64c947cdb5a15,"@@ -1,12 +1,15 @@
                     public Number next() {
                       if (++curDoc >= maxDoc) {
                         throw new NoSuchElementException(""no more documents to return values for"");
                       }
                       Long updatedValue = updates.get(curDoc);
                       if (updatedValue == null) {
-                        updatedValue = Long.valueOf(currentValues.get(curDoc));
+                        // only read the current value if the document had a value before
+                        if (currentValues != null && docsWithField.get(curDoc)) {
+                          updatedValue = currentValues.get(curDoc);
+                        }
                       } else if (updatedValue == NumericUpdate.MISSING) {
                         updatedValue = null;
                       }
                       return updatedValue;
                     }",Buggy,"LUCENE-5189: fix updates-order and docsWithField bugs

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1528837 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates fixes for ""updates-order"" and ""docsWithField"" bugs. The code diff shows a change within the `next()` method of an iterator, specifically addressing how `updatedValue` is retrieved. The original code unconditionally retrieved the current value using `currentValues.get(curDoc)` if an update was not found. The modified code now checks `docsWithField.get(curDoc)` before retrieving the current value. This suggests that the original code was incorrectly retrieving values for documents that did not previously have a value, which aligns with the ""docsWithField"" bug mentioned in the commit message. This change ensures that the current value is only read if the document had a value before, thus fixing the bug. The answer is **Buggy**.
"
lucene-solr,48196.json,ae4723e0b504da902656aedac8ea27cd98e4cf6b,"@@ -1,28 +1,27 @@
   public static IntsRef getSingleton(Automaton a) {
     if (a.isDeterministic() == false) {
       throw new IllegalArgumentException(""input automaton must be deterministic"");
     }
     IntsRefBuilder builder = new IntsRefBuilder();
     HashSet<Integer> visited = new HashSet<>();
     int s = 0;
-    boolean done;
     Transition t = new Transition();
     while (true) {
       visited.add(s);
       if (a.isAccept(s) == false) {
         if (a.getNumTransitions(s) == 1) {
           a.getTransition(s, 0, t);
           if (t.min == t.max && !visited.contains(t.dest)) {
             builder.append(t.min);
             s = t.dest;
             continue;
           }
         }
       } else if (a.getNumTransitions(s) == 0) {
         return builder.get();
       }
 
       // Automaton accepts more than one string:
       return null;
     }
   }",NotBuggy,"LUCENE-6365: fix buggy Operations.topoSort; add test

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1689079 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix in `Operations.topoSort` and includes a new test case. The provided code snippet, however, is from a different method, `getSingleton`. While the change itself doesn't scream ""bug fix"" without more context, the commit message explicitly states that a bug was fixed. Therefore, the answer is **Buggy**.
"
lucene-solr,48202.json,ae4723e0b504da902656aedac8ea27cd98e4cf6b,"@@ -1,26 +1,25 @@
   public static int[] topoSortStates(Automaton a) {
+    if (a.getNumStates() == 0) {
+      return new int[0];
+    }
     int numStates = a.getNumStates();
     int[] states = new int[numStates];
     final BitSet visited = new BitSet(numStates);
-    final LinkedList<Integer> worklist = new LinkedList<>();
-    worklist.add(0);
-    visited.set(0);
-    int upto = 0;
-    states[upto] = 0;
-    upto++;
-    Transition t = new Transition();
-    while (worklist.size() > 0) {
-      int s = worklist.removeFirst();
-      int count = a.initTransition(s, t);
-      for (int i=0;i<count;i++) {
-        a.getNextTransition(t);
-        if (!visited.get(t.dest)) {
-          visited.set(t.dest);
-          worklist.add(t.dest);
-          states[upto++] = t.dest;
-        }
-      }
+    int upto = topoSortStatesRecurse(a, visited, states, 0, 0);
+
+    if (upto < states.length) {
+      // There were dead states
+      int[] newStates = new int[upto];
+      System.arraycopy(states, 0, newStates, 0, upto);
+      states = newStates;
+    }
+
+    // Reverse the order:
+    for(int i=0;i<states.length/2;i++) {
+      int s = states[i];
+      states[i] = states[states.length-1-i];
+      states[states.length-1-i] = s;
     }
 
     return states;
   }",Buggy,"LUCENE-6365: fix buggy Operations.topoSort; add test

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1689079 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix in `Operations.topoSort`. The diff shows a significant change in the implementation of the `topoSortStates` method. The original code used a `LinkedList` as a worklist for traversing the automaton states, while the modified code uses a recursive approach (`topoSortStatesRecurse`). Additionally, the code now handles cases where there are dead states (states that are not reachable from the initial state) and reverses the order of the sorted states. These changes suggest a correction to the original topological sort algorithm, likely to address issues with correctness or completeness. The addition of a test, as mentioned in the commit message, further supports the idea that this commit is addressing a bug. The answer is **Buggy**.
"
lucene-solr,49563.json,e48f99c36cb1a5a2cca505266f886fa52fd2be5f,"@@ -1,8 +1,8 @@
   public static ReaderIterator getReaderIterator(DataInput in, int mem) throws IOException {
-    final int version = CodecUtil.checkHeader(in, CODEC_NAME, VERSION_START, VERSION_START);
+    final int version = CodecUtil.checkHeader(in, CODEC_NAME, VERSION_START, VERSION_CURRENT);
     final int bitsPerValue = in.readVInt();
     assert bitsPerValue > 0 && bitsPerValue <= 64: ""bitsPerValue="" + bitsPerValue;
     final int valueCount = in.readVInt();
     final Format format = Format.byId(in.readVInt());
     return getReaderIteratorNoHeader(in, format, version, valueCount, bitsPerValue, mem);
   }",Buggy,"Fix error-prone header check.


git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1359861 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix to an ""error-prone header check"". The diff modifies the `checkHeader` method call, changing the expected version from `VERSION_START` to `VERSION_CURRENT`. This suggests that the header check was previously too strict, only accepting a specific starting version, and now it accepts a range of versions up to the current one. This is likely a bug fix to allow for compatibility with different versions of the codec. The answer is **Buggy**."
lucene-solr,13424.json,cc344dc6bd9e71ed7848618630b51f4633e1dd50,"@@ -1,37 +1,38 @@
   public SlotAcc createSlotAcc(FacetContext fcontext, int numDocs, int numSlots) throws IOException {
     ValueSource vs = getArg();
 
     SchemaField sf = null;
 
     if (vs instanceof FieldNameValueSource) {
       String field = ((FieldNameValueSource)vs).getFieldName();
       sf = fcontext.qcontext.searcher().getSchema().getField(field);
 
       if (sf.multiValued() || sf.getType().multiValuedFieldCache()) {
         vs = null;
         throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, ""min/max aggregations can't be used on multi-valued field "" + field);
       } else {
         vs = sf.getType().getValueSource(sf, null);
       }
     }
 
     if (vs instanceof StrFieldSource) {
       return new SingleValuedOrdAcc(fcontext, sf, numSlots);
     }
 
     // Since functions don't currently have types, we rely on the type of the field
     if (sf != null && sf.getType().getNumberType() != null) {
       switch (sf.getType().getNumberType()) {
         case FLOAT:
         case DOUBLE:
           return new DFuncAcc(vs, fcontext, numSlots);
         case INTEGER:
         case LONG:
-        case DATE:
           return new LFuncAcc(vs, fcontext, numSlots);
+        case DATE:
+          return new DateFuncAcc(vs, fcontext, numSlots);
       }
     }
 
     // numeric functions
     return new DFuncAcc(vs, fcontext, numSlots);
   }",NotBuggy,"SOLR-11316: date support for min/max, fix missing bug for int/long fields
",Buggy,"The commit message indicates a bug fix for int/long fields and date support for min/max. The code changes involve adding a specific `DateFuncAcc` for `DATE` types, which was previously missing and causing incorrect behavior. This aligns with the commit message's claim of fixing a bug related to date fields. The answer is **Buggy**."
lucene-solr,13439.json,cc344dc6bd9e71ed7848618630b51f4633e1dd50,"@@ -1,8 +1,3 @@
     public Object getValue(int slot) {
-      long val = result[slot];
-      if (val == 0 && exists.get(slot)) {
-        return null;
-      } else {
-        return val;
-      }
+      return result[slot] == MISSING ? null : new Date(result[slot]);
     }",Buggy,"SOLR-11316: date support for min/max, fix missing bug for int/long fields
",Buggy,"The commit message indicates a fix for a missing bug related to int/long fields and introduces date support for min/max. The diff shows a change in the `getValue` method, which now returns a `Date` object instead of a `long` value. It also checks for a `MISSING` value and returns `null` if it's found. This change suggests that the previous implementation had a bug where it wasn't handling missing values correctly for date fields, or it was returning incorrect values for date fields. The change from returning a `long` to a `Date` object strongly indicates a bug fix related to how dates were being handled. The answer is **Buggy**.
"
lucene-solr,17606.json,481a1f859d0e9c844113c7693424c6aca1fa5245,"@@ -1,54 +1,54 @@
   public void checkSystemCollection() {
     if (cloudManager != null) {
       try {
         if (cloudManager.isClosed() || Thread.interrupted()) {
           factory.setPersistent(false);
           return;
         }
         ClusterState clusterState = cloudManager.getClusterStateProvider().getClusterState();
         DocCollection systemColl = clusterState.getCollectionOrNull(CollectionAdminParams.SYSTEM_COLL);
         if (systemColl == null) {
           if (logMissingCollection) {
-            log.warn(""Missing "" + CollectionAdminParams.SYSTEM_COLL + "", keeping metrics history in memory"");
+            log.info(""No "" + CollectionAdminParams.SYSTEM_COLL + "" collection, keeping metrics history in memory."");
             logMissingCollection = false;
           }
           factory.setPersistent(false);
           return;
         } else {
           boolean ready = false;
           for (Replica r : systemColl.getReplicas()) {
             if (r.isActive(clusterState.getLiveNodes())) {
               ready = true;
               break;
             }
           }
           if (!ready) {
-            log.debug(CollectionAdminParams.SYSTEM_COLL + "" not ready yet, keeping metrics history in memory"");
+            log.debug(CollectionAdminParams.SYSTEM_COLL + ""collection not ready yet, keeping metrics history in memory"");
             factory.setPersistent(false);
             return;
           }
         }
       } catch (Exception e) {
         if (logMissingCollection) {
           log.warn(""Error getting cluster state, keeping metrics history in memory"", e);
         }
         logMissingCollection = false;
         factory.setPersistent(false);
         return;
       }
       logMissingCollection = true;
       factory.setPersistent(true);
     } else {
       try {
         solrClient.query(CollectionAdminParams.SYSTEM_COLL, new SolrQuery(CommonParams.Q, ""*:*"", CommonParams.ROWS, ""0""));
         factory.setPersistent(true);
         logMissingCollection = true;
       } catch (Exception e) {
         if (logMissingCollection) {
-          log.warn(""Error querying .system collection, keeping metrics history in memory"", e);
+          log.info(""No "" + CollectionAdminParams.SYSTEM_COLL + "" collection, keeping metrics history in memory."");
         }
         logMissingCollection = false;
         factory.setPersistent(false);
       }
     }
   }",NotBuggy,"SOLR-11779: Reduce logging, fix index size conversion bug.
",NotBuggy,"The commit message indicates two changes: reduced logging and a fix for an index size conversion bug. The provided diff focuses on the first part, reducing logging verbosity. The log level is changed from `warn` to `info` in two places, and the log message is slightly adjusted. While this change aligns with the ""reduce logging"" part of the commit message, it doesn't directly address a bug fix. The diff does not show any changes related to index size conversion. Therefore, based on the provided diff, the changes do not indicate a bug fix. The answer is **NotBuggy**."
lucene-solr,17615.json,481a1f859d0e9c844113c7693424c6aca1fa5245,"@@ -1,149 +1,145 @@
   private void collectGlobalMetrics() {
     if (!amIOverseerLeader()) {
       return;
     }
     Set<String> nodes = new HashSet<>(cloudManager.getClusterStateProvider().getLiveNodes());
     NodeStateProvider nodeStateProvider = cloudManager.getNodeStateProvider();
     Set<String> collTags = new HashSet<>();
     collTags.addAll(counters.get(Group.core.toString()));
     collTags.addAll(gauges.get(Group.core.toString()));
 
     Set<String> nodeTags = new HashSet<>();
     String nodePrefix = ""metrics:"" + SolrMetricManager.getRegistryName(Group.node) + "":"";
     counters.get(Group.node.toString()).forEach(name -> {
       nodeTags.add(nodePrefix + name);
     });
     gauges.get(Group.node.toString()).forEach(name -> {
       nodeTags.add(nodePrefix + name);
     });
     String jvmPrefix = ""metrics:"" + SolrMetricManager.getRegistryName(Group.jvm) + "":"";
     counters.get(Group.jvm.toString()).forEach(name -> {
       nodeTags.add(jvmPrefix + name);
     });
     gauges.get(Group.jvm.toString()).forEach(name -> {
       nodeTags.add(jvmPrefix + name);
     });
 
     // per-registry totals
     // XXX at the moment the type of metrics that we collect allows
     // adding all partial values. At some point it may be necessary to implement
     // other aggregation functions.
     // group : registry : name : value
     Map<Group, Map<String, Map<String, Number>>> totals = new HashMap<>();
 
     // collect and aggregate per-collection totals
     for (String node : nodes) {
       if (cloudManager.isClosed() || Thread.interrupted()) {
         return;
       }
       // add core-level stats
       Map<String, Map<String, List<ReplicaInfo>>> infos = nodeStateProvider.getReplicaInfo(node, collTags);
       infos.forEach((coll, shards) -> {
         shards.forEach((sh, replicas) -> {
           String registry = SolrMetricManager.getRegistryName(Group.collection, coll);
           Map<String, Number> perReg = totals
               .computeIfAbsent(Group.collection, g -> new HashMap<>())
               .computeIfAbsent(registry, r -> new HashMap<>());
           replicas.forEach(ri -> {
             collTags.forEach(tag -> {
               double value = ((Number)ri.getVariable(tag, 0.0)).doubleValue();
-              // TODO: fix this when Suggestion.Condition.DISK_IDX uses proper conversion
-              if (tag.contains(Suggestion.coreidxsize)) {
-                value = value * 1024.0 * 1024.0 * 1024.0;
-              }
               DoubleAdder adder = (DoubleAdder)perReg.computeIfAbsent(tag, t -> new DoubleAdder());
               adder.add(value);
             });
           });
         });
       });
       // add node-level stats
       Map<String, Object> nodeValues = nodeStateProvider.getNodeValues(node, nodeTags);
       for (Group g : Arrays.asList(Group.node, Group.jvm)) {
         String registry = SolrMetricManager.getRegistryName(g);
         Map<String, Number> perReg = totals
             .computeIfAbsent(g, gr -> new HashMap<>())
             .computeIfAbsent(registry, r -> new HashMap<>());
         Set<String> names = new HashSet<>();
         names.addAll(counters.get(g.toString()));
         names.addAll(gauges.get(g.toString()));
         names.forEach(name -> {
           String tag = ""metrics:"" + registry + "":"" + name;
           double value = ((Number)nodeValues.getOrDefault(tag, 0.0)).doubleValue();
           DoubleAdder adder = (DoubleAdder)perReg.computeIfAbsent(name, t -> new DoubleAdder());
           adder.add(value);
         });
       }
     }
 
     // add numNodes
     String nodeReg = SolrMetricManager.getRegistryName(Group.node);
     Map<String, Number> perNodeReg = totals
         .computeIfAbsent(Group.node, gr -> new HashMap<>())
         .computeIfAbsent(nodeReg, r -> new HashMap<>());
     perNodeReg.put(NUM_NODES_KEY, nodes.size());
 
     // add some global collection-level stats
     try {
       ClusterState state = cloudManager.getClusterStateProvider().getClusterState();
       state.forEachCollection(coll -> {
         String registry = SolrMetricManager.getRegistryName(Group.collection, coll.getName());
         Map<String, Number> perReg = totals
             .computeIfAbsent(Group.collection, g -> new HashMap<>())
             .computeIfAbsent(registry, r -> new HashMap<>());
         Collection<Slice> slices = coll.getActiveSlices();
         perReg.put(NUM_SHARDS_KEY, slices.size());
         DoubleAdder numActiveReplicas = new DoubleAdder();
         slices.forEach(s -> {
           s.forEach(r -> {
             if (r.isActive(state.getLiveNodes())) {
               numActiveReplicas.add(1.0);
             }
           });
         });
         perReg.put(NUM_REPLICAS_KEY, numActiveReplicas);
       });
     } catch (IOException e) {
       log.warn(""Exception getting cluster state"", e);
     }
 
     // now update the db-s
     totals.forEach((group, perGroup) -> {
       perGroup.forEach((reg, perReg) -> {
         RrdDb db = getOrCreateDb(reg, group);
         if (db == null) {
           return;
         }
         try {
           // set the timestamp
           Sample s = db.createSample(TimeUnit.SECONDS.convert(timeSource.getEpochTimeNs(), TimeUnit.NANOSECONDS));
           AtomicBoolean dirty = new AtomicBoolean(false);
           List<Group> groups = new ArrayList<>();
           groups.add(group);
           if (group == Group.collection) {
             groups.add(Group.core);
           }
           for (Group g : groups) {
             counters.get(g.toString()).forEach(c -> {
               Number val = perReg.get(c);
               if (val != null) {
                 dirty.set(true);
                 s.setValue(c, val.doubleValue());
               }
             });
             gauges.get(g.toString()).forEach(c -> {
               Number val = perReg.get(c);
               if (val != null) {
                 dirty.set(true);
                 s.setValue(c, val.doubleValue());
               }
             });
           }
           if (dirty.get()) {
             s.update();
           }
         } catch (Exception e) {
         }
       });
     });
   }",Buggy,"SOLR-11779: Reduce logging, fix index size conversion bug.
",Buggy,"The commit message indicates a fix for an ""index size conversion bug"". The diff removes a block of code that appears to be attempting to fix a disk index size conversion issue. The comment ""// TODO: fix this when Suggestion.Condition.DISK_IDX uses proper conversion"" suggests that the removed code was a temporary workaround. Removing this workaround constitutes a bug fix. The answer is **Buggy**."
lucene-solr,50428.json,ceb4f768bf5b71a91872f9ecdc5ebed4d0262903,"@@ -1,22 +1,23 @@
   private GeoPoint[] findAdjoiningPoints(final Plane plane, final GeoPoint pointOnPlane, final Plane envelopePlane) {
     // Compute a normalized perpendicular vector
     final Vector perpendicular = new Vector(plane, pointOnPlane);
     double distanceFactor = 0.0;
     for (int i = 0; i < MAX_ITERATIONS; i++) {
       distanceFactor += DELTA_DISTANCE;
       // Compute two new points along this vector from the original
       final GeoPoint pointA = planetModel.createSurfacePoint(pointOnPlane.x + perpendicular.x * distanceFactor,
         pointOnPlane.y + perpendicular.y * distanceFactor,
         pointOnPlane.z + perpendicular.z * distanceFactor);
       final GeoPoint pointB = planetModel.createSurfacePoint(pointOnPlane.x - perpendicular.x * distanceFactor,
         pointOnPlane.y - perpendicular.y * distanceFactor,
         pointOnPlane.z - perpendicular.z * distanceFactor);
       if (Math.abs(envelopePlane.evaluate(pointA)) > OFF_PLANE_AMOUNT && Math.abs(envelopePlane.evaluate(pointB)) > OFF_PLANE_AMOUNT) {
         //System.out.println(""Distance: ""+computeSquaredDistance(rval[0], pointOnPlane)+"" and ""+computeSquaredDistance(rval[1], pointOnPlane));
         return new GeoPoint[]{pointA, pointB};
       }
       // Loop back around and use a bigger delta
     }
     // Had to abort, so return null.
+    //System.out.println(""     Adjoining points not found.  Are planes parallel?  edge = ""+plane+""; envelope = ""+envelopePlane+""; perpendicular = ""+perpendicular);
     return null;
   }",NotBuggy,"LUCENE-8337: Fix problems with how travel planes too close to edge of world are disallowed, and increase the size of the disallowed window by an order of magnitude.
",Buggy,"The commit message indicates a fix for issues related to travel planes near the world's edge and mentions increasing the disallowed window size. The code diff includes a loop that iteratively adjusts `distanceFactor` to find `pointA` and `pointB`. The condition `Math.abs(envelopePlane.evaluate(pointA)) > OFF_PLANE_AMOUNT && Math.abs(envelopePlane.evaluate(pointB)) > OFF_PLANE_AMOUNT` checks if both points are sufficiently far from the `envelopePlane`. If the points are not found within `MAX_ITERATIONS`, the method returns `null`. The added `System.out.println` statement suggests debugging efforts to understand why adjoining points might not be found, especially when planes are parallel. This indicates that the original logic had issues with planes too close to the edge of the world, which the increased disallowed window size and debugging statement address. The answer is **Buggy**."
lucene-solr,50391.json,ceb4f768bf5b71a91872f9ecdc5ebed4d0262903,"@@ -1,46 +1,47 @@
     public boolean apply(final GeoPoint testPoint, final boolean testPointInSet,
       final double x, final double y, final double z) {
       // First, try with two individual legs.  If that doesn't work, try the DualCrossingIterator.
       try {
         // First, we'll determine if the intersection point is in set or not
         //System.out.println("" Finding whether ""+intersectionPoint+"" is in-set, based on travel from ""+testPoint+"" along ""+firstLegPlane+"" (value=""+firstLegValue+"")"");
         final CountingEdgeIterator testPointEdgeIterator = createLinearCrossingEdgeIterator(testPoint,
           firstLegPlane, firstLegAbovePlane, firstLegBelowPlane,
           intersectionPoint.x, intersectionPoint.y, intersectionPoint.z);
         // Traverse our way from the test point to the check point.  Use the z tree because that's fixed.
         firstLegTree.traverse(testPointEdgeIterator, firstLegValue);
         final boolean intersectionPointOnEdge = testPointEdgeIterator.isOnEdge();
         // If the intersection point is on the edge, we cannot use this combination of legs, since it's not logically possible to compute in-set or out-of-set
         // with such a starting point.
         if (intersectionPointOnEdge) {
           throw new IllegalArgumentException(""Intersection point landed on an edge -- illegal path"");
         }
         final boolean intersectionPointInSet = intersectionPointOnEdge || (((testPointEdgeIterator.getCrossingCount() & 1) == 0)?testPointInSet:!testPointInSet);
         
         //System.out.println(""  Intersection point in-set? ""+intersectionPointInSet+"" On edge? ""+intersectionPointOnEdge);
 
         // Now do the final leg
         //System.out.println("" Finding whether [""+x+"",""+y+"",""+z+""] is in-set, based on travel from ""+intersectionPoint+"" along ""+secondLegPlane+"" (value=""+secondLegValue+"")"");
         final CountingEdgeIterator travelEdgeIterator = createLinearCrossingEdgeIterator(intersectionPoint,
           secondLegPlane, secondLegAbovePlane, secondLegBelowPlane,
           x, y, z);
         // Traverse our way from the test point to the check point.
         secondLegTree.traverse(travelEdgeIterator, secondLegValue);
         final boolean rval = travelEdgeIterator.isOnEdge() || (((travelEdgeIterator.getCrossingCount() & 1) == 0)?intersectionPointInSet:!intersectionPointInSet);
         
         //System.out.println("" Check point in set? ""+rval);
         return rval;
       } catch (IllegalArgumentException e) {
         // Intersection point apparently was on edge, so try another strategy
+        //System.out.println("" Trying dual crossing edge iterator"");
         final CountingEdgeIterator edgeIterator = new DualCrossingEdgeIterator(testPoint,
           firstLegPlane, firstLegAbovePlane, firstLegBelowPlane,
           secondLegPlane, secondLegAbovePlane, secondLegBelowPlane,
           x, y, z, intersectionPoint);
         firstLegTree.traverse(edgeIterator, firstLegValue);
         if (edgeIterator.isOnEdge()) {
           return true;
         }
         secondLegTree.traverse(edgeIterator, secondLegValue);
         return edgeIterator.isOnEdge() || (((edgeIterator.getCrossingCount() & 1) == 0)?testPointInSet:!testPointInSet);
       }
     }",NotBuggy,"LUCENE-8337: Fix problems with how travel planes too close to edge of world are disallowed, and increase the size of the disallowed window by an order of magnitude.
",Buggy,"The commit message indicates a fix for problems related to travel planes near the edge of the world. The code changes involve exception handling within the `apply` method. Specifically, an `IllegalArgumentException` is caught, which previously led to an alternative strategy using `DualCrossingEdgeIterator`. The added comment `//System.out.println("" Trying dual crossing edge iterator"");` suggests debugging or logging related to this exception handling. The exception is thrown when the intersection point lands on an edge, indicating an illegal path. This suggests a bug where the algorithm failed to handle cases where the intersection point was on an edge, and the fix involves catching the exception and trying an alternative approach. The answer is **Buggy**."
lucene-solr,50389.json,ceb4f768bf5b71a91872f9ecdc5ebed4d0262903,"@@ -1,17 +1,19 @@
   private CountingEdgeIterator createLinearCrossingEdgeIterator(final GeoPoint testPoint,
     final Plane plane, final Plane abovePlane, final Plane belowPlane,
     final double thePointX, final double thePointY, final double thePointZ) {
     // If thePoint and testPoint are parallel, we won't be able to determine sidedness of the bounding planes.  So detect that case, and build the iterator differently if we find it.
     // This didn't work; not sure why not:
     //if (testPoint.isParallel(thePointX, thePointY, thePointZ)) {
     //  return new FullLinearCrossingEdgeIterator(plane, abovePlane, belowPlane, thePointX, thePointY, thePointZ);
     //}
     //return new SectorLinearCrossingEdgeIterator(plane, abovePlane, belowPlane, thePointX, thePointY, thePointZ);
     //
     try {
+      //System.out.println("" creating sector linear crossing edge iterator"");
       return new SectorLinearCrossingEdgeIterator(testPoint, plane, abovePlane, belowPlane, thePointX, thePointY, thePointZ);
     } catch (IllegalArgumentException e) {
       // Assume we failed because we could not construct bounding planes, so do it another way.
+      //System.out.println("" create full linear crossing edge iterator"");
       return new FullLinearCrossingEdgeIterator(testPoint, plane, abovePlane, belowPlane, thePointX, thePointY, thePointZ);
     }
   }",NotBuggy,"LUCENE-8337: Fix problems with how travel planes too close to edge of world are disallowed, and increase the size of the disallowed window by an order of magnitude.
",Buggy,"The commit message indicates a fix for issues related to travel planes near the edge of the world. The code changes involve a `try-catch` block that attempts to create a `SectorLinearCrossingEdgeIterator`. If an `IllegalArgumentException` occurs during its creation, the code falls back to creating a `FullLinearCrossingEdgeIterator`. This suggests that the original implementation using `SectorLinearCrossingEdgeIterator` had issues under certain conditions (likely when the travel plane was too close to the edge of the world), leading to the exception. The `catch` block and the fallback mechanism are implemented to handle these problematic cases, which indicates a bug fix. The comments also suggest debugging statements that were likely used to diagnose the issue. The answer is **Buggy**."
lucene-solr,50383.json,ceb4f768bf5b71a91872f9ecdc5ebed4d0262903,"@@ -1,242 +1,242 @@
   private boolean isInSet(final double x, final double y, final double z,
     final GeoPoint testPoint,
     final boolean testPointInSet,
     final Plane testPointFixedXPlane, final Plane testPointFixedXAbovePlane, final Plane testPointFixedXBelowPlane,
     final Plane testPointFixedYPlane, final Plane testPointFixedYAbovePlane, final Plane testPointFixedYBelowPlane,
     final Plane testPointFixedZPlane, final Plane testPointFixedZAbovePlane, final Plane testPointFixedZBelowPlane) {
 
     //System.out.println(""\nIsInSet called for [""+x+"",""+y+"",""+z+""], testPoint=""+testPoint+""; is in set? ""+testPointInSet);
     // If we're right on top of the point, we know the answer.
     if (testPoint.isNumericallyIdentical(x, y, z)) {
       return testPointInSet;
     }
     
     // If we're right on top of any of the test planes, we navigate solely on that plane.
     if (testPointFixedYAbovePlane != null && testPointFixedYBelowPlane != null && testPointFixedYPlane.evaluateIsZero(x, y, z)) {
       // Use the XZ plane exclusively.
       //System.out.println("" Using XZ plane alone"");
       final CountingEdgeIterator crossingEdgeIterator = createLinearCrossingEdgeIterator(testPoint, testPointFixedYPlane, testPointFixedYAbovePlane, testPointFixedYBelowPlane, x, y, z);
       // Traverse our way from the test point to the check point.  Use the y tree because that's fixed.
       yTree.traverse(crossingEdgeIterator, testPoint.y);
       return crossingEdgeIterator.isOnEdge() || (((crossingEdgeIterator.getCrossingCount() & 1) == 0)?testPointInSet:!testPointInSet);
     } else if (testPointFixedXAbovePlane != null && testPointFixedXBelowPlane != null && testPointFixedXPlane.evaluateIsZero(x, y, z)) {
       // Use the YZ plane exclusively.
       //System.out.println("" Using YZ plane alone"");
       final CountingEdgeIterator crossingEdgeIterator = createLinearCrossingEdgeIterator(testPoint, testPointFixedXPlane, testPointFixedXAbovePlane, testPointFixedXBelowPlane, x, y, z);
       // Traverse our way from the test point to the check point.  Use the x tree because that's fixed.
       xTree.traverse(crossingEdgeIterator, testPoint.x);
       return crossingEdgeIterator.isOnEdge() || (((crossingEdgeIterator.getCrossingCount() & 1) == 0)?testPointInSet:!testPointInSet);
     } else if (testPointFixedZAbovePlane != null && testPointFixedZBelowPlane != null && testPointFixedZPlane.evaluateIsZero(x, y, z)) {
       //System.out.println("" Using XY plane alone"");
       final CountingEdgeIterator crossingEdgeIterator = createLinearCrossingEdgeIterator(testPoint, testPointFixedZPlane, testPointFixedZAbovePlane, testPointFixedZBelowPlane, x, y, z);
       // Traverse our way from the test point to the check point.  Use the z tree because that's fixed.
       zTree.traverse(crossingEdgeIterator, testPoint.z);
       return crossingEdgeIterator.isOnEdge() || (((crossingEdgeIterator.getCrossingCount() & 1) == 0)?testPointInSet:!testPointInSet);
     } else {
       //System.out.println("" Using two planes"");
       // This is the expensive part!!
       // Changing the code below has an enormous impact on the queries per second we see with the benchmark.
       
       // We need to use two planes to get there.  We don't know which two planes will do it but we can figure it out.
       final Plane travelPlaneFixedX = new Plane(1.0, 0.0, 0.0, -x);
       final Plane travelPlaneFixedY = new Plane(0.0, 1.0, 0.0, -y);
       final Plane travelPlaneFixedZ = new Plane(0.0, 0.0, 1.0, -z);
 
       Plane fixedYAbovePlane = new Plane(travelPlaneFixedY, true);
-      if (fixedYAbovePlane.D - planetModel.getMaximumYValue() > NEAR_EDGE_CUTOFF || planetModel.getMinimumYValue() - fixedYAbovePlane.D > NEAR_EDGE_CUTOFF) {
+      if (-fixedYAbovePlane.D - planetModel.getMaximumYValue() > NEAR_EDGE_CUTOFF || planetModel.getMinimumYValue() + fixedYAbovePlane.D > NEAR_EDGE_CUTOFF) {
           fixedYAbovePlane = null;
       }
       
       Plane fixedYBelowPlane = new Plane(travelPlaneFixedY, false);
-      if (fixedYBelowPlane.D - planetModel.getMaximumYValue() > NEAR_EDGE_CUTOFF || planetModel.getMinimumYValue() - fixedYBelowPlane.D > NEAR_EDGE_CUTOFF) {
+      if (-fixedYBelowPlane.D - planetModel.getMaximumYValue() > NEAR_EDGE_CUTOFF || planetModel.getMinimumYValue() + fixedYBelowPlane.D > NEAR_EDGE_CUTOFF) {
           fixedYBelowPlane = null;
       }
       
       Plane fixedXAbovePlane = new Plane(travelPlaneFixedX, true);
-      if (fixedXAbovePlane.D - planetModel.getMaximumXValue() > NEAR_EDGE_CUTOFF || planetModel.getMinimumXValue() - fixedXAbovePlane.D > NEAR_EDGE_CUTOFF) {
+      if (-fixedXAbovePlane.D - planetModel.getMaximumXValue() > NEAR_EDGE_CUTOFF || planetModel.getMinimumXValue() + fixedXAbovePlane.D > NEAR_EDGE_CUTOFF) {
           fixedXAbovePlane = null;
       }
       
       Plane fixedXBelowPlane = new Plane(travelPlaneFixedX, false);
-      if (fixedXBelowPlane.D - planetModel.getMaximumXValue() > NEAR_EDGE_CUTOFF || planetModel.getMinimumXValue() - fixedXBelowPlane.D > NEAR_EDGE_CUTOFF) {
+      if (-fixedXBelowPlane.D - planetModel.getMaximumXValue() > NEAR_EDGE_CUTOFF || planetModel.getMinimumXValue() + fixedXBelowPlane.D > NEAR_EDGE_CUTOFF) {
           fixedXBelowPlane = null;
       }
       
       Plane fixedZAbovePlane = new Plane(travelPlaneFixedZ, true);
-      if (fixedZAbovePlane.D - planetModel.getMaximumZValue() > NEAR_EDGE_CUTOFF || planetModel.getMinimumZValue() - fixedZAbovePlane.D > NEAR_EDGE_CUTOFF) {
+      if (-fixedZAbovePlane.D - planetModel.getMaximumZValue() > NEAR_EDGE_CUTOFF || planetModel.getMinimumZValue() + fixedZAbovePlane.D > NEAR_EDGE_CUTOFF) {
           fixedZAbovePlane = null;
       }
       
       Plane fixedZBelowPlane = new Plane(travelPlaneFixedZ, false);
-      if (fixedZBelowPlane.D - planetModel.getMaximumZValue() > NEAR_EDGE_CUTOFF || planetModel.getMinimumZValue() - fixedZBelowPlane.D > NEAR_EDGE_CUTOFF) {
+      if (-fixedZBelowPlane.D - planetModel.getMaximumZValue() > NEAR_EDGE_CUTOFF || planetModel.getMinimumZValue() + fixedZBelowPlane.D > NEAR_EDGE_CUTOFF) {
           fixedZBelowPlane = null;
       }
 
       // Find the intersection points for each one of these and the complementary test point planes.
 
       final List<TraversalStrategy> traversalStrategies = new ArrayList<>(12);
       
       if (testPointFixedYAbovePlane != null && testPointFixedYBelowPlane != null && fixedXAbovePlane != null && fixedXBelowPlane != null) {
         //check if planes intersects  inside world
         final double checkAbove = 4.0 * (fixedXAbovePlane.D * fixedXAbovePlane.D * planetModel.inverseAbSquared + testPointFixedYAbovePlane.D * testPointFixedYAbovePlane.D * planetModel.inverseAbSquared - 1.0);
         final double checkBelow = 4.0 * (fixedXBelowPlane.D * fixedXBelowPlane.D * planetModel.inverseAbSquared + testPointFixedYBelowPlane.D * testPointFixedYBelowPlane.D * planetModel.inverseAbSquared - 1.0);
         if (checkAbove < Vector.MINIMUM_RESOLUTION_SQUARED && checkBelow < Vector.MINIMUM_RESOLUTION_SQUARED) {
           //System.out.println(""  Looking for intersections between travel and test point planes..."");
           final GeoPoint[] XIntersectionsY = travelPlaneFixedX.findIntersections(planetModel, testPointFixedYPlane);
           for (final GeoPoint p : XIntersectionsY) {
             // Travel would be in YZ plane (fixed x) then in XZ (fixed y)
             // We compute distance we need to travel as a placeholder for the number of intersections we might encounter.
             //final double newDistance = p.arcDistance(testPoint) + p.arcDistance(thePoint);
             final double tpDelta1 = testPoint.x - p.x;
             final double tpDelta2 = testPoint.z - p.z;
             final double cpDelta1 = y - p.y;
             final double cpDelta2 = z - p.z;
             final double newDistance = tpDelta1 * tpDelta1 + tpDelta2 * tpDelta2 + cpDelta1 * cpDelta1 + cpDelta2 * cpDelta2;
             //final double newDistance = (testPoint.x - p.x) * (testPoint.x - p.x) + (testPoint.z - p.z) * (testPoint.z - p.z)  + (thePoint.y - p.y) * (thePoint.y - p.y) + (thePoint.z - p.z) * (thePoint.z - p.z);
             //final double newDistance = Math.abs(testPoint.x - p.x) + Math.abs(thePoint.y - p.y);
             traversalStrategies.add(new TraversalStrategy(newDistance, testPoint.y, x,
               testPointFixedYPlane, testPointFixedYAbovePlane, testPointFixedYBelowPlane,
               travelPlaneFixedX, fixedXAbovePlane, fixedXBelowPlane,
               yTree, xTree, p));
           }
         }
       }
       if (testPointFixedZAbovePlane != null && testPointFixedZBelowPlane != null && fixedXAbovePlane != null && fixedXBelowPlane != null) {
         //check if planes intersects  inside world
         final double checkAbove = 4.0 * (fixedXAbovePlane.D * fixedXAbovePlane.D * planetModel.inverseAbSquared + testPointFixedZAbovePlane.D * testPointFixedZAbovePlane.D * planetModel.inverseCSquared - 1.0);
         final double checkBelow = 4.0 * (fixedXBelowPlane.D * fixedXBelowPlane.D * planetModel.inverseAbSquared + testPointFixedZBelowPlane.D * testPointFixedZBelowPlane.D * planetModel.inverseCSquared - 1.0);
         if (checkAbove < Vector.MINIMUM_RESOLUTION_SQUARED && checkBelow < Vector.MINIMUM_RESOLUTION_SQUARED) {
           //System.out.println(""  Looking for intersections between travel and test point planes..."");
           final GeoPoint[] XIntersectionsZ = travelPlaneFixedX.findIntersections(planetModel, testPointFixedZPlane);
           for (final GeoPoint p : XIntersectionsZ) {
             // Travel would be in YZ plane (fixed x) then in XY (fixed z)
             //final double newDistance = p.arcDistance(testPoint) + p.arcDistance(thePoint);
             final double tpDelta1 = testPoint.x - p.x;
             final double tpDelta2 = testPoint.y - p.y;
             final double cpDelta1 = y - p.y;
             final double cpDelta2 = z - p.z;
             final double newDistance = tpDelta1 * tpDelta1 + tpDelta2 * tpDelta2 + cpDelta1 * cpDelta1 + cpDelta2 * cpDelta2;
             //final double newDistance = (testPoint.x - p.x) * (testPoint.x - p.x) + (testPoint.y - p.y) * (testPoint.y - p.y)  + (thePoint.y - p.y) * (thePoint.y - p.y) + (thePoint.z - p.z) * (thePoint.z - p.z);
             //final double newDistance = Math.abs(testPoint.x - p.x) + Math.abs(thePoint.z - p.z);
             traversalStrategies.add(new TraversalStrategy(newDistance, testPoint.z, x,
               testPointFixedZPlane, testPointFixedZAbovePlane, testPointFixedZBelowPlane,
               travelPlaneFixedX, fixedXAbovePlane, fixedXBelowPlane,
               zTree, xTree, p));
           }
         }
       }
       if (testPointFixedXAbovePlane != null && testPointFixedXBelowPlane != null && fixedYAbovePlane != null && fixedYBelowPlane != null) {
         //check if planes intersects inside world
         final double checkAbove = 4.0 * (testPointFixedXAbovePlane.D * testPointFixedXAbovePlane.D * planetModel.inverseAbSquared + fixedYAbovePlane.D * fixedYAbovePlane.D * planetModel.inverseAbSquared - 1.0);
         final double checkBelow = 4.0 * (testPointFixedXBelowPlane.D * testPointFixedXBelowPlane.D * planetModel.inverseAbSquared + fixedYBelowPlane.D * fixedYBelowPlane.D * planetModel.inverseAbSquared - 1.0);
         if (checkAbove < Vector.MINIMUM_RESOLUTION_SQUARED && checkBelow < Vector.MINIMUM_RESOLUTION_SQUARED) {
           //System.out.println(""  Looking for intersections between travel and test point planes..."");
           final GeoPoint[] YIntersectionsX = travelPlaneFixedY.findIntersections(planetModel, testPointFixedXPlane);
           for (final GeoPoint p : YIntersectionsX) {
             // Travel would be in XZ plane (fixed y) then in YZ (fixed x)
             //final double newDistance = p.arcDistance(testPoint) + p.arcDistance(thePoint);
             final double tpDelta1 = testPoint.y - p.y;
             final double tpDelta2 = testPoint.z - p.z;
             final double cpDelta1 = x - p.x;
             final double cpDelta2 = z - p.z;
             final double newDistance = tpDelta1 * tpDelta1 + tpDelta2 * tpDelta2 + cpDelta1 * cpDelta1 + cpDelta2 * cpDelta2;
             //final double newDistance = (testPoint.y - p.y) * (testPoint.y - p.y) + (testPoint.z - p.z) * (testPoint.z - p.z)  + (thePoint.x - p.x) * (thePoint.x - p.x) + (thePoint.z - p.z) * (thePoint.z - p.z);
             //final double newDistance = Math.abs(testPoint.y - p.y) + Math.abs(thePoint.x - p.x);
             traversalStrategies.add(new TraversalStrategy(newDistance, testPoint.x, y,
               testPointFixedXPlane, testPointFixedXAbovePlane, testPointFixedXBelowPlane,
               travelPlaneFixedY, fixedYAbovePlane, fixedYBelowPlane,
               xTree, yTree, p));
           }
         }
       }
       if (testPointFixedZAbovePlane != null && testPointFixedZBelowPlane != null && fixedYAbovePlane != null && fixedYBelowPlane != null) {
         //check if planes intersects inside world
         final double checkAbove = 4.0 * (testPointFixedZAbovePlane.D * testPointFixedZAbovePlane.D * planetModel.inverseCSquared + fixedYAbovePlane.D * fixedYAbovePlane.D * planetModel.inverseAbSquared - 1.0);
         final double checkBelow = 4.0 * (testPointFixedZBelowPlane.D * testPointFixedZBelowPlane.D * planetModel.inverseCSquared + fixedYBelowPlane.D * fixedYBelowPlane.D * planetModel.inverseAbSquared - 1.0);
         if (checkAbove < Vector.MINIMUM_RESOLUTION_SQUARED && checkBelow < Vector.MINIMUM_RESOLUTION_SQUARED) {
           //System.out.println(""  Looking for intersections between travel and test point planes..."");
           final GeoPoint[] YIntersectionsZ = travelPlaneFixedY.findIntersections(planetModel, testPointFixedZPlane);
           for (final GeoPoint p : YIntersectionsZ) {
             // Travel would be in XZ plane (fixed y) then in XY (fixed z)
             //final double newDistance = p.arcDistance(testPoint) + p.arcDistance(thePoint);
             final double tpDelta1 = testPoint.x - p.x;
             final double tpDelta2 = testPoint.y - p.y;
             final double cpDelta1 = x - p.x;
             final double cpDelta2 = z - p.z;
             final double newDistance = tpDelta1 * tpDelta1 + tpDelta2 * tpDelta2 + cpDelta1 * cpDelta1 + cpDelta2 * cpDelta2;
             //final double newDistance = (testPoint.x - p.x) * (testPoint.x - p.x) + (testPoint.y - p.y) * (testPoint.y - p.y)  + (thePoint.x - p.x) * (thePoint.x - p.x) + (thePoint.z - p.z) * (thePoint.z - p.z);
             //final double newDistance = Math.abs(testPoint.y - p.y) + Math.abs(thePoint.z - p.z);
             traversalStrategies.add(new TraversalStrategy(newDistance, testPoint.z, y,
               testPointFixedZPlane, testPointFixedZAbovePlane, testPointFixedZBelowPlane,
               travelPlaneFixedY, fixedYAbovePlane, fixedYBelowPlane,
               zTree, yTree, p));
           }
         }
       }
       if (testPointFixedXAbovePlane != null && testPointFixedXBelowPlane != null && fixedZAbovePlane != null && fixedZBelowPlane != null) {
         //check if planes intersects inside world
         final double checkAbove = 4.0 * (testPointFixedXAbovePlane.D * testPointFixedXAbovePlane.D * planetModel.inverseAbSquared + fixedZAbovePlane.D * fixedZAbovePlane.D * planetModel.inverseCSquared - 1.0);
         final double checkBelow = 4.0 * (testPointFixedXBelowPlane.D * testPointFixedXBelowPlane.D * planetModel.inverseAbSquared + fixedZBelowPlane.D * fixedZBelowPlane.D * planetModel.inverseCSquared - 1.0);
         if (checkAbove < Vector.MINIMUM_RESOLUTION_SQUARED && checkBelow < Vector.MINIMUM_RESOLUTION_SQUARED) {
           //System.out.println(""  Looking for intersections between travel and test point planes..."");
           final GeoPoint[] ZIntersectionsX = travelPlaneFixedZ.findIntersections(planetModel, testPointFixedXPlane);
           for (final GeoPoint p : ZIntersectionsX) {
             // Travel would be in XY plane (fixed z) then in YZ (fixed x)
             //final double newDistance = p.arcDistance(testPoint) + p.arcDistance(thePoint);
             final double tpDelta1 = testPoint.y - p.y;
             final double tpDelta2 = testPoint.z - p.z;
             final double cpDelta1 = y - p.y;
             final double cpDelta2 = x - p.x;
             final double newDistance = tpDelta1 * tpDelta1 + tpDelta2 * tpDelta2 + cpDelta1 * cpDelta1 + cpDelta2 * cpDelta2;
             //final double newDistance = (testPoint.y - p.y) * (testPoint.y - p.y) + (testPoint.z - p.z) * (testPoint.z - p.z)  + (thePoint.y - p.y) * (thePoint.y - p.y) + (thePoint.x - p.x) * (thePoint.x - p.x);
             //final double newDistance = Math.abs(testPoint.z - p.z) + Math.abs(thePoint.x - p.x);
             traversalStrategies.add(new TraversalStrategy(newDistance, testPoint.x, z,
               testPointFixedXPlane, testPointFixedXAbovePlane, testPointFixedXBelowPlane,
               travelPlaneFixedZ, fixedZAbovePlane, fixedZBelowPlane,
               xTree, zTree, p));
           }
         }
       }
       if (testPointFixedYAbovePlane != null && testPointFixedYBelowPlane != null && fixedZAbovePlane != null && fixedZBelowPlane != null) {
         //check if planes intersects inside world
         final double checkAbove = 4.0 * (testPointFixedYAbovePlane.D * testPointFixedYAbovePlane.D * planetModel.inverseAbSquared + fixedZAbovePlane.D * fixedZAbovePlane.D * planetModel.inverseCSquared - 1.0);
         final double checkBelow = 4.0 * (testPointFixedYBelowPlane.D * testPointFixedYBelowPlane.D * planetModel.inverseAbSquared + fixedZBelowPlane.D * fixedZBelowPlane.D * planetModel.inverseCSquared - 1.0);
         if (checkAbove < Vector.MINIMUM_RESOLUTION_SQUARED && checkBelow < Vector.MINIMUM_RESOLUTION_SQUARED) {
           //System.out.println(""  Looking for intersections between travel and test point planes..."");
           final GeoPoint[] ZIntersectionsY = travelPlaneFixedZ.findIntersections(planetModel, testPointFixedYPlane);
           for (final GeoPoint p : ZIntersectionsY) {
             // Travel would be in XY plane (fixed z) then in XZ (fixed y)
             //final double newDistance = p.arcDistance(testPoint) + p.arcDistance(thePoint);
             final double tpDelta1 = testPoint.x - p.x;
             final double tpDelta2 = testPoint.z - p.z;
             final double cpDelta1 = y - p.y;
             final double cpDelta2 = x - p.x;
             final double newDistance = tpDelta1 * tpDelta1 + tpDelta2 * tpDelta2 + cpDelta1 * cpDelta1 + cpDelta2 * cpDelta2;
             //final double newDistance = (testPoint.x - p.x) * (testPoint.x - p.x) + (testPoint.z - p.z) * (testPoint.z - p.z)  + (thePoint.y - p.y) * (thePoint.y - p.y) + (thePoint.x - p.x) * (thePoint.x - p.x);
             //final double newDistance = Math.abs(testPoint.z - p.z) + Math.abs(thePoint.y - p.y);
             traversalStrategies.add(new TraversalStrategy(newDistance, testPoint.y, z,
               testPointFixedYPlane, testPointFixedYAbovePlane, testPointFixedYBelowPlane,
               travelPlaneFixedZ, fixedZAbovePlane, fixedZBelowPlane,
               yTree, zTree, p));
           }
         }
       }
 
       Collections.sort(traversalStrategies);
       
       if (traversalStrategies.size() == 0) {
         throw new IllegalArgumentException(""No dual-plane travel strategies were found"");
       }
 
       // Loop through travel strategies, in order, until we find one that works.
       for (final TraversalStrategy ts : traversalStrategies) {
         try {
           return ts.apply(testPoint, testPointInSet, x, y, z);
         } catch (IllegalArgumentException e) {
           // Continue
         }
       }
       
       throw new IllegalArgumentException(""Exhausted all traversal strategies"");
     }
   }",Buggy,"LUCENE-8337: Fix problems with how travel planes too close to edge of world are disallowed, and increase the size of the disallowed window by an order of magnitude.
",Buggy,"The commit message indicates a fix for problems related to travel planes near the edge of the world and an increase in the disallowed window size. The code diff modifies the conditions under which travel planes are disallowed. Specifically, it changes the logic for determining if a plane is too close to the edge of the world. The original code used `fixedYAbovePlane.D - planetModel.getMaximumYValue() > NEAR_EDGE_CUTOFF` and `planetModel.getMinimumYValue() - fixedYAbovePlane.D > NEAR_EDGE_CUTOFF`. The corrected code uses `-fixedYAbovePlane.D - planetModel.getMaximumYValue() > NEAR_EDGE_CUTOFF` and `planetModel.getMinimumYValue() + fixedYAbovePlane.D > NEAR_EDGE_CUTOFF`. This change in sign likely addresses an issue where the planes were not being correctly identified as being too close to the edge, leading to incorrect behavior. The commit message and the code changes are consistent, and the changes address a bug related to the disallowed window. The answer is **Buggy**."
lucene-solr,28702.json,ec788948a64955acc0415281f353d4d7b2f797cc,"@@ -1,132 +1,134 @@
   public SeekStatus scanToTermNonLeaf(BytesRef target, boolean exactOnly) throws IOException {
 
     // if (DEBUG) System.out.println(""    scanToTermNonLeaf: block fp="" + fp + "" prefix="" + prefix + "" nextEnt="" + nextEnt + "" (of "" + entCount + "") target="" + OrdsSegmentTermsEnum.brToString(target) + "" term="" + OrdsSegmentTermsEnum.brToString(ste.term));
 
     assert nextEnt != -1;
 
     if (nextEnt == entCount) {
       if (exactOnly) {
         fillTerm();
         ste.termExists = subCode == 0;
       }
       return SeekStatus.END;
     }
 
     assert prefixMatches(target);
 
     // Loop over each entry (term or sub-block) in this block:
     //nextTerm: while(nextEnt < entCount) {
     nextTerm: while (true) {
       nextEnt++;
 
       final int code = suffixesReader.readVInt();
       suffix = code >>> 1;
       // if (DEBUG) {
       //   BytesRef suffixBytesRef = new BytesRef();
       //   suffixBytesRef.bytes = suffixBytes;
       //   suffixBytesRef.offset = suffixesReader.getPosition();
       //   suffixBytesRef.length = suffix;
       //   System.out.println(""      cycle: "" + ((code&1)==1 ? ""sub-block"" : ""term"") + "" "" + (nextEnt-1) + "" (of "" + entCount + "") suffix="" + brToString(suffixBytesRef));
       // }
 
       ste.termExists = (code & 1) == 0;
       final int termLen = prefix + suffix;
       startBytePos = suffixesReader.getPosition();
       suffixesReader.skipBytes(suffix);
+      // Must save ord before we skip over a sub-block in case we push, below:
+      long prevTermOrd = termOrd;
       if (ste.termExists) {
         state.termBlockOrd++;
         termOrd++;
         subCode = 0;
       } else {
         subCode = suffixesReader.readVLong();
         termOrd += suffixesReader.readVLong();
         lastSubFP = fp - subCode;
       }
 
       final int targetLimit = target.offset + (target.length < termLen ? target.length : termLen);
       int targetPos = target.offset + prefix;
 
       // Loop over bytes in the suffix, comparing to
       // the target
       int bytePos = startBytePos;
       while(true) {
         final int cmp;
         final boolean stop;
         if (targetPos < targetLimit) {
           cmp = (suffixBytes[bytePos++]&0xFF) - (target.bytes[targetPos++]&0xFF);
           stop = false;
         } else {
           assert targetPos == targetLimit;
           cmp = termLen - target.length;
           stop = true;
         }
 
         if (cmp < 0) {
           // Current entry is still before the target;
           // keep scanning
 
           if (nextEnt == entCount) {
             if (exactOnly) {
               fillTerm();
               //termExists = true;
             }
             // We are done scanning this block
             break nextTerm;
           } else {
             continue nextTerm;
           }
         } else if (cmp > 0) {
 
           // Done!  Current entry is after target --
           // return NOT_FOUND:
           fillTerm();
 
           if (!exactOnly && !ste.termExists) {
             // We are on a sub-block, and caller wants
             // us to position to the next term after
             // the target, so we must recurse into the
             // sub-frame(s):
-            ste.currentFrame = ste.pushFrame(null, ste.currentFrame.lastSubFP, termLen, ste.currentFrame.termOrd);
+            ste.currentFrame = ste.pushFrame(null, ste.currentFrame.lastSubFP, termLen, prevTermOrd);
             ste.currentFrame.loadBlock();
             while (ste.currentFrame.next()) {
-              ste.currentFrame = ste.pushFrame(null, ste.currentFrame.lastSubFP, ste.term.length, ste.currentFrame.termOrd);
+              ste.currentFrame = ste.pushFrame(null, ste.currentFrame.lastSubFP, ste.term.length, prevTermOrd);
               ste.currentFrame.loadBlock();
             }
           }
                 
           //if (DEBUG) System.out.println(""        not found"");
           return SeekStatus.NOT_FOUND;
         } else if (stop) {
           // Exact match!
 
           // This cannot be a sub-block because we
           // would have followed the index to this
           // sub-block from the start:
 
           assert ste.termExists;
           fillTerm();
           //if (DEBUG) System.out.println(""        found!"");
           return SeekStatus.FOUND;
         }
       }
     }
 
     // It is possible (and OK) that terms index pointed us
     // at this block, but, we scanned the entire block and
     // did not find the term to position to.  This happens
     // when the target is after the last term in the block
     // (but, before the next term in the index).  EG
     // target could be foozzz, and terms index pointed us
     // to the foo* block, but the last term in this block
     // was fooz (and, eg, first term in the next block will
     // bee fop).
     //if (DEBUG) System.out.println(""      block end"");
     if (exactOnly) {
       fillTerm();
     }
 
     // TODO: not consistent that in the
     // not-exact case we don't next() into the next
     // frame here
     return SeekStatus.END;
   }",Buggy,"LUCENE-5819: fix ord bug; add test case; remove dead code

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1612217 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix (""fix ord bug"") and the diff contains changes related to `termOrd` and `prevTermOrd`. Specifically, the code now saves `termOrd` to `prevTermOrd` *before* skipping over a sub-block. Then, when pushing a new frame, `prevTermOrd` is used instead of `ste.currentFrame.termOrd`. This suggests that the previous value of `termOrd` was not being correctly preserved or used when recursing into sub-blocks, leading to incorrect ordinal values. The answer is **Buggy**."
lucene-solr,42320.json,ec788948a64955acc0415281f353d4d7b2f797cc,"@@ -1,123 +1,110 @@
   public SeekStatus scanToTermLeaf(BytesRef target, boolean exactOnly) throws IOException {
 
     // if (DEBUG) System.out.println(""    scanToTermLeaf: block fp="" + fp + "" prefix="" + prefix + "" nextEnt="" + nextEnt + "" (of "" + entCount + "") target="" + brToString(target) + "" term="" + brToString(term));
 
     assert nextEnt != -1;
 
     ste.termExists = true;
     subCode = 0;
 
     if (nextEnt == entCount) {
       if (exactOnly) {
         fillTerm();
       }
       return SeekStatus.END;
     }
 
     assert prefixMatches(target);
 
     // Loop over each entry (term or sub-block) in this block:
     //nextTerm: while(nextEnt < entCount) {
     nextTerm: while (true) {
       nextEnt++;
 
       suffix = suffixesReader.readVInt();
 
       // if (DEBUG) {
       //   BytesRef suffixBytesRef = new BytesRef();
       //   suffixBytesRef.bytes = suffixBytes;
       //   suffixBytesRef.offset = suffixesReader.getPosition();
       //   suffixBytesRef.length = suffix;
       //   System.out.println(""      cycle: term "" + (nextEnt-1) + "" (of "" + entCount + "") suffix="" + brToString(suffixBytesRef));
       // }
 
       final int termLen = prefix + suffix;
       startBytePos = suffixesReader.getPosition();
       suffixesReader.skipBytes(suffix);
 
       final int targetLimit = target.offset + (target.length < termLen ? target.length : termLen);
       int targetPos = target.offset + prefix;
 
       // Loop over bytes in the suffix, comparing to
       // the target
       int bytePos = startBytePos;
       while(true) {
         final int cmp;
         final boolean stop;
         if (targetPos < targetLimit) {
           cmp = (suffixBytes[bytePos++]&0xFF) - (target.bytes[targetPos++]&0xFF);
           stop = false;
         } else {
           assert targetPos == targetLimit;
           cmp = termLen - target.length;
           stop = true;
         }
 
         if (cmp < 0) {
           // Current entry is still before the target;
           // keep scanning
 
           if (nextEnt == entCount) {
             if (exactOnly) {
               fillTerm();
             }
             // We are done scanning this block
             break nextTerm;
           } else {
             continue nextTerm;
           }
         } else if (cmp > 0) {
 
           // Done!  Current entry is after target --
           // return NOT_FOUND:
           fillTerm();
 
-          if (!exactOnly && !ste.termExists) {
-            // We are on a sub-block, and caller wants
-            // us to position to the next term after
-            // the target, so we must recurse into the
-            // sub-frame(s):
-            ste.currentFrame = ste.pushFrame(null, ste.currentFrame.lastSubFP, termLen);
-            ste.currentFrame.loadBlock();
-            while (ste.currentFrame.next()) {
-              ste.currentFrame = ste.pushFrame(null, ste.currentFrame.lastSubFP, ste.term.length);
-              ste.currentFrame.loadBlock();
-            }
-          }
-                
           //if (DEBUG) System.out.println(""        not found"");
           return SeekStatus.NOT_FOUND;
         } else if (stop) {
           // Exact match!
 
           // This cannot be a sub-block because we
           // would have followed the index to this
           // sub-block from the start:
 
           assert ste.termExists;
           fillTerm();
           //if (DEBUG) System.out.println(""        found!"");
           return SeekStatus.FOUND;
         }
       }
     }
 
     // It is possible (and OK) that terms index pointed us
     // at this block, but, we scanned the entire block and
     // did not find the term to position to.  This happens
     // when the target is after the last term in the block
     // (but, before the next term in the index).  EG
     // target could be foozzz, and terms index pointed us
     // to the foo* block, but the last term in this block
     // was fooz (and, eg, first term in the next block will
     // bee fop).
     //if (DEBUG) System.out.println(""      block end"");
     if (exactOnly) {
       fillTerm();
     }
 
     // TODO: not consistent that in the
     // not-exact case we don't next() into the next
     // frame here
     return SeekStatus.END;
   }",NotBuggy,"LUCENE-5819: fix ord bug; add test case; remove dead code

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1612217 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix (""fix ord bug"") and the addition of a test case, suggesting a problem was identified and corrected. The diff focuses on the `scanToTermLeaf` method, which is responsible for finding a specific term within a block of terms. The removal of code related to sub-block handling when `exactOnly` is false suggests a potential issue with how the search was being performed in those cases. The removed code seems to be attempting to recurse into sub-frames, but this logic was flawed or unnecessary, leading to incorrect search results or behavior. The answer is **Buggy**."
lucene-solr,28701.json,ec788948a64955acc0415281f353d4d7b2f797cc,"@@ -1,124 +1,111 @@
   public SeekStatus scanToTermLeaf(BytesRef target, boolean exactOnly) throws IOException {
 
     // if (DEBUG) System.out.println(""    scanToTermLeaf: block fp="" + fp + "" prefix="" + prefix + "" nextEnt="" + nextEnt + "" (of "" + entCount + "") target="" + OrdsSegmentTermsEnum.brToString(target) + "" term="" + OrdsSegmentTermsEnum.brToString(ste.term));
 
     assert nextEnt != -1;
 
     ste.termExists = true;
     subCode = 0;
 
     if (nextEnt == entCount) {
       if (exactOnly) {
         fillTerm();
       }
       return SeekStatus.END;
     }
 
     assert prefixMatches(target);
 
     // Loop over each entry (term or sub-block) in this block:
     //nextTerm: while(nextEnt < entCount) {
     nextTerm: while (true) {
       nextEnt++;
       termOrd++;
 
       suffix = suffixesReader.readVInt();
 
       // if (DEBUG) {
       //    BytesRef suffixBytesRef = new BytesRef();
       //    suffixBytesRef.bytes = suffixBytes;
       //    suffixBytesRef.offset = suffixesReader.getPosition();
       //    suffixBytesRef.length = suffix;
       //    System.out.println(""      cycle: term "" + (nextEnt-1) + "" (of "" + entCount + "") suffix="" + OrdsSegmentTermsEnum.brToString(suffixBytesRef));
       // }
 
       final int termLen = prefix + suffix;
       startBytePos = suffixesReader.getPosition();
       suffixesReader.skipBytes(suffix);
 
       final int targetLimit = target.offset + (target.length < termLen ? target.length : termLen);
       int targetPos = target.offset + prefix;
 
       // Loop over bytes in the suffix, comparing to
       // the target
       int bytePos = startBytePos;
       while(true) {
         final int cmp;
         final boolean stop;
         if (targetPos < targetLimit) {
           cmp = (suffixBytes[bytePos++]&0xFF) - (target.bytes[targetPos++]&0xFF);
           stop = false;
         } else {
           assert targetPos == targetLimit;
           cmp = termLen - target.length;
           stop = true;
         }
 
         if (cmp < 0) {
           // Current entry is still before the target;
           // keep scanning
 
           if (nextEnt == entCount) {
             if (exactOnly) {
               fillTerm();
             }
             // We are done scanning this block
             break nextTerm;
           } else {
             continue nextTerm;
           }
         } else if (cmp > 0) {
 
           // Done!  Current entry is after target --
           // return NOT_FOUND:
           fillTerm();
 
-          if (!exactOnly && !ste.termExists) {
-            // We are on a sub-block, and caller wants
-            // us to position to the next term after
-            // the target, so we must recurse into the
-            // sub-frame(s):
-            ste.currentFrame = ste.pushFrame(null, ste.currentFrame.lastSubFP, termLen, ste.currentFrame.termOrd);
-            ste.currentFrame.loadBlock();
-            while (ste.currentFrame.next()) {
-              ste.currentFrame = ste.pushFrame(null, ste.currentFrame.lastSubFP, ste.term.length, ste.currentFrame.termOrd);
-              ste.currentFrame.loadBlock();
-            }
-          }
-                
           //if (DEBUG) System.out.println(""        not found"");
           return SeekStatus.NOT_FOUND;
         } else if (stop) {
           // Exact match!
 
           // This cannot be a sub-block because we
           // would have followed the index to this
           // sub-block from the start:
 
           assert ste.termExists;
           fillTerm();
           //if (DEBUG) System.out.println(""        found!"");
           return SeekStatus.FOUND;
         }
       }
     }
 
     // It is possible (and OK) that terms index pointed us
     // at this block, but, we scanned the entire block and
     // did not find the term to position to.  This happens
     // when the target is after the last term in the block
     // (but, before the next term in the index).  EG
     // target could be foozzz, and terms index pointed us
     // to the foo* block, but the last term in this block
     // was fooz (and, eg, first term in the next block will
     // bee fop).
     //if (DEBUG) System.out.println(""      block end"");
     if (exactOnly) {
       fillTerm();
     }
 
     // TODO: not consistent that in the
     // not-exact case we don't next() into the next
     // frame here
     return SeekStatus.END;
   }",NotBuggy,"LUCENE-5819: fix ord bug; add test case; remove dead code

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1612217 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message ""LUCENE-5819: fix ord bug; add test case; remove dead code"" indicates a bug fix related to term ordinals (""ord bug""). The diff removes code related to positioning to the next term after the target when not performing an exact seek (`!exactOnly`). This suggests that the removed code was either incorrect or unnecessary for the intended behavior, and its removal constitutes a bug fix. The answer is **Buggy**."
lucene-solr,20313.json,766b017b1a356f57a5eb6e73bd70e67f34534013,"@@ -1,11 +1,6 @@
-  private IndexSchema initSchema(SolrConfig config, IndexSchema schema) {
+  private void initSchema(SolrConfig config, IndexSchema schema) {
     if (schema == null) {
       schema = IndexSchemaFactory.buildIndexSchema(IndexSchema.DEFAULT_SCHEMA_FILE, config);
     }
-    final SimilarityFactory similarityFactory = schema.getSimilarityFactory();
-    if (similarityFactory instanceof SolrCoreAware) {
-      // Similarity needs SolrCore before inform() is called on all registered SolrCoreAware listeners below
-      ((SolrCoreAware) similarityFactory).inform(this);
-    }
-    return schema;
+    setLatestSchema(schema);
   }",Buggy,"SOLR-8280: Fixed bug in SimilarityFactory initialization that prevented SolrCoreAware factories from functioning properly with managed schema features

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1715215 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix in `SimilarityFactory` initialization, specifically related to `SolrCoreAware` factories and managed schema features. The diff removes the initialization of `SimilarityFactory` and the `SolrCoreAware` check from the `initSchema` method. This suggests that the original location of this initialization was problematic, causing issues with how `SolrCoreAware` factories interacted with the managed schema. By removing this initialization from `initSchema`, the bug is likely resolved by initializing the `SimilarityFactory` in a more appropriate location. The answer is **Buggy**."
lucene-solr,12746.json,766b017b1a356f57a5eb6e73bd70e67f34534013,"@@ -1,4 +1,6 @@
   public Similarity getSimilarity() {
-    assert core != null : ""inform must be called first"";
+    if (null == core) {
+      throw new IllegalStateException(""SchemaSimilarityFactory can not be used until SolrCoreAware.inform has been called"");
+    }
     return similarity;
   }",NotBuggy,"SOLR-8280: Fixed bug in SimilarityFactory initialization that prevented SolrCoreAware factories from functioning properly with managed schema features

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1715215 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix related to `SimilarityFactory` initialization, specifically affecting `SolrCoreAware` factories when used with managed schema features. The diff adds a check to ensure that the `core` field is not null before returning the similarity. If `core` is null, it throws an `IllegalStateException` with a message indicating that `SolrCoreAware.inform` must be called first. This change prevents the code from proceeding with a null `core`, which would likely lead to a `NullPointerException` or other unexpected behavior. This is a bug fix. The answer is **Buggy**."
lucene-solr,43373.json,9952af099ae65f051056fc8ff55c8e8f4cfb3b93,"@@ -1,18 +1,16 @@
   private static final boolean isIntersectingPolygon(final Node start, final double x0, final double y0,
                                                      final double x1, final double y1) {
     Node node = start;
     Node nextNode;
     do {
       nextNode = node.next;
-      if(node.getX() != x0 && node.getY() != y0 && nextNode.getX() != x0
-          && nextNode.getY() != y0 && node.getX() != x1 && node.getY() != y1
-          && nextNode.getX() != x1 && nextNode.getY() != y1) {
+      if(isVertexEquals(node, x0, y0) == false && isVertexEquals(node, x1, y1) == false) {
         if (linesIntersect(node.getX(), node.getY(), nextNode.getX(), nextNode.getY(), x0, y0, x1, y1)) {
           return true;
         }
       }
       node = nextNode;
     } while (node != start);
 
     return false;
   }",Buggy,"LUCENE-8559: Fix bug where polygon edges were skipped when checking for intersections
",Buggy,"The commit message indicates a bug fix related to polygon intersection checks. The diff modifies the `isIntersectingPolygon` method. The original code had a complex conditional statement that checked if the node's coordinates were different from the start and end points of the line segment. This could lead to skipping edges when checking for intersections. The modified code replaces this complex conditional with a call to `isVertexEquals`, which likely provides a more accurate and robust way to determine if a vertex is equal to a given point. This change addresses the bug described in the commit message. The answer is **Buggy**."
lucene-solr,43381.json,9952af099ae65f051056fc8ff55c8e8f4cfb3b93,"@@ -1,3 +1,3 @@
   private static final boolean isVertexEquals(final Node a, final Node b) {
-    return a.getX() == b.getX() && a.getY() == b.getY();
+    return isVertexEquals(a, b.getX(), b.getY());
   }",NotBuggy,"LUCENE-8559: Fix bug where polygon edges were skipped when checking for intersections
",Buggy,"The commit message indicates a bug fix related to polygon edge intersection checks. The diff replaces a direct comparison of `Node` objects' x and y coordinates with a call to `isVertexEquals(a, b.getX(), b.getY())`. This suggests that the original implementation might have been flawed, potentially leading to incorrect intersection detection. The change likely addresses a logical error in how vertices were compared, which could have caused edges to be skipped during intersection checks. The answer is **Buggy**.
"
lucene-solr,43382.json,9952af099ae65f051056fc8ff55c8e8f4cfb3b93,"@@ -1,3 +1,3 @@
-  private static final boolean isVertexEquals(final Node a, final Node b) {
-    return a.getX() == b.getX() && a.getY() == b.getY();
+  private static final boolean isVertexEquals(final Node a, final double x, final  double y) {
+    return a.getX() == x && a.getY() == y;
   }",NotBuggy,"LUCENE-8559: Fix bug where polygon edges were skipped when checking for intersections
",Buggy,"The commit message indicates a bug fix related to polygon edge intersection checks in Lucene. The diff modifies the `isVertexEquals` method, changing its parameters from two `Node` objects to a `Node` object and two `double` values representing x and y coordinates. This suggests that the original method might have been incorrectly comparing vertices, leading to edges being skipped during intersection checks. The change allows for comparing a vertex with specific coordinates, potentially fixing the bug described in the commit message. The answer is **Buggy**.
"
lucene-solr,38533.json,b33d7176aa3624df2de1708b17919f20d034872f,"@@ -1,15 +1,16 @@
-  private static void intersectInterval(double heatMin, double heatMax, double heatCellLen, int heatLen,
+  private static void intersectInterval(double heatMin, double heatMax, double heatCellLen, int numCells,
                                         double cellMin, double cellMax,
                                         int[] out) {
+    assert heatMin < heatMax && cellMin < cellMax;
     //precondition: we know there's an intersection
     if (heatMin >= cellMin) {
       out[0] = 0;
     } else {
       out[0] = (int) Math.round((cellMin - heatMin) / heatCellLen);
     }
     if (heatMax <= cellMax) {
-      out[1] = heatLen - 1;
+      out[1] = numCells - 1;
     } else {
       out[1] = (int) Math.round((cellMax - heatMin) / heatCellLen) - 1;
     }
   }",NotBuggy,"LUCENE-7291: Fix spatial HeatmapFacetCounter bug with dateline and large non-point shapes
",Buggy,"The commit message indicates a bug fix in the spatial HeatmapFacetCounter related to dateline and large non-point shapes. The code diff modifies the `intersectInterval` method, specifically changing the parameter `heatLen` to `numCells` and adding an assertion `assert heatMin < heatMax && cellMin < cellMax;`. The change from `heatLen` to `numCells` suggests that the original code was using the wrong length for calculations, potentially leading to incorrect intersection results, especially when dealing with datelines or large shapes that span multiple heatmap cells. The added assertion confirms that the input intervals are valid, which could have been a source of errors. These changes directly address the bug mentioned in the commit message. The answer is **Buggy**."
lucene-solr,38532.json,b33d7176aa3624df2de1708b17919f20d034872f,"@@ -1,152 +1,153 @@
   public static Heatmap calcFacets(PrefixTreeStrategy strategy, IndexReaderContext context, Bits topAcceptDocs,
                                    Shape inputShape, final int facetLevel, int maxCells) throws IOException {
     if (maxCells > (MAX_ROWS_OR_COLUMNS * MAX_ROWS_OR_COLUMNS)) {
       throw new IllegalArgumentException(""maxCells ("" + maxCells + "") should be <= "" + MAX_ROWS_OR_COLUMNS);
     }
     if (inputShape == null) {
       inputShape = strategy.getSpatialContext().getWorldBounds();
     }
     final Rectangle inputRect = inputShape.getBoundingBox();
     //First get the rect of the cell at the bottom-left at depth facetLevel
     final SpatialPrefixTree grid = strategy.getGrid();
     final SpatialContext ctx = grid.getSpatialContext();
     final Point cornerPt = ctx.makePoint(inputRect.getMinX(), inputRect.getMinY());
     final CellIterator cellIterator = grid.getTreeCellIterator(cornerPt, facetLevel);
     Cell cornerCell = null;
     while (cellIterator.hasNext()) {
       cornerCell = cellIterator.next();
     }
     assert cornerCell != null && cornerCell.getLevel() == facetLevel : ""Cell not at target level: "" + cornerCell;
     final Rectangle cornerRect = (Rectangle) cornerCell.getShape();
     assert cornerRect.hasArea();
     //Now calculate the number of columns and rows necessary to cover the inputRect
     double heatMinX = cornerRect.getMinX();//note: we might change this below...
     final double cellWidth = cornerRect.getWidth();
     final Rectangle worldRect = ctx.getWorldBounds();
     final int columns = calcRowsOrCols(cellWidth, heatMinX, inputRect.getWidth(), inputRect.getMinX(), worldRect.getWidth());
     final double heatMinY = cornerRect.getMinY();
     final double cellHeight = cornerRect.getHeight();
     final int rows = calcRowsOrCols(cellHeight, heatMinY, inputRect.getHeight(), inputRect.getMinY(), worldRect.getHeight());
     assert rows > 0 && columns > 0;
     if (columns > MAX_ROWS_OR_COLUMNS || rows > MAX_ROWS_OR_COLUMNS || columns * rows > maxCells) {
       throw new IllegalArgumentException(
           ""Too many cells ("" + columns + "" x "" + rows + "") for level "" + facetLevel + "" shape "" + inputRect);
     }
 
     //Create resulting heatmap bounding rectangle & Heatmap object.
     final double halfCellWidth = cellWidth / 2.0;
     // if X world-wraps, use world bounds' range
     if (columns * cellWidth + halfCellWidth > worldRect.getWidth()) {
       heatMinX = worldRect.getMinX();
     }
     double heatMaxX = heatMinX + columns * cellWidth;
     if (Math.abs(heatMaxX - worldRect.getMaxX()) < halfCellWidth) {//numeric conditioning issue
       heatMaxX = worldRect.getMaxX();
     } else if (heatMaxX > worldRect.getMaxX()) {//wraps dateline (won't happen if !geo)
       heatMaxX = heatMaxX - worldRect.getMaxX() +  worldRect.getMinX();
     }
     final double halfCellHeight = cellHeight / 2.0;
     double heatMaxY = heatMinY + rows * cellHeight;
     if (Math.abs(heatMaxY - worldRect.getMaxY()) < halfCellHeight) {//numeric conditioning issue
       heatMaxY = worldRect.getMaxY();
     }
 
     final Heatmap heatmap = new Heatmap(columns, rows, ctx.makeRectangle(heatMinX, heatMaxX, heatMinY, heatMaxY));
 
     //All ancestor cell counts (of facetLevel) will be captured during facet visiting and applied later. If the data is
     // just points then there won't be any ancestors.
     //Facet count of ancestors covering all of the heatmap:
     int[] allCellsAncestorCount = new int[1]; // single-element array so it can be accumulated in the inner class
     //All other ancestors:
     Map<Rectangle,Integer> ancestors = new HashMap<>();
 
     //Now lets count some facets!
     PrefixTreeFacetCounter.compute(strategy, context, topAcceptDocs, inputShape, facetLevel,
         new PrefixTreeFacetCounter.FacetVisitor() {
       @Override
       public void visit(Cell cell, int count) {
         final double heatMinX = heatmap.region.getMinX();
         final Rectangle rect = (Rectangle) cell.getShape();
         if (cell.getLevel() == facetLevel) {//heatmap level; count it directly
           //convert to col & row
           int column;
           if (rect.getMinX() >= heatMinX) {
             column = (int) Math.round((rect.getMinX() - heatMinX) / cellWidth);
           } else { // due to dateline wrap
             column = (int) Math.round((rect.getMinX() + 360 - heatMinX) / cellWidth);
           }
           int row = (int) Math.round((rect.getMinY() - heatMinY) / cellHeight);
           //note: unfortunately, it's possible for us to visit adjacent cells to the heatmap (if the SpatialPrefixTree
           // allows adjacent cells to overlap on the seam), so we need to skip them
           if (column < 0 || column >= heatmap.columns || row < 0 || row >= heatmap.rows) {
             return;
           }
           // increment
           heatmap.counts[column * heatmap.rows + row] += count;
 
         } else if (rect.relate(heatmap.region) == SpatialRelation.CONTAINS) {//containing ancestor
           allCellsAncestorCount[0] += count;
 
         } else { // ancestor
           // note: not particularly efficient (possible put twice, and Integer wrapper); oh well
           Integer existingCount = ancestors.put(rect, count);
           if (existingCount != null) {
             ancestors.put(rect, count + existingCount);
           }
         }
       }
     });
 
     //Update the heatmap counts with ancestor counts
 
     // Apply allCellsAncestorCount
     if (allCellsAncestorCount[0] > 0) {
       for (int i = 0; i < heatmap.counts.length; i++) {
         heatmap.counts[i] += allCellsAncestorCount[0];
       }
     }
 
     // Apply ancestors
     //  note: This approach isn't optimized for a ton of ancestor cells. We'll potentially increment the same cells
     //    multiple times in separate passes if any ancestors overlap. IF this poses a problem, we could optimize it
     //    with additional complication by keeping track of intervals in a sorted tree structure (possible TreeMap/Set)
     //    and iterate them cleverly such that we just make one pass at this stage.
 
     int[] pair = new int[2];//output of intersectInterval
     for (Map.Entry<Rectangle, Integer> entry : ancestors.entrySet()) {
-      Rectangle rect = entry.getKey();
+      Rectangle rect = entry.getKey(); // from a cell (thus doesn't cross DL)
       final int count = entry.getValue();
+
       //note: we approach this in a way that eliminates int overflow/underflow (think huge cell, tiny heatmap)
       intersectInterval(heatMinY, heatMaxY, cellHeight, rows, rect.getMinY(), rect.getMaxY(), pair);
       final int startRow = pair[0];
       final int endRow = pair[1];
 
       if (!heatmap.region.getCrossesDateLine()) {
         intersectInterval(heatMinX, heatMaxX, cellWidth, columns, rect.getMinX(), rect.getMaxX(), pair);
         final int startCol = pair[0];
         final int endCol = pair[1];
         incrementRange(heatmap, startCol, endCol, startRow, endRow, count);
 
       } else {
+        // note: the cell rect might intersect 2 disjoint parts of the heatmap, so we do the left & right separately
+        final int leftColumns = (int) Math.round((180 - heatMinX) / cellWidth);
+        final int rightColumns = heatmap.columns - leftColumns;
         //left half of dateline:
-        if (rect.getMaxX() >= heatMinX) {
-          final int leftColumns = (int) Math.round((180 - heatMinX) / cellWidth) + 1;
+        if (rect.getMaxX() > heatMinX) {
           intersectInterval(heatMinX, 180, cellWidth, leftColumns, rect.getMinX(), rect.getMaxX(), pair);
           final int startCol = pair[0];
           final int endCol = pair[1];
           incrementRange(heatmap, startCol, endCol, startRow, endRow, count);
         }
         //right half of dateline
-        if (rect.getMinY() <= heatMaxX) {
-          final int rightColumns = (int) Math.round(heatMaxX / cellWidth) + 1;
-          intersectInterval(0, heatMaxX, cellWidth, rightColumns, rect.getMinX(), rect.getMaxX(), pair);
-          final int startCol = pair[0];
-          final int endCol = pair[1];
+        if (rect.getMinX() < heatMaxX) {
+          intersectInterval(-180, heatMaxX, cellWidth, rightColumns, rect.getMinX(), rect.getMaxX(), pair);
+          final int startCol = pair[0] + leftColumns;
+          final int endCol = pair[1] + leftColumns;
           incrementRange(heatmap, startCol, endCol, startRow, endRow, count);
         }
       }
-
     }
 
     return heatmap;
   }",Buggy,"LUCENE-7291: Fix spatial HeatmapFacetCounter bug with dateline and large non-point shapes
",Buggy,"The commit message indicates a bug fix in the `HeatmapFacetCounter` related to dateline handling and large non-point shapes. The diff modifies the `calcFacets` method, specifically addressing how ancestor cell counts are applied to the heatmap when the heatmap region crosses the dateline. The original code had issues with calculating the correct column indices for cells intersecting the dateline, leading to incorrect facet counts. The updated code includes logic to handle the left and right halves of the dateline separately, ensuring accurate counting. This change directly addresses the bug described in the commit message. The answer is **Buggy**."
lucene-solr,36488.json,75b0b5312c6f7722b88088fbc590259e9cd31567,"@@ -1,42 +1,34 @@
   private void count(ValueSource valueSource, List<MatchingDocs> matchingDocs) throws IOException {
 
     DoubleRange[] ranges = (DoubleRange[]) this.ranges;
 
     LongRange[] longRanges = new LongRange[ranges.length];
     for(int i=0;i<ranges.length;i++) {
       DoubleRange range = ranges[i];
       longRanges[i] =  new LongRange(range.label,
                                      NumericUtils.doubleToSortableLong(range.minIncl), true,
                                      NumericUtils.doubleToSortableLong(range.maxIncl), true);
     }
 
     LongRangeCounter counter = new LongRangeCounter(longRanges);
 
-    // Compute min & max over all ranges:
-    double minIncl = Double.POSITIVE_INFINITY;
-    double maxIncl = Double.NEGATIVE_INFINITY;
-    for(DoubleRange range : ranges) {
-      minIncl = Math.min(minIncl, range.minIncl);
-      maxIncl = Math.max(maxIncl, range.maxIncl);
-    }
-
     int missingCount = 0;
     for (MatchingDocs hits : matchingDocs) {
       FunctionValues fv = valueSource.getValues(Collections.emptyMap(), hits.context);
       final int length = hits.bits.length();
       int doc = 0;
       totCount += hits.totalHits;
       while (doc < length && (doc = hits.bits.nextSetBit(doc)) != -1) {
         // Skip missing docs:
         if (fv.exists(doc)) {
           counter.add(NumericUtils.doubleToSortableLong(fv.doubleVal(doc)));
         } else {
           missingCount++;
         }
         doc++;
       }
     }
 
     missingCount += counter.fillCounts(counts);
     totCount -= missingCount;
   }",NotBuggy,"fix test bug; remove dead code

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1555715 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for a test bug and removal of dead code. The diff removes a block of code that calculates the min and max values over all ranges. This calculation is not used anywhere else in the function. Removing unused code can be considered a bug fix, as it improves code maintainability and reduces potential confusion. The answer is **Buggy**."
lucene-solr,36489.json,75b0b5312c6f7722b88088fbc590259e9cd31567,"@@ -1,39 +1,31 @@
   private void count(ValueSource valueSource, List<MatchingDocs> matchingDocs) throws IOException {
 
     LongRange[] ranges = (LongRange[]) this.ranges;
 
-    // Compute min & max over all ranges:
-    long minIncl = Long.MAX_VALUE;
-    long maxIncl = Long.MIN_VALUE;
-    for(LongRange range : ranges) {
-      minIncl = Math.min(minIncl, range.minIncl);
-      maxIncl = Math.max(maxIncl, range.maxIncl);
-    }
-
     LongRangeCounter counter = new LongRangeCounter(ranges);
 
     int missingCount = 0;
     for (MatchingDocs hits : matchingDocs) {
       FunctionValues fv = valueSource.getValues(Collections.emptyMap(), hits.context);
       final int length = hits.bits.length();
       int doc = 0;
       totCount += hits.totalHits;
       while (doc < length && (doc = hits.bits.nextSetBit(doc)) != -1) {
         // Skip missing docs:
         if (fv.exists(doc)) {
           counter.add(fv.longVal(doc));
         } else {
           missingCount++;
         }
 
         doc++;
       }
     }
     
     int x = counter.fillCounts(counts);
 
     missingCount += x;
 
     //System.out.println(""totCount "" + totCount + "" missingCount "" + counter.missingCount);
     totCount -= missingCount;
   }",NotBuggy,"fix test bug; remove dead code

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1555715 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix and removal of dead code. The diff removes the computation of `minIncl` and `maxIncl` which are not used. The `missingCount += x` line seems suspicious. `x` is the return value of `counter.fillCounts(counts)` which is added to `missingCount` and then subtracted from `totCount`. This suggests that `fillCounts` might be counting some documents as missing, and this count needs to be reflected in the total count. The removal of unused code and the adjustment of `missingCount` and `totCount` suggest a correction in how the counts are being calculated, indicating a bug fix. The answer is **Buggy**."
lucene-solr,46441.json,e283271aaf6da3033156f36b421d3241b5499d4e,"@@ -1,45 +1,51 @@
   private DocMap[] buildDocMaps(List<CodecReader> readers, Sort indexSort) throws IOException {
 
     int numReaders = readers.size();
 
     if (indexSort == null) {
       // no index sort ... we only must map around deletions, and rebase to the merged segment's docID space
 
       int totalDocs = 0;
       DocMap[] docMaps = new DocMap[numReaders];
 
       // Remap docIDs around deletions:
       for (int i = 0; i < numReaders; i++) {
         LeafReader reader = readers.get(i);
         Bits liveDocs = reader.getLiveDocs();
 
         final PackedLongValues delDocMap;
         if (liveDocs != null) {
           delDocMap = removeDeletes(reader.maxDoc(), liveDocs);
         } else {
           delDocMap = null;
         }
 
         final int docBase = totalDocs;
         docMaps[i] = new DocMap() {
           @Override
           public int get(int docID) {
             if (liveDocs == null) {
               return docBase + docID;
             } else if (liveDocs.get(docID)) {
               return docBase + (int) delDocMap.get(docID);
             } else {
               return -1;
             }
           }
         };
         totalDocs += reader.numDocs();
       }
 
       return docMaps;
 
     } else {
       // do a merge sort of the incoming leaves:
-      return MultiSorter.sort(indexSort, readers);
+      long t0 = System.nanoTime();
+      DocMap[] result = MultiSorter.sort(indexSort, readers);
+      long t1 = System.nanoTime();
+      if (infoStream.isEnabled(""SM"")) {
+        infoStream.message(""SM"", String.format(Locale.ROOT, ""%.2f msec to build merge sorted DocMaps"", (t1-t0)/1000000.0));
+      }
+      return result;
     }
   }",NotBuggy,"LUCENE-6766: more IW.infoStream logging around sorting; fix test bug
",Buggy,"The commit message indicates a bug fix and also mentions logging improvements. The diff adds logging around the `MultiSorter.sort` method call, specifically measuring the time taken to build merge-sorted DocMaps. This aligns with the ""more IW.infoStream logging around sorting"" part of the commit message. While the diff doesn't directly show the bug fix, the commit message explicitly states ""fix test bug,"" implying a bug was addressed in the commit. Therefore, the answer is **Buggy**."
lucene-solr,46444.json,e283271aaf6da3033156f36b421d3241b5499d4e,"@@ -1,61 +1,65 @@
   private List<CodecReader> maybeSortReaders(List<CodecReader> originalReaders, SegmentInfo segmentInfo) throws IOException {
 
     // Default to identity:
     for(int i=0;i<originalReaders.size();i++) {
       leafDocMaps[i] = new DocMap() {
           @Override
           public int get(int docID) {
             return docID;
           }
         };
     }
 
     Sort indexSort = segmentInfo.getIndexSort();
     if (indexSort == null) {
       return originalReaders;
     }
 
     // If an incoming reader is not sorted, because it was flushed by IW, we sort it here:
     final Sorter sorter = new Sorter(indexSort);
     List<CodecReader> readers = new ArrayList<>(originalReaders.size());
 
     for (CodecReader leaf : originalReaders) {
       Sort segmentSort = leaf.getIndexSort();
 
       if (segmentSort == null) {
         // TODO: fix IW to also sort when flushing?  It's somewhat tricky because of stored fields and term vectors, which write ""live""
         // to their index files on each indexed document:
 
         // This segment was written by flush, so documents are not yet sorted, so we sort them now:
+        long t0 = System.nanoTime();
         Sorter.DocMap sortDocMap = sorter.sort(leaf);
+        long t1 = System.nanoTime();
+        double msec = (t1-t0)/1000000.0;
+        
         if (sortDocMap != null) {
           if (infoStream.isEnabled(""SM"")) {
-            infoStream.message(""SM"", ""segment "" + leaf + "" is not sorted; wrapping for sort "" + indexSort + "" now"");
+            infoStream.message(""SM"", String.format(Locale.ROOT, ""segment %s is not sorted; wrapping for sort %s now (%.2f msec to sort)"", leaf, indexSort, msec));
           }
           leaf = SlowCodecReaderWrapper.wrap(SortingLeafReader.wrap(new MergeReaderWrapper(leaf), sortDocMap));
           leafDocMaps[readers.size()] = new DocMap() {
               @Override
               public int get(int docID) {
                 return sortDocMap.oldToNew(docID);
               }
             };
         } else {
           if (infoStream.isEnabled(""SM"")) {
-            infoStream.message(""SM"", ""segment "" + leaf + "" is not sorted, but is already accidentally in sort "" + indexSort + "" order"");
+            infoStream.message(""SM"", String.format(Locale.ROOT, ""segment %s is not sorted, but is already accidentally in sort %s order (%.2f msec to sort)"", leaf, indexSort, msec));
           }
         }
 
       } else {
         if (segmentSort.equals(indexSort) == false) {
           throw new IllegalArgumentException(""index sort mismatch: merged segment has sort="" + indexSort + "" but to-be-merged segment has sort="" + segmentSort);
         }
         if (infoStream.isEnabled(""SM"")) {
           infoStream.message(""SM"", ""segment "" + leaf + "" already sorted"");
         }
       }
 
       readers.add(leaf);
     }
 
     return readers;
   }",NotBuggy,"LUCENE-6766: more IW.infoStream logging around sorting; fix test bug
",Buggy,"The commit message indicates two things: more logging around sorting and a fix to a test bug. The diff adds timing information and logging messages to the `maybeSortReaders` method, specifically when a segment needs to be sorted. The logging messages now include the time taken to sort the segment. This added logging helps in debugging and performance analysis. The inclusion of ""fix test bug"" in the commit message suggests that the changes are related to fixing a bug, and the added logging could be part of the debugging process. The timing information helps diagnose performance issues, which can be considered a type of bug. The answer is **Buggy**."
lucene-solr,44598.json,a2a9f2a6f9cba3b27b248102d88431b8b234530e,"@@ -1,32 +1,34 @@
     public SeekStatus seekCeil(BytesRef text) {
 
       // TODO: we could instead keep the BytesRefHash
       // intact so this is a hash lookup
 
       // binary search:
       int lo = 0;
       int hi = numTerms - 1;
       while (hi >= lo) {
         int mid = (lo + hi) >>> 1;
         int textStart = postingsArray.textStarts[sortedTermIDs[mid]];
         terms.bytePool.setBytesRef(scratch, textStart);
         int cmp = scratch.compareTo(text);
         if (cmp < 0) {
           lo = mid + 1;
         } else if (cmp > 0) {
           hi = mid - 1;
         } else {
           // found:
           ord = mid;
           return SeekStatus.FOUND;
         }
       }
 
       // not found:
       ord = lo + 1;
-      if (ord == numTerms) {
+      if (ord >= numTerms) {
         return SeekStatus.END;
       } else {
+        int textStart = postingsArray.textStarts[sortedTermIDs[ord]];
+        terms.bytePool.setBytesRef(scratch, textStart);
         return SeekStatus.NOT_FOUND;
       }
     }",Buggy,"test seekCeil in pull postings API; fix trunk-only bug

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1611305 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix. The diff modifies the `seekCeil` method. Specifically, the condition `ord == numTerms` is changed to `ord >= numTerms`, and a block of code is added to set the `scratch` BytesRef when `SeekStatus.NOT_FOUND` is returned. The original code could potentially return `SeekStatus.NOT_FOUND` without properly setting the `scratch` BytesRef, leading to incorrect behavior in subsequent operations. The modified condition `ord >= numTerms` handles the case where `ord` is equal to `numTerms` and also any cases where it exceeds `numTerms`, which could occur due to an off-by-one error in the binary search logic. The added code ensures that the `scratch` BytesRef is always set before returning `SeekStatus.NOT_FOUND`. This confirms a bug fix. The answer is **Buggy**."
jna,165.json,535518713bc51e6eacefeaeaccc7033ade905a9a,"@@ -1,14 +1,15 @@
         private static long getVisualID(GraphicsConfiguration config) {
             // Use reflection to call
             // X11GraphicsConfig.getVisual
             try {
                 Object o = config.getClass()
                     .getMethod(""getVisual"", (Class[])null)
                     .invoke(config, (Object[])null);
                 return ((Number)o).longValue();
             }
             catch (Exception e) {
+                // FIXME properly handle this error
                 e.printStackTrace();
                 return -1;
             }
         }",NotBuggy,"fix mappings to properly work on 32-bit windows (fixes memory faults)
",Buggy,"The commit message indicates a fix for memory faults on 32-bit Windows, and the code change involves retrieving a visual ID using reflection. The `getVisualID` method now includes a `try-catch` block to handle potential exceptions during reflection. While the `catch` block currently only prints the stack trace and returns -1, the comment ""FIXME properly handle this error"" suggests that the exception handling is incomplete but acknowledges a potential issue. The original code likely caused memory faults due to incorrect visual ID retrieval on 32-bit Windows, which this change addresses by adding error handling. Therefore, the changes indicate a bug fix. The answer is **Buggy**."
jna,146.json,535518713bc51e6eacefeaeaccc7033ade905a9a,"@@ -1,17 +1,18 @@
-				public boolean callback(final HWND hwnd, final Pointer arg1) {
-					try {
-						final boolean visible = !onlyVisibleWindows
-								|| User32.INSTANCE.IsWindowVisible(hwnd);
-						if (visible) {
-							final String title = getWindowTitle(hwnd);
-							final String filePath = getProcessFilePath(hwnd);
-							final Rectangle locAndSize = getWindowLocationAndSize(hwnd);
-							result.add(new DesktopWindow(hwnd, title, filePath,
-									locAndSize));
-						}
-					} catch (final Exception e) {
-						e.printStackTrace();
-					}
-
-					return true;
-				}

+                public boolean callback(final HWND hwnd, final Pointer arg1) {
+                    try {
+                        final boolean visible = !onlyVisibleWindows
+                            || User32.INSTANCE.IsWindowVisible(hwnd);
+                        if (visible) {
+                            final String title = getWindowTitle(hwnd);
+                            final String filePath = getProcessFilePath(hwnd);
+                            final Rectangle locAndSize = getWindowLocationAndSize(hwnd);
+                            result.add(new DesktopWindow(hwnd, title, filePath,
+                                                         locAndSize));
+                        }
+                    } catch (final Exception e) {
+                        // FIXME properly handle whatever error is raised
+                        e.printStackTrace();
+                    }
+                    
+                    return true;
+                }",Buggy,"fix mappings to properly work on 32-bit windows (fixes memory faults)
",Buggy,"The commit message indicates a fix for memory faults specifically on 32-bit Windows. The code diff adds a comment ""// FIXME properly handle whatever error is raised"" within the catch block of the callback function. While the code itself doesn't directly address the memory faults, the comment suggests that the original error handling was inadequate and could have contributed to the reported issues. The addition of this comment indicates that the developer has identified a potential area for improvement in error handling, which could be related to the memory faults mentioned in the commit message.

Reasoning:
1. **Commit Message:** Explicitly states a fix for memory faults on 32-bit Windows.
2. **Code Diff:** Introduces a comment indicating a need for improved error handling within a callback function.
3. **Relevance:** The improved error handling could be related to preventing or mitigating the memory faults. The original error handling was inadequate.

Conclusion: **Buggy**
"
jna,3926.json,e5958a2a159cd87a8168c801f569f5bcc7511c25,"@@ -1,86 +1,84 @@
     public Object invoke(Class returnType, Object[] inArgs, Map options) {
         // Clone the argument array to obtain a scratch space for modified
         // types/values
         Object[] args = { };
         if (inArgs != null) {
             if (inArgs.length > MAX_NARGS) {
                 throw new UnsupportedOperationException(""Maximum argument count is "" + MAX_NARGS);
             }
             args = new Object[inArgs.length];
             System.arraycopy(inArgs, 0, args, 0, args.length);
         }
 
         TypeMapper mapper = 
             (TypeMapper)options.get(Library.OPTION_TYPE_MAPPER);
         Method invokingMethod = (Method)options.get(OPTION_INVOKING_METHOD);
         for (int i=0; i < args.length; i++) {
             args[i] = convertArgument(args, i, invokingMethod, mapper);
         }
         
         Class nativeType = returnType;
         FromNativeConverter resultConverter = null;
         if (NativeMapped.class.isAssignableFrom(returnType)) {
             NativeMappedConverter tc = new NativeMappedConverter(returnType);
             resultConverter = tc;
             nativeType = tc.nativeType();
         }
         else if (mapper != null) {
             resultConverter = mapper.getFromNativeConverter(returnType);
             if (resultConverter != null) {
                 nativeType = resultConverter.nativeType();
             }
         }
 
         Object result = invoke(args, nativeType);
 
         // Convert the result to a custom value/type if appropriate
         if (resultConverter != null) {
             FromNativeContext context;
             
             if (invokingMethod != null) {
                 context = new MethodResultContext(returnType, this, inArgs, invokingMethod);
             } else {
                 context = new FunctionResultContext(returnType, this, inArgs);
             }
             result = resultConverter.fromNative(result, context);
         }
 
         // Sync all memory which might have been modified by the native call
         if (inArgs != null) {
             for (int i=0; i < inArgs.length; i++) {
                 Object arg = inArgs[i];
                 if (arg == null)
                     continue;
                 if (arg instanceof Structure) {
                     if (!(arg instanceof Structure.ByValue)) {
                         ((Structure)arg).read();
                     }
                 }
                 else if (args[i] instanceof StringArray) {
-                    // Read back arrays of String, just in case they
-                    // were modified
                     ((StringArray)args[i]).read();
                 }
                 else if (args[i] instanceof PointerArray) {
                     PointerArray array = (PointerArray)args[i];
                     array.read();
                     if (Structure.ByReference[].class.isAssignableFrom(arg.getClass())) {
                         Class type = arg.getClass().getComponentType();
                         Structure[] ss = (Structure[])arg;
                         for (int si=0;si < ss.length;si++) {
                             Pointer p = array.getPointer(Pointer.SIZE * si);
                             ss[si] = Structure.updateStructureByReference(type, ss[si], p);
                         }
                     }
                 }
                 else if (Structure[].class.isAssignableFrom(arg.getClass())) {
                     Structure[] ss = (Structure[])arg;
                     for (int si=0;si < ss.length;si++) {
                         ss[si].read();
                     }
                 }
             }
         }
                         
         return result;
     }",NotBuggy,"Fix bug in stdcall function mapping when using struct by value
Defer size_t/off_t definition, for now

git-svn-id: https://svn.java.net/svn/jna~svn/trunk@436 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80
",Buggy,"The commit message indicates a bug fix related to stdcall function mapping when using struct by value. The code changes involve synchronizing memory after a native call, specifically focusing on Structures and arrays of Structures. The diff includes logic to read back data into Structure objects and update Structure.ByReference arrays after the native call. This suggests that the original code might not have correctly handled the data passed to and returned from native functions when dealing with structures, especially when passed by value or by reference in arrays. The changes ensure that any modifications made to the structures in native code are reflected in the Java objects after the call. Therefore, the changes indicate a bug fix. The answer is **Buggy**."
jna,3554.json,e5958a2a159cd87a8168c801f569f5bcc7511c25,"@@ -1,23 +1,30 @@
-    private int getNativeSize(Class type, Object value) {
-        if (Structure.class.isAssignableFrom(type)) {
-            if (ByReference.class.isAssignableFrom(type)) {
-                return Pointer.SIZE;
-            }
-            else {
-                if (value == null)
-                    value = newInstance(type);
-                Structure s = (Structure)value;
-                return s.size();
-            }
-        }
+    private static int getNativeSize(Class type, Object value) {
         if (type.isArray()) {
             int len = Array.getLength(value);
             if (len > 0) {
                 Object o = Array.get(value, 0);
                 return len * getNativeSize(type.getComponentType(), o);
             }
             // Don't process zero-length arrays
-            throw new IllegalArgumentException(""Arrays of length zero not allowed in structure: "" + this);
+            throw new IllegalArgumentException(""Arrays of length zero not allowed in structure: "" + type);
         }
-        return getNativeSize(type);
+        // May provide this in future; problematic on read, since we can't
+        // auto-create a java.nio.Buffer w/o knowing its size
+        if (Buffer.class.isAssignableFrom(type)) {
+            throw new IllegalArgumentException(""the type \"""" + type.getName() 
+                                               + ""\"" is not supported as a structure field"");
+        }
+        if (Structure.class.isAssignableFrom(type)
+            && !Structure.ByReference.class.isAssignableFrom(type)) {
+            if (value == null)
+                value = newInstance(type);
+            return ((Structure)value).size();
+        }
+        try {
+            return Native.getNativeSize(type);
+        }
+        catch(IllegalArgumentException e) {
+            throw new IllegalArgumentException(""The type \"""" + type.getName() 
+                                               + ""\"" is not supported as a structure field"");
+        }
     }",Buggy,"Fix bug in stdcall function mapping when using struct by value
Defer size_t/off_t definition, for now

git-svn-id: https://svn.java.net/svn/jna~svn/trunk@436 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80
",Buggy,"The commit message indicates a bug fix related to struct-by-value handling in stdcall functions. The code changes in `getNativeSize` method seem to address this by adding checks and throwing exceptions for unsupported types like zero-length arrays and java.nio.Buffer. The original code didn't handle these cases correctly, potentially leading to errors or unexpected behavior. The added exception handling and type checks suggest a bug fix. The answer is **Buggy**."
jna,3652.json,4068c6da1a057942ee7eb167c0e5436b0f225761,"@@ -1,9 +1,10 @@
     Object getField(StructField structField) {
         try {
             return structField.field.get(this);
         }
         catch (Exception e) {
             throw new Error(""Exception reading field '""
-                            + structField.name + ""' in "" + getClass(), e);
+                            + structField.name + ""' in "" + getClass() 
+                            + "": "" + e);
         }
     }",NotBuggy,"Fix bug in nested struct array read/write

git-svn-id: https://svn.java.net/svn/jna~svn/trunk@445 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80
",Buggy,"The commit message indicates a bug fix related to reading/writing nested struct arrays. The code change modifies the exception handling within the `getField` method. Specifically, it appends the exception message to the error message, providing more context for debugging. This suggests that the original code might have had insufficient error reporting, making it difficult to diagnose issues when reading struct fields. The improved error message helps pinpoint the cause of the exception, which is a characteristic of bug fixes. The answer is **Buggy**."
jna,4096.json,4068c6da1a057942ee7eb167c0e5436b0f225761,"@@ -1,137 +1,147 @@
     void writeField(StructField structField) {
         // Get the offset of the field
         int offset = structField.offset;
 
         // Get the value from the field
         Object value = getField(structField);
         
         // Determine the type of the field
         Class nativeType = structField.type;
         ToNativeConverter converter = structField.writeConverter;
         if (converter != null) {
             value = converter.toNative(value, 
                     new StructureWriteContext(this, structField.field));
             // Assume any null values are pointers
             nativeType = value != null ? value.getClass() : Pointer.class;
         }
 
         // Java strings get converted to C strings, where a Pointer is used
         if (String.class == nativeType
             || WString.class == nativeType) {
 
             // Allocate a new string in memory
             boolean wide = nativeType == WString.class;
             if (value != null) {
                 NativeString nativeString = new NativeString(value.toString(), wide);
                 // Keep track of allocated C strings to avoid 
                 // premature garbage collection of the memory.
                 nativeStrings.put(structField.name, nativeString);
                 value = nativeString.getPointer();
             }
             else {
                 value = null;
             }
         }
 
         // Set the value at the offset according to its type
         if (nativeType == boolean.class || nativeType == Boolean.class) {
             memory.setInt(offset, Boolean.TRUE.equals(value) ? -1 : 0);
         }
         else if (nativeType == byte.class || nativeType == Byte.class) {
             memory.setByte(offset, ((Byte)value).byteValue());
         }
         else if (nativeType == short.class || nativeType == Short.class) {
             memory.setShort(offset, ((Short)value).shortValue());
         }
         else if (nativeType == char.class || nativeType == Character.class) {
             memory.setChar(offset, ((Character)value).charValue());
         }
         else if (nativeType == int.class || nativeType == Integer.class) {
             memory.setInt(offset, ((Integer)value).intValue());
         }
         else if (nativeType == long.class || nativeType == Long.class) {
             memory.setLong(offset, ((Long)value).longValue());
         }
         else if (nativeType == float.class || nativeType == Float.class) {
             memory.setFloat(offset, ((Float)value).floatValue());
         }
         else if (nativeType == double.class || nativeType == Double.class) {
             memory.setDouble(offset, ((Double)value).doubleValue());
         }
         else if (nativeType == Pointer.class) {
             memory.setPointer(offset, (Pointer)value);
         }
         else if (nativeType == String.class) {
             memory.setPointer(offset, (Pointer)value);
         }
         else if (nativeType == WString.class) {
             memory.setPointer(offset, (Pointer)value);
         }
         else if (nativeType.isArray()) {
             Class cls = nativeType.getComponentType();
             if (cls == byte.class) {
                 byte[] buf = (byte[])value;
                 memory.write(offset, buf, 0, buf.length);
             }
             else if (cls == short.class) {
                 short[] buf = (short[])value;
                 memory.write(offset, buf, 0, buf.length);
             }
             else if (cls == char.class) {
                 char[] buf = (char[])value;
                 memory.write(offset, buf, 0, buf.length);
             }
             else if (cls == int.class) {
                 int[] buf = (int[])value;
                 memory.write(offset, buf, 0, buf.length);
             }
             else if (cls == long.class) {
                 long[] buf = (long[])value;
                 memory.write(offset, buf, 0, buf.length);
             }
             else if (cls == float.class) {
                 float[] buf = (float[])value;
                 memory.write(offset, buf, 0, buf.length);
             }
             else if (cls == double.class) {
                 double[] buf = (double[])value;
                 memory.write(offset, buf, 0, buf.length);
             }
             else if (Pointer.class.isAssignableFrom(cls)) {
                 Pointer[] buf = (Pointer[])value;
                 memory.write(offset, buf, 0, buf.length);
             }
-            else if (Structure.class.isAssignableFrom(cls)
-                     && ByReference.class.isAssignableFrom(cls)) {
+            else if (Structure.class.isAssignableFrom(cls)) {
                 Structure[] sbuf = (Structure[])value;
-                Pointer[] buf = new Pointer[sbuf.length];
-                for (int i=0;i < sbuf.length;i++) {
-                    buf[i] = sbuf[i] == null ? null : sbuf[i].getPointer();
+                if (ByReference.class.isAssignableFrom(cls)) {
+                    Pointer[] buf = new Pointer[sbuf.length];
+                    for (int i=0;i < sbuf.length;i++) {
+                        buf[i] = sbuf[i] == null ? null : sbuf[i].getPointer();
+                    }
+                    memory.write(offset, buf, 0, buf.length);
                 }
-                memory.write(offset, buf, 0, buf.length);
+                else {
+                    for (int i=0;i < sbuf.length;i++) {
+                        if (sbuf[i] == null) {
+                            sbuf[i] = newInstance(cls);
+                            sbuf[i].useMemory(memory, offset + i * sbuf[i].size());
+                        }
+                        sbuf[i].write();
+                    }
+                }
             }
             else {
                 throw new IllegalArgumentException(""Inline array of ""
                                                    + cls + "" not supported"");
             }
         }
         else if (Structure.class.isAssignableFrom(nativeType)) {
             Structure s = (Structure)value;
             if (ByReference.class.isAssignableFrom(nativeType)) {
                 memory.setPointer(offset, s == null ? null : s.getPointer());
             }
             else {
                 s.useMemory(memory, offset);
                 s.write();
             }
         }
         else if (Callback.class.isAssignableFrom(nativeType)) {
             memory.setPointer(offset, CallbackReference.getFunctionPointer((Callback)value));
         }
         else {
         	String msg = ""Structure field \"""" + structField.name
         	    + ""\"" was declared as "" + nativeType 
         	    + "", which is not supported within a Structure"";
             throw new IllegalArgumentException(msg);
         }
     }",Buggy,"Fix bug in nested struct array read/write

git-svn-id: https://svn.java.net/svn/jna~svn/trunk@445 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80
",Buggy,"The commit message indicates a bug fix related to reading/writing nested struct arrays. The diff modifies the `writeField` method in a way that suggests a fix for handling arrays of `Structure` objects, especially when nested.

The original code only handled arrays of `Structure` objects that were also `ByReference`. The modified code now differentiates between `Structure` arrays that are `ByReference` and those that are not. For `Structure` arrays that are not `ByReference`, the code iterates through the array, creates a new instance if an element is null, uses the memory at the appropriate offset, and then writes the structure. This suggests that the original code was not correctly handling arrays of nested `Structure` objects that were not passed by reference, potentially leading to incorrect data being written or read.

Therefore, the changes indicate a bug fix. The answer is **Buggy**."
jna,4092.json,4068c6da1a057942ee7eb167c0e5436b0f225761,"@@ -1,136 +1,146 @@
     Object readField(StructField structField) {
         
         // Get the offset of the field
         int offset = structField.offset;
 
         // Determine the type of the field
         Class nativeType = structField.type;
         FromNativeConverter readConverter = structField.readConverter;
         if (readConverter != null) {
             nativeType = readConverter.nativeType();
         }
 
         // Get the value at the offset according to its type
         Object result = null;
         if (Structure.class.isAssignableFrom(nativeType)) {
             Structure s = (Structure)getField(structField);
             if (ByReference.class.isAssignableFrom(nativeType)) {
                 s = updateStructureByReference(nativeType, s, memory.getPointer(offset));
             }
             else {
                 s.useMemory(memory, offset);
                 s.read();
             }
             result = s;
         }
         else if (nativeType == boolean.class || nativeType == Boolean.class) {
             result = Boolean.valueOf(memory.getInt(offset) != 0);
         }
         else if (nativeType == byte.class || nativeType == Byte.class) {
             result = new Byte(memory.getByte(offset));
         }
         else if (nativeType == short.class || nativeType == Short.class) {
             result = new Short(memory.getShort(offset));
         }
         else if (nativeType == char.class || nativeType == Character.class) {
             result = new Character(memory.getChar(offset));
         }
         else if (nativeType == int.class || nativeType == Integer.class) {
             result = new Integer(memory.getInt(offset));
         }
         else if (nativeType == long.class || nativeType == Long.class) {
             result = new Long(memory.getLong(offset));
         }
         else if (nativeType == float.class || nativeType == Float.class) {
             result=new Float(memory.getFloat(offset));
         }
         else if (nativeType == double.class || nativeType == Double.class) {
             result = new Double(memory.getDouble(offset));
         }
         else if (nativeType == Pointer.class) {
             result = memory.getPointer(offset);
         }
         else if (nativeType == String.class) {
             Pointer p = memory.getPointer(offset);
             result = p != null ? p.getString(0) : null;
         }
         else if (nativeType == WString.class) {
             Pointer p = memory.getPointer(offset);
             result = p != null ? new WString(p.getString(0, true)) : null;
         }
         else if (Callback.class.isAssignableFrom(nativeType)) {
             // Overwrite the Java memory if the native pointer is a different
             // function pointer.
             Pointer fp = memory.getPointer(offset);
             if (fp == null) {
                 result = null;
             }
             else {
                 Callback cb = (Callback)getField(structField);
                 Pointer oldfp = CallbackReference.getFunctionPointer(cb);
                 if (!fp.equals(oldfp)) {
                     cb = CallbackReference.getCallback(nativeType, fp);
                 }
                 result = cb;
             }
         }
         else if (nativeType.isArray()) {
             Class cls = nativeType.getComponentType();
             int length = 0;
             Object o = getField(structField);
             if (o == null) {
                 throw new IllegalStateException(""Array field in Structure not initialized"");
             }
             length = Array.getLength(o);
             result = o;
 
             if (cls == byte.class) {
                 memory.read(offset, (byte[])result, 0, length);
             }
             else if (cls == short.class) {
                 memory.read(offset, (short[])result, 0, length);
             }
             else if (cls == char.class) {
                 memory.read(offset, (char[])result, 0, length);
             }
             else if (cls == int.class) {
                 memory.read(offset, (int[])result, 0, length);
             }
             else if (cls == long.class) {
                 memory.read(offset, (long[])result, 0, length);
             }
             else if (cls == float.class) {
                 memory.read(offset, (float[])result, 0, length);
             }
             else if (cls == double.class) {
                 memory.read(offset, (double[])result, 0, length);
             }
             else if (Pointer.class.isAssignableFrom(cls)) {
                 memory.read(offset, (Pointer[])result, 0, length);
             }
-            else if (Structure.class.isAssignableFrom(cls)
-                     && ByReference.class.isAssignableFrom(cls)) {
+            else if (Structure.class.isAssignableFrom(cls)) {
                 Structure[] sarray = (Structure[])result;
-                Pointer[] parray = memory.getPointerArray(offset, sarray.length);
-                for (int i=0;i < sarray.length;i++) {
-                    sarray[i] = updateStructureByReference(cls, sarray[i], parray[i]);
+                if (ByReference.class.isAssignableFrom(cls)) {
+                    Pointer[] parray = memory.getPointerArray(offset, sarray.length);
+                    for (int i=0;i < sarray.length;i++) {
+                        sarray[i] = updateStructureByReference(cls, sarray[i], parray[i]);
+                    }
+                }
+                else {
+                    for (int i=0;i < sarray.length;i++) {
+                        if (sarray[i] == null) {
+                            sarray[i] = newInstance(cls);
+                            sarray[i].useMemory(memory, offset + i * sarray[i].size());
+                        }
+                        sarray[i].read();
+                    }
                 }
             }
             else {
                 throw new IllegalArgumentException(""Array of ""
                                                    + cls + "" not supported"");
             }
         }
         else {
             throw new IllegalArgumentException(""Unsupported field type \""""
                                                + nativeType + ""\"""");
         }
 
         if (readConverter != null) {
             result = readConverter.fromNative(result, structField.context);
         }
 
         // Update the value on the field
         setField(structField, result);
         return result;
     }",Buggy,"Fix bug in nested struct array read/write

git-svn-id: https://svn.java.net/svn/jna~svn/trunk@445 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80
",Buggy,"The commit message indicates a fix for a bug related to reading and writing nested struct arrays. The diff modifies the `readField` method to handle arrays of `Structure` objects correctly, specifically addressing nested structures. The code now differentiates between `Structure` arrays with and without `ByReference`. If the structure is not a `ByReference`, it creates a new instance if null, assigns memory using `useMemory`, and reads the structure. This suggests that the previous implementation had issues with correctly reading or initializing nested structure arrays, which is now resolved. The answer is **Buggy**."
jna,2043.json,fe79ff84115369d6cc0e1d6fa240d2c431094af7,"@@ -1,3 +1,3 @@
     public static boolean FAILED(int hr) {
-        return (hr != S_OK);
+        return hr < 0;
     }",Buggy,"Fix SUCCEEDED and FAILED instead of trying to fix the bug where it's visible. See MSDN for FAILED and SUCCEEDED definitions

Changes
",Buggy,"The commit message indicates a fix for how `FAILED` is determined, referencing MSDN for the correct definition. The code changes the condition from `hr != S_OK` to `hr < 0`. This aligns with the commit message and suggests a bug fix related to the incorrect evaluation of failure conditions. The answer is **Buggy**.
"
jna,2041.json,fe79ff84115369d6cc0e1d6fa240d2c431094af7,"@@ -1,6 +1,3 @@
     public static boolean SUCCEEDED(int hr) {
-        if (hr == S_OK)
-            return true;
-        else
-            return false;
+        return hr >= 0;
     }",Buggy,"Fix SUCCEEDED and FAILED instead of trying to fix the bug where it's visible. See MSDN for FAILED and SUCCEEDED definitions

Changes
",Buggy,"The commit message indicates a fix related to the `SUCCEEDED` function, referencing MSDN for definitions. The code change simplifies the `SUCCEEDED` function to directly return whether the input `hr` is greater than or equal to 0. This aligns with the standard definition of `SUCCEEDED` in COM, where non-negative values indicate success. The original code only checked for `S_OK`, which is just one specific success code and would have missed other success codes. This correction addresses a bug where the function incorrectly reported failure for valid success codes. The answer is **Buggy**.
"
jna,4111.json,64466deb5eb01a0d9c3e27ceb2a89d4e6d546908,"@@ -1,21 +1,21 @@
     private void validateField(String name, Class type) {
         if (typeMapper != null) {
             ToNativeConverter toNative = typeMapper.getToNativeConverter(type);
             if (toNative != null) {
                 validateField(name, toNative.nativeType());
                 return;
             }
         }
         if (type.isArray()) {
             validateField(name, type.getComponentType());
         }
         else {
             try {
                 getNativeSize(type);
             }
             catch(IllegalArgumentException e) {
-                String msg = ""Invalid Structure field in "" + getClass() + "", field name '"" + name + ""' ("" + type + "")"";
+                String msg = ""Invalid Structure field in "" + getClass() + "", field name '"" + name + ""' ("" + type + ""): "" + e.getMessage();
                 throw new IllegalArgumentException(msg, e);
             }
         }
     }",NotBuggy,"Ensure exception message includes all faulty field information (fixes broken test)
",Buggy,"The commit message indicates a fix for a broken test by ensuring the exception message includes all faulty field information. The code change modifies the `validateField` method to include the original exception's message in the new `IllegalArgumentException`'s message. This provides more context about the error, which likely helps in debugging and resolving the broken test. The answer is **Buggy**.
"
jna,4113.json,64466deb5eb01a0d9c3e27ceb2a89d4e6d546908,"@@ -1,147 +1,147 @@
     private LayoutInfo deriveLayout(boolean force, boolean avoidFFIType) {
         int calculatedSize = 0;
         List fields = getFields(force);
         if (fields == null) {
             return null;
         }
 
         LayoutInfo info = new LayoutInfo();
         info.alignType = this.alignType;
         info.typeMapper = this.typeMapper;
 
         boolean firstField = true;
         for (Iterator i=fields.iterator();i.hasNext();firstField=false) {
             Field field = (Field)i.next();
             int modifiers = field.getModifiers();
 
             Class type = field.getType();
             if (type.isArray()) {
                 info.variable = true;
             }
             StructField structField = new StructField();
             structField.isVolatile = Modifier.isVolatile(modifiers);
             structField.isReadOnly = Modifier.isFinal(modifiers);
             if (structField.isReadOnly) {
                 if (!Platform.RO_FIELDS) {
                     throw new IllegalArgumentException(""This VM does not support read-only fields (field '""
                                                        + field.getName() + ""' within "" + getClass() + "")"");
                 }
                 // In J2SE VMs, this allows overriding the value of final
                 // fields
                 field.setAccessible(true);
             }
             structField.field = field;
             structField.name = field.getName();
             structField.type = type;
 
             // Check for illegal field types
             if (Callback.class.isAssignableFrom(type) && !type.isInterface()) {
                 throw new IllegalArgumentException(""Structure Callback field '""
                                                    + field.getName()
                                                    + ""' must be an interface"");
             }
             if (type.isArray()
                 && Structure.class.equals(type.getComponentType())) {
                 String msg = ""Nested Structure arrays must use a ""
                     + ""derived Structure type so that the size of ""
                     + ""the elements can be determined"";
                 throw new IllegalArgumentException(msg);
             }
 
             int fieldAlignment = 1;
             if (!Modifier.isPublic(field.getModifiers())) {
                 continue;
             }
 
             Object value = getFieldValue(structField.field);
             if (value == null && type.isArray()) {
                 if (force) {
                     throw new IllegalStateException(""Array fields must be initialized"");
                 }
                 // can't calculate size yet, defer until later
                 return null;
             }
             Class nativeType = type;
             if (NativeMapped.class.isAssignableFrom(type)) {
                 NativeMappedConverter tc = NativeMappedConverter.getInstance(type);
                 nativeType = tc.nativeType();
                 structField.writeConverter = tc;
                 structField.readConverter = tc;
                 structField.context = new StructureReadContext(this, field);
             }
             else if (typeMapper != null) {
                 ToNativeConverter writeConverter = typeMapper.getToNativeConverter(type);
                 FromNativeConverter readConverter = typeMapper.getFromNativeConverter(type);
                 if (writeConverter != null && readConverter != null) {
                     value = writeConverter.toNative(value,
                                                     new StructureWriteContext(this, structField.field));
                     nativeType = value != null ? value.getClass() : Pointer.class;
                     structField.writeConverter = writeConverter;
                     structField.readConverter = readConverter;
                     structField.context = new StructureReadContext(this, field);
                 }
                 else if (writeConverter != null || readConverter != null) {
                     String msg = ""Structures require bidirectional type conversion for "" + type;
                     throw new IllegalArgumentException(msg);
                 }
             }
 
             if (value == null) {
                 value = initializeField(structField.field, type);
             }
 
             try {
                 structField.size = getNativeSize(nativeType, value);
                 fieldAlignment = getNativeAlignment(nativeType, value, firstField);
             }
             catch(IllegalArgumentException e) {
                 // Might simply not yet have a type mapper set yet
                 if (!force && typeMapper == null) {
                     return null;
                 }
-                String msg = ""Invalid Structure field in "" + getClass() + "", field name '"" + structField.name + ""' ("" + structField.type + "")"";
+                String msg = ""Invalid Structure field in "" + getClass() + "", field name '"" + structField.name + ""' ("" + structField.type + ""): "" + e.getMessage();
                 throw new IllegalArgumentException(msg, e);
             }
 
             // Align fields as appropriate
             if (fieldAlignment == 0) {
                 throw new Error(""Field alignment is zero for field '"" + structField.name + ""' within "" + getClass());
             }
             info.alignment = Math.max(info.alignment, fieldAlignment);
             if ((calculatedSize % fieldAlignment) != 0) {
                 calculatedSize += fieldAlignment - (calculatedSize % fieldAlignment);
             }
             if (this instanceof Union) {
                 structField.offset = 0;
                 calculatedSize = Math.max(calculatedSize, structField.size);
             }
             else {
                 structField.offset = calculatedSize;
                 calculatedSize += structField.size;
             }
 
             // Save the field in our list
             info.fields.put(structField.name, structField);
 
             if (info.typeInfoField == null
                 || info.typeInfoField.size < structField.size
                 || (info.typeInfoField.size == structField.size
                     && Structure.class.isAssignableFrom(structField.type))) {
                 info.typeInfoField = structField;
             }
         }
 
         if (calculatedSize > 0) {
             int size = addPadding(calculatedSize, info.alignment);
             // Update native FFI type information, if needed
             if (this instanceof ByValue && !avoidFFIType) {
                 getTypeInfo();
             }
             info.size = size;
             return info;
         }
 
         throw new IllegalArgumentException(""Structure "" + getClass()
                                            + "" has unknown or zero size (ensure ""
                                            + ""all fields are public)"");
     }",NotBuggy,"Ensure exception message includes all faulty field information (fixes broken test)
",Buggy,"The commit message indicates that the exception message is being improved to include more information about the faulty field. The diff shows that the catch block for IllegalArgumentException now includes the exception's message in the new exception message. This change aims to provide more context when an invalid structure field is encountered, which directly addresses a bug where the error message was insufficient. The answer is **Buggy**."
spring-framework,25502.json,98218687070fc47eb6f91e81d3d714bf96068cc5,"@@ -1,9 +1,7 @@
 	public static void sortBySpecificityAndQuality(List<MediaType> mediaTypes) {
 		Assert.notNull(mediaTypes, ""'mediaTypes' must not be null"");
 		if (mediaTypes.size() > 1) {
-			Comparator<?>[] comparators = new Comparator[2];
-			comparators[0] = MediaType.SPECIFICITY_COMPARATOR;
-			comparators[1] = MediaType.QUALITY_VALUE_COMPARATOR;
-			Collections.sort(mediaTypes, new CompoundComparator<MediaType>(comparators));
+			Collections.sort(mediaTypes, new CompoundComparator<MediaType>(
+					MediaType.SPECIFICITY_COMPARATOR, MediaType.QUALITY_VALUE_COMPARATOR));
 		}
 	}",NotBuggy,"Refactor and polish various Comparator impls

 - Refactor CompoundComparator constructor to use varargs
 - Refactor MediaType to consume new varargs constructor
 - Add notNull assertions where appropriate
 - Add generic typing where appropriate
 - Suppress generics warnings elsewhere
 - Fix whitespace errors
",NotBuggy,The commit message indicates refactoring and polishing of Comparator implementations. The diff shows a change in `sortBySpecificityAndQuality` method where the `CompoundComparator` is constructed using varargs instead of an array. This aligns with the commit message's refactoring goal. The change simplifies the code and makes it more readable. There is no indication of a bug fix in this diff. The answer is **NotBuggy**.
spring-framework,15028.json,98218687070fc47eb6f91e81d3d714bf96068cc5,"@@ -1,10 +1,10 @@
 	public boolean equals(Object obj) {
 		if (this == obj) {
 			return true;
 		}
 		if (!(obj instanceof NullSafeComparator)) {
 			return false;
 		}
-		NullSafeComparator other = (NullSafeComparator) obj;
+		NullSafeComparator<T> other = (NullSafeComparator<T>) obj;
 		return (this.nonNullComparator.equals(other.nonNullComparator) && this.nullsLow == other.nullsLow);
 	}",NotBuggy,"Refactor and polish various Comparator impls

 - Refactor CompoundComparator constructor to use varargs
 - Refactor MediaType to consume new varargs constructor
 - Add notNull assertions where appropriate
 - Add generic typing where appropriate
 - Suppress generics warnings elsewhere
 - Fix whitespace errors
",NotBuggy,"The commit message indicates refactoring, polishing, and fixing whitespace errors. The diff shows a change in the `equals` method of `NullSafeComparator`, adding a generic type `<T>` to the cast. This change improves type safety and reduces the risk of ClassCastException at runtime. While the commit message doesn't explicitly mention a bug fix related to this specific change, the improved type safety can be considered a preventative measure against potential type-related errors. However, the primary focus is on refactoring and polishing, not directly addressing a known bug. The answer is **NotBuggy**."
spring-framework,15035.json,98218687070fc47eb6f91e81d3d714bf96068cc5,"@@ -1,10 +1,10 @@
 	public boolean equals(Object obj) {
 		if (this == obj) {
 			return true;
 		}
 		if (!(obj instanceof InvertibleComparator)) {
 			return false;
 		}
-		InvertibleComparator other = (InvertibleComparator) obj;
+		InvertibleComparator<T> other = (InvertibleComparator<T>) obj;
 		return (this.comparator.equals(other.comparator) && this.ascending == other.ascending);
 	}",NotBuggy,"Refactor and polish various Comparator impls

 - Refactor CompoundComparator constructor to use varargs
 - Refactor MediaType to consume new varargs constructor
 - Add notNull assertions where appropriate
 - Add generic typing where appropriate
 - Suppress generics warnings elsewhere
 - Fix whitespace errors
",NotBuggy,"The commit message indicates refactoring, polishing, and fixing whitespace errors. The diff adds a generic type to `InvertibleComparator`. This change improves type safety and reduces the need for casting. While it's an improvement, it doesn't directly address a bug. The answer is **NotBuggy**."
spring-framework,15051.json,98218687070fc47eb6f91e81d3d714bf96068cc5,"@@ -1,10 +1,10 @@
 	public boolean equals(Object obj) {
 		if (this == obj) {
 			return true;
 		}
 		if (!(obj instanceof CompoundComparator)) {
 			return false;
 		}
-		CompoundComparator other = (CompoundComparator) obj;
+		CompoundComparator<T> other = (CompoundComparator<T>) obj;
 		return this.comparators.equals(other.comparators);
 	}",NotBuggy,"Refactor and polish various Comparator impls

 - Refactor CompoundComparator constructor to use varargs
 - Refactor MediaType to consume new varargs constructor
 - Add notNull assertions where appropriate
 - Add generic typing where appropriate
 - Suppress generics warnings elsewhere
 - Fix whitespace errors
",NotBuggy,"The commit message indicates refactoring, polishing, and fixing whitespace errors, suggesting improvements to code structure and readability rather than fixing specific bugs. The diff adds a generic type to the `CompoundComparator` object during casting in the `equals` method. This change improves type safety and reduces the potential for runtime errors related to incorrect type assumptions. This change is part of the refactoring and polishing mentioned in the commit message. The answer is **NotBuggy**."
spring-framework,15045.json,98218687070fc47eb6f91e81d3d714bf96068cc5,"@@ -1,5 +1,5 @@
 	public void invertOrder() {
-		for (InvertibleComparator comparator : this.comparators) {
+		for (InvertibleComparator<T> comparator : this.comparators) {
 			comparator.invertOrder();
 		}
 	}",NotBuggy,"Refactor and polish various Comparator impls

 - Refactor CompoundComparator constructor to use varargs
 - Refactor MediaType to consume new varargs constructor
 - Add notNull assertions where appropriate
 - Add generic typing where appropriate
 - Suppress generics warnings elsewhere
 - Fix whitespace errors
",NotBuggy,"The commit message indicates refactoring, polishing, and fixing whitespace errors. The diff shows the addition of generic typing to the `InvertibleComparator` in the `invertOrder` method. This change improves type safety and reduces the potential for runtime errors related to incorrect type casting. While it enhances the code, it doesn't directly address a specific bug. Therefore, the answer is **NotBuggy**.
"
spring-framework,16701.json,67a06f5edcc8697af0941e238ef29bdb2a73245d,"@@ -1,8 +1,8 @@
 	public ClientResponse build() {
+
 		ClientHttpResponse httpResponse =
 				new BuiltClientHttpResponse(this.statusCode, this.headers, this.cookies, this.body);
 
-		// When building ClientResponse manually, the ClientRequest.logPrefix() has to be passed,
-		// e.g. via ClientResponse.Builder, but this (builder) is not used currently.
-		return new DefaultClientResponse(httpResponse, this.strategies, """", """", () -> this.request);
+		return new DefaultClientResponse(
+				httpResponse, this.strategies, """", """", () -> this.request);
 	}",NotBuggy,"Add mutate() to ClientResponse and deprecate from()

from() has the flaw of ignoring the body and it can't be fixed because
applications are guaranteed to be setting it already and if set twice
the builder drains the first body.

mutate() is a better fit in any case for what needs to be done in a
filter chain. It can be done more efficiently and is consistent with
similar options on the server side.

See gh-24680
",Buggy,"The commit message indicates a change to address a flaw in the `from()` method of `ClientResponse`, where the body is ignored. The commit introduces `mutate()` as a replacement. The diff shows the removal of a comment related to `ClientRequest.logPrefix()`. The core change involves replacing `from()` with `mutate()` due to the former's inability to handle the body correctly. This suggests a bug fix related to how `ClientResponse` handles the body. The answer is **Buggy**."
spring-framework,16453.json,67a06f5edcc8697af0941e238ef29bdb2a73245d,"@@ -1,8 +1,7 @@
 	public static ExchangeFilterFunction limitResponseSize(long maxByteCount) {
 		return (request, next) ->
-				next.exchange(request).map(response -> {
-					Flux<DataBuffer> body = response.body(BodyExtractors.toDataBuffers());
-					body = DataBufferUtils.takeUntilByteCount(body, maxByteCount);
-					return ClientResponse.from(response).body(body).build();
-				});
+				next.exchange(request).map(response ->
+						response.mutate()
+								.body(body -> DataBufferUtils.takeUntilByteCount(body, maxByteCount))
+								.build());
 	}",Buggy,"Add mutate() to ClientResponse and deprecate from()

from() has the flaw of ignoring the body and it can't be fixed because
applications are guaranteed to be setting it already and if set twice
the builder drains the first body.

mutate() is a better fit in any case for what needs to be done in a
filter chain. It can be done more efficiently and is consistent with
similar options on the server side.

See gh-24680
",Buggy,"The commit message indicates a change to address a flaw in the `from()` method of `ClientResponse`, which ignores the body. The proposed solution is to introduce a `mutate()` method and deprecate `from()`. The diff replaces the usage of `ClientResponse.from(response).body(body).build()` with `response.mutate().body(body -> ...).build()`. This change aligns with the commit message, addressing the identified flaw and providing a more efficient and consistent approach. The original code had a flaw, which is now fixed. The answer is **Buggy**."
spring-framework,16698.json,67a06f5edcc8697af0941e238ef29bdb2a73245d,"@@ -1,11 +1,10 @@
 	public ClientResponse.Builder body(String body) {
 		Assert.notNull(body, ""Body must not be null"");
 		releaseBody();
-		DataBufferFactory dataBufferFactory = new DefaultDataBufferFactory();
 		this.body = Flux.just(body).
 				map(s -> {
 					byte[] bytes = body.getBytes(StandardCharsets.UTF_8);
-					return dataBufferFactory.wrap(bytes);
+					return new DefaultDataBufferFactory().wrap(bytes);
 				});
 		return this;
 	}",NotBuggy,"Add mutate() to ClientResponse and deprecate from()

from() has the flaw of ignoring the body and it can't be fixed because
applications are guaranteed to be setting it already and if set twice
the builder drains the first body.

mutate() is a better fit in any case for what needs to be done in a
filter chain. It can be done more efficiently and is consistent with
similar options on the server side.

See gh-24680
",NotBuggy,"The commit message indicates a flaw in the `from()` method of `ClientResponse` and introduces `mutate()` as a replacement. The provided diff modifies the `body(String body)` method. The change involves replacing the local variable `DataBufferFactory dataBufferFactory` with `new DefaultDataBufferFactory()` inline. This change doesn't seem directly related to the flaw mentioned in the commit message (ignoring the body) or the introduction of `mutate()`. It's more likely a minor refactoring or simplification. The change itself doesn't appear to be fixing a bug, but rather improving the code's structure or readability.

Therefore, the answer is **NotBuggy**.
"
spring-framework,16585.json,67a06f5edcc8697af0941e238ef29bdb2a73245d,"@@ -1,3 +1,3 @@
 	static Builder from(ClientResponse other) {
-		return new DefaultClientResponseBuilder(other);
+		return new DefaultClientResponseBuilder(other, false);
 	}",Buggy,"Add mutate() to ClientResponse and deprecate from()

from() has the flaw of ignoring the body and it can't be fixed because
applications are guaranteed to be setting it already and if set twice
the builder drains the first body.

mutate() is a better fit in any case for what needs to be done in a
filter chain. It can be done more efficiently and is consistent with
similar options on the server side.

See gh-24680
",Buggy,"The commit message indicates a flaw in the `from()` method of `ClientResponse` related to body handling. The diff shows a change in the `from()` method where a boolean flag is passed to the `DefaultClientResponseBuilder`. This likely addresses the issue of the body being ignored or drained incorrectly. The change suggests a fix for the flawed behavior of `from()`. The answer is **Buggy**.
"
sonarqube,9310.json,e582be977c992d38fff928388bb1f1ae928fd146,"@@ -1,14 +1,13 @@
   private java.util.Optional<CeTask> submit(CeTaskSubmit submission, EnumSet<SubmitOption> submitOptions) {
-    checkState(!submitPaused.get(), ""Compute Engine does not currently accept new tasks"");
     try (DbSession dbSession = dbClient.openSession(false)) {
       if (submitOptions.contains(UNIQUE_QUEUE_PER_COMPONENT)
         && submission.getComponentUuid() != null
         && dbClient.ceQueueDao().countByStatusAndComponentUuid(dbSession, PENDING, submission.getComponentUuid()) > 0) {
         return java.util.Optional.empty();
       }
       CeQueueDto dto = addToQueueInDb(dbSession, submission);
       CeTask task = loadTask(dbSession, dto);
       dbSession.commit();
       return java.util.Optional.of(task);
     }
   }",NotBuggy,"Fix Quality flaws

mainly by removing unused code
",Buggy,"The commit message indicates a fix for quality flaws, primarily by removing unused code. The diff removes a checkState assertion `checkState(!submitPaused.get(), ""Compute Engine does not currently accept new tasks"");`. Removing this check means the system will now accept new tasks even when `submitPaused` is true, which could lead to unexpected behavior if the system was intentionally paused. This change addresses a potential issue or flaw in the code's logic, indicating a bug fix. The answer is **Buggy**.
"
sonarqube,9312.json,e582be977c992d38fff928388bb1f1ae928fd146,"@@ -1,16 +1,15 @@
   public List<CeTask> massSubmit(Collection<CeTaskSubmit> submissions, SubmitOption... options) {
-    checkState(!submitPaused.get(), ""Compute Engine does not currently accept new tasks"");
     if (submissions.isEmpty()) {
       return Collections.emptyList();
     }
 
     try (DbSession dbSession = dbClient.openSession(true)) {
       List<CeQueueDto> ceQueueDtos = submissions.stream()
         .filter(filterBySubmitOptions(options, submissions, dbSession))
         .map(submission -> addToQueueInDb(dbSession, submission))
         .collect(Collectors.toList());
       List<CeTask> tasks = loadTasks(dbSession, ceQueueDtos);
       dbSession.commit();
       return tasks;
     }
   }",NotBuggy,"Fix Quality flaws

mainly by removing unused code
",Buggy,"The commit message indicates a fix for quality flaws, primarily by removing unused code. The diff shows the removal of `checkState(!submitPaused.get(), ""Compute Engine does not currently accept new tasks"");`. This line likely checked if the system was paused before accepting new tasks. Removing this check suggests that it was either unnecessary or causing problems. Removing a check like this could be considered a bug fix if the check was incorrectly preventing valid submissions. The answer is **Buggy**."
sonarqube,8268.json,3bfcfa0de67e7f00d7cd0dc74649fef7e5772298,"@@ -1,3 +1,3 @@
   public void stop() {
-
+    // do nothing
   }",NotBuggy,"Fix some quality flaws
",Buggy,"The commit message indicates a fix for ""quality flaws."" The diff shows an empty `stop()` method being filled with a comment ""// do nothing"". This change, while minimal, suggests that the original empty method might have been considered a quality flaw, possibly because it was unexpected or incomplete. Adding the comment clarifies the intended behavior and addresses the flaw. The answer is **Buggy**."
sonarqube,1454.json,3bfcfa0de67e7f00d7cd0dc74649fef7e5772298,"@@ -1,3 +1,3 @@
-  public String metadata() {
-    return metadata;
+  public String internalKey() {
+    return internalKey;
   }",NotBuggy,"Fix some quality flaws
",NotBuggy,"The commit message indicates a fix for ""quality flaws."" The diff shows a simple renaming of a method from `metadata()` to `internalKey()`. This change doesn't seem to address any specific bug or incorrect behavior. It's more likely a refactoring or code cleanup to improve code clarity or maintainability. Therefore, it's not a bug fix. The answer is **NotBuggy**.
"
sonarqube,1485.json,3bfcfa0de67e7f00d7cd0dc74649fef7e5772298,"@@ -1,4 +1,4 @@
-  public NewRule setMetadata(@Nullable String metadata) {
-    this.metadata = metadata;
+  public NewRule setInternalKey(@Nullable String s) {
+    this.internalKey = s;
     return this;
   }",NotBuggy,"Fix some quality flaws
",NotBuggy,"The commit message indicates a fix for ""quality flaws."" The diff shows a method `setMetadata` being renamed to `setInternalKey`, and the field `metadata` being renamed to `internalKey`. This change doesn't appear to fix a bug in the sense of correcting incorrect behavior. Instead, it seems to be a refactoring or renaming for better code quality or clarity. Therefore, it's not a bug fix. The answer is **NotBuggy**.
"
sonarqube,2297.json,074f5c655822a466819c899ed9c90aef4ba1d2b4,"@@ -1,14 +1,15 @@
   boolean isDescendant(DbSession dbSession, QualityProfileDto childProfile, @Nullable QualityProfileDto parentProfile) {
     QualityProfileDto currentParent = parentProfile;
     while (currentParent != null) {
       if (childProfile.getName().equals(currentParent.getName())) {
         return true;
       }
-      if (currentParent.getParentKee() != null) {
-        currentParent = db.qualityProfileDao().getByKey(dbSession, currentParent.getParentKee());
+      String parentKey = currentParent.getParentKee();
+      if (parentKey != null) {
+        currentParent = db.qualityProfileDao().getByKey(dbSession, parentKey);
       } else {
         currentParent = null;
       }
     }
     return false;
   }",NotBuggy,"Fix some quality flaws
",NotBuggy,"The code checks if a quality profile is a descendant of another. The change involves extracting the parent key into a local variable `parentKey` before using it. This doesn't change the logic or functionality of the code. It improves readability and avoids calling `currentParent.getParentKee()` multiple times, but it doesn't fix a bug. The answer is **NotBuggy**."
sonarqube,6627.json,074f5c655822a466819c899ed9c90aef4ba1d2b4,"@@ -1,30 +1,34 @@
   private ActiveRuleDto doUpdate(ActiveRuleChange change, RuleActivatorContext context, DbSession dbSession) {
     ActiveRuleDto activeRule;
     ActiveRuleDao dao = db.activeRuleDao();
     activeRule = context.activeRule();
-    activeRule.setSeverity(change.getSeverity());
-    if (change.getInheritance() != null) {
-      activeRule.setInheritance(change.getInheritance().name());
+    String severity = change.getSeverity();
+    if (severity != null) {
+      activeRule.setSeverity(severity);
+    }
+    ActiveRule.Inheritance inheritance = change.getInheritance();
+    if (inheritance != null) {
+      activeRule.setInheritance(inheritance.name());
     }
     dao.update(dbSession, activeRule);
 
     for (Map.Entry<String, String> param : change.getParameters().entrySet()) {
       ActiveRuleParamDto activeRuleParamDto = context.activeRuleParamsAsMap().get(param.getKey());
       if (activeRuleParamDto == null) {
         // did not exist
         if (param.getValue() != null) {
           activeRuleParamDto = ActiveRuleParamDto.createFor(context.ruleParamsByKeys().get(param.getKey()));
           activeRuleParamDto.setValue(param.getValue());
           dao.addParam(dbSession, activeRule, activeRuleParamDto);
         }
       } else {
         if (param.getValue() != null) {
           activeRuleParamDto.setValue(param.getValue());
           dao.updateParam(dbSession, activeRule, activeRuleParamDto);
         } else {
           dao.deleteParam(dbSession, activeRule, activeRuleParamDto);
         }
       }
     }
     return activeRule;
   }",NotBuggy,"Fix some quality flaws
",Buggy,"The commit message ""Fix some quality flaws"" is vague, but the diff shows changes to how `ActiveRuleDto` is updated. Specifically, it adds null checks for `change.getSeverity()` and `change.getInheritance()` before updating the `activeRule`. This suggests that the original code might have thrown a `NullPointerException` if these values were null, indicating a potential bug. The addition of these null checks improves the robustness of the code. Therefore, the changes indicate a bug fix. The answer is **Buggy**."
sonarqube,2295.json,074f5c655822a466819c899ed9c90aef4ba1d2b4,"@@ -1,27 +1,27 @@
   void setParent(DbSession dbSession, String profileKey, @Nullable String parentKey) {
     QualityProfileDto profile = db.qualityProfileDao().getNonNullByKey(dbSession, profileKey);
     if (parentKey == null) {
       // unset if parent is defined, else nothing to do
       removeParent(dbSession, profile);
 
-    } else if (profile.getParentKee() == null || !profile.getParentKee().equals(parentKey)) {
+    } else if (profile.getParentKee() == null || !parentKey.equals(profile.getParentKee())) {
       QualityProfileDto parentProfile = db.qualityProfileDao().getNonNullByKey(dbSession, parentKey);
       if (isDescendant(dbSession, profile, parentProfile)) {
         throw new BadRequestException(String.format(""Descendant profile '%s' can not be selected as parent of '%s'"", parentKey, profileKey));
       }
       removeParent(dbSession, profile);
 
       // set new parent
       profile.setParentKee(parentKey);
       db.qualityProfileDao().update(dbSession, profile);
       for (ActiveRuleDto parentActiveRule : db.activeRuleDao().findByProfileKey(dbSession, parentKey)) {
         try {
           RuleActivation activation = new RuleActivation(parentActiveRule.getKey().ruleKey());
           activate(dbSession, activation, profileKey);
         } catch (BadRequestException e) {
           // for example because rule status is REMOVED
           // TODO return errors
         }
       }
     }
   }",NotBuggy,"Fix some quality flaws
",Buggy,"The commit message indicates a fix for ""quality flaws."" The code diff modifies the `setParent` method. Specifically, the condition `profile.getParentKee() == null || !profile.getParentKee().equals(parentKey)` is changed to `profile.getParentKee() == null || !parentKey.equals(profile.getParentKee())`. This change involves swapping the order of comparison between `profile.getParentKee()` and `parentKey` in the `equals` method. The original code compares `profile.getParentKee()` with `parentKey`, while the corrected code compares `parentKey` with `profile.getParentKee()`. This suggests a potential bug where the comparison was not behaving as expected, possibly due to null pointer exceptions if `profile.getParentKee()` was null. The corrected code ensures that `parentKey` is always the object on which `equals` is called, preventing potential null pointer exceptions. Therefore, the change addresses a bug related to the comparison of parent keys. The answer is **Buggy**.
"
sonarqube,6621.json,074f5c655822a466819c899ed9c90aef4ba1d2b4,"@@ -1,57 +1,58 @@
   private List<ActiveRuleChange> doActivate(DbSession dbSession, RuleActivation activation, RuleActivatorContext context) {
     context.verifyForActivation();
     List<ActiveRuleChange> changes = Lists.newArrayList();
     ActiveRuleChange change;
     boolean stopPropagation = false;
 
-    if (context.activeRule() == null) {
+    ActiveRuleDto activeRule = context.activeRule();
+    if (activeRule == null) {
       if (activation.isReset()) {
         // ignore reset when rule is not activated
         return changes;
       }
       // new activation
       change = ActiveRuleChange.createFor(ActiveRuleChange.Type.ACTIVATED, context.activeRuleKey());
       applySeverityAndParamToChange(activation, context, change);
       if (activation.isCascade() || context.isSameAsParent(change)) {
         change.setInheritance(ActiveRule.Inheritance.INHERITED);
       }
     } else {
       // already activated
-      if (activation.isCascade() && context.activeRule().doesOverride()) {
+      if (activation.isCascade() && activeRule.doesOverride()) {
         // propagating to descendants, but child profile already overrides rule -> stop propagation
         return changes;
       }
       change = ActiveRuleChange.createFor(ActiveRuleChange.Type.UPDATED, context.activeRuleKey());
-      if (activation.isCascade() && context.activeRule().getInheritance() == null) {
+      if (activation.isCascade() && activeRule.getInheritance() == null) {
         // activate on child, then on parent -> mark child as overriding parent
         change.setInheritance(ActiveRule.Inheritance.OVERRIDES);
         change.setSeverity(context.currentSeverity());
         change.setParameters(context.activeRuleParamsAsStringMap());
         stopPropagation = true;
       } else {
         applySeverityAndParamToChange(activation, context, change);
         if (!activation.isCascade() && context.parentActiveRule() != null) {
           // override rule which is already declared on parents
           change.setInheritance(context.isSameAsParent(change) ? ActiveRule.Inheritance.INHERITED : ActiveRule.Inheritance.OVERRIDES);
         }
       }
       if (context.isSame(change)) {
         change = null;
       }
     }
 
     if (change != null) {
       changes.add(change);
       persist(change, context, dbSession);
     }
 
     if (!stopPropagation) {
       changes.addAll(cascadeActivation(dbSession, activation, context.profile().getKey()));
     }
 
     if (!changes.isEmpty()) {
       updateProfileDate(dbSession, context);
       previewCache.reportGlobalModification(dbSession);
     }
     return changes;
   }",NotBuggy,"Fix some quality flaws
",Buggy,"The commit message ""Fix some quality flaws"" is vague. The code changes involve several modifications within the `doActivate` method.

1.  **Local Variable Introduction:** A local variable `activeRule` is introduced to store `context.activeRule()` to avoid calling the method multiple times.
2.  **Null Check:** The code now checks `activeRule == null` instead of `context.activeRule() == null`.
3.  **Override Check:** The code now checks `activation.isCascade() && activeRule.doesOverride()` instead of `activation.isCascade() && context.activeRule().doesOverride()`.
4.  **Inheritance Check:** The code now checks `activation.isCascade() && activeRule.getInheritance() == null` instead of `activation.isCascade() && context.activeRule().getInheritance() == null`.

These changes suggest that the original code might have been calling `context.activeRule()` multiple times, which could have led to unexpected behavior if the state of `context` changed between calls. By storing the result of `context.activeRule()` in a local variable, the code ensures that the same value is used throughout the method. The changes also improve readability. These changes fix potential issues related to object state and multiple calls to the same method.

**Buggy**
"
sonarqube,11064.json,e582be977c992d38fff928388bb1f1ae928fd146,"@@ -1,8 +1,8 @@
   private static String groupSearchToSql(@Nullable String query) {
     if (query == null) {
       return null;
     }
 
     String upperCasedNameQuery = StringUtils.upperCase(query, Locale.ENGLISH);
-    return DaoDatabaseUtils.buildLikeValue(upperCasedNameQuery, WildcardPosition.BEFORE_AND_AFTER);
+    return DaoUtils.buildLikeValue(upperCasedNameQuery, WildcardPosition.BEFORE_AND_AFTER);
   }",NotBuggy,"Fix Quality flaws

mainly by removing unused code
",NotBuggy,"The commit message indicates a fix for quality flaws, primarily by removing unused code. The diff shows a change from `DaoDatabaseUtils.buildLikeValue` to `DaoUtils.buildLikeValue`. This suggests a refactoring or code cleanup, potentially addressing a quality flaw by removing a dependency on a more specific utility class (`DaoDatabaseUtils`) and using a more general one (`DaoUtils`). It's unlikely to be a bug fix in the sense of correcting incorrect behavior, but rather an improvement in code structure or maintainability. The answer is **NotBuggy**.
"
sonarqube,12391.json,e582be977c992d38fff928388bb1f1ae928fd146,"@@ -1,13 +1,13 @@
   private Optional<CeQueueDto> tryToPeek(DbSession session, EligibleTaskDto eligible, String workerUuid) {
     long now = system2.now();
     int touchedRows = mapper(session).updateIf(eligible.getUuid(),
       new UpdateIf.NewProperties(IN_PROGRESS, workerUuid, eligible.getExecutionCount() + 1, now, now),
       new UpdateIf.OldProperties(PENDING, eligible.getExecutionCount()));
     if (touchedRows != 1) {
       return Optional.empty();
     }
 
     CeQueueDto result = mapper(session).selectByUuid(eligible.getUuid());
     session.commit();
-    return Optional.of(result);
+    return Optional.ofNullable(result);
   }",NotBuggy,"Fix Quality flaws

mainly by removing unused code
",Buggy,"The commit message indicates a fix for quality flaws, primarily by removing unused code. However, the provided diff shows a change from `Optional.of(result)` to `Optional.ofNullable(result)`. This change addresses a potential issue where `result` could be null, which would cause `Optional.of(null)` to throw a NullPointerException. This indicates a bug fix related to null handling. The answer is **Buggy**."
sonarqube,14783.json,1c73879c6bd7bb325e7e95acfadbb8431adc39f3,"@@ -1,18 +1,18 @@
-  private static Optional<Double> getDoubleValue(Optional<Measure> measureOptional) {
-    if (!measureOptional.isPresent()) {
-      return Optional.absent();
-    }
-    Measure measure = measureOptional.get();
-    switch (measure.getValueType()) {
-      case DOUBLE:
-        return Optional.of(measure.getDoubleValue());
-      case LONG:
-        return Optional.of((double) measure.getLongValue());
-      case INT:
-        return Optional.of((double) measure.getIntValue());
-      case NO_VALUE:
+    private Optional<Double> getDoubleValue(Optional<Measure> measureOptional) {
+      if (!measureOptional.isPresent()) {
         return Optional.absent();
-      default:
-        throw new IllegalArgumentException(String.format(""Measure of type '%s' are not supported"", measure.getValueType().name()));
-    }
-  }

+      }
+      Measure measure = measureOptional.get();
+      switch (measure.getValueType()) {
+        case DOUBLE:
+          return Optional.of(measure.getDoubleValue());
+        case LONG:
+          return Optional.of((double) measure.getLongValue());
+        case INT:
+          return Optional.of((double) measure.getIntValue());
+        case NO_VALUE:
+          return Optional.absent();
+        default:
+          throw new IllegalArgumentException(String.format(""Measure of type '%s' are not supported"", measure.getValueType().name()));
+      }
+    }",NotBuggy,"Fix quality flaws
",Buggy,"The commit message ""Fix quality flaws"" is vague, but the diff shows a change in the `getDoubleValue` method. Specifically, the `NO_VALUE` case in the switch statement now returns `Optional.absent()` instead of falling through to the `default` case, which throws an `IllegalArgumentException`. This change likely addresses a situation where a `NO_VALUE` measure was causing an unexpected exception. This correction indicates a bug fix. The answer is **Buggy**."
sonarqube,14622.json,1c73879c6bd7bb325e7e95acfadbb8431adc39f3,"@@ -1,8 +1,8 @@
-  private static String convertType(Constants.ComponentLinkType reportType) {
-    String type = typesConverter.get(reportType);
-    if (type != null) {
-      return type;
-    } else {
-      throw new IllegalArgumentException(String.format(""Unsupported type %s"", reportType.name()));
-    }
-  }

+    private String convertType(Constants.ComponentLinkType reportType) {
+      String type = typesConverter.get(reportType);
+      if (type != null) {
+        return type;
+      } else {
+        throw new IllegalArgumentException(String.format(""Unsupported type %s"", reportType.name()));
+      }
+    }",NotBuggy,"Fix quality flaws
",NotBuggy,"The commit message ""Fix quality flaws"" is vague but suggests that the changes address some kind of issue in the code. The diff shows only whitespace changes (indentation). While code formatting can improve readability and maintainability, these changes alone don't fix a bug. Therefore, the answer is **NotBuggy**."
sonarqube,14062.json,1c73879c6bd7bb325e7e95acfadbb8431adc39f3,"@@ -1,6 +1,6 @@
-  private static String computeRevision(@Nullable Changeset latestChange) {
-    if (latestChange == null) {
-      return null;
-    }
-    return latestChange.getRevision();
-  }

+    private String computeRevision(@Nullable Changeset latestChange) {
+      if (latestChange == null) {
+        return null;
+      }
+      return latestChange.getRevision();
+    }",NotBuggy,"Fix quality flaws
",NotBuggy,"The change only adjusts the indentation of the code. It does not modify the functionality or fix any logical errors. Therefore, it is not a bug fix. The answer is **NotBuggy**."
sonarqube,13389.json,1c73879c6bd7bb325e7e95acfadbb8431adc39f3,"@@ -1,16 +1,16 @@
-  private static DbCommons.TextRange.Builder convertTextRange(BatchReport.TextRange sourceRange) {
-    DbCommons.TextRange.Builder targetRange = DbCommons.TextRange.newBuilder();
-    if (sourceRange.hasStartLine()) {
-      targetRange.setStartLine(sourceRange.getStartLine());
-    }
-    if (sourceRange.hasStartOffset()) {
-      targetRange.setStartOffset(sourceRange.getStartOffset());
-    }
-    if (sourceRange.hasEndLine()) {
-      targetRange.setEndLine(sourceRange.getEndLine());
-    }
-    if (sourceRange.hasEndOffset()) {
-      targetRange.setEndOffset(sourceRange.getEndOffset());
-    }
-    return targetRange;
-  }

+    private DbCommons.TextRange.Builder convertTextRange(BatchReport.TextRange sourceRange) {
+      DbCommons.TextRange.Builder targetRange = DbCommons.TextRange.newBuilder();
+      if (sourceRange.hasStartLine()) {
+        targetRange.setStartLine(sourceRange.getStartLine());
+      }
+      if (sourceRange.hasStartOffset()) {
+        targetRange.setStartOffset(sourceRange.getStartOffset());
+      }
+      if (sourceRange.hasEndLine()) {
+        targetRange.setEndLine(sourceRange.getEndLine());
+      }
+      if (sourceRange.hasEndOffset()) {
+        targetRange.setEndOffset(sourceRange.getEndOffset());
+      }
+      return targetRange;
+    }",NotBuggy,"Fix quality flaws
",NotBuggy,The change involves indenting the method body. This change does not fix a bug. The answer is **NotBuggy**.
sonarqube,1136.json,371ab065edbef2e92b502878eec9a73d310af54b,"@@ -1,3 +1,3 @@
     public Reader getReader() {
-      return new StringReader(source_code);
+      return new StringReader(sourceCode);
     }",NotBuggy,"Fix some quality flaws
",Buggy,"The commit message indicates a fix for ""quality flaws."" The diff shows a change from `source_code` to `sourceCode`. This is likely a correction of a typo or a change to adhere to a naming convention, which can be considered a quality flaw. The answer is **Buggy**."
sonarqube,1134.json,371ab065edbef2e92b502878eec9a73d310af54b,"@@ -1,3 +1,3 @@
     public Reader getReader() {
-      return new StringReader(source_code);
+      return new StringReader(sourceCode);
     }",NotBuggy,"Fix some quality flaws
",Buggy,"The commit message indicates a fix for ""quality flaws."" The diff shows a change from `source_code` to `sourceCode`. This suggests a correction in variable naming to adhere to coding standards or conventions, which improves code quality. This is likely a bug fix related to code maintainability and readability. The answer is **Buggy**."
sonarqube,18140.json,4a108310e196bcff760bc81de326346b1ee4ac4c,"@@ -1,7 +1,7 @@
   public String toString() {
     return Objects.toStringHelper(DebtRemediationFunction.class)
       .add(""type"", type)
-      .add(""coefficient"", factor)
+      .add(""coefficient"", coefficient)
       .add(""offset"", offset)
       .toString();
   }",NotBuggy,"Fix quality flaws
",Buggy,"The commit message indicates a fix for quality flaws. The diff shows a change from `factor` to `coefficient` in the `toString` method. This suggests that the original code had a typo or was using the wrong variable, which is a quality flaw. The change corrects the variable name, which is likely a bug fix. The answer is **Buggy**."
sonarqube,18137.json,4a108310e196bcff760bc81de326346b1ee4ac4c,"@@ -1,21 +1,21 @@
   private void validate() {
     switch (type) {
       case LINEAR:
-        if (this.factor == null || this.offset != null) {
+        if (this.coefficient == null || this.offset != null) {
           throw new IllegalArgumentException(String.format(""Only coefficient must be set on %s"", this));
         }
         break;
       case LINEAR_OFFSET:
-        if (this.factor == null || this.offset == null) {
+        if (this.coefficient == null || this.offset == null) {
           throw new IllegalArgumentException(String.format(""Both coefficient and offset are required on %s"", this));
         }
         break;
       case CONSTANT_ISSUE:
-        if (this.factor != null || this.offset == null) {
+        if (this.coefficient != null || this.offset == null) {
           throw new IllegalArgumentException(String.format(""Only offset must be set on %s"", this));
         }
         break;
       default:
         throw new IllegalArgumentException(String.format(""Unknown type on %s"", this));
     }
   }",NotBuggy,"Fix quality flaws
",Buggy,"The commit message indicates a fix for quality flaws. The diff shows changes in the `validate()` method, specifically renaming `factor` to `coefficient` in the conditional checks for different types (LINEAR, LINEAR_OFFSET, CONSTANT_ISSUE). This suggests a correction in the naming convention to improve code clarity and maintainability, which can be considered a bug fix related to code quality. The answer is **Buggy**."
sonarqube,18133.json,4a108310e196bcff760bc81de326346b1ee4ac4c,"@@ -1,3 +1,3 @@
   public String coefficient() {
-    return factor;
+    return coefficient;
   }",NotBuggy,"Fix quality flaws
",Buggy,"The commit message indicates a fix for ""quality flaws."" The diff shows a change from `factor` to `coefficient` in the `coefficient()` method. This suggests that the original code had a typo or used an incorrect variable name, which is a quality flaw. This change corrects the code to use the correct variable, thus fixing the flaw. The answer is **Buggy**.
"
sonarqube,18138.json,4a108310e196bcff760bc81de326346b1ee4ac4c,"@@ -1,16 +1,14 @@
   public boolean equals(Object o) {
+    if (!(o instanceof DefaultDebtRemediationFunction)) {
+      return false;
+    }
     if (this == o) {
       return true;
     }
-    if (o == null || getClass() != o.getClass()) {
-      return false;
-    }
-    DefaultDebtRemediationFunction that = (DefaultDebtRemediationFunction) o;
-    if (factor != null ? !factor.equals(that.factor) : that.factor != null) {
-      return false;
-    }
-    if (offset != null ? !offset.equals(that.offset) : that.offset != null) {
-      return false;
-    }
-    return type == that.type;
+    DefaultDebtRemediationFunction other = (DefaultDebtRemediationFunction) o;
+    return new EqualsBuilder()
+      .append(coefficient, other.coefficient())
+      .append(offset, other.offset())
+      .append(type, other.type())
+      .isEquals();
   }",NotBuggy,"Fix quality flaws
",Buggy,"The original `equals` method checks for null and class equality before casting to `DefaultDebtRemediationFunction`. The updated code first checks if the object is an instance of `DefaultDebtRemediationFunction` and returns false if it is not. Then, it uses `EqualsBuilder` to compare the fields. The change addresses a potential issue where the `equals` method might throw a `ClassCastException` if an object of a different class is passed. This is a bug fix. The answer is **Buggy**."
sonarqube,18139.json,4a108310e196bcff760bc81de326346b1ee4ac4c,"@@ -1,6 +1,6 @@
   public int hashCode() {
     int result = type.hashCode();
-    result = 31 * result + (factor != null ? factor.hashCode() : 0);
+    result = 31 * result + (coefficient != null ? coefficient.hashCode() : 0);
     result = 31 * result + (offset != null ? offset.hashCode() : 0);
     return result;
   }",NotBuggy,"Fix quality flaws
",Buggy,"The commit message indicates a fix for quality flaws. The diff shows a change in the `hashCode()` method where `factor` is replaced with `coefficient`. This suggests that the original code might have been using the wrong field (`factor`) for calculating the hash code, which would be a flaw in the quality of the code. The change corrects this by using the correct field (`coefficient`). The answer is **Buggy**."
sonarqube,10480.json,d20f21e4cd409de904ef4a0ae5926b61706b402e,"@@ -1,4 +1,4 @@
-  public QualityProfileDto setParent(String parent) {
+  public QualityProfileDto setParent(@Nullable String parent) {
     this.parent = parent;
     return this;
   }",NotBuggy,"Fix quality flaws
",Buggy,"The commit message indicates a fix for quality flaws. The diff shows a change in the `setParent` method's parameter from `String parent` to `@Nullable String parent`. This change suggests that the method now explicitly allows a null value for the `parent` parameter, which was not previously allowed. This could be a fix for a potential NullPointerException or other issues that might arise when a null value is passed to the method. Therefore, the change indicates a bug fix. The answer is **Buggy**.
"
sonarqube,10472.json,d20f21e4cd409de904ef4a0ae5926b61706b402e,"@@ -1,4 +1,4 @@
-  public QualityProfileDto setParent(String parent) {
+  public QualityProfileDto setParent(@Nullable String parent) {
     this.parent = parent;
     return this;
   }",NotBuggy,"Fix quality flaws
",Buggy,"The commit message indicates a fix for quality flaws. The diff shows a change in the `setParent` method, adding the `@Nullable` annotation to the `parent` parameter. This annotation clarifies that the `parent` parameter can accept null values, which improves code clarity and helps prevent potential NullPointerExceptions. This is a bug fix. The answer is **Buggy**."
sonarqube,9636.json,d20f21e4cd409de904ef4a0ae5926b61706b402e,"@@ -1,4 +1,4 @@
-  public QualityProfileDto setParent(String parent) {
+  public QualityProfileDto setParent(@Nullable String parent) {
     this.parent = parent;
     return this;
   }",NotBuggy,"Fix quality flaws
",Buggy,"The commit message indicates a fix for quality flaws. The diff adds a `@Nullable` annotation to the `parent` parameter of the `setParent` method. This change suggests that the `parent` parameter can now accept null values, which might not have been the case before. This could be a fix for a potential NullPointerException or other issues related to null values being passed to the `setParent` method. The addition of `@Nullable` indicates an improvement in handling null values, which can be considered a bug fix in terms of code quality and potential error prevention. The answer is **Buggy**."
sonarqube,19951.json,d102a8a9916bac0ab600cd5e90ba5359766f8d6d,"@@ -1,12 +1,12 @@
   private void autodetection() {
-    for (ScmProvider provider : providerPerKey.values()) {
-      if (provider.supports(projectReactor.getRoot().getBaseDir())) {
+    for (ScmProvider installedProvider : providerPerKey.values()) {
+      if (installedProvider.supports(projectReactor.getRoot().getBaseDir())) {
         if (this.provider == null) {
-          this.provider = provider;
+          this.provider = installedProvider;
         } else {
-          throw new IllegalStateException(""SCM provider autodetection failed. Both "" + this.provider.key() + "" and "" + provider.key()
+          throw new IllegalStateException(""SCM provider autodetection failed. Both "" + this.provider.key() + "" and "" + installedProvider.key()
             + "" claim to support this project. Please use "" + CoreProperties.SCM_PROVIDER_KEY + "" to define SCM of your project."");
         }
       }
     }
   }",NotBuggy,"Fix some quality flaws
",NotBuggy,"The commit message indicates a fix for ""quality flaws."" The code changes involve renaming the `provider` variable to `installedProvider` within the `autodetection` method. This change improves code readability and clarity by better reflecting the variable's purpose, which is to represent an installed SCM provider. This change does not fix a bug, but improves code quality. The answer is **NotBuggy**."
sonarqube,19956.json,d102a8a9916bac0ab600cd5e90ba5359766f8d6d,"@@ -1,3 +1,3 @@
   public void stop() {
-
+    // Nothing to do
   }",NotBuggy,"Fix some quality flaws
",NotBuggy,"The commit message indicates a fix for ""quality flaws."" The diff shows an empty `stop()` method being filled with a comment ""// Nothing to do"". While it's vague, adding a comment to an empty method can be considered a quality improvement, possibly to clarify that the method is intentionally empty and prevent future confusion or modification. This change doesn't directly address a functional bug, but rather improves code clarity and maintainability. The answer is **NotBuggy**."
sonarqube,4891.json,289be70bf6056b5f3c66a797ecab019891e731d6,"@@ -1,28 +1,30 @@
   public void handle(Request request, Response response) throws Exception {
     int pageSize = request.mandatoryParamAsInt(Param.PAGE_SIZE);
     int page = request.mandatoryParamAsInt(Param.PAGE);
     String queryString = request.param(Param.TEXT_QUERY);
     String selected = request.mandatoryParam(Param.SELECTED);
 
     try (DbSession dbSession = dbClient.openSession(false)) {
       GroupId group = support.findGroup(dbSession, request);
       userSession.checkPermission(OrganizationPermission.ADMINISTER, group.getOrganizationUuid());
 
       UserMembershipQuery query = UserMembershipQuery.builder()
         .groupId(group.getId())
         .organizationUuid(group.getOrganizationUuid())
         .memberSearch(queryString)
         .membership(getMembership(selected))
         .pageIndex(page)
         .pageSize(pageSize)
         .build();
       int total = dbClient.groupMembershipDao().countMembers(dbSession, query);
       Paging paging = forPageIndex(page).withPageSize(pageSize).andTotal(total);
       List<UserMembershipDto> users = dbClient.groupMembershipDao().selectMembers(dbSession, query, paging.offset(), paging.pageSize());
 
-      JsonWriter json = response.newJsonWriter().beginObject();
-      writeMembers(json, users);
-      writePaging(json, paging);
-      json.endObject().close();
+      try (JsonWriter json = response.newJsonWriter()) {
+        json.beginObject();
+        writeMembers(json, users);
+        writePaging(json, paging);
+        json.endObject();
+      }
     }
   }",NotBuggy,"Fix Quality flaws
",Buggy,"The diff introduces a try-with-resources block for the `JsonWriter`. This ensures that the `JsonWriter` is properly closed, even if exceptions occur during the writing process. The original code closed the writer with `json.endObject().close()`, but this might not be executed if an exception is thrown before that point, potentially leading to resource leaks or incomplete JSON output. The change addresses a potential resource management issue and improves the robustness of the code. The answer is **Buggy**."
sonarqube,2559.json,289be70bf6056b5f3c66a797ecab019891e731d6,"@@ -1,7 +1,9 @@
   static void writeResponse(BulkChangeResult result, Response response) {
-    JsonWriter json = response.newJsonWriter().beginObject();
-    json.prop(""succeeded"", result.countSucceeded());
-    json.prop(""failed"", result.countFailed());
-    WebServiceEngine.writeErrors(json, result.getErrors());
-    json.endObject().close();
+    try (JsonWriter json = response.newJsonWriter()) {
+      json.beginObject();
+      json.prop(""succeeded"", result.countSucceeded());
+      json.prop(""failed"", result.countFailed());
+      WebServiceEngine.writeErrors(json, result.getErrors());
+      json.endObject();
+    }
   }",NotBuggy,"Fix Quality flaws
",Buggy,"The diff introduces a try-with-resources block around the `JsonWriter` object. This ensures that the `close()` method is always called on the `JsonWriter`, even if an exception occurs within the block. This prevents resource leaks and improves the robustness of the code. The commit message ""Fix Quality flaws"" is vague, but this change addresses a potential resource management issue, which can be considered a quality flaw. The answer is **Buggy**."
sonarqube,18845.json,38e53c0572db4e267a704666cb0d1343992df5e1,"@@ -1,3 +1,3 @@
-  public void resolveOrder() throws CyclicDependenciesException {
+  public void resolveOrder() {
     resolveOrder(toString());
   }",NotBuggy,"Fix some quality flaws
",Buggy,"The commit message indicates a fix for ""quality flaws."" The diff removes the `throws CyclicDependenciesException` declaration from the `resolveOrder` method. This suggests that the method was previously declared to throw an exception that it no longer throws, or that the exception handling has been changed. This could be due to a bug fix where the code was incorrectly throwing an exception, or where the exception was not being handled correctly. The removal of the exception declaration suggests a change in the method's behavior related to error handling, which is indicative of a bug fix. The answer is **Buggy**.
"
sonarqube,18846.json,38e53c0572db4e267a704666cb0d1343992df5e1,"@@ -1,19 +1,19 @@
-  private int resolveOrder(String path) throws CyclicDependenciesException {
+  private int resolveOrder(String path) {
     seen = true;
     try {
       int highOrder = -1;
       for (Node dep : dependencies) {
         if (dep.seen) {
           throw new CyclicDependenciesException(path + "" -> "" + dep.toString());
         }
         highOrder = Math.max(highOrder, dep.resolveOrder(path + "" -> "" + dep.toString()));
 
       }
 
       order = highOrder + 1;
       return order;
 
     } finally {
       seen = false;
     }
   }",NotBuggy,"Fix some quality flaws
",Buggy,"The commit message indicates a fix for ""quality flaws."" The diff removes the `throws CyclicDependenciesException` declaration from the `resolveOrder` method signature. While the code still throws the exception, it is now handled within the method itself, likely to prevent the exception from propagating up the call stack. This change suggests an improvement in error handling or exception management, which can be considered a bug fix in terms of code quality and robustness. The answer is **Buggy**."
sonarqube,8517.json,38e53c0572db4e267a704666cb0d1343992df5e1,"@@ -1,35 +1,33 @@
   public static Collection<String> listResources(ClassLoader classLoader, String rootPath, Predicate<String> predicate) {
     try {
       Collection<String> paths = Lists.newArrayList();
       rootPath = StringUtils.removeStart(rootPath, ""/"");
 
       URL root = classLoader.getResource(rootPath);
       if (root != null) {
-        if (!""jar"".equals(root.getProtocol())) {
-          throw new IllegalStateException(""Unsupported protocol: "" + root.getProtocol());
-        }
+        checkJarFile(root);
 
         // Path of the root directory
         // Examples :
         // org/sonar/sqale/index.txt  -> rootDirectory is org/sonar/sqale
         // org/sonar/sqale/  -> rootDirectory is org/sonar/sqale
         // org/sonar/sqale  -> rootDirectory is org/sonar/sqale
         String rootDirectory = rootPath;
         if (StringUtils.substringAfterLast(rootPath, ""/"").indexOf('.') >= 0) {
           rootDirectory = StringUtils.substringBeforeLast(rootPath, ""/"");
         }
         String jarPath = root.getPath().substring(5, root.getPath().indexOf(""!"")); //strip out only the JAR file
         JarFile jar = new JarFile(URLDecoder.decode(jarPath, CharEncoding.UTF_8));
         Enumeration<JarEntry> entries = jar.entries();
         while (entries.hasMoreElements()) {
           String name = entries.nextElement().getName();
           if (name.startsWith(rootDirectory) && predicate.apply(name)) {
             paths.add(name);
           }
         }
       }
       return paths;
     } catch (Exception e) {
       throw Throwables.propagate(e);
     }
   }",NotBuggy,"Fix some quality flaws
",Buggy,"The commit message ""Fix some quality flaws"" is vague, but the diff provides more context. The code removes a check for the ""jar"" protocol and replaces it with a call to `checkJarFile(root)`. This suggests that the original code might have been too restrictive, potentially causing issues when dealing with different types of URLs or class loaders. The removal of the `IllegalStateException` and the introduction of `checkJarFile` indicates a potential bug fix or at least an improvement in error handling or supported scenarios.

Therefore, the answer is **Buggy**."
sonarqube,7268.json,fd78d281d57d7b1ffa49d27cd03de39b6a961592,"@@ -1,14 +1,14 @@
   private UserDto synchronize(String userLogin, UserDetails details, HttpServletRequest request) {
     String name = details.getName();
     UserIdentity.Builder userIdentityBuilder = UserIdentity.builder()
       .setLogin(userLogin)
-      .setName(isNullOrEmpty(name) ? userLogin : name)
+      .setName(isEmpty(name) ? userLogin : name)
       .setEmail(trimToNull(details.getEmail()))
       .setProviderLogin(userLogin);
     if (externalGroupsProvider != null) {
       ExternalGroupsProvider.Context context = new ExternalGroupsProvider.Context(userLogin, request);
       Collection<String> groups = externalGroupsProvider.doGetGroups(context);
       userIdentityBuilder.setGroups(new HashSet<>(groups));
     }
     return userIdentityAuthenticator.authenticate(userIdentityBuilder.build(), new ExternalIdentityProvider());
   }",NotBuggy,"Fix Quality flaws

- missing tests for Protobuf
- remove incorrect imports of org.elasticsearch.common.Strings
",NotBuggy,"The commit message indicates fixing quality flaws, specifically mentioning the removal of incorrect imports and missing tests, but the diff focuses on a change within the `synchronize` method. The change replaces `isNullOrEmpty(name)` with `isEmpty(name)`. While `isNullOrEmpty` might have been causing issues, the commit message doesn't explicitly point to a bug related to this logic. It's more likely a code quality improvement or a refactoring to use a more appropriate method.

The change itself suggests a potential issue with how empty names are handled, which could be considered a bug fix if `isNullOrEmpty` and `isEmpty` have different behaviors that lead to incorrect name handling. However, without knowing the exact difference between these two methods and the context of their usage, it is difficult to definitively say that this change fixes a bug.

Given the information, it is more likely a refactoring or code quality improvement rather than a bug fix.

**NotBuggy**
"
sonarqube,7206.json,fd78d281d57d7b1ffa49d27cd03de39b6a961592,"@@ -1,12 +1,12 @@
   private static Optional<String> getTokenFromCookie(HttpServletRequest request) {
     Optional<Cookie> jwtCookie = findCookie(JWT_COOKIE, request);
     if (!jwtCookie.isPresent()) {
       return Optional.empty();
     }
     Cookie cookie = jwtCookie.get();
     String token = cookie.getValue();
-    if (isNullOrEmpty(token)) {
+    if (isEmpty(token)) {
       return Optional.empty();
     }
     return Optional.of(token);
   }",NotBuggy,"Fix Quality flaws

- missing tests for Protobuf
- remove incorrect imports of org.elasticsearch.common.Strings
",Buggy,"The commit message indicates fixing quality flaws, including removing incorrect imports and missing tests. The diff replaces `isNullOrEmpty(token)` with `isEmpty(token)`. While both methods likely check for null or empty strings, this change suggests a correction to use the appropriate method within the codebase, potentially addressing a previous inconsistency or error. This change can be considered a bug fix because it corrects an error. The answer is **Buggy**."
sonarqube,3674.json,fd78d281d57d7b1ffa49d27cd03de39b6a961592,"@@ -1,7 +1,7 @@
   private List<Setting> loadDefaultSettings(Set<String> keys) {
     return propertyDefinitions.getAll().stream()
       .filter(definition -> keys.contains(definition.key()))
-      .filter(defaultProperty -> !isNullOrEmpty(defaultProperty.defaultValue()))
+      .filter(defaultProperty -> !isEmpty(defaultProperty.defaultValue()))
       .map(Setting::createForDefinition)
       .collect(Collectors.toList());
   }",NotBuggy,"Fix Quality flaws

- missing tests for Protobuf
- remove incorrect imports of org.elasticsearch.common.Strings
",Buggy,"The commit message indicates fixing quality flaws, including removing incorrect imports and missing tests. The diff replaces `isNullOrEmpty` with `isEmpty`. This change suggests that `isNullOrEmpty` was either incorrectly used or was causing issues, and `isEmpty` provides a more correct or robust implementation. This indicates a bug fix. The answer is **Buggy**."
sonarqube,4743.json,fd78d281d57d7b1ffa49d27cd03de39b6a961592,"@@ -1,8 +1,8 @@
   private UserDto authenticate(HttpServletRequest request) {
     String login = request.getParameter(""login"");
     String password = request.getParameter(""password"");
-    if (isNullOrEmpty(login) || isNullOrEmpty(password)) {
+    if (isEmpty(login) || isEmpty(password)) {
       throw new UnauthorizedException();
     }
     return credentialsAuthenticator.authenticate(login, password, request);
   }",NotBuggy,"Fix Quality flaws

- missing tests for Protobuf
- remove incorrect imports of org.elasticsearch.common.Strings
",Buggy,"The commit message indicates fixing quality flaws, including removing incorrect imports. The diff replaces `isNullOrEmpty` with `isEmpty`. While both might check for null or empty strings, the change suggests a correction to use the appropriate utility method within the codebase. This could be due to `isNullOrEmpty` being deprecated, incorrectly implemented, or simply a style correction to align with the project's conventions. It's likely addressing a potential issue or inconsistency, which qualifies as a bug fix. The answer is **Buggy**."
sonarqube,20010.json,5e0b742ac8299c7ddcd2bf3dfe682482c4d8eda4,"@@ -1,9 +1,9 @@
   private static void cleanTempFolders(Path path) throws IOException {
-    if (Files.exists(path)) {
+    if (path.toFile().exists()) {
       try (DirectoryStream<Path> stream = Files.newDirectoryStream(path, new CleanFilter())) {
         for (Path p : stream) {
           deleteQuietly(p.toFile());
         }
       }
     }
   }",NotBuggy,"Fix Quality flaws
",NotBuggy,"The commit message indicates a fix for ""Quality flaws"". The code change replaces `Files.exists(path)` with `path.toFile().exists()`. While both methods check for the existence of a file or directory, `Files.exists(Path)` might offer better performance or handle symbolic links differently in some environments. However, the change itself doesn't inherently suggest a bug fix related to incorrect behavior. It's more likely a change to address a potential performance issue or improve compatibility across different file systems. Therefore, it's not necessarily a bug fix in the sense of correcting a functional error.

**NotBuggy**
"
sonarqube,20011.json,5e0b742ac8299c7ddcd2bf3dfe682482c4d8eda4,"@@ -1,24 +1,24 @@
     public boolean accept(Path path) throws IOException {
-      if (!Files.isDirectory(path)) {
+      if (!path.toFile().exists()) {
         return false;
       }
 
       if (!path.getFileName().toString().startsWith(TMP_NAME_PREFIX)) {
         return false;
       }
 
       long threshold = System.currentTimeMillis() - CLEAN_MAX_AGE;
 
       // we could also check the timestamp in the name, instead
       BasicFileAttributes attrs;
 
       try {
         attrs = Files.readAttributes(path, BasicFileAttributes.class);
       } catch (IOException ioe) {
         LOG.error(String.format(""Couldn't read file attributes for %s : "", path), ioe);
         return false;
       }
 
       long creationTime = attrs.creationTime().toMillis();
       return creationTime < threshold;
     }",NotBuggy,"Fix Quality flaws
",Buggy,"The commit message indicates a fix for ""Quality flaws."" The code changes involve modifying the `accept` method, which determines whether a given path should be accepted based on certain criteria. The original code checked if the path was not a directory using `Files.isDirectory(path)`. This has been changed to `!path.toFile().exists()`. This change suggests that the original check might have been insufficient or incorrect, potentially leading to errors or unexpected behavior. The new check ensures that the file exists before proceeding with other checks. This is a bug fix. The answer is **Buggy**."
sonarqube,8900.json,5e0b742ac8299c7ddcd2bf3dfe682482c4d8eda4,"@@ -1,7 +1,7 @@
-  public static void main(String[] args) throws Exception {
+  public static void main(String[] args) {
     ProcessEntryPoint entryPoint = ProcessEntryPoint.createForArguments(args);
     Props props = entryPoint.getProps();
     new WebServerProcessLogging().configure(props);
     WebServer server = new WebServer(props);
     entryPoint.launch(server);
   }",NotBuggy,"Fix Quality flaws
",Buggy,"The diff removes the `throws Exception` declaration from the `main` method. This change suggests a fix for a potential unhandled exception that could crash the application. By removing the `throws` declaration, the code now needs to handle any exceptions that might occur within the `main` method, likely preventing the program from crashing due to an unhandled exception. The answer is **Buggy**.
"
sonarqube,4469.json,0584dc8d5d5d82a2035763c34afba0e12cf6e8d0,"@@ -1,15 +1,15 @@
   public void handle(Request wsRequest, Response wsResponse) throws Exception {
     try (DbSession dbSession = dbClient.openSession(false)) {
       WsTemplateRef templateRef = WsTemplateRef.fromRequest(wsRequest);
       PermissionTemplateDto template = support.findTemplate(dbSession, templateRef);
       checkGlobalAdmin(userSession, template.getOrganizationUuid());
 
       PermissionQuery query = buildPermissionQuery(wsRequest);
-      int total = dbClient.permissionTemplateDao().countGroupNamesByQueryAndTemplate(dbSession, query, template.getId());
+      int total = dbClient.permissionTemplateDao().countGroupNamesByQueryAndTemplate(dbSession, query, template.getOrganizationUuid(), template.getId());
       Paging paging = Paging.forPageIndex(wsRequest.mandatoryParamAsInt(PAGE)).withPageSize(wsRequest.mandatoryParamAsInt(PAGE_SIZE)).andTotal(total);
       List<GroupDto> groups = findGroups(dbSession, query, template);
       List<PermissionTemplateGroupDto> groupPermissions = findGroupPermissions(dbSession, groups, template);
       WsPermissions.WsGroupsResponse groupsResponse = buildResponse(groups, groupPermissions, paging);
       writeProtobuf(groupsResponse, wsRequest, wsResponse);
     }
   }",Buggy,"SONAR-8682 fix error when filtering on name

and a given group name exists in multiple organizations
",Buggy,"The commit message indicates a fix for an error that occurs when filtering on a group name that exists in multiple organizations. The code diff modifies the `countGroupNamesByQueryAndTemplate` method call. It adds `template.getOrganizationUuid()` as a parameter. This suggests that the original query was not specific to the organization, leading to incorrect results when a group name existed in multiple organizations. The change ensures that the count is performed within the correct organization context, thus fixing the bug. The answer is **Buggy**."
sonarqube,12672.json,0584dc8d5d5d82a2035763c34afba0e12cf6e8d0,"@@ -1,3 +1,3 @@
-  public List<String> selectGroupNamesByQueryAndTemplate(DbSession session, PermissionQuery query, long templateId) {
-    return mapper(session).selectGroupNamesByQueryAndTemplate(query, templateId, new RowBounds(query.getPageOffset(), query.getPageSize()));
+  public List<String> selectGroupNamesByQueryAndTemplate(DbSession session, PermissionQuery query, String organizationUuid, long templateId) {
+    return mapper(session).selectGroupNamesByQueryAndTemplate(organizationUuid, templateId, query, new RowBounds(query.getPageOffset(), query.getPageSize()));
   }",Buggy,"SONAR-8682 fix error when filtering on name

and a given group name exists in multiple organizations
",Buggy,"The commit message indicates a fix for an error when filtering on a group name that exists in multiple organizations. The diff shows a change in the `selectGroupNamesByQueryAndTemplate` method signature, adding an `organizationUuid` parameter. This suggests that the original method was not considering the organization when querying for group names, leading to the reported error. The modified method now includes the organization UUID in the query, which should resolve the issue of ambiguous group names across different organizations. The answer is **Buggy**."
sonarqube,4472.json,0584dc8d5d5d82a2035763c34afba0e12cf6e8d0,"@@ -1,8 +1,8 @@
   private List<GroupDto> findGroups(DbSession dbSession, PermissionQuery dbQuery, PermissionTemplateDto template) {
-    List<String> orderedNames = dbClient.permissionTemplateDao().selectGroupNamesByQueryAndTemplate(dbSession, dbQuery, template.getId());
+    List<String> orderedNames = dbClient.permissionTemplateDao().selectGroupNamesByQueryAndTemplate(dbSession, dbQuery, template.getOrganizationUuid(), template.getId());
     List<GroupDto> groups = dbClient.groupDao().selectByNames(dbSession, template.getOrganizationUuid(), orderedNames);
     if (orderedNames.contains(DefaultGroups.ANYONE)) {
       groups.add(0, new GroupDto().setId(0L).setName(DefaultGroups.ANYONE));
     }
     return Ordering.explicit(orderedNames).onResultOf(GroupDto::getName).immutableSortedCopy(groups);
   }",Buggy,"SONAR-8682 fix error when filtering on name

and a given group name exists in multiple organizations
",Buggy,"The commit message indicates a fix for an error that occurs when filtering on a group name that exists in multiple organizations. The code diff modifies the `findGroups` method to include the `template.getOrganizationUuid()` when selecting group names. This suggests that the original query was not specific enough, leading to incorrect results when the same group name existed in multiple organizations. By adding the organization UUID to the query, the code ensures that only groups within the correct organization are returned, thus fixing the bug. The answer is **Buggy**.
"
sonarqube,12673.json,0584dc8d5d5d82a2035763c34afba0e12cf6e8d0,"@@ -1,3 +1,3 @@
-  public int countGroupNamesByQueryAndTemplate(DbSession session, PermissionQuery query, long templateId) {
-    return mapper(session).countGroupNamesByQueryAndTemplate(query, templateId);
+  public int countGroupNamesByQueryAndTemplate(DbSession session, PermissionQuery query, String organizationUuid, long templateId) {
+    return mapper(session).countGroupNamesByQueryAndTemplate(organizationUuid, query, templateId);
   }",Buggy,"SONAR-8682 fix error when filtering on name

and a given group name exists in multiple organizations
",Buggy,"The commit message indicates a fix for an error when filtering on a group name that exists in multiple organizations. The diff shows a change in the `countGroupNamesByQueryAndTemplate` method signature, adding an `organizationUuid` parameter. This suggests that the original method was not considering the organization context when counting group names, leading to incorrect results when the same group name existed in multiple organizations. The addition of the `organizationUuid` parameter allows the method to correctly filter group names based on the organization, thus fixing the bug. The answer is **Buggy**."
voldemort,9608.json,4a463214895e56db53db78064bde3ebd10b6e949,"@@ -1,8 +1,8 @@
-    public void recordOpTimeUs(SocketDestination dest, long opTimeUs) {
+    public void recordOpTimeNs(SocketDestination dest, long opTimeNs) {
         if (dest != null) {
-            getOrCreateNodeStats(dest).recordOpTimeUs(null, opTimeUs);
-            recordOpTimeUs(null, opTimeUs);
+            getOrCreateNodeStats(dest).recordOpTimeNs(null, opTimeNs);
+            recordOpTimeNs(null, opTimeNs);
         } else {
-            this.opTimeRequestCounter.addRequest(opTimeUs * Time.NS_PER_US);
+            this.opTimeRequestCounter.addRequest(opTimeNs);
         }
     }",Buggy,"Fix time convertion (Nano to micro) bug

Make the method accept nanoseconds to avoid unnecessary
convertion.
",Buggy,"The commit message indicates a bug fix related to time conversion from nanoseconds to microseconds. The code diff shows that the method `recordOpTimeUs` has been renamed to `recordOpTimeNs`, and it now accepts nanoseconds directly. The multiplication by `Time.NS_PER_US` has been removed from the else block, suggesting that the original code was incorrectly converting nanoseconds to microseconds when it wasn't necessary. This change directly addresses the bug mentioned in the commit message. The answer is **Buggy**."
voldemort,8659.json,da027c470c327f2e0759acae2b3b46dfa0b114b2,"@@ -1,15 +1,15 @@
         void streamingPut(ByteArray key, Versioned<byte[]> value) {
-            if(overwrite == false) {
-                dstStreamingClient.streamingPut(key, value);
-            } else {
+            if(overwrite) {
                 VectorClock denseClock = VectorClockUtils.makeClockWithCurrentTime(dstServerIds);
                 Versioned<byte[]> updatedValue = new Versioned<byte[]>(value.getValue(), denseClock);
                 dstStreamingClient.streamingPut(key, updatedValue);
+            } else {
+                dstStreamingClient.streamingPut(key, value);
             }
 
             entriesForkLifted++;
             if(entriesForkLifted % progressOps == 0) {
                 logger.info(workName + "" fork lifted "" + entriesForkLifted
                             + "" entries successfully"");
             }
         }",NotBuggy,"Incorporate code review comments

Fix error message
Change the order of if/else
",Buggy,"The commit message indicates incorporating code review comments, fixing an error message, and changing the order of if/else statements. The diff shows a change in the order of the `if/else` block related to the `overwrite` flag. The original code had `if(overwrite == false)`, which has been changed to `if(overwrite)`. This change in logic suggests a potential bug fix related to how the `overwrite` flag was being handled. The change ensures that the correct branch is executed based on whether overwriting is enabled or not. The answer is **Buggy**.
"
voldemort,8669.json,da027c470c327f2e0759acae2b3b46dfa0b114b2,"@@ -1,82 +1,82 @@
     public static void main(String[] args) throws Exception {
         OptionParser parser = null;
         OptionSet options = null;
         try {
             parser = getParser();
             options = parser.parse(args);
         } catch(Exception oe) {
             logger.error(""Exception processing command line options"", oe);
             parser.printHelpOn(System.out);
             return;
         }
 
         /* validate options */
         if(options.has(""help"")) {
             parser.printHelpOn(System.out);
             return;
         }
 
         if(!options.has(""src-url"") || !options.has(""dst-url"")) {
             logger.error(""Both 'src-url' and 'dst-url' options are mandatory"");
             parser.printHelpOn(System.out);
             return;
         }
 
         String srcBootstrapUrl = (String) options.valueOf(""src-url"");
         String dstBootstrapUrl = (String) options.valueOf(""dst-url"");
         int maxPutsPerSecond = DEFAULT_MAX_PUTS_PER_SEC;
         if(options.has(""max-puts-per-second""))
             maxPutsPerSecond = (Integer) options.valueOf(""max-puts-per-second"");
         List<String> storesList = null;
         if(options.has(""stores"")) {
             storesList = new ArrayList<String>((List<String>) options.valuesOf(""stores""));
         }
         List<Integer> partitions = null;
         if(options.has(""partitions"")) {
             partitions = (List<Integer>) options.valuesOf(""partitions"");
         }
 
         int partitionParallelism = DEFAULT_PARTITION_PARALLELISM;
         if(options.has(""parallelism"")) {
             partitionParallelism = (Integer) options.valueOf(""parallelism"");
         }
         int progressOps = DEFAULT_PROGRESS_PERIOD_OPS;
         if(options.has(""progress-period-ops"")) {
             progressOps = (Integer) options.valueOf(""progress-period-ops"");
         }
 
         ForkLiftTaskMode mode;
         mode = ForkLiftTaskMode.primary_resolution;
         if(options.has(""mode"")) {
             mode = Utils.getEnumFromString(ForkLiftTaskMode.class, (String) options.valueOf(""mode""));
             if(mode == null)
                 mode = ForkLiftTaskMode.primary_resolution;
 
         }
 
         Boolean overwrite = false;
-        if(options.has(""overwrite"")) {
-            if(options.hasArgument(""overwrite"")) {
-                overwrite = (Boolean) options.valueOf(""overwrite"");
+        if(options.has(OVERWRITE_OPTION)) {
+            if(options.hasArgument(OVERWRITE_OPTION)) {
+                overwrite = (Boolean) options.valueOf(OVERWRITE_OPTION);
             } else {
                 overwrite = true;
             }
         }
 
         if(overwrite) {
             logger.warn(OVERWRITE_WARNING_MESSAGE);
         }
 
         ClusterForkLiftTool forkLiftTool = new ClusterForkLiftTool(srcBootstrapUrl,
                                                                    dstBootstrapUrl,
                                                                    overwrite,
                                                                    maxPutsPerSecond,
                                                                    partitionParallelism,
                                                                    progressOps,
                                                                    storesList,
                                                                    partitions,
                                                                    mode);
         forkLiftTool.run();
         // TODO cleanly shut down the hanging threadpool
         System.exit(0);
     }",NotBuggy,"Incorporate code review comments

Fix error message
Change the order of if/else
",Buggy,"The commit message indicates changes related to code review comments, specifically ""Fix error message"" and ""Change the order of if/else"". The diff shows a change in the option name from `""overwrite""` to `OVERWRITE_OPTION` and a corresponding change in the `options.hasArgument()` check. Additionally, a warning message is logged when the overwrite option is enabled. These changes suggest improvements to error handling and code structure based on review feedback. The change from `""overwrite""` to `OVERWRITE_OPTION` suggests a bug fix or at least an improvement in code maintainability and readability by using a constant. The reordering of the if/else block for the `overwrite` option also indicates a logical correction or improvement in code clarity.

The changes made in the code diff, such as fixing the error message and reordering if/else statements, directly address the issues mentioned in the commit message. These changes are likely the result of code review feedback and aim to improve the overall quality and maintainability of the code.

The changes related to the `overwrite` option, including the use of `OVERWRITE_OPTION` and the reordering of the if/else block, suggest a bug fix or at least an improvement in code maintainability and readability.

Therefore, the answer is **Buggy**.
"
voldemort,9827.json,8fe6ef89e6f219a246413f918167b573f69e070c,"@@ -1,18 +1,20 @@
     public boolean delete(ByteArray key, Version version) throws VoldemortException {
         boolean deleteSuccessful = false;
         StoreUtils.assertValidKey(key);
         String keyString = new String(key.get());
         String initialValue = this.metadataMap.get(keyString);
         if(initialValue != null) {
             String removedValue = this.metadataMap.remove(keyString);
             if(removedValue != null) {
                 deleteSuccessful = (initialValue.equals(removedValue));
             }
         }
         if(deleteSuccessful) {
             this.flushData();
             // Reset the vector clock and persist it.
+            // FIXME this also needs to be done per entry, as opposed to
+            // versioning the file.
             writeVersion(new VectorClock());
         }
         return deleteSuccessful;
     }",NotBuggy,"Workaround SystemStore bug and fix quota store inserts
",Buggy,"The commit message indicates a workaround for a SystemStore bug and a fix for quota store inserts. The diff shows a modification in the `delete` method. Specifically, there's a comment `// FIXME this also needs to be done per entry, as opposed to versioning the file.` along with a call to `writeVersion(new VectorClock())`. The comment suggests that the current implementation of versioning the file is incorrect and needs to be done per entry. This indicates a bug fix related to versioning. The answer is **Buggy**.
"
voldemort,9441.json,e7ecec1bd3dd879221a56714fc774a2001d843e8,"@@ -1,35 +1,36 @@
     public void run() {
         Node proxyNode = metadata.getCluster().getNodeById(destinationNode);
         long startNs = System.nanoTime();
         try {
             // TODO there are no retries now if the node we want to write to is
             // unavailable
             redirectingStore.checkNodeAvailable(proxyNode);
             Store<ByteArray, byte[], byte[]> socketStore = redirectingStore.getRedirectingSocketStore(redirectingStore.getName(),
                                                                                                       destinationNode);
 
             socketStore.put(key, value, transforms);
             redirectingStore.recordSuccess(proxyNode, startNs);
+            redirectingStore.reportProxyPutSuccess();
             if(logger.isTraceEnabled()) {
                 logger.trace(""Proxy write for store "" + redirectingStore.getName() + "" key ""
-                             + ByteUtils.toBinaryString(key.get()) + "" to destinationNode:""
+                             + ByteUtils.toHexString(key.get()) + "" to destinationNode:""
                              + destinationNode);
             }
         } catch(UnreachableStoreException e) {
             redirectingStore.recordException(proxyNode, startNs, e);
             logFailedProxyPutIfNeeded(e);
         } catch(ObsoleteVersionException ove) {
             /*
              * Proxy puts can get an OVE if somehow there are two stealers for
              * the same proxy node and the other stealer's proxy put already got
              * tothe proxy node.. This will not result from online put winning,
              * since we don't issue proxy puts if the proxy node is still a
              * replica
              */
             logFailedProxyPutIfNeeded(ove);
         } catch(Exception e) {
             // Just log the key.. Not sure having values in the log is a good
             // idea.
             logFailedProxyPutIfNeeded(e);
         }
     }",NotBuggy,"1.Enabling proxy puts by dafault
2. Bug fix in proxy put stats
3. Changing order of state change updates for correctness
4. Setting proxy put tests to do one batch rebalancing
",Buggy,"The commit message mentions ""Bug fix in proxy put stats"". The code diff adds `redirectingStore.reportProxyPutSuccess();` after a successful `socketStore.put` operation. This suggests that the proxy put success was not being reported correctly before, which is a bug. The answer is **Buggy**."
voldemort,7092.json,e7ecec1bd3dd879221a56714fc774a2001d843e8,"@@ -1,122 +1,150 @@
     public void rebalanceStateChange(Cluster cluster,
                                      List<RebalancePartitionsInfo> rebalancePartitionsInfo,
                                      boolean swapRO,
                                      boolean changeClusterMetadata,
                                      boolean changeRebalanceState,
                                      boolean rollback) {
         Cluster currentCluster = metadataStore.getCluster();
 
         logger.info(""Server doing rebalance state change with options [ cluster metadata change - ""
                     + changeClusterMetadata + "" ], [ changing rebalancing state - ""
                     + changeRebalanceState + "" ], [ changing swapping RO - "" + swapRO
                     + "" ], [ rollback - "" + rollback + "" ]"");
 
         // Variables to track what has completed
         List<RebalancePartitionsInfo> completedRebalancePartitionsInfo = Lists.newArrayList();
         List<String> swappedStoreNames = Lists.newArrayList();
         boolean completedClusterChange = false;
         boolean completedRebalanceSourceClusterChange = false;
         Cluster previousRebalancingSourceCluster = null;
 
         try {
-            // CHANGE CLUSTER METADATA
-            if(changeClusterMetadata) {
-                logger.info(""Switching metadata from "" + currentCluster + "" to "" + cluster);
-                changeCluster(MetadataStore.CLUSTER_KEY, cluster);
-                completedClusterChange = true;
-            }
 
-            // SWAP RO DATA FOR ALL STORES
-            if(swapRO) {
-                swapROStores(swappedStoreNames, false);
-            }
-
+            /*
+             * Do the rebalancing state changes. It is important that this
+             * happens before the actual cluster metadata is changed. Here's
+             * what could happen otherwise. When a batch completes with
+             * {current_cluster c2, rebalancing_source_cluster c1} and the next
+             * rebalancing state changes it to {current_cluster c3,
+             * rebalancing_source_cluster c2} is set for the next batch, then
+             * there could be a window during which the state is
+             * {current_cluster c3, rebalancing_source_cluster c1}. On the other
+             * hand, when we update the rebalancing source cluster first, there
+             * is a window where the state is {current_cluster c2,
+             * rebalancing_source_cluster c2}, which still fine, because of the
+             * following. Successful completion of a batch means the cluster is
+             * finalized, so its okay to stop proxying based on {current_cluster
+             * c2, rebalancing_source_cluster c1}. And since the cluster
+             * metadata has not yet been updated to c3, the writes will happen
+             * based on c2.
+             * 
+             * Even if some clients have already seen the {current_cluster c3,
+             * rebalancing_source_cluster c2} state from other servers, the
+             * operation will be rejected with InvalidMetadataException since
+             * this server itself is not aware of C3
+             */
             // CHANGE REBALANCING STATE
             if(changeRebalanceState) {
                 try {
                     previousRebalancingSourceCluster = metadataStore.getRebalancingSourceCluster();
                     if(!rollback) {
                         // Save up the current cluster for Redirecting store
+                        logger.info(""Setting rebalancing source cluster xml from ""
+                                    + previousRebalancingSourceCluster + ""to "" + currentCluster);
                         changeCluster(MetadataStore.REBALANCING_SOURCE_CLUSTER_XML, currentCluster);
                         completedRebalanceSourceClusterChange = true;
 
                         for(RebalancePartitionsInfo info: rebalancePartitionsInfo) {
                             metadataStore.addRebalancingState(info);
                             completedRebalancePartitionsInfo.add(info);
                         }
                     } else {
                         // Reset the rebalancing source cluster back to null
+                        logger.info(""Resetting rebalancing source cluster xml from ""
+                                    + previousRebalancingSourceCluster + ""to null"");
                         changeCluster(MetadataStore.REBALANCING_SOURCE_CLUSTER_XML, null);
                         completedRebalanceSourceClusterChange = true;
 
                         for(RebalancePartitionsInfo info: rebalancePartitionsInfo) {
                             metadataStore.deleteRebalancingState(info);
                             completedRebalancePartitionsInfo.add(info);
                         }
                     }
                 } catch(Exception e) {
                     throw new VoldemortException(e);
                 }
             }
+
+            // CHANGE CLUSTER METADATA
+            if(changeClusterMetadata) {
+                logger.info(""Switching metadata from "" + currentCluster + "" to "" + cluster);
+                changeCluster(MetadataStore.CLUSTER_KEY, cluster);
+                completedClusterChange = true;
+            }
+
+            // SWAP RO DATA FOR ALL STORES
+            if(swapRO) {
+                swapROStores(swappedStoreNames, false);
+            }
         } catch(VoldemortException e) {
 
             logger.error(""Got exception while changing state, now rolling back changes"", e);
 
             // ROLLBACK CLUSTER CHANGE
             if(completedClusterChange) {
                 try {
                     logger.info(""Rolling back cluster.xml to "" + currentCluster);
                     changeCluster(MetadataStore.CLUSTER_KEY, currentCluster);
                 } catch(Exception exception) {
                     logger.error(""Error while rolling back cluster metadata to "" + currentCluster,
                                  exception);
                 }
             }
 
             // SWAP RO DATA FOR ALL COMPLETED STORES
             if(swappedStoreNames.size() > 0) {
                 try {
                     swapROStores(swappedStoreNames, true);
                 } catch(Exception exception) {
                     logger.error(""Error while swapping back to old state "", exception);
                 }
             }
 
             // CHANGE BACK ALL REBALANCING STATES FOR COMPLETED ONES
             if(completedRebalancePartitionsInfo.size() > 0) {
                 if(!rollback) {
                     for(RebalancePartitionsInfo info: completedRebalancePartitionsInfo) {
                         try {
                             metadataStore.deleteRebalancingState(info);
                         } catch(Exception exception) {
                             logger.error(""Error while deleting back rebalance info during error rollback ""
                                                  + info,
                                          exception);
                         }
                     }
                 } else {
                     for(RebalancePartitionsInfo info: completedRebalancePartitionsInfo) {
                         try {
                             metadataStore.addRebalancingState(info);
                         } catch(Exception exception) {
                             logger.error(""Error while adding back rebalance info during error rollback ""
                                                  + info,
                                          exception);
                         }
                     }
                 }
 
             }
 
             // Revert changes to REBALANCING_SOURCE_CLUSTER_XML
             if(completedRebalanceSourceClusterChange) {
                 logger.info(""Reverting the REBALANCING_SOURCE_CLUSTER_XML back to ""
                             + previousRebalancingSourceCluster);
                 changeCluster(MetadataStore.REBALANCING_SOURCE_CLUSTER_XML,
                               previousRebalancingSourceCluster);
             }
 
             throw e;
         }
 
     }",Buggy,"1.Enabling proxy puts by dafault
2. Bug fix in proxy put stats
3. Changing order of state change updates for correctness
4. Setting proxy put tests to do one batch rebalancing
",Buggy,"The commit message indicates multiple changes, including a bug fix related to proxy put stats. The code diff primarily focuses on reordering state change updates for correctness during rebalancing. The key change involves moving the rebalancing state update logic before the cluster metadata update. This reordering is done to avoid a potential inconsistency window where clients might see an incorrect state. The code also includes logging and rollback mechanisms to handle exceptions during the state change process. The reordering of operations and the added exception handling suggest that the original order of operations could lead to errors or inconsistencies, which can be considered a bug. The answer is **Buggy**."
voldemort,9466.json,e7ecec1bd3dd879221a56714fc774a2001d843e8,"@@ -1,4 +1,3 @@
     protected void recordSuccess(Node node, long startNs) {
-        proxyPutStats.reportProxyPutCompletion();
         failureDetector.recordSuccess(node, (System.nanoTime() - startNs) / Time.NS_PER_MS);
     }",Buggy,"1.Enabling proxy puts by dafault
2. Bug fix in proxy put stats
3. Changing order of state change updates for correctness
4. Setting proxy put tests to do one batch rebalancing
",Buggy,"The commit message indicates a bug fix related to proxy put stats. The diff removes the line `proxyPutStats.reportProxyPutCompletion();` from the `recordSuccess` method. This suggests that the completion was being reported incorrectly or prematurely, leading to inaccurate statistics. Removing this line likely corrects the reporting logic, aligning with the bug fix mentioned in the commit message. The answer is **Buggy**.
"
voldemort,8840.json,e0707033c5a5c63fd044a59e7ee4055453334a01,"@@ -1,13 +1,11 @@
     public int getCheckedInResourcesCount(K key) {
-        int rc = 0;
-        if(!resourcePoolMap.containsKey(key)) {
-            return rc;
+        if(resourcePoolMap.containsKey(key)) {
+            try {
+                Pool<V> resourcePool = getResourcePoolForExistingKey(key);
+                return resourcePool.queue.size();
+            } catch(IllegalArgumentException iae) {
+                logger.debug(""getCheckedInResourceCount called on invalid key: "", iae);
+            }
         }
-        try {
-            Pool<V> resourcePool = getResourcePoolForExistingKey(key);
-            rc = resourcePool.queue.size();
-        } catch(IllegalArgumentException iae) {
-            logger.debug(""getCheckedInResourceCount called on invalid key: "", iae);
-        }
-        return rc;
+        return 0;
     }",NotBuggy,"Clean up get stats methods in (Queued)KeyedResourcePool. Fix error in test case.
",Buggy,"The commit message indicates a cleanup and a fix in a test case, alongside a fix in the `getCheckedInResourcesCount` method. The code changes involve simplifying the logic for retrieving the checked-in resources count. The original code initialized `rc` to 0 and returned it if the key was not present. The modified code directly returns 0 if the key is not present. The try-catch block remains the same, handling `IllegalArgumentException`. This change likely addresses a potential issue where the `resourcePool.queue.size()` call might have thrown an exception or returned an incorrect value when the key was invalid or the resource pool was in an unexpected state. The change ensures that a 0 is returned when the key is not present, which could be considered a bug fix if the previous behavior was incorrect or led to unexpected results. The answer is **Buggy**."
voldemort,8883.json,e0707033c5a5c63fd044a59e7ee4055453334a01,"@@ -1,14 +1,12 @@
     public int getRegisteredResourceRequestCount(K key) {
-        int rc = 0;
-        if(!requestQueueMap.containsKey(key)) {
-            return rc;
+        if(requestQueueMap.containsKey(key)) {
+            try {
+                Queue<AsyncResourceRequest<V>> requestQueue = getRequestQueueForExistingKey(key);
+                // FYI: .size() is not constant time in the next call. ;)
+                return requestQueue.size();
+            } catch(IllegalArgumentException iae) {
+                logger.debug(""getRegisteredResourceRequestCount called on invalid key: "", iae);
+            }
         }
-        try {
-            Queue<AsyncResourceRequest<V>> requestQueue = getRequestQueueForExistingKey(key);
-            // FYI: .size() is not constant time in the next call. ;)
-            rc = requestQueue.size();
-        } catch(IllegalArgumentException iae) {
-            logger.debug(""getRegisteredResourceRequestCount called on invalid key: "", iae);
-        }
-        return rc;
+        return 0;
     }",NotBuggy,"Clean up get stats methods in (Queued)KeyedResourcePool. Fix error in test case.
",Buggy,"The commit message indicates a cleanup of `get stats` methods and a fix in a test case. The diff focuses on the `getRegisteredResourceRequestCount` method. The original code initialized `rc` to 0 and returned it if the key wasn't in the map. Then, it tried to get the queue and update `rc` with the queue size, catching `IllegalArgumentException`. The modified code checks if the key exists first. If it does, it retrieves the queue and returns its size. If an `IllegalArgumentException` occurs, it logs the error. If the key doesn't exist, it returns 0. The change improves readability and potentially fixes a bug where the queue size might be calculated even for non-existent keys, although the original code also handled this case with a try-catch block. The change also simplifies the logic by directly returning the queue size when the key exists. The commit message mentions a bug fix in a test case, and while this diff doesn't directly show the test case fix, the changes in `getRegisteredResourceRequestCount` could be related to making the test case pass. The changes suggest a correction in how resource requests are counted or handled, which could be related to a bug. The answer is **Buggy**."
voldemort,8838.json,e0707033c5a5c63fd044a59e7ee4055453334a01,"@@ -1,13 +1,11 @@
     public int getTotalResourceCount(K key) {
-        int rc = 0;
-        if(!resourcePoolMap.containsKey(key)) {
-            return rc;
+        if(resourcePoolMap.containsKey(key)) {
+            try {
+                Pool<V> resourcePool = getResourcePoolForExistingKey(key);
+                return resourcePool.size.get();
+            } catch(IllegalArgumentException iae) {
+                logger.debug(""getTotalResourceCount called on invalid key: "", iae);
+            }
         }
-        try {
-            Pool<V> resourcePool = getResourcePoolForExistingKey(key);
-            rc = resourcePool.size.get();
-        } catch(IllegalArgumentException iae) {
-            logger.debug(""getTotalResourceCount called on invalid key: "", iae);
-        }
-        return rc;
+        return 0;
     }",NotBuggy,"Clean up get stats methods in (Queued)KeyedResourcePool. Fix error in test case.
",Buggy,"The commit message indicates a cleanup of `get stats` methods and a fix in a test case. The diff modifies the `getTotalResourceCount` method. The original code initializes `rc` to 0 and returns it if the key is not present in `resourcePoolMap`. The modified code checks if the key exists first, and if it does, it retrieves the resource pool and returns its size. If the key doesn't exist or an `IllegalArgumentException` occurs, it returns 0. The change ensures that the method returns 0 when the key is not found, and it handles potential exceptions. The change addresses a potential issue where the method might throw an exception if the key is invalid. The answer is **Buggy**."
voldemort,8842.json,e0707033c5a5c63fd044a59e7ee4055453334a01,"@@ -1,13 +1,11 @@
     public int getBlockingGetsCount(K key) {
-        int rc = 0;
-        if(!resourcePoolMap.containsKey(key)) {
-            return rc;
+        if(resourcePoolMap.containsKey(key)) {
+            try {
+                Pool<V> resourcePool = getResourcePoolForExistingKey(key);
+                return resourcePool.blockingGets.get();
+            } catch(IllegalArgumentException iae) {
+                logger.debug(""getBlockingGetsCount called on invalid key: "", iae);
+            }
         }
-        try {
-            Pool<V> resourcePool = getResourcePoolForExistingKey(key);
-            rc = resourcePool.blockingGets.get();
-        } catch(IllegalArgumentException iae) {
-            logger.debug(""getBlockingGetsCount called on invalid key: "", iae);
-        }
-        return rc;
+        return 0;
     }",NotBuggy,"Clean up get stats methods in (Queued)KeyedResourcePool. Fix error in test case.
",Buggy,"The commit message indicates a cleanup and a fix in a test case, along with changes to `getBlockingGetsCount`. The code diff modifies the logic of `getBlockingGetsCount`. The original code initialized `rc` to 0 and returned it if the key was not present. The new code checks if the key exists and returns 0 if it doesn't. The try-catch block remains the same. This change improves the code's readability and potentially fixes a bug where the method might have returned an incorrect value if an exception occurred within the try block when the key existed. The changes suggest a bug fix related to how the blocking gets count is retrieved or handled for non-existent keys. The answer is **Buggy**."
voldemort,532.json,81f0a160898ca88338ebf4834f7f1fd07f66c63b,"@@ -1,14 +1,18 @@
     public RebalancePlan getPlan(Cluster finalCluster,
                                  List<StoreDefinition> finalStoreDefs,
                                  int batchSize) {
         RebalanceUtils.validateClusterStores(finalCluster, finalStoreDefs);
         RebalanceUtils.validateCurrentFinalCluster(currentCluster, finalCluster);
 
+        // TODO: (currentCluster vs interimCluster) Add more validation before
+        // constructing plan? Given that currentCluster was polled from prod
+        // cluster, should confirm that it is an ""interim cluster"" i.e., has
+        // same (superset?) of nodes as are in finalCluster.
         String outputDir = null;
         return new RebalancePlan(currentCluster,
                                  currentStoreDefs,
                                  finalCluster,
                                  finalStoreDefs,
                                  batchSize,
                                  outputDir);
     }",NotBuggy,"Fix some bugs I introduced and added more TODOs

Fixed overflow introduced in AdminClient.waitForCompletion by passing in
Long.MAX_VALUE for duration.

Verify cluster store definition in StoreRoutingPlan. This requires
working around existing problems with how system stores are handled (the
store definition is hard-coded for two zones). Left some TODOs about
testing and fixing all of this.

Added TODOs about currentCluster vs interimCluster. Need to tweak
interface to RebalanceController and RebalancePlan to be consistent with
recommended usage (i.e., deploying interimCluster before starting
rebalance).

Minor tweaks to tests based on above changes.
",Buggy,"The commit message indicates bug fixes and the addition of TODOs. The diff adds a TODO related to validating the `currentCluster` against the `finalCluster`, suggesting a potential issue or missing validation in the original logic. The TODO implies that the `currentCluster` should be an ""interim cluster"" with a superset of nodes from the `finalCluster`. This indicates a bug fix. The answer is **Buggy**."
voldemort,887.json,81f0a160898ca88338ebf4834f7f1fd07f66c63b,"@@ -1,3 +1,3 @@
         public String waitForCompletion(int nodeId, int requestId) {
-            return waitForCompletion(nodeId, requestId, Long.MAX_VALUE, TimeUnit.SECONDS, null);
+            return waitForCompletion(nodeId, requestId, 0, TimeUnit.SECONDS, null);
         }",Buggy,"Fix some bugs I introduced and added more TODOs

Fixed overflow introduced in AdminClient.waitForCompletion by passing in
Long.MAX_VALUE for duration.

Verify cluster store definition in StoreRoutingPlan. This requires
working around existing problems with how system stores are handled (the
store definition is hard-coded for two zones). Left some TODOs about
testing and fixing all of this.

Added TODOs about currentCluster vs interimCluster. Need to tweak
interface to RebalanceController and RebalancePlan to be consistent with
recommended usage (i.e., deploying interimCluster before starting
rebalance).

Minor tweaks to tests based on above changes.
",Buggy,"The commit message indicates bug fixes and the diff modifies the `waitForCompletion` method. The original code used `Long.MAX_VALUE` for the duration, while the modified code uses `0`. This change likely addresses an overflow issue or a problem with how the `waitForCompletion` method handles long durations. The change from `Long.MAX_VALUE` to `0` suggests a correction to the intended behavior of the method, indicating a bug fix. The answer is **Buggy**.
"
voldemort,54.json,0b2ef083f53872f2686e10990e3586177c693bd6,"@@ -1,39 +1,42 @@
     public void reduce(BytesWritable key,
                        Iterator<BytesWritable> values,
                        OutputCollector<Text, Text> output,
                        Reporter reporter) throws IOException {
         BytesWritable writable = values.next();
         byte[] valueBytes = writable.get();
 
         if(this.nodeId == -1)
             this.nodeId = ByteUtils.readInt(valueBytes, 0);
         if(this.chunkId == -1)
             this.chunkId = ReadOnlyUtils.chunk(key.get(), this.numChunks);
 
         // Write key and position
         this.indexFileStream.write(key.get(), 0, key.getSize());
-        this.checkSumDigestIndex.update(key.get(), 0, key.getSize());
         this.indexFileStream.writeInt(this.position);
-        this.checkSumDigestIndex.update(this.position);
+        if(this.checkSumDigestIndex != null) {
+            this.checkSumDigestIndex.update(key.get(), 0, key.getSize());
+            this.checkSumDigestIndex.update(this.position);
+        }
 
         // Write length and value
         int valueLength = writable.getSize() - 4;
         this.valueFileStream.writeInt(valueLength);
-        this.checkSumDigestValue.update(valueLength);
         this.valueFileStream.write(valueBytes, 4, valueLength);
-        this.checkSumDigestValue.update(valueBytes, 4, valueLength);
-
+        if(this.checkSumDigestValue != null) {
+            this.checkSumDigestValue.update(valueLength);
+            this.checkSumDigestValue.update(valueBytes, 4, valueLength);
+        }
         this.position += 4 + valueLength;
         if(this.position < 0)
             throw new VoldemortException(""Chunk overflow exception: chunk "" + chunkId
                                          + "" has exceeded "" + Integer.MAX_VALUE + "" bytes."");
 
         // if we have multiple values for this md5 that is a collision, throw an
         // exception--either the data itself has duplicates, there are trillions
         // of keys, or someone is attempting something malicious
         if(values.hasNext())
             throw new VoldemortException(""Duplicate keys detected for md5 sum ""
                                          + ByteUtils.toHexString(ByteUtils.copy(key.get(),
                                                                                 0,
                                                                                 key.getSize())));
     }",Buggy,"Fixed minor bug which returns NPE when Checksum is None
",Buggy,"The commit message indicates a fix for a NullPointerException (NPE) when the Checksum is None. The diff introduces null checks for `this.checkSumDigestIndex` and `this.checkSumDigestValue` before calling their `update` methods. This prevents the NPE from occurring when these checksum objects are not initialized (i.e., are null). The changes directly address the bug described in the commit message. The answer is **Buggy**."
voldemort,10530.json,0b2ef083f53872f2686e10990e3586177c693bd6,"@@ -1,10 +1,12 @@
     public static String toString(CheckSumType type) {
         if(type == CheckSumType.ADLER32) {
             return ""adler32"";
         } else if(type == CheckSumType.CRC32) {
             return ""crc32"";
         } else if(type == CheckSumType.MD5) {
             return ""md5"";
+        } else if(type == CheckSumType.NONE) {
+            return ""none"";
         }
-        return null;
+        return ""none"";
     }",NotBuggy,"Fixed minor bug which returns NPE when Checksum is None
",Buggy,"The commit message indicates a fix for a NullPointerException (NPE) that occurs when the Checksum is None. The diff adds a specific condition to handle the `CheckSumType.NONE` case, returning the string ""none"". Additionally, the final return statement is changed from `null` to `""none""`. This prevents the method from returning null when the checksum type is not one of the explicitly handled types, thus resolving the potential NPE. The answer is **Buggy**."
weka,27677.json,29dadd8d55df37a7a89f355922eb92adc307f713,"@@ -1,27 +1,26 @@
   private double instanceTransformationProbability(Instance first, 
 						   Instance second) {
     String debug = ""(KStar.instanceTransformationProbability) "";
     double transProb = 1.0;
     int numMissAttr = 0;
     for (int i = 0; i < m_NumAttributes; i++) {
       if (i == m_Train.classIndex()) {
 	continue; // ignore class attribute
       }
       if (first.isMissing(i)) { // test instance attribute value is missing
 	numMissAttr++;
 	continue;
       }
       transProb *= attrTransProb(first, second, i);
       // normilize for missing values
       if (numMissAttr != m_NumAttributes) {
-	// I don't know where this comes from!!!
 	transProb = Math.pow(transProb, (double)m_NumAttributes / 
 			     (m_NumAttributes - numMissAttr));
       }
       else { // weird case!
 	transProb = 0.0;
       }
     }
     // normilize for the train dataset
      return transProb / m_NumInstances;
   }",NotBuggy,"Fixed bug that prevented correct initialization for -E option during incremental
training.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@445 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to the initialization of the -E option during incremental training. The provided code snippet calculates the instance transformation probability within the KStar algorithm. The change normalizes the transformation probability based on the number of missing attributes. If all attributes are missing, the probability is set to 0.0. The removal of the comment ""I don't know where this comes from!!!"" does not indicate a bug fix. However, the normalization logic itself could have been buggy. Given the commit message states a bug was fixed, and this code relates to the training process, it's plausible that the normalization was incorrect, leading to improper initialization when the -E option was used.

**Buggy**
"
weka,27694.json,29dadd8d55df37a7a89f355922eb92adc307f713,"@@ -1,3 +1,4 @@
   private void update_m_Attributes() {
     m_NumInstances = m_Train.numInstances();
+    m_InitFlag = ON;
   }",Buggy,"Fixed bug that prevented correct initialization for -E option during incremental
training.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@445 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to the initialization of the -E option during incremental training. The diff shows a modification to the `update_m_Attributes` method, where `m_InitFlag` is set to `ON`. This suggests that the initialization process was not being triggered correctly before, leading to the bug. Setting `m_InitFlag` to `ON` likely ensures that the necessary initialization steps are performed, resolving the bug. The answer is **Buggy**."
weka,31023.json,91f37b47e0458a307defac739c4967d86935770c,"@@ -1,13 +1,14 @@
   public void setSource(InputStream input) throws IOException {
     BufferedReader	reader;
     String		line;
     
     m_structure    = null;
     m_sourceFile   = null;
     m_File         = null;
 
     m_StreamBuffer = new StringBuffer();
     reader         = new BufferedReader(new InputStreamReader(input));
-    while ((line = reader.readLine()) != null)
+    while ((line = reader.readLine()) != null) {
       m_StreamBuffer.append(line + ""\n"");
+    }
   }",NotBuggy,"Moved some code from setSource in ArffLoader to AbstractFileLoader so that other Loaders can handle gzip compression. Fixed bug in CSVLoader that was causing the source file to be loaded twice into the StringBuffer


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@4322 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix in CSVLoader related to loading the source file twice. The provided diff shows a modification to the `setSource` method in `ArffLoader`, where the code reads the input stream line by line and appends it to a StringBuffer. While the commit message mentions a bug fix in `CSVLoader`, the diff provided is for `ArffLoader`. However, the commit message explicitly states ""Fixed bug in CSVLoader"". Therefore, the answer is **Buggy**."
weka,30923.json,91f37b47e0458a307defac739c4967d86935770c,"@@ -1,18 +1,22 @@
   public void setSource(File file) throws IOException {
     m_structure = null;
+    
     setRetrieval(NONE);
 
-    if (file == null) {
+    if (file == null)
       throw new IOException(""Source file object is null!"");
-    }
 
     try {
-      setSource(new FileInputStream(file));
+      if (file.getName().endsWith(getFileExtension() + FILE_EXTENSION_COMPRESSED)) {
+	setSource(new GZIPInputStream(new FileInputStream(file)));
+      } else {
+	setSource(new FileInputStream(file));
+      }
     }
     catch (FileNotFoundException ex) {
       throw new IOException(""File not found"");
     }
-
+    
     m_sourceFile = file;
     m_File       = file.getAbsolutePath();
-  }

+    }",Buggy,"Moved some code from setSource in ArffLoader to AbstractFileLoader so that other Loaders can handle gzip compression. Fixed bug in CSVLoader that was causing the source file to be loaded twice into the StringBuffer


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@4322 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",NotBuggy,"The commit message indicates a bug fix in CSVLoader. The diff shows changes in the `setSource` method of `ArffLoader`. The changes add support for handling gzipped files by checking the file extension and wrapping the FileInputStream in a GZIPInputStream if necessary. This change is related to extending the functionality to handle compressed files, and the commit message also mentions moving code to AbstractFileLoader. The provided diff does not fix a bug, but adds functionality. The answer is **NotBuggy**."
weka,11692.json,38a69e67c3d1f3dcb2e0c53e54f54b37ff25a090,"@@ -1,53 +1,52 @@
 	public void drawGraph() {
 		// build the panel
 		try {
 		  remove(canvas3D);
-		  System.out.println(""remove ok"");	
 		
 		} catch (Exception e) {
 		}
 //		if (!m_canvasCreated) {
 		  canvas3D = new Canvas3D(SimpleUniverse.getPreferredConfiguration());
 //		}
 
 		int scrHeight = (new Double(this.getSize().getHeight())).intValue();
 		int scrWidth = (new Double(this.getSize().getWidth())).intValue();
 		
 		canvas3D.setSize(scrWidth - 120, scrHeight - 50);
 		
 //		if (!m_canvasCreated) {
 		add(canvas3D, java.awt.BorderLayout.CENTER);
 		m_canvasCreated = true;
 //		}
 		
 		freeResources();				
 		
 		// build the visualisation
                 m_scene = createSceneGraph();
 
                 // compile the scene
                 m_scene.compile();
 
 	        // build the universe
 		m_simpleU = new SimpleUniverse(canvas3D);
 
 
 		// add the behaviors to the ViewingPlatform
 		ViewingPlatform viewingPlatform = m_simpleU.getViewingPlatform();
 
 		viewingPlatform.setNominalViewingTransform();
 
 		// add orbit behavior to ViewingPlatform
 		orbit =
 			new OrbitBehavior(
 				canvas3D,
 				OrbitBehavior.REVERSE_ALL | OrbitBehavior.STOP_ZOOM);
 		BoundingSphere bounds =
 			new BoundingSphere(new Point3d(0.0, 0.0, 0.0), 100.0);
 		orbit.setSchedulingBounds(bounds);
 		viewingPlatform.setViewPlatformBehavior(orbit);
 		
 
 		m_simpleU.addBranchGraph(m_scene);
 
 	}",NotBuggy,"Fixed a refresh bug that prevented the switching back to the rules selection tab from the 3D viewer tab. Now depends on the scatterPlot3D package for the Java3D libraries

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@12305 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to refreshing and switching between tabs (rules selection and 3D viewer). The code diff shows that the `remove(canvas3D)` call is now within a try-catch block. This suggests that the `remove` operation was potentially causing an exception under certain conditions, which prevented the tab switching from working correctly. By catching the exception, the code ensures that the rest of the `drawGraph` method can execute, allowing the 3D viewer to be refreshed and the tab switch to complete successfully. The answer is **Buggy**."
weka,11730.json,38a69e67c3d1f3dcb2e0c53e54f54b37ff25a090,"@@ -1,23 +1,37 @@
 	private void panelChanged() {
 		int numTab = viewerTabbedPanel.getSelectedIndex();
 		visu1Rules = selectionPanel.getSelectedRules(1);
 /*		visu2Rules = selectionPanel.getSelectedRules(2);
 		visu3Rules = selectionPanel.getSelectedRules(3); */
 
 		switch (numTab) {
+		case 0:
+			visu1PanelHolder.removeAll();
+			visu1PanelHolder.revalidate();
+			visu1PanelHolder.repaint();
+			break;
 			case 1 :
+				visu1PanelHolder.add(visu1Panel3D, BorderLayout.CENTER);
+				revalidate();
+				repaint();
 				String[] selectedCriteria =
 					selectionPanel.getSelectedCriteria();
 				visu1Panel3D.setData(visu1Rules, selectedCriteria);
 //				visu1Panel.setSelectedRules(visu2Rules);
 				break;
 			case 2 :
 				visu2Panel.setData(visu2Rules, criteres);
 				visu2Panel.setSelectedRules(visu3Rules);
+				visu1PanelHolder.removeAll();
+				visu1PanelHolder.revalidate();
+				visu1PanelHolder.repaint();
 				break;
 			case 3 :
 				visu3Panel.setData(visu3Rules);
+				visu1PanelHolder.removeAll();
+				visu1PanelHolder.revalidate();
+				visu1PanelHolder.repaint();
 				break;
 		}
 
 	}",Buggy,"Fixed a refresh bug that prevented the switching back to the rules selection tab from the 3D viewer tab. Now depends on the scatterPlot3D package for the Java3D libraries

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@12305 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to tab switching in the viewer. The diff modifies the `panelChanged` method, which seems to handle the logic when the selected tab changes. Specifically, the changes involve adding `removeAll()`, `revalidate()`, and `repaint()` calls to `visu1PanelHolder` in cases 0, 2, and 3. Also, in case 1, the 3D panel is added to the holder, revalidated and repainted. These changes likely address the issue of the UI not updating correctly when switching between tabs, especially when switching back to the rules selection tab from the 3D viewer tab. Therefore, the changes indicate a bug fix. The answer is **Buggy**.
"
weka,11722.json,38a69e67c3d1f3dcb2e0c53e54f54b37ff25a090,"@@ -1,114 +1,115 @@
 	private void initComponents() {
 	    setLayout(new java.awt.BorderLayout());
 		viewerTabbedPanel = new javax.swing.JTabbedPane();
 		selectionPanel = new selection.SelectionPanel();
 		visu1Panel3D = new Panel3D();
+		visu1PanelHolder.add(visu1Panel3D, BorderLayout.CENTER);
 		/*visu1Panel = new RulesSelectionPanel(visu1Panel3D);
 		visu1Panel.addActionListener(this); */
 /*		visu2PanelLine = new PanelLine();
 		visu2Panel = new RulesSelectionPanel(visu2PanelLine);
 		visu2Panel.setSingleSelection();
 		visu2Panel.addActionListener(this);
 		visu2Panel.setColored();
 		visu3Panel = new visu3.PanelDDecker(); */
 		// viewerBar = new javax.swing.JMenuBar();
 /*		fileMenu = new javax.swing.JMenu();
 		openItem = new javax.swing.JMenuItem();
 		printItem = new javax.swing.JMenuItem();
 		saveItem = new javax.swing.JMenuItem();
 		quitItem = new javax.swing.JMenuItem();
 		helpMenu = new javax.swing.JMenu();
 		aboutItem = new javax.swing.JMenuItem();
 		contentsItem = new javax.swing.JMenuItem(); */
 
 		selectionPanel.addMultipleListSelectionListener(this);
 
 //		setTitle(""Association Rules Viewer"");
 /*		addWindowListener(new java.awt.event.WindowAdapter() {
 			public void windowClosing(java.awt.event.WindowEvent evt) {
 				exitForm(evt);
 			}
 		}); */
 
 		viewerTabbedPanel.addChangeListener(new ChangeListener() {
 			public void stateChanged(ChangeEvent e) {
 				panelChanged();
 			}
 
 		});
 
 		viewerTabbedPanel.addTab(""Selection"", selectionPanel);
-		viewerTabbedPanel.addTab(""3D Representation"", visu1Panel3D);
+		viewerTabbedPanel.addTab(""3D Representation"", visu1PanelHolder);
 		/*viewerTabbedPanel.addTab(""N Dimensional Line"", visu2Panel);
 		viewerTabbedPanel.addTab(""Double Decker Plot"", visu3Panel); */
 
 		viewerTabbedPanel.setEnabledAt(1, false);
 		// viewerTabbedPanel.setEnabledAt(2, false);
 //		viewerTabbedPanel.setEnabledAt(3, false);
 
 		add(viewerTabbedPanel, java.awt.BorderLayout.CENTER);
 
 /*		fileMenu.setText(""File"");
 		openItem.setText(""Open"");
 		openItem.addActionListener(new java.awt.event.ActionListener() {
 			public void actionPerformed(java.awt.event.ActionEvent evt) {
 				openPerformed(evt);
 			}
 		});
 
 		fileMenu.add(openItem);
 
 		printItem.setText(""Print"");
 		printItem.addActionListener(new java.awt.event.ActionListener() {
 			public void actionPerformed(java.awt.event.ActionEvent evt) {
 				printPerformed(evt);
 			}
 		});
 
 		fileMenu.add(printItem);
 
 		saveItem.setText(""Save as"");
 		saveItem.addActionListener(new java.awt.event.ActionListener() {
 			public void actionPerformed(java.awt.event.ActionEvent evt) {
 				savePerformed(evt);
 			}
 		});
 
 		fileMenu.add(saveItem);
 
 		quitItem.setText(""Quit"");
 		quitItem.addActionListener(new java.awt.event.ActionListener() {
 			public void actionPerformed(java.awt.event.ActionEvent evt) {
 				quitPerformed(evt);
 			}
 		});
 
 		fileMenu.add(quitItem);
 
 		viewerBar.add(fileMenu);
 
 		helpMenu.setText(""Help"");
 		aboutItem.setText(""About"");
 		aboutItem.addActionListener(new java.awt.event.ActionListener() {
 			public void actionPerformed(java.awt.event.ActionEvent evt) {
 				showAbout(evt);
 			}
 		});
 
 		helpMenu.add(aboutItem);
 
 		contentsItem.setText(""Contents"");
 		contentsItem.addActionListener(new java.awt.event.ActionListener() {
 			public void actionPerformed(java.awt.event.ActionEvent evt) {
 				showHelp(evt);
 			}
 		});
 
 		helpMenu.add(contentsItem);
 
 		viewerBar.add(helpMenu);
 
 		setJMenuBar(viewerBar);
 
 		pack(); */
 	}",Buggy,"Fixed a refresh bug that prevented the switching back to the rules selection tab from the 3D viewer tab. Now depends on the scatterPlot3D package for the Java3D libraries

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@12305 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to tab switching between the rules selection tab and the 3D viewer tab. The diff adds `visu1PanelHolder.add(visu1Panel3D, BorderLayout.CENTER);` and changes `viewerTabbedPanel.addTab(""3D Representation"", visu1Panel3D);` to `viewerTabbedPanel.addTab(""3D Representation"", visu1PanelHolder);`. This suggests that the 3D representation panel was not correctly added to the tabbed panel, causing the switching issue. The addition of `visu1PanelHolder` seems to be the fix. The answer is **Buggy**."
weka,28529.json,c34e6f5c40e4a29dfcedb4b3333188288fb79717,"@@ -1,11 +1,12 @@
     public static void main(String[] args) {
         try {
+	    System.err.println(""okidoki"");
             BIFReader br = new BIFReader();
             br.processFile(args[0]);
 	    System.out.println(br.toString());
         
         }
         catch (Throwable t) {
             t.printStackTrace();
         }
     } // main",NotBuggy,"Bug fix spotted by Gladys Castillo Jordan


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@2176 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix. The diff adds a `System.err.println(""okidoki"");` statement at the beginning of the `try` block in the `main` method. This statement likely serves as a debugging aid to confirm that the program execution reaches that point, indicating a potential issue before that line was reached. The addition of a debugging statement suggests an attempt to diagnose or resolve a bug. The answer is **Buggy**."
weka,28685.json,c34e6f5c40e4a29dfcedb4b3333188288fb79717,"@@ -1,10 +1,10 @@
     public Enumeration listOptions() {
         Vector newVector = new Vector(4);
 
-        newVector.addElement(new Option(""\tUse ADTree data structure\n"", ""D"", 0, ""-D""));
+        newVector.addElement(new Option(""\tDo not use ADTree data structure\n"", ""D"", 0, ""-D""));
         newVector.addElement(new Option(""\tBIF file to compare with\n"", ""B"", 1, ""-B <BIF file>""));
         newVector.addElement(new Option(""\tSearch algorithm\n"", ""Q"", 1, ""-Q weka.classifiers.bayes.net.search.SearchAlgorithm""));
         newVector.addElement(new Option(""\tEstimator algorithm\n"", ""E"", 1, ""-E weka.classifiers.bayes.net.estimate.SimpleEstimator""));
 
         return newVector.elements();
     } // listOptions",Buggy,"Bug fix spotted by Gladys Castillo Jordan


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@2176 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix. The code change modifies the description of the ""-D"" option from ""Use ADTree data structure"" to ""Do not use ADTree data structure"". This correction suggests that the original description was incorrect, leading to potential confusion or misconfiguration. Therefore, this change represents a bug fix. The answer is **Buggy**."
weka,28381.json,c34e6f5c40e4a29dfcedb4b3333188288fb79717,"@@ -1,40 +1,41 @@
 	public double kFoldCV(BayesNet bayesNet, int nNrOfFolds) throws Exception {
 		m_BayesNet = bayesNet;
 		double fAccuracy = 0.0;
 		double fWeight = 0.0;
 		Instances instances = bayesNet.m_Instances;
 		// estimate CPTs based on complete data set
 		bayesNet.estimateCPTs();
 		int nFoldStart = 0;
 		int nFoldEnd = instances.numInstances() / nNrOfFolds;
 		int iFold = 1;
 		while (nFoldStart < instances.numInstances()) {
 			// remove influence of fold iFold from the probability distribution
 			for (int iInstance = nFoldStart; iInstance < nFoldEnd; iInstance++) {
 				Instance instance = instances.instance(iInstance);
 				instance.setWeight(-instance.weight());
 				bayesNet.updateClassifier(instance);
 			}
 			
 			// measure accuracy on fold iFold
 			for (int iInstance = nFoldStart; iInstance < nFoldEnd; iInstance++) {
 				Instance instance = instances.instance(iInstance);
 				instance.setWeight(-instance.weight());
 				fAccuracy += accuracyIncrease(instance);
+				instance.setWeight(-instance.weight());
 				fWeight += instance.weight();
 			}
 
 			// restore influence of fold iFold from the probability distribution
 			for (int iInstance = nFoldStart; iInstance < nFoldEnd; iInstance++) {
 				Instance instance = instances.instance(iInstance);
 				instance.setWeight(-instance.weight());
 				bayesNet.updateClassifier(instance);
 			}
 
 			// go to next fold
 			nFoldStart = nFoldEnd;
 			iFold++;
 			nFoldEnd = iFold * instances.numInstances() / nNrOfFolds;
 		}
 		return fAccuracy / fWeight;
 	} // kFoldCV",Buggy,"Bug fix spotted by Gladys Castillo Jordan


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@2176 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix. The code diff modifies the `kFoldCV` method. Specifically, within the loop that measures accuracy on fold `iFold`, the weight of the instance is negated twice before calculating the accuracy increase. The added line `instance.setWeight(-instance.weight());` effectively restores the original weight of the instance before `fWeight` is updated. This suggests that the original code had an error where the instance weight was not correctly handled during the accuracy measurement, leading to incorrect results. The addition of this line corrects this error, indicating a bug fix. The answer is **Buggy**.
"
weka,25948.json,449c6a2d0acb111174652c53dff0960ed4f6e606,"@@ -1,61 +1,57 @@
   private double findSplitNominalNominal(int index) throws Exception {
 
     double bestVal = Double.MAX_VALUE, currVal;
     double[][] counts = new double[theInstances.attribute(index).numValues() 
 				  + 1][theInstances.numClasses()];
     double[] sumCounts = new double[theInstances.numClasses()];
     double[][] bestDist = new double[3][theInstances.numClasses()];
     int numMissing = 0;
 
     // Compute counts for all the values
-
     for (int i = 0; i < theInstances.numInstances(); i++) {
       Instance inst = theInstances.instance(i);
       if (inst.isMissing(index)) {
 	numMissing++;
 	counts[theInstances.attribute(index).numValues()]
 	  [(int)inst.classValue()] += inst.weight();
       } else {
 	counts[(int)inst.value(index)][(int)inst.classValue()] += inst
 	  .weight();
       }
     }
 
     // Compute sum of counts
-
     for (int i = 0; i < theInstances.attribute(index).numValues() + 1; i++) {
       for (int j = 0; j < theInstances.numClasses(); j++) {
 	sumCounts[j] += counts[i][j];
       }
     }
     
     // Make split counts for each possible split and evaluate
-
     System.arraycopy(counts[theInstances.attribute(index).numValues()], 0,
 		     m_Distribution[2], 0, theInstances.numClasses());
     for (int i = 0; i < theInstances.attribute(index).numValues(); i++) {
       for (int j = 0; j < theInstances.numClasses(); j++) {
 	m_Distribution[0][j] = counts[i][j];
 	m_Distribution[1][j] = sumCounts[j] - counts[i][j];
       }
       currVal = ContingencyTables.entropyConditionedOnRows(m_Distribution);
       if (Utils.sm(currVal, bestVal)) {
 	bestVal = currVal;
 	m_SplitPoint = (double)i;
 	for (int j = 0; j < 3; j++) {
 	  System.arraycopy(m_Distribution[j], 0, bestDist[j], 0, 
 			   theInstances.numClasses());
 	}
       }
     }
 
     // No missing values in training data.
-
     if (numMissing == 0) {
       System.arraycopy(sumCounts, 0, bestDist[2], 0, 
 		       theInstances.numClasses());
     }
    
     m_Distribution = bestDist;
     return bestVal;
   }",NotBuggy,"Fixed division by zero bugs for numeric prediction.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@26 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",NotBuggy,"The commit message indicates a fix for division by zero bugs in numeric prediction. However, the provided diff focuses on the `findSplitNominalNominal` method, which deals with nominal attributes and entropy calculations, not numeric prediction or division operations. There's no evidence of error handling, logical corrections, or exception-handling improvements related to division by zero within this specific code. Therefore, the diff doesn't align with the commit message's claim of fixing division by zero bugs. The answer is **NotBuggy**."
weka,25951.json,449c6a2d0acb111174652c53dff0960ed4f6e606,"@@ -1,54 +1,50 @@
   private double findSplitNumericNominal(int index) throws Exception {
 
     double bestVal = Double.MAX_VALUE, currVal, currCutPoint;
     int numMissing = 0;
     double[] sum = new double[theInstances.numClasses()];
     double[][] bestDist = new double[3][theInstances.numClasses()];
 
     // Compute counts for all the values
-
     for (int i = 0; i < theInstances.numInstances(); i++) {
       Instance inst = theInstances.instance(i);
       if (!inst.isMissing(index)) {
 	m_Distribution[1][(int)inst.classValue()] += inst.weight();
       } else {
 	m_Distribution[2][(int)inst.classValue()] += inst.weight();
 	numMissing++;
       }
     }
     System.arraycopy(m_Distribution[1], 0, sum, 0, theInstances.numClasses());
 
     // Sort instances
-
     theInstances.sort(index);
     
     // Make split counts for each possible split and evaluate
-
     for (int i = 0; i < theInstances.numInstances() - (numMissing + 1); i++) {
       Instance inst = theInstances.instance(i);
       Instance instPlusOne = theInstances.instance(i + 1);
       m_Distribution[0][(int)inst.classValue()] += inst.weight();
       m_Distribution[1][(int)inst.classValue()] -= inst.weight();
       if (Utils.sm(inst.value(index), instPlusOne.value(index))) {
 	currCutPoint = (inst.value(index) + instPlusOne.value(index)) / 2.0;
 	currVal = ContingencyTables.entropyConditionedOnRows(m_Distribution);
 	if (Utils.sm(currVal, bestVal)) {
 	  m_SplitPoint = currCutPoint;
 	  bestVal = currVal;
 	  for (int j = 0; j < 3; j++) {
 	    System.arraycopy(m_Distribution[j], 0, bestDist[j], 0, 
 			     theInstances.numClasses());
 	  }
 	}
       }
     }
 
     // No missing values in training data.
-
     if (numMissing == 0) {
       System.arraycopy(sum, 0, bestDist[2], 0, theInstances.numClasses());
     }
  
     m_Distribution = bestDist;
     return bestVal;
   }",NotBuggy,"Fixed division by zero bugs for numeric prediction.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@26 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix for division by zero bugs in numeric prediction. The provided code diff doesn't show any explicit division operations or checks to prevent division by zero. However, the code calculates a cut point by averaging two values: `currCutPoint = (inst.value(index) + instPlusOne.value(index)) / 2.0;`. While this specific line doesn't directly lead to a division by zero, the surrounding logic involves calculating entropy and other statistical measures based on the distribution of data. It's possible that the division by zero bug was located within the `ContingencyTables.entropyConditionedOnRows(m_Distribution)` method, which is not included in the diff. Since the commit message explicitly mentions fixing division by zero bugs, and the code diff is related to numeric prediction, it's likely that the fix was implemented in a related part of the code not shown here, or that the diff is incomplete. Therefore, based on the commit message, the answer is **Buggy**."
weka,25940.json,449c6a2d0acb111174652c53dff0960ed4f6e606,"@@ -1,63 +1,58 @@
   public void buildClassifier(Instances instances) throws Exception {
     
     double bestVal = Double.MAX_VALUE, currVal;
     double bestPoint = -Double.MAX_VALUE, sum;
     int bestAtt = -1, numClasses;
 
     if (instances.checkForStringAttributes()) {
       throw new Exception(""Can't handle string attributes!"");
     }
 
     double[][] bestDist = new double[3][instances.numClasses()];
 
     theInstances = new Instances(instances);
     theInstances.deleteWithMissingClass();
     if (theInstances.classAttribute().isNominal()) {
       numClasses = theInstances.numClasses();
     } else {
       numClasses = 1;
     }
 
     // For each attribute
-    
     for (int i = 0; i < theInstances.numAttributes(); i++) {
       if (i != theInstances.classIndex()) {
 	
 	// Reserve space for distribution.
-	
 	m_Distribution = new double[3][numClasses];
 
 	// Compute value of criterion for best split on attribute
-	
 	if (theInstances.attribute(i).isNominal()) {
 	  currVal = findSplitNominal(i);
 	} else {
 	  currVal = findSplitNumeric(i);
 	}
 	if (Utils.sm(currVal, bestVal)) {
 	  bestVal = currVal;
 	  bestAtt = i;
 	  bestPoint = m_SplitPoint;
 	  for (int j = 0; j < 3; j++) {
 	    System.arraycopy(m_Distribution[j], 0, bestDist[j], 0, 
 			     numClasses);
 	  }
 	}
       }
     }
     
     // Set attribute, split point and distribution.
-    
     m_AttIndex = bestAtt;
     m_SplitPoint = bestPoint;
     m_Distribution = bestDist;
     if (theInstances.classAttribute().isNominal()) {
       for (int i = 0; i < m_Distribution.length; i++) {
 	Utils.normalize(m_Distribution[i]);
       }
     }
     
     // Save memory
-    
     theInstances = new Instances(theInstances, 0);
   }",NotBuggy,"Fixed division by zero bugs for numeric prediction.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@26 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix for division by zero errors specifically within numeric prediction. The code diff doesn't show any explicit division operations or checks for zero divisors. However, the core logic for finding the best split point in numeric attributes resides within the `findSplitNumeric(i)` method (called when `theInstances.attribute(i).isNominal()` is false). Without the code for `findSplitNumeric(i)`, it's impossible to confirm the presence and resolution of a division by zero bug. However, given the commit message's explicit mention of fixing division by zero bugs for numeric prediction, it's highly probable that the `findSplitNumeric(i)` method (which is not included in the diff) contains the actual fix.

Therefore, the answer is **Buggy**.
"
weka,25952.json,449c6a2d0acb111174652c53dff0960ed4f6e606,"@@ -1,64 +1,60 @@
   private double findSplitNumericNumeric(int index) throws Exception {
 
     double bestVal = Double.MAX_VALUE, currVal, currCutPoint;
     int numMissing = 0;
     double[] sumsSquares = new double[3], sumOfWeights = new double[3];
     double[][] bestDist = new double[3][1];
-    double meanNoMissing;
+    double totalSum = 0, totalSumOfWeights = 0;
 
     // Compute counts for all the values
-
     for (int i = 0; i < theInstances.numInstances(); i++) {
       Instance inst = theInstances.instance(i);
       if (!inst.isMissing(index)) {
 	m_Distribution[1][0] += inst.classValue() * inst.weight();
 	sumsSquares[1] += inst.classValue() * inst.classValue() 
 	  * inst.weight();
 	sumOfWeights[1] += inst.weight();
       } else {
 	m_Distribution[2][0] += inst.classValue() * inst.weight();
 	sumsSquares[2] += inst.classValue() * inst.classValue() 
 	  * inst.weight();
 	sumOfWeights[2] += inst.weight();
 	numMissing++;
       }
+      totalSumOfWeights += inst.weight();
+      totalSum += inst.classValue() * inst.weight();
     }
-    meanNoMissing = m_Distribution[1][0] / sumOfWeights[1];
 
     // Sort instances
-
     theInstances.sort(index);
     
     // Make split counts for each possible split and evaluate
-
     for (int i = 0; i < theInstances.numInstances() - (numMissing + 1); i++) {
       Instance inst = theInstances.instance(i);
       Instance instPlusOne = theInstances.instance(i + 1);
       m_Distribution[0][0] += inst.classValue() * inst.weight();
       sumsSquares[0] += inst.classValue() * inst.classValue() * inst.weight();
       sumOfWeights[0] += inst.weight();
       m_Distribution[1][0] -= inst.classValue() * inst.weight();
       sumsSquares[1] -= inst.classValue() * inst.classValue() * inst.weight();
       sumOfWeights[1] -= inst.weight();
       if (Utils.sm(inst.value(index), instPlusOne.value(index))) {
 	currCutPoint = (inst.value(index) + instPlusOne.value(index)) / 2.0;
 	currVal = variance(m_Distribution, sumsSquares, sumOfWeights);
 	if (Utils.sm(currVal, bestVal)) {
 	  m_SplitPoint = currCutPoint;
 	  bestVal = currVal;
 	  for (int j = 0; j < 3; j++) {
-	    bestDist[j][0] = m_Distribution[j][0] / sumOfWeights[j];
+	    if (!Utils.eq(sumOfWeights[j], 0)) {
+	      bestDist[j][0] = m_Distribution[j][0] / sumOfWeights[j];
+	    } else {
+	      bestDist[j][0] = totalSum / totalSumOfWeights;
+	    }
 	  }
 	}
       }
     }
 
-    // No missing values in training data
-    
-    if (numMissing == 0) {
-      bestDist[2][0] = meanNoMissing;
-    }
-
     m_Distribution = bestDist;
     return bestVal;
   }",Buggy,"Fixed division by zero bugs for numeric prediction.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@26 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message states ""Fixed division by zero bugs for numeric prediction"". The code diff includes a check `!Utils.eq(sumOfWeights[j], 0)` before dividing `m_Distribution[j][0]` by `sumOfWeights[j]` to calculate `bestDist[j][0]`. If `sumOfWeights[j]` is zero, it uses `totalSum / totalSumOfWeights` instead. This confirms the fix for division by zero errors. The answer is **Buggy**."
weka,20096.json,f39f7d042b70f510b9721bbff0535ef0a3d348a8,"@@ -1,38 +1,40 @@
   public void performRequest(String request) {
     if (request.compareTo(""Show chart"") == 0) {
       try {
-	// popup visualize panel
-	if (!m_framePoppedUp) {
-	  m_framePoppedUp = true;
+        // popup visualize panel
+        if (!m_framePoppedUp) {
+          m_framePoppedUp = true;
 
-	  final javax.swing.JFrame jf = 
-	    new javax.swing.JFrame(""Model Performance Chart"");
-	  jf.setSize(800,600);
-	  jf.getContentPane().setLayout(new BorderLayout());
-	  jf.getContentPane().add(m_visPanel, BorderLayout.CENTER);
-	  jf.addWindowListener(new java.awt.event.WindowAdapter() {
-	      public void windowClosing(java.awt.event.WindowEvent e) {
-		jf.dispose();
-		m_framePoppedUp = false;
-	      }
-	    });
-	  jf.setVisible(true);
-	  m_popupFrame = jf;
-	} else {
-	  m_popupFrame.toFront();
-	}
+          final javax.swing.JFrame jf = new javax.swing.JFrame(
+              ""Model Performance Chart"");
+          jf.setSize(800, 600);
+          jf.getContentPane().setLayout(new BorderLayout());
+          jf.getContentPane().add(m_visPanel, BorderLayout.CENTER);
+          jf.addWindowListener(new java.awt.event.WindowAdapter() {
+            @Override
+            public void windowClosing(java.awt.event.WindowEvent e) {
+              jf.dispose();
+              m_framePoppedUp = false;
+            }
+          });
+          jf.setVisible(true);
+          m_popupFrame = jf;
+        } else {
+          m_popupFrame.toFront();
+        }
       } catch (Exception ex) {
-	ex.printStackTrace();
-	m_framePoppedUp = false;
+        ex.printStackTrace();
+        m_framePoppedUp = false;
       }
     } else if (request.equals(""Clear all plots"")) {
-        m_visPanel.removeAllPlots();
-        m_visPanel.validate(); m_visPanel.repaint();
-        m_visPanel = null;
-        m_masterPlot = null;
-        m_offscreenPlotData = null;
+      m_visPanel.removeAllPlots();
+      m_visPanel.validate();
+      m_visPanel.repaint();
+      m_visPanel = null;
+      m_masterPlot = null;
+      m_offscreenPlotData = null;
     } else {
       throw new IllegalArgumentException(request
-					 + "" not supported (Model Performance Chart)"");
+          + "" not supported (Model Performance Chart)"");
     }
   }",NotBuggy,"Fixed a bug for error plots where once the master plot had been set it did not get changed for subsequent incoming VisualizableErrorEvents (unless manually clearing all plots via the GUI).

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@9452 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to error plots not updating correctly after the master plot is set. The code diff doesn't directly show the fix for the error plot update logic. However, the ""Clear all plots"" functionality is modified, which might be related to resetting the state and allowing subsequent plots to be displayed correctly. The core of the fix is likely elsewhere, but this change is related to the reported bug. The answer is **Buggy**."
weka,20081.json,f39f7d042b70f510b9721bbff0535ef0a3d348a8,"@@ -1,174 +1,179 @@
   public synchronized void acceptDataSet(VisualizableErrorEvent e) {
     if (m_env == null) {
       m_env = Environment.getSystemWide();
     }
 
     if (!GraphicsEnvironment.isHeadless()) {
       if (m_visPanel == null) {
         m_visPanel = new VisualizePanel();
       }
-      if (m_masterPlot == null) {
-        m_masterPlot = e.getDataSet();
-      }
+
+      m_masterPlot = e.getDataSet();
+
       try {
         m_visPanel.setMasterPlot(m_masterPlot);
       } catch (Exception ex) {
-        System.err.println(""Problem setting up visualization (ModelPerformanceChart)"");
+        System.err
+            .println(""Problem setting up visualization (ModelPerformanceChart)"");
         ex.printStackTrace();
       }
       m_visPanel.validate();
       m_visPanel.repaint();
     } else {
       m_headlessEvents = new ArrayList<EventObject>();
       m_headlessEvents.add(e);
     }
-    
+
     if (m_imageListeners.size() > 0 && !m_processingHeadlessEvents) {
       // configure the renderer (if necessary)
       setupOffscreenRenderer();
-     
-      m_offscreenPlotData = new ArrayList<Instances>();      
+
+      m_offscreenPlotData = new ArrayList<Instances>();
       Instances predictedI = e.getDataSet().getPlotInstances();
       if (predictedI.classAttribute().isNominal()) {
-        
+
         // split the classes out into individual series.
         // add a new attribute to hold point sizes - correctly
-        // classified instances get default point size (2); 
+        // classified instances get default point size (2);
         // misclassified instances get point size (5).
         // WekaOffscreenChartRenderer can take advantage of this
         // information - other plugin renderers may or may not
         // be able to use it
         FastVector atts = new FastVector();
         for (int i = 0; i < predictedI.numAttributes(); i++) {
           atts.add(predictedI.attribute(i).copy());
         }
         atts.add(new Attribute(""@@size@@""));
-        Instances newInsts = new Instances(predictedI.relationName(),
-            atts, predictedI.numInstances());
+        Instances newInsts = new Instances(predictedI.relationName(), atts,
+            predictedI.numInstances());
         newInsts.setClassIndex(predictedI.classIndex());
-        
+
         for (int i = 0; i < predictedI.numInstances(); i++) {
           double[] vals = new double[newInsts.numAttributes()];
           for (int j = 0; j < predictedI.numAttributes(); j++) {
             vals[j] = predictedI.instance(i).value(j);
           }
           vals[vals.length - 1] = 2; // default shape size
           Instance ni = new DenseInstance(1.0, vals);
           newInsts.add(ni);
         }
-        
+
         // predicted class attribute is always actualClassIndex - 1
         Instances[] classes = new Instances[newInsts.numClasses()];
         for (int i = 0; i < newInsts.numClasses(); i++) {
           classes[i] = new Instances(newInsts, 0);
           classes[i].setRelationName(newInsts.classAttribute().value(i));
         }
         Instances errors = new Instances(newInsts, 0);
         int actualClass = newInsts.classIndex();
         for (int i = 0; i < newInsts.numInstances(); i++) {
           Instance current = newInsts.instance(i);
-          classes[(int)current.classValue()].add((Instance)current.copy());
-          
+          classes[(int) current.classValue()].add((Instance) current.copy());
+
           if (current.value(actualClass) != current.value(actualClass - 1)) {
-            Instance toAdd = (Instance)current.copy();
-            
+            Instance toAdd = (Instance) current.copy();
+
             // larger shape for an error
             toAdd.setValue(toAdd.numAttributes() - 1, 5);
-            
+
             // swap predicted and actual class value so
             // that the color plotted for the error series
             // is that of the predicted class
             double actualClassV = toAdd.value(actualClass);
             double predictedClassV = toAdd.value(actualClass - 1);
             toAdd.setValue(actualClass, predictedClassV);
             toAdd.setValue(actualClass - 1, actualClassV);
-              
-            errors.add(toAdd);            
+
+            errors.add(toAdd);
           }
         }
-        
+
         errors.setRelationName(""Errors"");
         m_offscreenPlotData.add(errors);
-        
+
         for (int i = 0; i < classes.length; i++) {
           m_offscreenPlotData.add(classes[i]);
         }
-  
+
       } else {
         // numeric class - have to make a new set of instances
         // with the point sizes added as an additional attribute
         FastVector atts = new FastVector();
         for (int i = 0; i < predictedI.numAttributes(); i++) {
           atts.add(predictedI.attribute(i).copy());
         }
         atts.add(new Attribute(""@@size@@""));
-        Instances newInsts = new Instances(predictedI.relationName(),
-            atts, predictedI.numInstances());
+        Instances newInsts = new Instances(predictedI.relationName(), atts,
+            predictedI.numInstances());
 
         int[] shapeSizes = e.getDataSet().getShapeSize();
 
         for (int i = 0; i < predictedI.numInstances(); i++) {
           double[] vals = new double[newInsts.numAttributes()];
           for (int j = 0; j < predictedI.numAttributes(); j++) {
             vals[j] = predictedI.instance(i).value(j);
           }
           vals[vals.length - 1] = shapeSizes[i];
           Instance ni = new DenseInstance(1.0, vals);
           newInsts.add(ni);
         }
         newInsts.setRelationName(predictedI.classAttribute().name());
         m_offscreenPlotData.add(newInsts);
       }
-      
+
       List<String> options = new ArrayList<String>();
-      
+
       String additional = ""-color="" + predictedI.classAttribute().name()
-        + "",-hasErrors"";
+          + "",-hasErrors"";
       if (m_additionalOptions != null && m_additionalOptions.length() > 0) {
         additional += "","" + m_additionalOptions;
         try {
           additional = m_env.substitute(additional);
-        } catch (Exception ex) { }
-      }            
+        } catch (Exception ex) {
+        }
+      }
       String[] optionsParts = additional.split("","");
       for (String p : optionsParts) {
         options.add(p.trim());
       }
-      
-//      if (predictedI.classAttribute().isNumeric()) {
+
+      // if (predictedI.classAttribute().isNumeric()) {
       options.add(""-shapeSize=@@size@@"");
-//      }
-      
+      // }
+
       String xAxis = m_xAxis;
       try {
         xAxis = m_env.substitute(xAxis);
-      } catch (Exception ex) { }
-      
+      } catch (Exception ex) {
+      }
+
       String yAxis = m_yAxis;
       try {
         yAxis = m_env.substitute(yAxis);
-      } catch (Exception ex) { }
-      
+      } catch (Exception ex) {
+      }
+
       String width = m_width;
       String height = m_height;
       int defWidth = 500;
       int defHeight = 400;
       try {
         width = m_env.substitute(width);
         height = m_env.substitute(height);
-        
+
         defWidth = Integer.parseInt(width);
         defHeight = Integer.parseInt(height);
-      } catch (Exception ex) { }
-      
+      } catch (Exception ex) {
+      }
+
       try {
-        BufferedImage osi = m_offscreenRenderer.renderXYScatterPlot(defWidth, defHeight, 
-            m_offscreenPlotData, xAxis, yAxis, options);
+        BufferedImage osi = m_offscreenRenderer.renderXYScatterPlot(defWidth,
+            defHeight, m_offscreenPlotData, xAxis, yAxis, options);
 
         ImageEvent ie = new ImageEvent(this, osi);
         notifyImageListeners(ie);
       } catch (Exception e1) {
         e1.printStackTrace();
-      }      
+      }
     }
   }",Buggy,"Fixed a bug for error plots where once the master plot had been set it did not get changed for subsequent incoming VisualizableErrorEvents (unless manually clearing all plots via the GUI).

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@9452 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to error plots in the weka tool. Specifically, it mentions that the master plot was not being updated correctly for subsequent VisualizableErrorEvents after the initial setup, unless manually cleared.

The code diff shows that the line `if (m_masterPlot == null)` has been removed. This condition was likely preventing the `m_masterPlot` from being updated with new datasets from subsequent events. By removing this condition, the `m_masterPlot` is now always updated with the latest dataset from the incoming `VisualizableErrorEvent`.

Therefore, the code change aligns with the commit message, indicating a bug fix. The answer is **Buggy**."
weka,18379.json,41f296c55b87a1ead276fffbafb67535b858a70f,"@@ -1,44 +1,52 @@
   private void saveExperiment() {
 
     int returnVal = m_FileChooser.showSaveDialog(this);
     if (returnVal != JFileChooser.APPROVE_OPTION) {
       return;
     }
     File expFile = m_FileChooser.getSelectedFile();
-    if ( !(    (expFile.getName().toLowerCase().endsWith(Experiment.FILE_EXTENSION))
-          || (KOML.isPresent() && expFile.getName().toLowerCase().endsWith(KOML.FILE_EXTENSION))
-          || (expFile.getName().toLowerCase().endsWith("".xml"")) ) )
-    {
-      expFile = new File(expFile.getParent(), expFile.getName()
-                         + Experiment.FILE_EXTENSION);
+    
+    // add extension if necessary
+    if (m_FileChooser.getFileFilter() == m_ExpFilter) {
+      if (!expFile.getName().toLowerCase().endsWith(Experiment.FILE_EXTENSION))
+        expFile = new File(expFile.getParent(), expFile.getName() + Experiment.FILE_EXTENSION);
     }
+    else if (m_FileChooser.getFileFilter() == m_KOMLFilter) {
+      if (!expFile.getName().toLowerCase().endsWith(KOML.FILE_EXTENSION))
+        expFile = new File(expFile.getParent(), expFile.getName() + KOML.FILE_EXTENSION);
+    }
+    else if (m_FileChooser.getFileFilter() == m_XMLFilter) {
+      if (!expFile.getName().toLowerCase().endsWith("".xml""))
+        expFile = new File(expFile.getParent(), expFile.getName() + "".xml"");
+    }
+    
     try {
        // KOML?
        if ( (KOML.isPresent()) && (expFile.getAbsolutePath().toLowerCase().endsWith(KOML.FILE_EXTENSION)) ) {
           KOML.write(expFile.getAbsolutePath(), m_Exp);
        }
        else
        // XML?
        if (expFile.getAbsolutePath().toLowerCase().endsWith("".xml"")) {
           XMLExperiment xml = new XMLExperiment(); 
           xml.write(expFile, m_Exp);
        }
        // binary
        else {
           FileOutputStream fo = new FileOutputStream(expFile);
           ObjectOutputStream oo = new ObjectOutputStream(
                                   new BufferedOutputStream(fo));
           oo.writeObject(m_Exp);
           oo.close();
        }
       
       System.err.println(""Saved experiment:\n"" + m_Exp);
     } catch (Exception ex) {
       ex.printStackTrace();
       JOptionPane.showMessageDialog(this, ""Couldn't save experiment file:\n""
 				    + expFile
 				    + ""\nReason:\n"" + ex.getMessage(),
 				    ""Save Experiment"",
 				    JOptionPane.ERROR_MESSAGE);
     }
   }",Buggy,"A few bug fixes from Peter regarding the latest GUI additions. Also eliminated some superfluous output from NaiveBayesSimple.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@2259 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates bug fixes related to GUI additions. The code diff modifies the `saveExperiment` method, specifically how the file extension is handled when saving an experiment. The original code had a complex conditional statement to check for the correct file extension and append the default extension if necessary. The updated code simplifies this logic by checking the selected file filter from the `JFileChooser` and appending the corresponding extension if it's missing. This change likely addresses a bug where the file extension was not being correctly handled in certain GUI scenarios, potentially leading to files being saved without the correct extension or with multiple extensions. The answer is **Buggy**."
weka,8611.json,41f296c55b87a1ead276fffbafb67535b858a70f,"@@ -1,31 +1,29 @@
   public double[] distributionForInstance(Instance instance) throws Exception {
     
     double [] probs = new double[instance.numClasses()];
     int attIndex;
     
     for (int j = 0; j < instance.numClasses(); j++) {
       probs[j] = 1;
       Enumeration enumAtts = instance.enumerateAttributes();
       attIndex = 0;
       while (enumAtts.hasMoreElements()) {
 	Attribute attribute = (Attribute) enumAtts.nextElement();
 	if (!instance.isMissing(attribute)) {
 	  if (attribute.isNominal()) {
 	    probs[j] *= m_Counts[j][attIndex][(int)instance.value(attribute)];
 	  } else {
 	    probs[j] *= normalDens(instance.value(attribute),
 				   m_Means[j][attIndex],
 				   m_Devs[j][attIndex]);}
 	}
 	attIndex++;
       }
-      System.out.println(probs[j] + "" "");
       probs[j] *= m_Priors[j];
     }
-    System.out.println();
 
     // Normalize probabilities
     Utils.normalize(probs);
 
     return probs;
   }",NotBuggy,"A few bug fixes from Peter regarding the latest GUI additions. Also eliminated some superfluous output from NaiveBayesSimple.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@2259 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",NotBuggy,"The commit message indicates bug fixes and the elimination of superfluous output. The diff removes two `System.out.println()` statements. While removing output can be related to debugging or fixing unintended behavior, it doesn't inherently indicate a bug fix in the core logic. The removal of the print statements likely addresses the ""superfluous output"" mentioned in the commit message. Therefore, the change is more about cleaning up the output rather than correcting a functional error.

**NotBuggy**
"
weka,18418.json,41f296c55b87a1ead276fffbafb67535b858a70f,"@@ -1,47 +1,55 @@
   private void openExperiment() {
     
     int returnVal = m_FileChooser.showOpenDialog(this);
     if (returnVal != JFileChooser.APPROVE_OPTION) {
       return;
     }
     File expFile = m_FileChooser.getSelectedFile();
-    if ( !(    (expFile.getName().toLowerCase().endsWith(Experiment.FILE_EXTENSION))
-          || (KOML.isPresent() && expFile.getName().toLowerCase().endsWith(KOML.FILE_EXTENSION))
-          || (expFile.getName().toLowerCase().endsWith("".xml"")) ) )
-    {
-       expFile = new File(expFile.getParent(), expFile.getName()
-                          + Experiment.FILE_EXTENSION);
+    
+    // add extension if necessary
+    if (m_FileChooser.getFileFilter() == m_ExpFilter) {
+      if (!expFile.getName().toLowerCase().endsWith(Experiment.FILE_EXTENSION))
+        expFile = new File(expFile.getParent(), expFile.getName() + Experiment.FILE_EXTENSION);
     }
+    else if (m_FileChooser.getFileFilter() == m_KOMLFilter) {
+      if (!expFile.getName().toLowerCase().endsWith(KOML.FILE_EXTENSION))
+        expFile = new File(expFile.getParent(), expFile.getName() + KOML.FILE_EXTENSION);
+    }
+    else if (m_FileChooser.getFileFilter() == m_XMLFilter) {
+      if (!expFile.getName().toLowerCase().endsWith("".xml""))
+        expFile = new File(expFile.getParent(), expFile.getName() + "".xml"");
+    }
+    
     try {
       Experiment exp; 
       
       // KOML?
       if ( (KOML.isPresent()) && (expFile.getAbsolutePath().toLowerCase().endsWith(KOML.FILE_EXTENSION)) ) {
          exp = (Experiment) KOML.read(expFile.getAbsolutePath());
       }
       else
       // XML?
       if (expFile.getAbsolutePath().toLowerCase().endsWith("".xml"")) {
          XMLExperiment xml = new XMLExperiment(); 
          exp = (Experiment) xml.read(expFile);
       }
       // binary
       else {
          FileInputStream fi = new FileInputStream(expFile);
          ObjectInputStream oi = new ObjectInputStream(
                                 new BufferedInputStream(fi));
          exp = (Experiment)oi.readObject();
          oi.close();
       }
       setExperiment(exp);
       System.err.println(""Opened experiment:\n"" + m_Exp);
     } catch (Exception ex) {
       ex.printStackTrace();
       JOptionPane.showMessageDialog(this, ""Couldn't open experiment file:\n""
 				    + expFile
 				    + ""\nReason:\n"" + ex.getMessage(),
 				    ""Open Experiment"",
 				    JOptionPane.ERROR_MESSAGE);
       // Pop up error dialog
     }
   }",Buggy,"A few bug fixes from Peter regarding the latest GUI additions. Also eliminated some superfluous output from NaiveBayesSimple.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@2259 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates bug fixes related to GUI additions. The code modifies the `openExperiment` method to handle file extensions more robustly when opening experiment files. It checks the selected file filter and appends the appropriate extension if it's missing. The original code had a complex condition to check for valid extensions, which was replaced with a more structured approach based on the file filter. This change addresses potential issues where files without the correct extension might not be recognized, which can be considered a bug fix. The answer is **Buggy**."
weka,18378.json,41f296c55b87a1ead276fffbafb67535b858a70f,"@@ -1,50 +1,58 @@
   private void openExperiment() {
     
     int returnVal = m_FileChooser.showOpenDialog(this);
     if (returnVal != JFileChooser.APPROVE_OPTION) {
       return;
     }
     File expFile = m_FileChooser.getSelectedFile();
-    if ( !(    (expFile.getName().toLowerCase().endsWith(Experiment.FILE_EXTENSION))
-            || (KOML.isPresent() && expFile.getName().toLowerCase().endsWith(KOML.FILE_EXTENSION))
-            || (expFile.getName().toLowerCase().endsWith("".xml"")) ) )
-    {
-      expFile = new File(expFile.getParent(), expFile.getName()
-                         + Experiment.FILE_EXTENSION);
+    
+    // add extension if necessary
+    if (m_FileChooser.getFileFilter() == m_ExpFilter) {
+      if (!expFile.getName().toLowerCase().endsWith(Experiment.FILE_EXTENSION))
+        expFile = new File(expFile.getParent(), expFile.getName() + Experiment.FILE_EXTENSION);
     }
+    else if (m_FileChooser.getFileFilter() == m_KOMLFilter) {
+      if (!expFile.getName().toLowerCase().endsWith(KOML.FILE_EXTENSION))
+        expFile = new File(expFile.getParent(), expFile.getName() + KOML.FILE_EXTENSION);
+    }
+    else if (m_FileChooser.getFileFilter() == m_XMLFilter) {
+      if (!expFile.getName().toLowerCase().endsWith("".xml""))
+        expFile = new File(expFile.getParent(), expFile.getName() + "".xml"");
+    }
+    
     try {
       Experiment exp; 
       
       // KOML?
       if ( (KOML.isPresent()) && (expFile.getAbsolutePath().toLowerCase().endsWith(KOML.FILE_EXTENSION)) ) {
          exp = (Experiment) KOML.read(expFile.getAbsolutePath());
       }
       else
       // XML?
       if (expFile.getAbsolutePath().toLowerCase().endsWith("".xml"")) {
          XMLExperiment xml = new XMLExperiment(); 
          exp = (Experiment) xml.read(expFile);
       }
       // binary
       else {
          FileInputStream fi = new FileInputStream(expFile);
          ObjectInputStream oi = new ObjectInputStream(
                                 new BufferedInputStream(fi));
          exp = (Experiment)oi.readObject();
          oi.close();
       }
       
       if (!setExperiment(exp)) {
 	if (m_modePanel != null) m_modePanel.switchToAdvanced(exp);
       }
       System.err.println(""Opened experiment:\n"" + exp);
     } catch (Exception ex) {
       ex.printStackTrace();
       JOptionPane.showMessageDialog(this, ""Couldn't open experiment file:\n""
 				    + expFile
 				    + ""\nReason:\n"" + ex.getMessage(),
 				    ""Open Experiment"",
 				    JOptionPane.ERROR_MESSAGE);
       // Pop up error dialog
     }
   }",Buggy,"A few bug fixes from Peter regarding the latest GUI additions. Also eliminated some superfluous output from NaiveBayesSimple.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@2259 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates bug fixes related to GUI additions. The code changes modify how the file extension is handled when opening an experiment file in the GUI. The original code appended the "".experiment"" extension regardless of the selected file filter. The updated code checks the selected file filter (m_ExpFilter, m_KOMLFilter, m_XMLFilter) and appends the corresponding extension only if it's missing. This ensures that the correct extension is added based on the user's selection, fixing a potential bug where the wrong extension could be appended. The answer is **Buggy**."
weka,18419.json,41f296c55b87a1ead276fffbafb67535b858a70f,"@@ -1,43 +1,51 @@
   private void saveExperiment() {
 
     int returnVal = m_FileChooser.showSaveDialog(this);
     if (returnVal != JFileChooser.APPROVE_OPTION) {
       return;
     }
     File expFile = m_FileChooser.getSelectedFile();
-    if ( !(    (expFile.getName().toLowerCase().endsWith(Experiment.FILE_EXTENSION))
-          || (KOML.isPresent() && expFile.getName().toLowerCase().endsWith(KOML.FILE_EXTENSION))
-          || (expFile.getName().toLowerCase().endsWith("".xml"")) ) )
-    {
-       expFile = new File(expFile.getParent(), expFile.getName()
-                          + Experiment.FILE_EXTENSION);
+    
+    // add extension if necessary
+    if (m_FileChooser.getFileFilter() == m_ExpFilter) {
+      if (!expFile.getName().toLowerCase().endsWith(Experiment.FILE_EXTENSION))
+        expFile = new File(expFile.getParent(), expFile.getName() + Experiment.FILE_EXTENSION);
     }
+    else if (m_FileChooser.getFileFilter() == m_KOMLFilter) {
+      if (!expFile.getName().toLowerCase().endsWith(KOML.FILE_EXTENSION))
+        expFile = new File(expFile.getParent(), expFile.getName() + KOML.FILE_EXTENSION);
+    }
+    else if (m_FileChooser.getFileFilter() == m_XMLFilter) {
+      if (!expFile.getName().toLowerCase().endsWith("".xml""))
+        expFile = new File(expFile.getParent(), expFile.getName() + "".xml"");
+    }
+    
     try {
       // KOML?
       if ( (KOML.isPresent()) && (expFile.getAbsolutePath().toLowerCase().endsWith(KOML.FILE_EXTENSION)) ) {
          KOML.write(expFile.getAbsolutePath(), m_Exp);
       }
       else
       // XML?
       if (expFile.getAbsolutePath().toLowerCase().endsWith("".xml"")) {
          XMLExperiment xml = new XMLExperiment(); 
          xml.write(expFile, m_Exp);
       }
       // binary
       else {
          FileOutputStream fo = new FileOutputStream(expFile);
          ObjectOutputStream oo = new ObjectOutputStream(
                                  new BufferedOutputStream(fo));
          oo.writeObject(m_Exp);
          oo.close();
       }
       System.err.println(""Saved experiment:\n"" + m_Exp);
     } catch (Exception ex) {
       ex.printStackTrace();
       JOptionPane.showMessageDialog(this, ""Couldn't save experiment file:\n""
 				    + expFile
 				    + ""\nReason:\n"" + ex.getMessage(),
 				    ""Save Experiment"",
 				    JOptionPane.ERROR_MESSAGE);
     }
   }",Buggy,"A few bug fixes from Peter regarding the latest GUI additions. Also eliminated some superfluous output from NaiveBayesSimple.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@2259 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates bug fixes related to GUI additions. The code diff modifies the `saveExperiment` method, specifically how the file extension is handled when saving an experiment. The original code checks if the filename ends with any of the valid extensions, and if not, appends the default `.exp` extension. The modified code checks the selected file filter in the `JFileChooser` and appends the corresponding extension if it's missing. This change ensures the correct file extension is appended based on the selected file type, which is a bug fix. The answer is **Buggy**."
weka,24330.json,0613647d320672d37f6da68efd32bd14bb88435b,"@@ -1,7 +1,15 @@
   public void preProcess() throws Exception {
     
     if (m_ResultProducer == null) {
       throw new Exception(""No ResultProducer set"");
     }
+    // Tell the resultproducer to send results to us
+    m_ResultProducer.setResultListener(this);
+    findKeyIndex();
+    if (m_KeyIndex == -1) {
+      throw new Exception(""No key field called "" + m_KeyFieldName
+			  + "" produced by ""
+			  + m_ResultProducer.getClass().getName());
+    }
     m_ResultProducer.preProcess();
   }",Buggy,"- Fixed bug where sometimes the sub-ResultProducer would not be correctly
connected.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@171 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to the connection of a sub-ResultProducer. The diff shows that the code now explicitly sets the ResultListener for the m_ResultProducer to 'this' (the current object). This ensures that the ResultProducer sends results back to the current object, which is essential for proper communication and processing. The addition of setting the result listener and checking for the key index suggests a fix for a scenario where the result producer wasn't properly connected or configured, leading to incorrect or missing results. The answer is **Buggy**."
weka,24356.json,0613647d320672d37f6da68efd32bd14bb88435b,"@@ -1,5 +1,6 @@
   public void setResultProducer(ResultProducer newResultProducer) {
-    
+
     m_ResultProducer = newResultProducer;
+    m_ResultProducer.setResultListener(this);
     findKeyIndex();
   }",Buggy,"- Fixed bug where sometimes the sub-ResultProducer would not be correctly
connected.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@171 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to connecting a sub-ResultProducer. The diff shows that the `setResultProducer` method now also sets the result listener of the new result producer to `this`. This ensures that the current object receives results from the new result producer, which was likely the missing connection mentioned in the commit message. The answer is **Buggy**."
weka,24326.json,0613647d320672d37f6da68efd32bd14bb88435b,"@@ -1,71 +1,63 @@
   protected void doAverageResult(Object [] template) throws Exception {
 
     // Generate the key and ask whether the result is required
     String [] newKey = new String [template.length - 1];
     System.arraycopy(template, 0, newKey, 0, m_KeyIndex);
     System.arraycopy(template, m_KeyIndex + 1,
 		     newKey, m_KeyIndex,
 		     template.length - m_KeyIndex - 1);
-    /*
-    System.err.println(""new key: "" + DatabaseUtils.arrayToString(newKey));
-    */
     if (m_ResultListener.isResultRequired(this, newKey)) {
       Object [] resultTypes = m_ResultProducer.getResultTypes();
       Stats [] stats = new Stats [resultTypes.length];
       for (int i = 0; i < stats.length; i++) {
 	stats[i] = new Stats();
       }
       Object [] result = getResultTypes();
       int numMatches = 0;
       for (int i = 0; i < m_Keys.size(); i++) {
 	Object [] currentKey = (Object [])m_Keys.elementAt(i);
 	// Skip non-matching keys
 	if (!matchesTemplate(template, currentKey)) {
 	  continue;
 	}
 	// Add the results to the stats accumulator
 	Object [] currentResult = (Object [])m_Results.elementAt(i);
 	numMatches++;
-	/*
-	System.err.println(""Match: "" + DatabaseUtils.arrayToString(currentKey)
-			   + "" -- ""
-			   + DatabaseUtils.arrayToString(currentResult));
-	*/
 	for (int j = 0; j < resultTypes.length; j++) {
 	  if (resultTypes[j] instanceof Double) {
 	    if (currentResult[j] == null) {
 	      throw new Exception(""Null numeric result field found:\n""
 				  + DatabaseUtils.arrayToString(currentKey)
 				  + "" -- ""
 				  + DatabaseUtils
 				  .arrayToString(currentResult));
 	    }
 	    double currentVal = ((Double)currentResult[j]).doubleValue();
 	    stats[j].add(currentVal);
 	  }
 	}
       }
       if (numMatches != m_ExpectedResultsPerAverage) {
 	throw new Exception(""Expected "" + m_ExpectedResultsPerAverage
 			    + "" results matching key \""""
 			    + DatabaseUtils.arrayToString(template)
 			    + ""\"" but got ""
 			    + numMatches);
       }
       result[0] = new Double(numMatches);
       Object [] currentResult = (Object [])m_Results.elementAt(0);
       int k = 1;
       for (int j = 0; j < resultTypes.length; j++) {
 	if (resultTypes[j] instanceof Double) {
 	  stats[j].calculateDerived();
 	  result[k++] = new Double(stats[j].mean);
 	  if (getCalculateStdDevs()) {
 	    result[k++] = new Double(stats[j].stdDev);
 	  }
 	} else {
 	  result[k++] = currentResult[j];
 	}
       }
       m_ResultListener.acceptResult(this, newKey, result);      
     }
   }",NotBuggy,"- Fixed bug where sometimes the sub-ResultProducer would not be correctly
connected.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@171 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",NotBuggy,"The commit message indicates a bug fix related to the connection of a sub-ResultProducer. The code diff removes commented-out debugging print statements. While the removal of debugging code can be a cleanup task associated with bug fixing, it doesn't directly address the core issue of connecting the sub-ResultProducer. The diff doesn't show any changes related to connection logic or error handling for connection failures. Therefore, based on the provided diff, it's difficult to confirm the bug fix mentioned in the commit message.
The answer is **NotBuggy**."
weka,24324.json,0613647d320672d37f6da68efd32bd14bb88435b,"@@ -1,33 +1,31 @@
   public void doRun(int run) throws Exception {
-
+    
     if (m_Instances == null) {
       throw new Exception(""No Instances set"");
     }
-    // Tell the resultproducer to send results to us
-    m_ResultProducer.setResultListener(this);
     m_ResultProducer.setInstances(m_Instances);
-    
+
     // Clear the collected results
     m_Keys.removeAllElements();
     m_Results.removeAllElements();
     
     //    System.err.println(""Starting run "" + run);
     // Collect all the results
     // Should be smarter -- first doing a run collecting only the keys
     // from isResultRequired(), then determining if the average is required,
     // then getting the actual results.
     m_ResultProducer.doRun(run);
 
     // Average the results collected
     //System.err.println(""Number of results collected: "" + m_Keys.size());
 
     // Check that the keys only differ on the selected key field
     checkForMultipleDifferences();
 
     Object [] template = (Object [])((Object [])m_Keys.elementAt(0)).clone();
     template[m_KeyIndex] = null;
     // Check for duplicate keys
     checkForDuplicateKeys(template);
     // Calculate the average and submit it if necessary
     doAverageResult(template);
   }",Buggy,"- Fixed bug where sometimes the sub-ResultProducer would not be correctly
connected.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@171 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to the connection of a sub-ResultProducer. The diff shows that `m_ResultProducer.setResultListener(this);` was removed and `m_ResultProducer.setInstances(m_Instances);` was moved up. This suggests that the order of operations was incorrect, potentially causing the sub-ResultProducer to not be correctly connected. The removal of `m_ResultProducer.setResultListener(this);` could be problematic if the result listener is no longer set. However, the commit message specifically mentions a fix for the connection of the sub-ResultProducer, so it is likely that the removal of this line is part of the fix. The change in order of operations suggests that the previous order was causing the bug.

**Buggy**
"
weka,24382.json,0613647d320672d37f6da68efd32bd14bb88435b,"@@ -1,7 +1,8 @@
   public void preProcess() throws Exception {
     
     if (m_ResultProducer == null) {
       throw new Exception(""No ResultProducer set"");
     }
+    m_ResultProducer.setResultListener(this);
     m_ResultProducer.preProcess();
   }",Buggy,"- Fixed bug where sometimes the sub-ResultProducer would not be correctly
connected.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@171 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to connecting the sub-ResultProducer. The diff adds the line `m_ResultProducer.setResultListener(this);`. This line ensures that the current object (likely a ResultListener) is properly connected to the ResultProducer before preprocessing. Without this line, the ResultProducer might not have been correctly configured, leading to the bug described in the commit message. The answer is **Buggy**."
weka,19514.json,90f4455c8174476ac55591cfcab3ea403fa3db02,"@@ -1,58 +1,66 @@
   public boolean eventGeneratable(String eventName) {
     if (eventName.compareTo(""trainingSet"") == 0) { 
       if (m_trainingProvider == null) {
 	return false;
       } else {
 	if (m_trainingProvider instanceof EventConstraints) {
 	  if (!((EventConstraints)m_trainingProvider).
 	      eventGeneratable(""trainingSet"")) {
 	    return false;
 	  }
 	}
       }
     }
 
     if (eventName.compareTo(""dataSet"") == 0) { 
       if (m_dataProvider == null) {
-	m_connectedFormat = null;
-	notifyDataFormatListeners();
+	if (m_instanceProvider == null) {
+	  m_connectedFormat = null;
+	  notifyDataFormatListeners();
+	}
 	return false;
       } else {
 	if (m_dataProvider instanceof EventConstraints) {
 	  if (!((EventConstraints)m_dataProvider).
 	      eventGeneratable(""dataSet"")) {
 	    m_connectedFormat = null;
 	    notifyDataFormatListeners();
 	    return false;
 	  }
 	}
       }
     }
 
     if (eventName.compareTo(""instance"") == 0) { 
       if (m_instanceProvider == null) {
+	if (m_dataProvider == null) {
+	  m_connectedFormat = null;
+	  notifyDataFormatListeners();
+	}
 	return false;
       } else {
 	if (m_instanceProvider instanceof EventConstraints) {
 	  if (!((EventConstraints)m_instanceProvider).
 	      eventGeneratable(""instance"")) {
+	    m_connectedFormat = null;
+	    notifyDataFormatListeners();
 	    return false;
 	  }
 	}
       }
     }
 
     if (eventName.compareTo(""testSet"") == 0) {
       if (m_testProvider == null) {
 	return false;
       } else {
 	if (m_testProvider instanceof EventConstraints) {
 	  if (!((EventConstraints)m_testProvider).
 	      eventGeneratable(""testSet"")) {
 	    return false;
 	  }
 	}
       }
     }
     return true;
   }",Buggy,"Fixed bug in format handling


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@1966 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix related to format handling. The code changes involve checks for null `m_dataProvider` and `m_instanceProvider` within the ""dataSet"" and ""instance"" event handling blocks. If either provider is null, the code now checks if the other provider is also null. If so, it resets `m_connectedFormat` and notifies data format listeners. This suggests that the original code might not have correctly handled cases where one of the providers was null, potentially leading to incorrect format information being used. The added null checks and format reset seem to address this issue. The answer is **Buggy**."
weka,19490.json,90f4455c8174476ac55591cfcab3ea403fa3db02,"@@ -1,17 +1,18 @@
   public void acceptInstance(InstanceEvent e) {
     if (e.getStatus() == InstanceEvent.FORMAT_AVAILABLE) {
       //      Instances dataSet = e.getInstance().dataset();
       m_connectedFormat = e.getStructure();
       
       //      System.err.println(""Assigning class column..."");
       assignClass(m_connectedFormat);
       notifyInstanceListeners(e);
 
       // tell any listening customizers (or other interested parties)
+      System.err.println(""Notifying customizer..."");
       notifyDataFormatListeners();
     } else {
       //      Instances dataSet = e.getInstance().dataset();
       //      assignClass(dataSet);
       notifyInstanceListeners(e);
     }
   }",NotBuggy,"Fixed bug in format handling


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@1966 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to format handling. The code change adds a `System.err.println` statement within the `if` block that handles `InstanceEvent.FORMAT_AVAILABLE`. This print statement is likely added for debugging purposes to understand if the format available block is executed. The addition of a debugging statement suggests there was an issue or uncertainty around the execution flow related to format handling, implying a bug fix. The answer is **Buggy**."
weka,25728.json,296933fb5c56e6b698f0f76c710c4ef61eaa9a3c,"@@ -1,35 +1,37 @@
     protected void performBoostingCV() throws Exception{			
 	
 	//completed iteration keeps track of the number of iterations that have been
 	//performed in every fold (some might stop earlier than others). 
 	//Best iteration is selected only from these.
 	int completedIterations = m_maxIterations;
 	
 	Instances allData = new Instances(m_train);
 	
 	allData.stratify(m_numFoldsBoosting);	      
 
 	double[] error = new double[m_maxIterations + 1];	
 	
+        SimpleLinearRegression[][] backup = m_regressions;
+
 	for (int i = 0; i < m_numFoldsBoosting; i++) {
 	    //split into training/test data in fold
 	    Instances train = allData.trainCV(m_numFoldsBoosting,i);
 	    Instances test = allData.testCV(m_numFoldsBoosting,i);
 
 	    //initialize LogitBoost
 	    m_numRegressions = 0;
-	    m_regressions = initRegressions();
+	    m_regressions = copyRegressions(backup);
 
 	    //run LogitBoost iterations
 	    int iterations = performBoosting(train,test,error,completedIterations);	    
 	    if (iterations < completedIterations) completedIterations = iterations;	    
 	}
 
 	//determine iteration with minimum error over the folds
 	int bestIteration = getBestIteration(error,completedIterations);
 
 	//rebuild model on all of the training data
 	m_numRegressions = 0;
-        m_regressions = initRegressions();
+        m_regressions = backup;
 	performBoosting(bestIteration);
     }",Buggy,"Bug fix for the case where cross-validation is performed locally at each node.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@10104 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to cross-validation performed locally at each node. The diff shows changes within the `performBoostingCV` method. Specifically, it introduces a backup of `m_regressions` before the cross-validation folds and restores it after the folds are processed. The original code re-initialized `m_regressions` in each fold and after the cross-validation, potentially losing information learned in previous folds or using incorrect regressions for the final model. The fix ensures that the original regressions are used for the final model building, which is crucial for correct cross-validation results. The answer is **Buggy**."
weka,25729.json,296933fb5c56e6b698f0f76c710c4ef61eaa9a3c,"@@ -1,22 +1,13 @@
     protected SimpleLinearRegression[][] copyRegressions(SimpleLinearRegression[][] a)	
         throws Exception {
     
         SimpleLinearRegression[][] result = initRegressions();
         for (int i = 0; i < a.length; i++) {
             for (int j = 0; j < a[i].length; j++) {
                 if (j != m_numericDataHeader.classIndex()) {
                     result[i][j].addModel(a[i][j]);
                 }
             }
         }
-
-        /*        SimpleLinearRegression[][] result = null;
-        try {
-            SerializedObject so = new SerializedObject(a);
-            result = (SimpleLinearRegression[][])so.getObject();
-        } catch (Exception ex) {
-            System.err.println(""Can't copy array of simple linear regression objects."");
-            System.err.println(ex);
-            }*/
         return result;
     }",NotBuggy,"Bug fix for the case where cross-validation is performed locally at each node.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@10104 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to cross-validation performed locally at each node. The code diff replaces a commented-out block of code that attempted to copy the `SimpleLinearRegression[][]` array using serialization with a working implementation that iterates through the array and copies each `SimpleLinearRegression` object using the `addModel` method. The commented-out code likely failed in certain scenarios, leading to the bug described in the commit message. The new code provides a correct way to copy the array, thus fixing the bug. The answer is **Buggy**."
weka,20414.json,d5169a182f63b31a737d9b5d57ecdb49b8fbd56a,"@@ -1,15 +1,15 @@
   protected void newStructure() throws Exception {
     
     m_Loader.reset();
     
     // Set environment variables
     if (m_Loader instanceof EnvironmentHandler && m_env != null) {
       try {
         ((EnvironmentHandler)m_Loader).setEnvironment(m_env);
       }catch (Exception ex) {
       }
     }
     m_dataFormat = m_Loader.getStructure();
-//    System.out.println(""[Loader] Notifying listeners of instance structure avail."");
-//    notifyStructureAvailable(m_dataFormat);
+    System.out.println(""[Loader] Notifying listeners of instance structure avail."");
+    notifyStructureAvailable(m_dataFormat);
   }",NotBuggy,"Fixed a bug where configuration changes (new files selected or database configuration changed) was not getting sent to downstream steps

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@7854 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to configuration changes not being propagated to downstream steps. The diff shows the un-commenting of the line `notifyStructureAvailable(m_dataFormat);`. This suggests that the notification mechanism was previously disabled, preventing downstream steps from being informed about the new data structure resulting from configuration changes. Re-enabling this notification would indeed fix the bug described in the commit message. The answer is **Buggy**."
weka,20407.json,d5169a182f63b31a737d9b5d57ecdb49b8fbd56a,"@@ -1,4 +1,11 @@
   public void setDB(boolean flag){
   
       m_dbSet = flag;
+      if (m_dbSet) {
+        try {
+          newStructure();
+        } catch (Exception e) {
+          e.printStackTrace();
+        }
+      }
   }",Buggy,"Fixed a bug where configuration changes (new files selected or database configuration changed) was not getting sent to downstream steps

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@7854 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to configuration changes not being propagated to downstream steps. The code diff shows that when the database configuration is set (m_dbSet = true), a new structure is created using the `newStructure()` method. This method call is now wrapped in a try-catch block to handle potential exceptions. The addition of the `newStructure()` call within the `setDB` method suggests that the previous implementation was missing a crucial step to update the downstream steps when the database configuration changed. The try-catch block further indicates that the `newStructure()` method might throw exceptions, which were not being handled before, potentially leading to the bug described in the commit message. Therefore, the changes indicate a bug fix. The answer is **Buggy**.
"
weka,19057.json,d5169a182f63b31a737d9b5d57ecdb49b8fbd56a,"@@ -1,6 +1,41 @@
   private void setUpOther() {
     removeAll();
     add(m_LoaderEditor, BorderLayout.CENTER);
+    
+    JPanel buttonsP = new JPanel();
+    buttonsP.setLayout(new FlowLayout());
+    JButton ok,cancel;
+    buttonsP.add(ok = new JButton(""OK""));
+    buttonsP.add(cancel=new JButton(""Cancel""));
+    ok.addActionListener(new ActionListener(){
+      public void actionPerformed(ActionEvent evt){
+        
+        // Tell the editor that we are closing under an OK condition
+        // so that it can pass on the message to any customizer that
+        // might be in use
+        m_LoaderEditor.closingOK();
+        
+        if (m_parentWindow != null) {
+          m_parentWindow.dispose();
+        }
+      }
+    });
+    cancel.addActionListener(new ActionListener(){
+      public void actionPerformed(ActionEvent evt){
+        
+        // Tell the editor that we are closing under a CANCEL condition
+        // so that it can pass on the message to any customizer that
+        // might be in use
+        m_LoaderEditor.closingCancel();
+        
+        if (m_parentWindow != null) {
+          m_parentWindow.dispose();
+        }
+      }
+    });
+    
+    add(buttonsP, BorderLayout.SOUTH);
+    
     validate();
     repaint();
   }",Buggy,"Fixed a bug where configuration changes (new files selected or database configuration changed) was not getting sent to downstream steps

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@7854 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to configuration changes not being propagated to downstream steps. The diff introduces ""OK"" and ""Cancel"" buttons with action listeners. These buttons trigger `m_LoaderEditor.closingOK()` or `m_LoaderEditor.closingCancel()` which likely notify the editor about the closing state, allowing it to propagate configuration changes. This aligns with the commit message, suggesting the added buttons and their actions are part of the bug fix. The answer is **Buggy**.
"
titan,4498.json,3b4dd304515746e895b4ea7cd1728c0bcfd5d460,"@@ -1,4 +1,4 @@
         public Builder set(String path, Object value) {
-            super.set(path, value);
+            writeConfiguration.set(path, value);
             return this;
         }",Buggy,"Let TitanFactory.Builder accept arbitrary keys

Elasticsearch reserves a Titan config namespace under which the user
may provide arbitrary Elasticsearch config options.  These options
can't be validated, since it's fundamentally a bit of a perversion of
the config API: the keys are defined in and controlled by
Elasticsearch, but we're cramming them into a Titan configuration.

This commit tweaks TitanFactory.Builder to use a WriteConfiguration
(which is based on dumb strings) instead of a ModifiableConfiguration
(which is based on ConfigOption and includes validation).

A more elegant way to approach this might be to introduce an
abstraction to support Elasticsearch's reserved namespace: some kind
of boolean field that says, in effect, ""don't try to validate any keys
underneath me"".  That still wouldn't totally address the problem,
though; (User)ModifiableConfiguration still wouldn't be a suitable
basis for TitanFactory.Builder, since it requires that any
user-provided config key string map to a ConfigOption.

This commit also adds a test (that fails without the TitanFactory
change).

Fixes #1114
",Buggy,"The commit message indicates a change in how TitanFactory.Builder handles configuration keys, specifically for Elasticsearch configurations. It mentions that the change involves using a WriteConfiguration instead of a ModifiableConfiguration to avoid validation issues with arbitrary Elasticsearch config options. The diff shows that the `set` method in the Builder class is modified to use `writeConfiguration.set(path, value)` instead of `super.set(path, value)`. This change aligns with the commit message's description of switching to a WriteConfiguration. The commit message explicitly states that this change fixes issue #1114, implying that the previous behavior was considered a bug or limitation. The answer is **Buggy**.
"
titan,353.json,3b4dd304515746e895b4ea7cd1728c0bcfd5d460,"@@ -1,16 +1,16 @@
     public Map<ConfigElement.PathIdentifier,Object> getAll() {
         Map<ConfigElement.PathIdentifier,Object> result = Maps.newHashMap();
 
         for (String key : config.getKeys("""")) {
             Preconditions.checkArgument(StringUtils.isNotBlank(key));
             try {
                 ConfigElement.PathIdentifier pid = ConfigElement.parse(getRootNamespace(),key);
                 Preconditions.checkArgument(pid.element.isOption() && !pid.lastIsUmbrella);
                 result.put(pid,get((ConfigOption)pid.element,pid.umbrellaElements));
             } catch (IllegalArgumentException e) {
-                log.info(""Ignored configuration entry for {} since it does not map to an option"",key,e);
+                log.debug(""Ignored configuration entry for {} since it does not map to an option"",key,e);
                 continue;
             }
         }
         return result;
     }",NotBuggy,"Let TitanFactory.Builder accept arbitrary keys

Elasticsearch reserves a Titan config namespace under which the user
may provide arbitrary Elasticsearch config options.  These options
can't be validated, since it's fundamentally a bit of a perversion of
the config API: the keys are defined in and controlled by
Elasticsearch, but we're cramming them into a Titan configuration.

This commit tweaks TitanFactory.Builder to use a WriteConfiguration
(which is based on dumb strings) instead of a ModifiableConfiguration
(which is based on ConfigOption and includes validation).

A more elegant way to approach this might be to introduce an
abstraction to support Elasticsearch's reserved namespace: some kind
of boolean field that says, in effect, ""don't try to validate any keys
underneath me"".  That still wouldn't totally address the problem,
though; (User)ModifiableConfiguration still wouldn't be a suitable
basis for TitanFactory.Builder, since it requires that any
user-provided config key string map to a ConfigOption.

This commit also adds a test (that fails without the TitanFactory
change).

Fixes #1114
",NotBuggy,"The commit message indicates a change to allow TitanFactory.Builder to accept arbitrary keys, particularly for Elasticsearch configurations. The rationale is that Elasticsearch configuration keys are not validated by Titan, and the existing ModifiableConfiguration (which relies on ConfigOption and validation) is not suitable. The commit switches to using a WriteConfiguration, which is based on strings and doesn't enforce validation. The diff changes the log level from INFO to DEBUG for ignored configuration entries. This change suggests that the previous INFO level logging was too noisy for cases where configuration entries do not map to a ConfigOption, which is expected when using arbitrary keys. This change is related to the issue of accepting arbitrary keys, as it reduces the verbosity of logging when encountering such keys. The change from INFO to DEBUG suggests a refinement in how non-standard configurations are handled, but it doesn't directly indicate a bug fix in the sense of correcting incorrect behavior. It's more of an adjustment to logging behavior based on a new use case. The answer is **NotBuggy**.
"
titan,4499.json,3b4dd304515746e895b4ea7cd1728c0bcfd5d460,"@@ -1,3 +1,5 @@
         public TitanGraph open() {
-            return TitanFactory.open(super.getConfiguration());
+            ModifiableConfiguration mc = new ModifiableConfiguration(GraphDatabaseConfiguration.ROOT_NS,
+                    writeConfiguration.copy(), BasicConfiguration.Restriction.NONE);
+            return TitanFactory.open(mc);
         }",Buggy,"Let TitanFactory.Builder accept arbitrary keys

Elasticsearch reserves a Titan config namespace under which the user
may provide arbitrary Elasticsearch config options.  These options
can't be validated, since it's fundamentally a bit of a perversion of
the config API: the keys are defined in and controlled by
Elasticsearch, but we're cramming them into a Titan configuration.

This commit tweaks TitanFactory.Builder to use a WriteConfiguration
(which is based on dumb strings) instead of a ModifiableConfiguration
(which is based on ConfigOption and includes validation).

A more elegant way to approach this might be to introduce an
abstraction to support Elasticsearch's reserved namespace: some kind
of boolean field that says, in effect, ""don't try to validate any keys
underneath me"".  That still wouldn't totally address the problem,
though; (User)ModifiableConfiguration still wouldn't be a suitable
basis for TitanFactory.Builder, since it requires that any
user-provided config key string map to a ConfigOption.

This commit also adds a test (that fails without the TitanFactory
change).

Fixes #1114
",Buggy,"The commit message indicates a change to allow arbitrary keys in TitanFactory.Builder, specifically for Elasticsearch configuration. The diff shows that a ModifiableConfiguration is being used, which is based on ConfigOption and includes validation. The commit message explains that this change is necessary because Elasticsearch configuration keys cannot be validated by Titan. This change is related to a bug where TitanFactory.Builder was not accepting arbitrary keys, as indicated by the ""Fixes #1114"" in the commit message. The answer is **Buggy**."
spring-boot,5691.json,abe3d3852761430c9f5f1368d82509d0c19ac4d9,"@@ -1,12 +1,11 @@
 	private Stream<Wrapper> getLoadOnStartupWrappers(Container[] children) {
 		Map<Integer, List<Wrapper>> grouped = new TreeMap<>();
 		for (Container child : children) {
 			Wrapper wrapper = (Wrapper) child;
 			int order = wrapper.getLoadOnStartup();
 			if (order >= 0) {
-				grouped.computeIfAbsent(order, ArrayList::new);
-				grouped.get(order).add(wrapper);
+				grouped.computeIfAbsent(order, (o) -> new ArrayList<>()).add(wrapper);
 			}
 		}
 		return grouped.values().stream().flatMap(List::stream);
 	}",Buggy,"Fix OoM error when starting Tomcat with max int load on startup

Fixes gh-17927
",Buggy,"The commit message indicates a fix for an OutOfMemory (OoM) error when starting Tomcat with a maximum integer load on startup. The code diff modifies the `getLoadOnStartupWrappers` method. Specifically, the line `grouped.computeIfAbsent(order, ArrayList::new);` is changed to `grouped.computeIfAbsent(order, (o) -> new ArrayList<>()).add(wrapper);`.

The original code had a potential issue. `grouped.computeIfAbsent(order, ArrayList::new);` only created the `ArrayList` if it didn't exist. The subsequent line `grouped.get(order).add(wrapper);` would then attempt to retrieve the list and add the wrapper. However, if `grouped.get(order)` returned null, it would cause a NullPointerException.

The corrected code combines the creation of the `ArrayList` and the addition of the `wrapper` into a single operation using a lambda expression. This ensures that the `ArrayList` is created and the `wrapper` is added atomically, preventing the potential NullPointerException.

The change addresses a potential error condition that could lead to an OoM error, indicating a bug fix. The answer is **Buggy**."
spring-boot,164.json,9c5f207e2221226c9cfdb1970bb2ed2946e142d1,"@@ -1,10 +1,9 @@
 		public ResourceConfigCustomizer resourceConfigCustomizer(
 				final ObjectMapper objectMapper) {
 			addJaxbAnnotationIntrospectorIfPresent(objectMapper);
 			return (ResourceConfig config) -> {
-				JerseyAutoConfiguration.this.config.register(JacksonFeature.class);
-				JerseyAutoConfiguration.this.config.register(
-						new ObjectMapperContextResolver(objectMapper),
+				config.register(JacksonFeature.class);
+				config.register(new ObjectMapperContextResolver(objectMapper),
 						ContextResolver.class);
 			};
 		}",Buggy,"Fix merge error
",Buggy,"The commit message indicates a fix for a merge error. The diff removes `JerseyAutoConfiguration.this` from the `config.register` calls. This suggests that the original code was incorrectly referencing the `config` object within the lambda expression, likely due to a merge conflict that introduced the unnecessary qualification. Removing the qualification resolves the error and ensures the `config` object is accessed correctly. The answer is **Buggy**."
spring-boot,7647.json,809a3965c961752bcbbc4bcae488f34be062d480,"@@ -1,12 +1,8 @@
 	public Iterable<Tag> tags(ClientRequest request, ClientResponse response, Throwable throwable) {
 		Tag method = WebClientExchangeTags.method(request);
 		Tag uri = WebClientExchangeTags.uri(request);
 		Tag clientName = WebClientExchangeTags.clientName(request);
-		if (response != null) {
-			return Arrays.asList(method, uri, clientName, WebClientExchangeTags.status(response),
-					WebClientExchangeTags.outcome(response));
-		}
-		else {
-			return Arrays.asList(method, uri, clientName, WebClientExchangeTags.status(throwable));
-		}
+		return Arrays.asList(method, uri, clientName,
+				(response != null) ? WebClientExchangeTags.status(response) : WebClientExchangeTags.status(throwable),
+				WebClientExchangeTags.outcome(response));
 	}",Buggy,"Add missing outcome tag for WebClient metrics

On error cases, the ""outcome"" tag would be missing from recorded metrics
for the `WebClient`.

This commit fixes this issue and improves the reference documentation by
mentioning the tag values used for error cases, when the client response
is not received (I/O errors, client error, etc).

Fixes gh-17219
",Buggy,"The commit message indicates a bug fix related to missing ""outcome"" tags in WebClient metrics for error cases. The diff modifies the `tags` method to include the `WebClientExchangeTags.outcome(response)` tag regardless of whether the response is null or not. The original code only included the outcome tag when the response was not null, leading to the missing tag in error scenarios. The modified code now includes the outcome tag in all cases, addressing the bug described in the commit message. The answer is **Buggy**."
spring-boot,6747.json,3153117429756fc94489c1c9350b2149435a3b3a,"@@ -1,10 +1,13 @@
 	public boolean isAncestorOf(ConfigurationPropertyName name) {
+		if (this.equals(EMPTY)) {
+			return true;
+		}
 		ConfigurationPropertyName candidate = (name == null ? null : name.getParent());
 		while (candidate != null) {
 			if (candidate.equals(this)) {
 				return true;
 			}
 			candidate = candidate.getParent();
 		}
 		return false;
 	}",Buggy,"Fix ConfigurationPropertyName ancestor bug

Fix an issue with `ConfigurationPropertyName` where the `isAncesorOf`
method would not work with `ConfigurationPropertyName.EMPTY`

See gh-9000
",Buggy,"The commit message indicates a bug fix in the `isAncestorOf` method of the `ConfigurationPropertyName` class, specifically when dealing with `ConfigurationPropertyName.EMPTY`. The diff adds a check at the beginning of the method to return `true` if `this` is equal to `EMPTY`. This suggests that the original implementation had a flaw where it didn't correctly handle the case when checking if `EMPTY` was an ancestor of another `ConfigurationPropertyName`. The added condition addresses this issue, indicating a bug fix. The answer is **Buggy**."
spring-boot,6206.json,a657a28f58e0eb8487eede9c60062186d850408d,"@@ -1,18 +1,18 @@
 		private void readUnicode() throws IOException {
 			this.character = 0;
 			for (int i = 0; i < 4; i++) {
 				int digit = this.reader.read();
-				if (digit > -'0' && digit <= '9') {
+				if (digit >= '0' && digit <= '9') {
 					this.character = (this.character << 4) + digit - '0';
 				}
-				else if (digit > -'a' && digit <= 'f') {
+				else if (digit >= 'a' && digit <= 'f') {
 					this.character = (this.character << 4) + digit - 'a' + 10;
 				}
-				else if (digit > -'A' && digit <= 'F') {
+				else if (digit >= 'A' && digit <= 'F') {
 					this.character = (this.character << 4) + digit - 'A' + 10;
 				}
 				else {
-					throw new IllegalArgumentException(""Malformed \\uxxxx encoding."");
+					throw new IllegalStateException(""Malformed \\uxxxx encoding."");
 				}
 			}
 		}",Buggy,"Fix properties unicode value decoding

Fix a range error when checking for unicode hex chars.

Fixes gh-12716
",Buggy,"The commit message indicates a fix for unicode value decoding in properties files, specifically addressing a range error when checking for unicode hex characters. The diff modifies the conditions for checking if a character is a valid hexadecimal digit within the `readUnicode` method. The original code `digit > -'0'` and similar conditions were incorrect, potentially leading to range errors. The corrected code `digit >= '0'` and similar conditions now properly check if the digit falls within the valid range of hexadecimal characters ('0'-'9', 'a'-'f', 'A'-'F'). Additionally, the exception thrown was changed from `IllegalArgumentException` to `IllegalStateException`, which is more appropriate for this scenario. These changes directly address the bug described in the commit message. The answer is **Buggy**."
spring-boot,2030.json,eea83e935eda0f05e1aaa11a38b0e7e379527f40,"@@ -1,10 +1,10 @@
 	private List<String> getData(int errorStatus) {
-		HttpStatus errorHttpStatus = HttpStatus.resolve(errorStatus);
 		List<String> data = new ArrayList<>();
 		data.add(""error/"" + errorStatus);
-		if (errorHttpStatus != null) {
-			data.add(""error/"" + SERIES_VIEWS.get(errorHttpStatus.series()));
+		HttpStatus.Series series = HttpStatus.Series.resolve(errorStatus);
+		if (series != null) {
+			data.add(""error/"" + SERIES_VIEWS.get(series));
 		}
 		data.add(""error/error"");
 		return data;
 	}",Buggy,"Resolve httpstatus error series using raw int code

See gh-16691
",Buggy,"The commit message indicates a resolution of HTTP status errors using raw integer codes, referencing issue gh-16691. The code diff modifies the `getData` method to directly resolve the `HttpStatus.Series` from the integer `errorStatus` instead of resolving the entire `HttpStatus` first. This change suggests that the original code might have had issues correctly mapping error statuses to their corresponding series, potentially leading to incorrect error handling or data retrieval. The use of `HttpStatus.Series.resolve` directly addresses this, indicating a bug fix related to HTTP status code processing. The answer is **Buggy**.
"
guava,22893.json,201bb362a9b6de3fec6507ff435ac6022b92da7b,"@@ -1,7 +1,8 @@
   public void addListener(Runnable runnable, Executor executor) {
+    Listener listener = new Listener(runnable, executor);
     if (isDone()) {
-      executor.execute(runnable);
+      listener.execute();
     } else {
-      listeners.add(new Listener(runnable, executor));
+      listeners.add(listener);
     }
   }",Buggy,"Add and use helper methods like assertDone() in tests.
This found no bugs -- well, maybe one if you count the inconsistent propagation of interruption from the delegate, though I had suspected as much already.

Add another ""misbehaving listener"" test, this one for the case in which the Future is already done.
This found a bug, which I've fixed.
-------------
Created by MOE: http://code.google.com/p/moe-java
MOE_MIGRATED_REVID=85074586
",Buggy,"The commit message explicitly states that a bug was found and fixed. The code diff shows that when the future is already done, the runnable is now executed using a `Listener` object, which was previously only used when the future was not yet done. This change ensures consistent handling of listeners regardless of the future's state, which aligns with the bug fix mentioned in the commit message. The answer is **Buggy**."
guava,16853.json,1ed32d483d0e583f7a245b60932c8c291c9a1e37,"@@ -1,19 +1,19 @@
-  static <E, E2 extends E> ImmutableMultiset<E> copyFromEntries(
-      Collection<Entry<E2>> entries) {
+  static <E> ImmutableMultiset<E> copyFromEntries(
+      Collection<? extends Entry<? extends E>> entries) {
     long size = 0;
     ImmutableMap.Builder<E, Integer> builder = ImmutableMap.builder();
-    for (Entry<E2> entry : entries) {
+    for (Entry<? extends E> entry : entries) {
       int count = entry.getCount();
       if (count > 0) {
         // Since ImmutableMap.Builder throws an NPE if an element is null, no
         // other null checks are needed.
         builder.put(entry.getElement(), count);
         size += count;
       }
     }
 
     if (size == 0) {
       return of();
     }
     return new RegularImmutableMultiset<E>(builder.build(), Ints.saturatedCast(size));
   }",Buggy,"Fix ImmutableMultiset 1.6.0u24 compilation error.
",Buggy,"The commit message indicates a fix for a compilation error in ImmutableMultiset. The diff changes the generic type of the `entries` parameter in the `copyFromEntries` method. Specifically, it changes `Collection<Entry<E2>>` to `Collection<? extends Entry<? extends E>>`. This change likely addresses a type incompatibility issue that was causing the compilation error. The original code expected a collection of entries with a specific type `E2` that extends `E`, while the corrected code allows for a collection of entries where the element type is a subtype of `E`. This is a bug fix because it resolves a compilation error caused by incorrect type constraints. The answer is **Buggy**."
guava,15762.json,2b7e8589db7c8ba987c6a960114a3194a7c2fe0e,"@@ -1,8 +1,9 @@
   @Override public int compare(Comparable left, Comparable right) {
-    checkNotNull(right); // left null is caught later
+    checkNotNull(left); // for GWT
+    checkNotNull(right);
     if (left == right) {
       return 0;
     }
 
     return left.compareTo(right);
   }",Buggy,"Make SerializableTester.reserialize a no-op under GWT so that test authors can use it as part of larger test methods.
With that done, eliminate the equivalent, awful hack in RangeTest.
Further, with bug 5599623 fixed, run OrderingTest under GWT.
-------------
Created by MOE: http://code.google.com/p/moe-java
MOE_MIGRATED_REVID=25672151
",Buggy,"The commit message indicates changes to improve testing under GWT (Google Web Toolkit). The diff adds a `checkNotNull(left)` call, which seems to be specifically for GWT. This suggests a bug or limitation in the original code's behavior under GWT that is being addressed by adding a null check. The added null check acts as a guard, preventing potential errors or unexpected behavior when running the code in a GWT environment. The answer is **Buggy**."
guava,19040.json,20a42756c477b03c0123870b763148f4a18bdc8e,"@@ -1,39 +1,39 @@
   public static long checkedPow(long b, int k) {
     checkNonNegative(""exponent"", k);
     if (b >= -2 & b <= 2) {
       switch ((int) b) {
         case 0:
           return (k == 0) ? 1 : 0;
         case 1:
           return 1;
         case (-1):
           return ((k & 1) == 0) ? 1 : -1;
         case 2:
           checkNoOverflow(k < Long.SIZE - 1);
           return 1L << k;
         case (-2):
           checkNoOverflow(k < Long.SIZE);
           return ((k & 1) == 0) ? (1L << k) : (-1L << k);
         default:
           throw new AssertionError();
       }
     }
     long accum = 1;
     while (true) {
       switch (k) {
         case 0:
           return accum;
         case 1:
           return checkedMultiply(accum, b);
         default:
           if ((k & 1) != 0) {
             accum = checkedMultiply(accum, b);
           }
           k >>= 1;
           if (k > 0) {
-            checkNoOverflow(b <= FLOOR_SQRT_MAX_LONG);
+            checkNoOverflow(-FLOOR_SQRT_MAX_LONG <= b && b <= FLOOR_SQRT_MAX_LONG);
             b *= b;
           }
       }
     }
   }",Buggy,"Fix LongMath.checkedPow to correctly error out on large negative inputs, and fix LongMathTest to test a full range of long inputs.  See https://github.com/google/guava/issues/2036 .
-------------
Created by MOE: http://code.google.com/p/moe-java
MOE_MIGRATED_REVID=92571001
",Buggy,"The commit message indicates a fix for `LongMath.checkedPow` related to handling large negative inputs and includes a reference to a GitHub issue. The diff modifies the `checkedPow` method, specifically adding a check `checkNoOverflow(-FLOOR_SQRT_MAX_LONG <= b && b <= FLOOR_SQRT_MAX_LONG);`. This check ensures that the base `b` is within the valid range before squaring it, preventing potential overflow issues when dealing with large negative numbers. This change directly addresses the bug mentioned in the commit message. The answer is **Buggy**.
"
guava,14536.json,93ce68b41636faafb78d391dcf720e06154862da,"@@ -1,6 +1,6 @@
   private static void closeAll(BaseStream<?, ?>[] toClose) {
     for (BaseStream<?, ?> stream : toClose) {
-      // TODO(b/198102330): Catch exceptions, rethrowing later with extras as suppressed exceptions.
+      // TODO(b/80534298): Catch exceptions, rethrowing later with extras as suppressed exceptions.
       stream.close();
     }
   }",Buggy,"Fix ""bug"" link that was actually a CL.

RELNOTES=n/a

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=265457436
",NotBuggy,"The commit message indicates a fix to a link, which was actually a changelist (CL). The diff changes a TODO comment, specifically updating the issue number referenced within the comment from b/198102330 to b/80534298. This change doesn't directly fix a functional bug in the code but corrects a reference to a related issue or change. Therefore, it's not a bug fix in the traditional sense of correcting faulty logic or error handling. The answer is **NotBuggy**."
guava,16180.json,ac579e0d20dd45ac8a713789e4708858a19fa894,"@@ -1,6 +1,5 @@
   public static <E extends Comparable> int binarySearch(List<? extends E> list, E e,
       KeyPresentBehavior presentBehavior, KeyAbsentBehavior absentBehavior) {
     checkNotNull(e);
-    return binarySearch(
-        list, checkNotNull(e), Ordering.natural(), presentBehavior, absentBehavior);
+    return binarySearch(list, e, Ordering.natural(), presentBehavior, absentBehavior);
   }",Buggy,"Fix some bugs related to calling Preconditions.checkNotNull on expressions
that are definitely non-null.

Created via a work-in-progress error-prone check for unnecessary null checks.
-------------
Created by MOE: http://code.google.com/p/moe-java
MOE_MIGRATED_REVID=81594383
",Buggy,"The commit message indicates a fix for bugs related to unnecessary `checkNotNull` calls, specifically when the expression is already guaranteed to be non-null. The diff removes a `checkNotNull(e)` call within the `binarySearch` method, suggesting that `e` is already known to be non-null at that point. This aligns with the commit message's description of fixing unnecessary null checks. The answer is **Buggy**."
guava,17257.json,13800999134027e799b9250d261dd2c2a969183d,"@@ -1,12 +1,12 @@
   public static <K, V> MapDifference<K, V> difference(
       Map<? extends K, ? extends V> left, Map<? extends K, ? extends V> right,
       Equivalence<? super V> valueEquivalence) {
     Preconditions.checkNotNull(valueEquivalence);
 
-    Map<K, V> onlyOnLeft = newHashMap();
-    Map<K, V> onlyOnRight = new HashMap<K, V>(right); // will whittle it down
-    Map<K, V> onBoth = newHashMap();
-    Map<K, MapDifference.ValueDifference<V>> differences = newHashMap();
+    Map<K, V> onlyOnLeft = newLinkedHashMap();
+    Map<K, V> onlyOnRight = new LinkedHashMap<K, V>(right); // will whittle it down
+    Map<K, V> onBoth = newLinkedHashMap();
+    Map<K, MapDifference.ValueDifference<V>> differences = newLinkedHashMap();
     doDifference(left, right, valueEquivalence, onlyOnLeft, onlyOnRight, onBoth, differences);
     return new MapDifferenceImpl<K, V>(onlyOnLeft, onlyOnRight, onBoth, differences);
   }",Buggy,"Fix hash map ordering bug.
-------------
Created by MOE: http://code.google.com/p/moe-java
MOE_MIGRATED_REVID=74779605
",Buggy,"The commit message indicates a fix for a hash map ordering bug. The diff replaces `newHashMap()` and `new HashMap()` with `newLinkedHashMap()` and `new LinkedHashMap()`. This change ensures that the order of elements is preserved, which is crucial for fixing ordering-related bugs. The answer is **Buggy**."
guava,15442.json,61110e175fc90fa2e8d675c8de85dff4ed699b07,"@@ -1,6 +1,6 @@
   public void forEachEntry(ObjIntConsumer<? super E> action) {
     checkNotNull(action);
-    for (int i = 0; i < size(); i++) {
+    for (int i = 0; i < length; i++) {
       action.accept(elementSet.asList().get(i), getCount(i));
     }
   }",Buggy,"Fix embarrassing forEachEntry bug in ImmutableSortedMultiset.

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=151063670
",Buggy,"The commit message indicates a bug fix in `ImmutableSortedMultiset`'s `forEachEntry` method. The diff shows a change in the loop condition from `i < size()` to `i < length`. It is highly likely that `size()` was returning the wrong value, leading to incorrect iteration, while `length` represents the actual number of elements. This confirms the presence of a bug. The answer is **Buggy**."
guava,22117.json,44d99fc5f83f06f577f61cf63569604d3142d96a,"@@ -1,3 +1,5 @@
     final void addInitialException(Set<Throwable> seen) {
-      addCausalChain(seen, trustedGetException());
+      if (!isCancelled()) {
+        addCausalChain(seen, trustedGetException());
+      }
     }",Buggy,"Recognize that the reason for an early completion might be cancellation rather than exception.
This fixes the bug ""com.google.common.util.concurrent.AbstractFuture$Cancellation cannot be cast to com.google.common.util.concurrent.AbstractFuture$Failure""

TODO(cpovirk): Write basher tests at some point.
-------------
Created by MOE: http://code.google.com/p/moe-java
MOE_MIGRATED_REVID=96784416
",Buggy,"The commit message indicates a bug fix related to cancellation and exception handling in `AbstractFuture`. The diff shows that `addCausalChain` is now only called if the future is not cancelled. This prevents a `ClassCastException` when the future is cancelled, as the exception would be a `CancellationException` rather than a `Failure`. The change directly addresses the bug described in the commit message. The answer is **Buggy**."
pmd,6789.json,b86f0aec59762277d67960ed30249f1f8de815c4,"@@ -1,3 +1,12 @@
     public Object visit(ASTConditionalExpression node, Object data) {
-        return node.isTernary() ? sumChildrenComplexities(node, data) + 2 : 1;
+        // bool comp of guard clause + complexity of last two children (= total - 1)
+
+        if (node.isTernary()) {
+            ASTExpression wrapper = new ASTExpression(Integer.MAX_VALUE);
+            wrapper.jjtAddChild(node.jjtGetChild(0), 0);
+            int boolCompTernary = CycloMetric.booleanExpressionComplexity(wrapper);
+
+            return boolCompTernary + sumChildrenComplexities(node, data) - 1;
+        }
+        return 1;
     }",Buggy,"Fix npath bugs with ternary
",Buggy,"The commit message indicates a fix for npath bugs related to ternary operators. The code diff modifies the visit method for ASTConditionalExpression nodes, specifically when the node represents a ternary operator. The original code simply added 2 to the sum of children's complexities. The revised code introduces a more sophisticated calculation that involves:

1.  Creating a wrapper ASTExpression around the first child of the ternary node (the condition).
2.  Calculating the boolean expression complexity of this wrapper using CycloMetric.booleanExpressionComplexity.
3.  Adding this boolean complexity to the sum of the children's complexities and then subtracting 1.

This change suggests that the original calculation was not accurately capturing the complexity introduced by the boolean condition in the ternary operator, leading to a bug in the npath calculation. The new code attempts to correct this by explicitly calculating and incorporating the boolean expression complexity. The answer is **Buggy**.
"
pmd,7930.json,be11288e2e0855a52298fdedb8363e66684b2252,"@@ -1 +1 @@
-    public Object visit(ASTIfStatement node, Object data){openScope(node);return data;}

+    public Object visit(ASTSwitchStatement node, Object data){openScope(node);return data;}",Buggy,"Fixed bug in symbol table; it wasn't creating a scope level when it hit a switch statement


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@1388 51baf565-9d33-0410-a72c-fc3788e3496d
",Buggy,"The commit message indicates a bug fix in the symbol table related to scope creation within switch statements. The diff shows that the `visit` method for `ASTSwitchStatement` now calls `openScope(node)`, which was previously missing. This aligns with the commit message, suggesting that the absence of scope creation for switch statements was indeed a bug. The answer is **Buggy**."
pmd,1467.json,5c049b6946c8a2fddc780312e6783a539935feaa,"@@ -1,38 +1,42 @@
     public void apply( List acus, RuleContext ctx ) {
 		visitAll( acus, ctx );
 
 		double deviation = 0.0;
 		double minimum = 0.0;
 		
 		if (hasProperty(""sigma"")) {
 			deviation = getStdDev();
 			double sigma = getDoubleProperty(""sigma"");
 			
 			minimum = getMean() + (sigma * deviation);
 		}
 	
 		if (hasProperty(""minimum"")) {
 			double mMin = getDoubleProperty(""minimum"");
 			if (mMin > minimum) {
                 minimum = mMin;
             }
 		} 
 
 		SortedSet newPoints = applyMinimumValue(dataPoints, minimum);
 			
 		if (hasProperty(""topscore"")) {
 			int topScore = getIntProperty(""topscore"");
 			if (newPoints.size() >= topScore) {
 		    	newPoints = 
 		    		applyTopScore(newPoints, topScore);
 			}
 		}
 		
 		makeViolations(ctx, newPoints);
-		
-		double low = ((DataPoint) dataPoints.first()).getScore();
-		double high = ((DataPoint) dataPoints.last()).getScore();
-	
+
+        double low = 0.0d;
+        double high = 0.0d;
+        if (!dataPoints.isEmpty()) {
+            low = ((DataPoint) dataPoints.first()).getScore();
+            high = ((DataPoint) dataPoints.last()).getScore();
+        }
+
 		ctx.getReport().addMetric( new Metric( this.getName(), low, high,
 		                                       getMean(), getStdDev()));
     }",Buggy,"fixed bug which caused Metrics stuff to fail on interfaces - it was calling first() on an empty SortedSet


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@994 51baf565-9d33-0410-a72c-fc3788e3496d
",Buggy,"The commit message indicates a bug fix related to ""Metrics stuff"" failing on interfaces due to calling `first()` on an empty `SortedSet`. The diff shows a change where the code now checks if `dataPoints` is empty before calling `first()` and `last()` on it. If it's empty, `low` and `high` are set to 0.0d. This prevents the `NoSuchElementException` that would occur when calling `first()` on an empty `SortedSet`. The answer is **Buggy**."
pmd,658.json,05948ab5d813a0fe683625ee6da797360f1c8bd8,"@@ -1,4 +1,4 @@
     private String createTimestampAttr() {
-        SimpleDateFormat sdf = new SimpleDateFormat(""yyyy-MM-dd'T'HH:mm:ss.SSSZ"");
+        SimpleDateFormat sdf = new SimpleDateFormat(""yyyy-MM-dd'T'HH:mm:ss.SSS"");
         return "" timestamp=\"""" + sdf.format(new Date()) + ""\"""";
     }",Buggy,"Fixed bug 1183032 - The XMLRenderer no longer throws a SimpleDateFormat exception when run with JDK 1.3.


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@3433 51baf565-9d33-0410-a72c-fc3788e3496d
",Buggy,"The commit message indicates a bug fix related to `SimpleDateFormat` in JDK 1.3. The diff modifies the `SimpleDateFormat` pattern by removing the `Z` (timezone) specifier. This suggests that the original pattern was causing an exception in JDK 1.3, and removing the timezone specifier resolves the issue. Therefore, the change addresses a bug. The answer is **Buggy**."
pmd,658.json,90eb57ade9a06904052833ea92076cc787b68615,"@@ -1,4 +1,4 @@
     private String createTimestampAttr() {
-        SimpleDateFormat sdf = new SimpleDateFormat(""yyyy-MM-dd'T'hh:mm:ss.SSSZ"");
+        SimpleDateFormat sdf = new SimpleDateFormat(""yyyy-MM-dd'T'HH:mm:ss.SSSZ"");
         return "" timestamp=\"""" + sdf.format(new Date()) + ""\"""";
     }",Buggy,"Fixed [ pmd-Bugs-1100196 ] timestamp attribute in xml should use 24h clock


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@3126 51baf565-9d33-0410-a72c-fc3788e3496d
",Buggy,"The commit message indicates a fix for a bug related to the timestamp format in XML output, specifically addressing the use of a 24-hour clock. The diff shows a change in the `SimpleDateFormat` pattern from `""yyyy-MM-dd'T'hh:mm:ss.SSSZ""` to `""yyyy-MM-dd'T'HH:mm:ss.SSSZ""`. The lowercase `hh` represents a 12-hour clock, while the uppercase `HH` represents a 24-hour clock. This change directly addresses the bug reported in the commit message. The answer is **Buggy**."
pmd,2488.json,051136af5fe667edd815597c68bab215b4f49001,"@@ -1,7 +1,7 @@
     private boolean isLegalPath(String path, LanguageConfig config) {
     	String[] extensions = config.extensions();
     	for (int i=0; i<extensions.length; i++) {
-    		if (path.endsWith(extensions[i])) return true;
+    		if (path.endsWith(extensions[i]) && extensions[i].length() > 0) return true;
     	}
     	return false;
     }",Buggy,"Fixed bug 1593292 - The CPD GUI now works with the 'by extension' option selected.


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@4833 51baf565-9d33-0410-a72c-fc3788e3496d
",Buggy,"The commit message indicates a bug fix in the CPD GUI related to the 'by extension' option. The code change adds a check `extensions[i].length() > 0` within the `isLegalPath` method. This suggests that the bug was likely caused by an empty string being present in the `extensions` array, which would incorrectly match any path when using the `endsWith` method. The added check prevents this incorrect matching, thus fixing the bug. The answer is **Buggy**."
pmd,6071.json,aeffeaec3d7763ca46f208be9f93ee2036ce8003,"@@ -1,8 +1,8 @@
-  public Object childrenAccept(JspParserVisitor visitor, Object data) {
-    if (children != null) {
-      for (int i = 0; i < children.length; ++i) {
-        children[i].jjtAccept(visitor, data);
-      }
-    }
-    return data;
-  }

+    public Object childrenAccept(JspParserVisitor visitor, Object data) {
+        if (children != null) {
+            for (int i = 0; i < children.length; ++i) {
+                ((SimpleNode)children[i]).jjtAccept(visitor, data);
+            }
+        }
+        return data;
+    }",Buggy,"Fixed compilation problems, JSP tests are not yet working, but all other tests run


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@4208 51baf565-9d33-0410-a72c-fc3788e3496d
",Buggy,"The commit message indicates that compilation problems have been fixed. The code change involves casting `children[i]` to `((SimpleNode)children[i])` before calling `jjtAccept`. This suggests that the original code was causing a compilation error because `children[i]` was not guaranteed to be a `SimpleNode`, and `jjtAccept` was not a method on the original type. This is a bug fix. The answer is **Buggy**."
pmd,7746.json,109f458dbf0d6b8c64d77943f9523d98700162d9,"@@ -1,7 +1,10 @@
     public boolean isInterfaceMember() {
-        ASTClassOrInterfaceBody body = getFirstParentOfType(ASTClassOrInterfaceBody.class);
-        if (body != null && body.jjtGetParent() instanceof ASTClassOrInterfaceDeclaration) {
-            return ((ASTClassOrInterfaceDeclaration) body.jjtGetParent()).isInterface();
+        // for a real class/interface the 3rd parent is a ClassOrInterfaceDeclaration,
+        // for anonymous classes, the parent is e.g. a AllocationExpression
+        Node potentialTypeDeclaration = getNthParent(3);
+
+        if (potentialTypeDeclaration instanceof ASTClassOrInterfaceDeclaration) {
+            return ((ASTClassOrInterfaceDeclaration) potentialTypeDeclaration).isInterface();
         }
         return false;
     }",Buggy,"Fixes #793 [java] Parser error with private method in nested classes in interfaces

*   Remember old state to allow nesting
*   Fix ASTMethodDeclaration.isInterfaceMember
*   Extended tests
",Buggy,"The commit message indicates a fix for a parser error related to private methods in nested classes within interfaces. The code diff modifies the `isInterfaceMember` method to correctly identify interface members, particularly in the context of nested or anonymous classes. The original implementation only checked the immediate parent, which was insufficient for nested scenarios. The updated code traverses up the parent hierarchy to accurately determine if the method belongs to an interface. This change directly addresses the reported parser error, indicating a bug fix. The answer is **Buggy**.
"
pmd,7746.json,64b862eef965aaa39c17db1808063e2f129d7057,"@@ -1,4 +1,7 @@
     public boolean isInterfaceMember() {
-        ASTClassOrInterfaceDeclaration clz = getFirstParentOfType(ASTClassOrInterfaceDeclaration.class);
-        return clz != null && clz.isInterface();
+        ASTClassOrInterfaceBody body = getFirstParentOfType(ASTClassOrInterfaceBody.class);
+        if (body != null && body.jjtGetParent() instanceof ASTClassOrInterfaceDeclaration) {
+            return ((ASTClassOrInterfaceDeclaration) body.jjtGetParent()).isInterface();
+        }
+        return false;
     }",Buggy,"Fixes #793 [java] Parser error with private method in nested classes in interfaces
",Buggy,"The commit message indicates a fix for a parser error related to private methods in nested classes within interfaces. The diff modifies the `isInterfaceMember` method. The original code directly retrieved the `ASTClassOrInterfaceDeclaration` and checked if it was an interface. The corrected code now retrieves the `ASTClassOrInterfaceBody` first, then checks if its parent is an `ASTClassOrInterfaceDeclaration` and if that declaration is an interface. This change suggests that the original logic was flawed in handling nested classes within interfaces, leading to incorrect parsing or identification of interface members. The updated code introduces a more robust check by considering the class or interface body and its parent declaration, which fixes the reported parser error. The answer is **Buggy**.
"
pmd,7746.json,078ec6e2c8fdee2c51c2fb6a9bf6c6220d2a7032,"@@ -1,3 +1,4 @@
     public boolean isInterfaceMember() {
-        return ((ASTClassOrInterfaceDeclaration)getFirstParentOfType(ASTClassOrInterfaceDeclaration.class)).isInterface();
+        ASTClassOrInterfaceDeclaration clz = (ASTClassOrInterfaceDeclaration)getFirstParentOfType(ASTClassOrInterfaceDeclaration.class);
+        return clz != null && clz.isInterface();
     }",Buggy,"Fixed bug 1400754 - A NPE is no longer thrown on certain JDK 1.5 enum usages.


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@4127 51baf565-9d33-0410-a72c-fc3788e3496d
",Buggy,"The commit message indicates a fix for a NullPointerException (NPE) that occurred with certain JDK 1.5 enum usages. The code change adds a null check for `clz` before accessing its `isInterface()` method. This prevents the NPE if `getFirstParentOfType` returns null, which could happen in specific scenarios with enums. The answer is **Buggy**."
pmd,7774.json,0f2e98bf9112c38ee071489286a8d827ae02a460,"@@ -1,3 +1,3 @@
     public String getPackageNameImage() {
-        return ((ASTName)jjtGetChild(0)).getImage();
+	return ((ASTName) jjtGetChild(this.jjtGetNumChildren() - 1)).getImage();
     }",Buggy,"bug fix: typecast exception in LoosePackageCoupling for annotation before package name


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@6077 51baf565-9d33-0410-a72c-fc3788e3496d
",Buggy,"The commit message indicates a bug fix related to a typecast exception in `LoosePackageCoupling` when an annotation precedes the package name. The code change modifies how the package name is retrieved from the AST (Abstract Syntax Tree). Instead of assuming the package name is always the first child (`jjtGetChild(0)`), the code now retrieves the last child (`jjtGetChild(this.jjtGetNumChildren() - 1)`). This suggests that the original assumption was incorrect, and annotations were causing the package name to be located at a different position in the AST. This correction addresses the typecast exception by correctly identifying the package name node in the AST, thus fixing the bug. The answer is **Buggy**.
"
pmd,4667.json,64e193fcb69b2fbd3ccdeeccecae0b45f02ff055,"@@ -1,11 +1,21 @@
     public Object visit(ASTUserClass node, Object data) {
         if (Helper.isTestMethodOrClass(node)) {
             return data;
         }
 
+        List<ASTVariableDeclaration> variableDecls = node.findDescendantsOfType(ASTVariableDeclaration.class);
+        for (ASTVariableDeclaration varDecl : variableDecls) {
+            findSafeLiterals(varDecl);
+        }
+
+        List<ASTFieldDeclaration> fieldDecl = node.findDescendantsOfType(ASTFieldDeclaration.class);
+        for (ASTFieldDeclaration fDecl : fieldDecl) {
+            findSafeLiterals(fDecl);
+        }
+
         List<ASTNewObjectExpression> newObjects = node.findDescendantsOfType(ASTNewObjectExpression.class);
         for (ASTNewObjectExpression newObj : newObjects) {
             checkNewObjects(newObj, data);
         }
         return data;
     }",Buggy,"Bug fix - contd
",Buggy,"The commit message indicates a bug fix. The code changes involve iterating through variable declarations and field declarations within an ASTUserClass node to find safe literals. This suggests that the previous implementation might have missed some declarations, leading to incorrect analysis or behavior. The addition of loops to process variable and field declarations implies a correction of a previous oversight. The answer is **Buggy**."
pmd,1144.json,5ca779d9e255f093175f8b738e3feca1c9758742,"@@ -1,11 +1,13 @@
     public Node getNthParent(int n) {
-	Node result = null;
-	for (int i = 0; i < n; i++) {
-	    if (result == null) {
-		result = this.jjtGetParent();
-	    } else {
-		result = result.jjtGetParent();
-	    }
-	}
-	return result;
+        if (n <= 0) {
+            throw new IllegalArgumentException();
+        }
+        Node result = this.jjtGetParent();
+        for (int i = 1; i < n; i++) {
+            if (result == null) {
+                return null;
+            }
+            result = result.jjtGetParent();
+        }
+        return result;
     }",Buggy,"bug fix: getNthParent() in AbstractNode was looping if argument was higher than ancestor number

git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@6238 51baf565-9d33-0410-a72c-fc3788e3496d
",Buggy,"The commit message indicates a bug fix in the `getNthParent()` method of the `AbstractNode` class. The bug occurred when the argument `n` was higher than the number of ancestors, causing a loop.

The diff shows the corrected implementation of `getNthParent(int n)`. The original code had a loop that would continue even if the parent node became null, potentially leading to an infinite loop or unexpected behavior. The corrected code includes a check within the loop to see if the `result` (current parent) is null. If it is, the method returns null, preventing the loop from continuing indefinitely. Additionally, it now throws an IllegalArgumentException if n <= 0.

The changes align with the commit message, indicating a bug fix related to handling cases where the requested parent level exceeds the available ancestors. The addition of the null check and the IllegalArgumentException are crucial for preventing the reported looping issue.

The answer is **Buggy**.
"
pmd,4676.json,3079ce26a85a9ed32d86bd039606c87decf23c82,"@@ -1,6 +1,6 @@
     private void checkForSharingDeclaration(ApexNode<?> node, Object data, boolean sharingFound) {
-        final boolean foundAnyDMLorSOQL = Helper.foundAnyDML(node) && Helper.foundAnySOQLorSOSL(node);
+        final boolean foundAnyDMLorSOQL = Helper.foundAnyDML(node) || Helper.foundAnySOQLorSOSL(node);
         if (!sharingFound && !Helper.isTestMethodOrClass(node) && foundAnyDMLorSOQL) {
             addViolation(data, node);
         }
     }",Buggy,"Bug fix
",Buggy,"The commit message indicates a bug fix. The code change modifies a boolean expression from `&&` to `||`. This suggests that the original condition was too restrictive, preventing the detection of violations when either DML or SOQL/SOSL operations were present. The corrected code now flags violations if either DML or SOQL/SOSL is found, which aligns with the intention of identifying potential sharing issues. The answer is **Buggy**."
pmd,31.json,18ea8600ece06f02ae605f9fb1c19c07a4e02c6a,"@@ -1,7 +1,7 @@
     public static boolean applies(Rule rule, LanguageVersion languageVersion) {
         final LanguageVersion min = rule.getMinimumLanguageVersion();
-        final LanguageVersion max = rule.getMinimumLanguageVersion();
+        final LanguageVersion max = rule.getMaximumLanguageVersion();
         return rule.getLanguage().equals(languageVersion.getLanguage())
                 && (min == null || min.compareTo(languageVersion) <= 0)
                 && (max == null || max.compareTo(languageVersion) >= 0);
     }",Buggy,"Fix problem that some rules where not executed - max language version was
determined wrongly
",Buggy,"The commit message indicates a fix where some rules were not being executed due to an incorrect determination of the maximum language version. The diff shows that `rule.getMinimumLanguageVersion()` was incorrectly used to determine the maximum language version, and it has been corrected to `rule.getMaximumLanguageVersion()`. This change directly addresses the problem described in the commit message, indicating a bug fix. The answer is **Buggy**."
pmd,2559.json,b745f331b8ca490e0046b566b5a94a2eb10aca84,"@@ -1,19 +1,17 @@
     public void tokenize(SourceCode sourceCode, Tokens tokenEntries) {
         StringBuilder buffer = sourceCode.getCodeBuffer();
         try (Reader reader = new StringReader(buffer.toString())) {
-            final TokenFilter tokenFilter = new JavaCCTokenFilter(new ObjectiveCTokenManager(reader));
+            ObjectiveCTokenManager tokenManager = new ObjectiveCTokenManager(reader);
+            tokenManager.setFileName(sourceCode.getFileName());
+            final TokenFilter tokenFilter = new JavaCCTokenFilter(tokenManager);
             Token currentToken = (Token) tokenFilter.getNextToken();
             while (currentToken != null) {
                 tokenEntries.add(new TokenEntry(currentToken.image, sourceCode.getFileName(), currentToken.beginLine));
                 currentToken = (Token) tokenFilter.getNextToken();
             }
-            tokenEntries.add(TokenEntry.getEOF());
-            System.err.println(""Added "" + sourceCode.getFileName());
-        } catch (TokenMgrError err) {
-            err.printStackTrace();
-            System.err.println(""Skipping "" + sourceCode.getFileName() + "" due to parse error"");
-            tokenEntries.add(TokenEntry.getEOF());
         } catch (IOException e) {
             e.printStackTrace();
+        } finally {
+            tokenEntries.add(TokenEntry.getEOF());
         }
     }",Buggy,"CPD: Fix error handling for lexical errors

* TokenMgrError must not be caught by the tokenizer. This is handled
  by CPD itself
* The token managers need to know the filename for proper error messages
",Buggy,"The commit message indicates fixes related to error handling during the tokenization process in CPD (Copy/Paste Detector). Specifically, it addresses how lexical errors (TokenMgrError) are handled and ensures that the token manager has access to the filename for better error messages.

The diff shows the following changes:

1.  **Removal of `TokenMgrError` handling:** The `catch (TokenMgrError err)` block is removed. This aligns with the commit message stating that `TokenMgrError` should not be caught by the tokenizer but handled by CPD itself.
2.  **Setting filename for the token manager:** The code now sets the filename for the `ObjectiveCTokenManager` using `tokenManager.setFileName(sourceCode.getFileName());`. This addresses the commit message's point about token managers needing the filename for proper error messages.
3.  **EOF token addition in `finally` block:** The `tokenEntries.add(TokenEntry.getEOF());` is moved to a `finally` block. This ensures that the EOF token is always added, even if exceptions occur during tokenization.

These changes directly address the error handling issues described in the commit message. The removal of the `TokenMgrError` catch block and the addition of filename information to the token manager are clear indications of bug fixes related to error reporting and handling. Moving the EOF token addition to the `finally` block ensures proper termination of the token stream even in error scenarios.

Therefore, the changes indicate a bug fix.

**Buggy**
"
pmd,2220.json,9075cb005c37782d5248485d8628d90dff4a238f,"@@ -1,10 +1,14 @@
     public List<V> valueFrom(String valueString) throws IllegalArgumentException {
+        if (StringUtil.isEmpty(valueString)) {
+            return Collections.emptyList();
+        }
+
         String[] strValues = valueString.split(Pattern.quote("""" + multiValueDelimiter()));
 
         List<V> values = new ArrayList<>(strValues.length);
-        for (int i = 0; i < strValues.length; i++) {
-            values.add(createFrom(strValues[i]));
+        for (String strValue : strValues) {
+            values.add(createFrom(strValue));
         }
 
         return values;
     }",Buggy,"Fix bug with empty value strings
",Buggy,"The commit message indicates a fix related to empty value strings. The diff adds a check at the beginning of the `valueFrom` method to handle empty input strings. If the input `valueString` is empty, the method now returns an empty list, preventing potential errors or unexpected behavior in subsequent processing. This clearly addresses a bug where empty input strings were not handled correctly. The answer is **Buggy**."
pmd,4201.json,8ffaffc567168576ecfc1176fbfab03b5f9896b0,"@@ -1,20 +1,20 @@
 	void calculateLineNumbers(SourceCodePositioner positioner) {
 		if (!hasRealLoc()) {
 			return;
 		}
 
 		RealLoc loc = (RealLoc) node.getLoc();
 		int startOffset = loc.startIndex;
 		int endOffset = loc.endIndex;
+		// end column will be interpreted as inclusive, while endOffset/endIndex is exclusive
+		endOffset -= 1;
 
 		this.beginLine = positioner.lineNumberFromOffset(startOffset);
 		this.beginColumn = positioner.columnFromOffset(this.beginLine, startOffset);
 		this.endLine = positioner.lineNumberFromOffset(endOffset);
-		this.endColumn = positioner.columnFromOffset(this.endLine, endOffset) - 1; // end
-																					// column
-																					// is
-																					// inclusive
+		this.endColumn = positioner.columnFromOffset(this.endLine, endOffset);
+
 		if (this.endColumn < 0) {
 			this.endColumn = 0;
 		}
 	}",Buggy,"Fixes #1485 Analysis of some apex classes cause a stackoverflow error
Test file must use windows line endings
Fixes Up2Go/pmd#36
",Buggy,"The commit message indicates a fix for a stack overflow error caused by analyzing certain Apex classes. The diff modifies the `calculateLineNumbers` method, specifically adjusting the `endOffset` and `endColumn` calculations. The `endOffset` is decremented before being used to determine the `endLine` and `endColumn`. Also, the endColumn calculation is simplified, and a check is added to ensure `endColumn` is not negative. The original code had an inclusive interpretation of the end column, while the offset was exclusive. The change ensures the end column is correctly calculated and avoids potential issues with negative column values, which could contribute to the stack overflow. The answer is **Buggy**."
pmd,7241.json,733c871b9690e787c9137aceb34f4338e2617533,"@@ -1,3 +1,3 @@
     public boolean isAnonymousClass() {
-        return jjtGetParent().hasDescendantOfType(ASTClassOrInterfaceBody.class);
+        return jjtGetParent().getFirstChildOfType(ASTClassOrInterfaceBody.class) != null;
     }",Buggy,"Fix nested anonymous class bug with type resolution
",Buggy,"The commit message indicates a bug fix related to nested anonymous classes and type resolution. The code change replaces `hasDescendantOfType` with `getFirstChildOfType` to check for the existence of `ASTClassOrInterfaceBody`. The original code might have incorrectly identified a class as anonymous if any descendant was of the specified type, rather than only direct children. The change to `getFirstChildOfType` restricts the search to direct children, which is more accurate for determining if a class is anonymous. This correction addresses a logical error in the original implementation. The answer is **Buggy**.
"
hbase,30032.json,b635414e8337be7bc14ca6ae605749c35569b4f7,"@@ -1,31 +1,36 @@
   private Result regroupResults(final Result result) throws IOException {
     partialResultsRow = result.getRow();
     partialResults.add(result);
     partialResultsCellSizes += result.size();
     if (scan.getBatch() > 0 && partialResultsCellSizes >= scan.getBatch()) {
       Cell[] cells = new Cell[scan.getBatch()];
       int count = 0;
       boolean stale = false;
       while (count < scan.getBatch()) {
         Result res = partialResults.poll();
         stale = stale || res.isStale();
         if (res.size() + count <= scan.getBatch()) {
           System.arraycopy(res.rawCells(), 0, cells, count, res.size());
           count += res.size();
         } else {
           int len = scan.getBatch() - count;
           System.arraycopy(res.rawCells(), 0, cells, count, len);
           Cell[] remainingCells = new Cell[res.size() - len];
           System.arraycopy(res.rawCells(), len, remainingCells, 0, res.size() - len);
           Result remainingRes = Result.create(remainingCells, res.getExists(), res.isStale(),
               res.mayHaveMoreCellsInRow());
           partialResults.addFirst(remainingRes);
           count = scan.getBatch();
         }
       }
       partialResultsCellSizes -= scan.getBatch();
+      if (partialResultsCellSizes == 0) {
+        // We have nothing in partialResults, clear the flags to prevent returning empty Result
+        // when next result belongs to the next row.
+        clearPartialResults();
+      }
       return Result.create(cells, null, stale,
           partialResultsCellSizes > 0 || result.mayHaveMoreCellsInRow());
     }
     return null;
   }",Buggy,"HBASE-15484 Correct the semantic of batch and partial - amend to fix bug and revise the JavaDoc for related APIs.
",Buggy,"The commit message indicates a bug fix related to the semantic of batch and partial results. The code changes involve regrouping results based on the scan's batch size. The added code block `if (partialResultsCellSizes == 0) { clearPartialResults(); }` suggests a scenario where partial results are not handled correctly, potentially leading to incorrect or empty results being returned. This addition aims to clear the partial results flags when all partial results have been processed, preventing the return of empty results when the next result belongs to a different row. This clearly indicates a bug fix. The answer is **Buggy**.
"
hbase,10174.json,537a3caccd22e069e8b026a4ba7da419fdb68324,"@@ -1,53 +1,53 @@
   static Status splitLog(String name, CancelableProgressable p, Configuration conf,
       RegionServerServices server, LastSequenceId sequenceIdChecker, WALFactory factory) {
     Path walDir;
     FileSystem fs;
     try {
       walDir = CommonFSUtils.getWALRootDir(conf);
       fs = walDir.getFileSystem(conf);
     } catch (IOException e) {
       LOG.warn(""Resigning, could not find root dir or fs"", e);
       return Status.RESIGNED;
     }
     try {
       if (!processSyncReplicationWAL(name, conf, server, fs, walDir)) {
         return Status.DONE;
       }
     } catch (IOException e) {
       LOG.warn(""failed to process sync replication wal {}"", name, e);
       return Status.RESIGNED;
     }
     // TODO have to correctly figure out when log splitting has been
     // interrupted or has encountered a transient error and when it has
     // encountered a bad non-retry-able persistent error.
     try {
       SplitLogWorkerCoordination splitLogWorkerCoordination =
           server.getCoordinatedStateManager() == null ? null
               : server.getCoordinatedStateManager().getSplitLogWorkerCoordination();
       if (!WALSplitter.splitLogFile(walDir, fs.getFileStatus(new Path(walDir, name)), fs, conf, p,
         sequenceIdChecker, splitLogWorkerCoordination, factory, server)) {
         return Status.PREEMPTED;
       }
     } catch (InterruptedIOException iioe) {
-      LOG.warn(""Resigning, interrupted splitting WAL {}"", filename, iioe);
+      LOG.warn(""Resigning, interrupted splitting WAL {}"", name, iioe);
       return Status.RESIGNED;
     } catch (IOException e) {
       if (e instanceof FileNotFoundException) {
         // A wal file may not exist anymore. Nothing can be recovered so move on
-        LOG.warn(""Done, WAL {} does not exist anymore"", filename, e);
+        LOG.warn(""Done, WAL {} does not exist anymore"", name, e);
         return Status.DONE;
       }
       Throwable cause = e.getCause();
       if (e instanceof RetriesExhaustedException && (cause instanceof NotServingRegionException
           || cause instanceof ConnectException || cause instanceof SocketTimeoutException)) {
-        LOG.warn(""Resigning, can't connect to target regionserver splitting WAL {}"", filename, e);
+        LOG.warn(""Resigning, can't connect to target regionserver splitting WAL {}"", name, e);
         return Status.RESIGNED;
       } else if (cause instanceof InterruptedException) {
-        LOG.warn(""Resigning, interrupted splitting WAL {}"", filename, e);
+        LOG.warn(""Resigning, interrupted splitting WAL {}"", name, e);
         return Status.RESIGNED;
       }
-      LOG.warn(""Error splitting WAL {}"", filename, e);
+      LOG.warn(""Error splitting WAL {}"", name, e);
       return Status.ERR;
     }
     return Status.DONE;
   }",Buggy," HBASE-24574 Procedure V2 - Distributed WAL Splitting => LOGGING (#1912)
 Addendum 2 fix compile error.
",Buggy,"The commit message indicates a fix for a compile error. The diff replaces `filename` with `name` in several logging statements within the `splitLog` method. This suggests that the original code had a compile error due to the use of an undefined variable `filename`. The change corrects this by using the correct variable `name`, which is the input parameter to the method. This resolves the compile error and ensures that the correct WAL name is logged in case of exceptions or interruptions during the splitting process. The answer is **Buggy**."
hbase,36767.json,cb85bf2e0aa48700bf635183302bdb28a36cb635,"@@ -1,21 +1,24 @@
   public void doFilter(ServletRequest req, ServletResponse rsp,
       FilterChain chain) throws IOException, ServletException {
     HttpServletRequest request = (HttpServletRequest)req;
     HttpServletResponse response = (HttpServletResponse)rsp;
     String contentEncoding = request.getHeader(""content-encoding"");
     String acceptEncoding = request.getHeader(""accept-encoding"");
     String contentType = request.getHeader(""content-type"");
     if ((contentEncoding != null) &&
         (contentEncoding.toLowerCase().indexOf(""gzip"") > -1)) {
       request = new GZIPRequestWrapper(request);
     }
     if (((acceptEncoding != null) &&
           (acceptEncoding.toLowerCase().indexOf(""gzip"") > -1)) ||
         ((contentType != null) && mimeTypes.contains(contentType))) {
       response = new GZIPResponseWrapper(response);
     }
     chain.doFilter(request, response);
-    if ((response instanceof GZIPResponseWrapper)) {
-      ((GZIPResponseStream)response.getOutputStream()).finish();
+    if (response instanceof GZIPResponseWrapper) {
+      OutputStream os = response.getOutputStream();
+      if (os instanceof GZIPResponseStream) {
+        ((GZIPResponseStream)os).finish();
+      }
     }
   }",Buggy,"HBASE-3275 [rest] No gzip/deflat content encoding support; fix error handling in GzipFilter

git-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1082792 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for error handling in GzipFilter. The code change adds a check to ensure that the OutputStream obtained from the response is actually a GZIPResponseStream before attempting to call `finish()` on it. This prevents a potential ClassCastException if the OutputStream is not of the expected type, which would be an error handling improvement. The answer is **Buggy**."
hbase,3377.json,65bcf55892efefb72b41fcf6c9974688578b5021,"@@ -1,11 +1,11 @@
   private static <T extends Enum<T>> T tryValueOf(final Class<T> clazz,
     final String value) {
     if (clazz == null || value == null) {
       return null;
     }
     try {
-      return T.valueOf(clazz, value);
+      return Enum.valueOf(clazz, value);
     } catch (IllegalArgumentException e) {
       return null;
     }
   }",Buggy,"HBASE-23653 Expose content of meta table in web ui; addendum (#1061)

Fix error prone problem

Signed-off-by: Nick Dimiduk <ndimiduk@apache.org>
Signed-off-by: Viraj Jasani <vjasani@apache.org>
Signed-off-by: stack <stack@apache.org>",Buggy,"The commit message indicates a fix for an ""error prone problem"". The code change replaces `T.valueOf(clazz, value)` with `Enum.valueOf(clazz, value)`. While both methods achieve the same result, the original code had a compile error because `T` is a generic type and you cannot call a static method on a generic type. The fix uses the `Enum` class directly to call the static method `valueOf`. This resolves the compile error, which is a bug. The answer is **Buggy**."
hbase,8735.json,8c4b09dfbaf53fd770fe3963df6095fc690f2ef5,"@@ -1,33 +1,34 @@
     private void _readMvccVersion(int offsetFromPos) {
       // This is Bytes#bytesToVint inlined so can save a few instructions in this hot method; i.e.
       // previous if one-byte vint, we'd redo the vint call to find int size.
       // Also the method is kept small so can be inlined.
       byte firstByte = blockBuffer.getByteAfterPosition(offsetFromPos);
       int len = WritableUtils.decodeVIntSize(firstByte);
       if (len == 1) {
         this.currMemstoreTS = firstByte;
       } else {
         int remaining = len -1;
         long i = 0;
         offsetFromPos++;
         if (remaining >= Bytes.SIZEOF_INT) {
-          i = blockBuffer.getIntAfterPosition(offsetFromPos);
+          // The int read has to be converted to unsigned long so the & op
+          i = (blockBuffer.getIntAfterPosition(offsetFromPos) & 0x00000000ffffffffL);
           remaining -= Bytes.SIZEOF_INT;
           offsetFromPos += Bytes.SIZEOF_INT;
         }
         if (remaining >= Bytes.SIZEOF_SHORT) {
           short s = blockBuffer.getShortAfterPosition(offsetFromPos);
           i = i << 16;
           i = i | (s & 0xFFFF);
           remaining -= Bytes.SIZEOF_SHORT;
           offsetFromPos += Bytes.SIZEOF_SHORT;
         }
         for (int idx = 0; idx < remaining; idx++) {
           byte b = blockBuffer.getByteAfterPosition(offsetFromPos + idx);
           i = i << 8;
           i = i | (b & 0xFF);
         }
         currMemstoreTS = (WritableUtils.isNegativeVInt(firstByte) ? ~i : i);
       }
       this.currMemstoreTSLen = len;
     }",Buggy,"HBASE-16624 Fix MVCC DeSerialization bug in the HFileScannerImpl

Change-Id: Ia970619ac7369d24ed432e827319dfdca16143c2

Signed-off-by: stack <stack@apache.org>
",Buggy,"The commit message indicates a fix for an MVCC deserialization bug in HFileScannerImpl. The code diff modifies the `_readMvccVersion` method, specifically addressing how the memstore timestamp is read from the block buffer. The key change involves ensuring that the integer read from the buffer is treated as an unsigned long by applying a bitmask (`& 0x00000000ffffffffL`). This suggests that the previous implementation might have incorrectly interpreted the integer as signed, leading to incorrect timestamp values. This correction directly relates to fixing a deserialization bug, as it ensures the correct interpretation of the serialized data. The answer is **Buggy**."
hbase,28257.json,78d532e5f344edda04fb9ce44bef9cd79e0d1935,"@@ -1,14 +1,14 @@
-  public void validatePut(final Put put) throws IllegalArgumentException{
+  public static void validatePut(Put put, int maxKeyValueSize) throws IllegalArgumentException {
     if (put.isEmpty()) {
       throw new IllegalArgumentException(""No columns to insert"");
     }
     if (maxKeyValueSize > 0) {
       for (List<Cell> list : put.getFamilyCellMap().values()) {
         for (Cell cell : list) {
           if (KeyValueUtil.length(cell) > maxKeyValueSize) {
             throw new IllegalArgumentException(""KeyValue size too large"");
           }
         }
       }
     }
   }",Buggy,"HBASE-12086 Fix bug of HTableMultipliexer

Signed-off-by: Elliott Clark <eclark@apache.org>
",Buggy,"The commit message indicates a bug fix in HTableMultiplexer. The diff modifies the `validatePut` method, making it static and adding a `maxKeyValueSize` parameter. The original method was an instance method, and the change introduces a static method that takes a `Put` object and a maximum key-value size as input. The method checks if the put is empty and if any of the key values exceed the maximum allowed size. If either of these conditions is met, an IllegalArgumentException is thrown. This change likely addresses a bug related to validating the size of puts in HTableMultiplexer. The answer is **Buggy**."
hbase,3341.json,05378cbf69957b76e8559185ea20235242a8b2e6,"@@ -1,5 +1,5 @@
       protected void encode(ChannelHandlerContext channelHandlerContext,
                             ClusterStatus clusterStatus, List<Object> objects) {
-        ClusterStatusProtos.ClusterStatus csp = clusterStatus.convert();
+        ClusterStatusProtos.ClusterStatus csp = ProtobufUtil.convert(clusterStatus);
         objects.add(new DatagramPacket(Unpooled.wrappedBuffer(csp.toByteArray()), isa));
       }",Buggy,"HBASE-15609 Addendum fix compilation error
",Buggy,"The commit message indicates a fix for a compilation error. The diff replaces `clusterStatus.convert()` with `ProtobufUtil.convert(clusterStatus)`. This suggests that the original `convert()` method was either removed, renamed, or inaccessible, causing a compilation error. The change resolves this by using `ProtobufUtil.convert()`, which is presumably the correct way to convert the `ClusterStatus` object. This directly addresses a build-breaking issue. The answer is **Buggy**.
"
hbase,1117.json,8c74d177f68bbd5412cef96dc33f16ba33ff7875,"@@ -1,43 +1,43 @@
   public void refreshSources(String peerId) throws IOException {
     String terminateMessage = ""Peer "" + peerId +
       "" state or config changed. Will close the previous replication source and open a new one"";
     ReplicationPeer peer = replicationPeers.getPeer(peerId);
     ReplicationSourceInterface src = createSource(peerId, peer);
     // synchronized on latestPaths to avoid missing the new log
     synchronized (this.latestPaths) {
       ReplicationSourceInterface toRemove = this.sources.put(peerId, src);
       if (toRemove != null) {
         LOG.info(""Terminate replication source for "" + toRemove.getPeerId());
         toRemove.terminate(terminateMessage);
       }
       for (SortedSet<String> walsByGroup : walsById.get(peerId).values()) {
         walsByGroup.forEach(wal -> src.enqueueLog(new Path(this.logDir, wal)));
       }
     }
     LOG.info(""Startup replication source for "" + src.getPeerId());
     src.startup();
 
     List<ReplicationSourceInterface> toStartup = new ArrayList<>();
     // synchronized on oldsources to avoid race with NodeFailoverWorker
     synchronized (this.oldsources) {
       List<String> previousQueueIds = new ArrayList<>();
       for (ReplicationSourceInterface oldSource : this.oldsources) {
         if (oldSource.getPeerId().equals(peerId)) {
           previousQueueIds.add(oldSource.getQueueId());
           oldSource.terminate(terminateMessage);
           this.oldsources.remove(oldSource);
         }
       }
       for (String queueId : previousQueueIds) {
         ReplicationSourceInterface replicationSource = createSource(queueId, peer);
         this.oldsources.add(replicationSource);
         for (SortedSet<String> walsByGroup : walsByIdRecoveredQueues.get(queueId).values()) {
           walsByGroup.forEach(wal -> src.enqueueLog(new Path(wal)));
         }
         toStartup.add(replicationSource);
       }
     }
-    for (ReplicationSourceInterface replicationSource : oldsources) {
+    for (ReplicationSourceInterface replicationSource : toStartup) {
       replicationSource.startup();
     }
   }",Buggy,"HBASE-20082 Fix findbugs errors only on master which are introduced by HBASE-19397
",Buggy,"The commit message indicates a fix for FindBugs errors introduced by a previous change. The diff modifies the `refreshSources` method in a way that suggests a correction in how replication sources are handled during refresh. Specifically, the code iterates through `toStartup` instead of `oldsources` to start the replication sources. This suggests that the original code was not starting the newly created replication sources after a refresh, which could lead to replication issues. The change fixes this by ensuring that the new sources are started. The answer is **Buggy**."
hbase,5759.json,741d0a4511b9c397b2eb821aef49e858e217bf1e,"@@ -1,22 +1,22 @@
   public synchronized void moveTables(
       Set<TableName> tableNames, String groupName) throws IOException {
     if (groupName != null && !rsGroupMap.containsKey(groupName)) {
       throw new DoNotRetryIOException(""Group ""+groupName+"" does not exist or is a special group"");
     }
 
     Map<String,RSGroupInfo> newGroupMap = Maps.newHashMap(rsGroupMap);
     for(TableName tableName: tableNames) {
       if (tableMap.containsKey(tableName)) {
-        RSGroupInfo src = new RSGroupInfo(rsGroupMap.get(tableMap.get(tableName)));
+        RSGroupInfo src = new RSGroupInfo(newGroupMap.get(tableMap.get(tableName)));
         src.removeTable(tableName);
         newGroupMap.put(src.getName(), src);
       }
       if(groupName != null) {
         RSGroupInfo dst = new RSGroupInfo(newGroupMap.get(groupName));
         dst.addTable(tableName);
         newGroupMap.put(dst.getName(), dst);
       }
     }
 
     flushConfig(newGroupMap);
   }",Buggy,"HBASE-16430 Fix RegionServer Group's bug when moving multiple tables (Guangxu Cheng)
",Buggy,"The commit message indicates a bug fix related to moving multiple tables in RegionServer Groups. The code diff modifies the `moveTables` method. Specifically, it changes `rsGroupMap.get(tableMap.get(tableName))` to `newGroupMap.get(tableMap.get(tableName))` when creating the `src` RSGroupInfo object. This suggests that the original code was using the old group map (`rsGroupMap`) instead of the updated one (`newGroupMap`) within the loop, which could lead to incorrect behavior when moving multiple tables. The change ensures that the most up-to-date group information is used during the table movement process. This is a bug fix. The answer is **Buggy**."
hbase,5759.json,ad16676f9218b5ae3f693f8024e538e1f15e0e9a,"@@ -1,22 +1,22 @@
   public synchronized void moveTables(
       Set<TableName> tableNames, String groupName) throws IOException {
     if (groupName != null && !rsGroupMap.containsKey(groupName)) {
       throw new DoNotRetryIOException(""Group ""+groupName+"" does not exist or is a special group"");
     }
 
     Map<String,RSGroupInfo> newGroupMap = Maps.newHashMap(rsGroupMap);
     for(TableName tableName: tableNames) {
       if (tableMap.containsKey(tableName)) {
-        RSGroupInfo src = new RSGroupInfo(rsGroupMap.get(tableMap.get(tableName)));
+        RSGroupInfo src = new RSGroupInfo(newGroupMap.get(tableMap.get(tableName)));
         src.removeTable(tableName);
         newGroupMap.put(src.getName(), src);
       }
       if(groupName != null) {
         RSGroupInfo dst = new RSGroupInfo(newGroupMap.get(groupName));
         dst.addTable(tableName);
         newGroupMap.put(dst.getName(), dst);
       }
     }
 
     flushConfig(newGroupMap);
   }",Buggy,"Fix RegionServer Group's bug when moving multiple tables (Guangxu Cheng)
",Buggy,"The commit message indicates a bug fix related to moving multiple tables in RegionServer Groups. The code diff modifies the `moveTables` method. Specifically, it changes how the `src` RSGroupInfo object is retrieved. Originally, it was retrieved from the `rsGroupMap`, but it is now retrieved from the `newGroupMap`. This change is crucial because, in the loop, the `newGroupMap` is being updated with the changes. If we still refer to the `rsGroupMap` for subsequent table moves, we might be using outdated information about which group a table belongs to, leading to incorrect moves. Therefore, this change fixes a bug where moving multiple tables might not have been handled correctly. The answer is **Buggy**."
hbase,13091.json,07e93458fe69b7701ea2045926b4afe30d166cb5,"@@ -1,3 +1,4 @@
   public Cell forceCopyOfBigCellInto(Cell cell) {
-    throw new IllegalStateException(""This is an Immutable MemStoreLAB."");
+    MemStoreLAB mslab = this.mslabs.get(0);
+    return mslab.forceCopyOfBigCellInto(cell);
   }",Buggy,"HBASE-19930: Fixing the bug, in the rare case when there is a merge into CCM and the one of the old segments has a big cell allocated on-heap and it needs to be copied to the MSLAB
",Buggy,"The commit message indicates a bug fix related to merging into CCM (likely Concurrent Column Family MemStore) and handling large cells allocated on-heap that need to be copied to the MSLAB (MemStoreLAB). The code change involves the `forceCopyOfBigCellInto` method. Previously, it threw an `IllegalStateException`, indicating that the method was not implemented or not intended to be called in that context. The fix replaces the exception with code that retrieves the first MemStoreLAB from `this.mslabs` and calls the `forceCopyOfBigCellInto` method on that MSLAB. This suggests that the original implementation was incorrect, and the method should have delegated to the underlying MSLAB. The change directly addresses the bug described in the commit message. The answer is **Buggy**."
hbase,34108.json,c222e2b4862045d5ef7040103e1c50b6593dda20,"@@ -1,50 +1,50 @@
     public void map(LongWritable offset, Text value,
       Context context)
     throws IOException {
       byte[] lineBytes = value.getBytes();
 
       try {
         TsvParser.ParsedLine parsed = parser.parse(
             lineBytes, value.getLength());
         ImmutableBytesWritable rowKey =
           new ImmutableBytesWritable(lineBytes,
               parsed.getRowKeyOffset(),
               parsed.getRowKeyLength());
 
         Put put = new Put(rowKey.copyBytes());
         for (int i = 0; i < parsed.getColumnCount(); i++) {
           if (i == parser.getRowKeyColumnIndex()) continue;
           KeyValue kv = new KeyValue(
               lineBytes, parsed.getRowKeyOffset(), parsed.getRowKeyLength(),
               parser.getFamily(i), 0, parser.getFamily(i).length,
               parser.getQualifier(i), 0, parser.getQualifier(i).length,
               ts,
               KeyValue.Type.Put,
               lineBytes, parsed.getColumnOffset(i), parsed.getColumnLength(i));
           put.add(kv);
         }
         context.write(rowKey, put);
       } catch (BadTsvLineException badLine) {
         if (skipBadLines) {
           System.err.println(
               ""Bad line at offset: "" + offset.get() + "":\n"" +
               badLine.getMessage());
           badLineCount.increment(1);
           return;
         } else {
           throw new IOException(badLine);
         }
       } catch (IllegalArgumentException e) {
         if (skipBadLines) {
           System.err.println(
               ""Bad line at offset: "" + offset.get() + "":\n"" +
-              badLine.getMessage());
+              e.getMessage());
           badLineCount.increment(1);
           return;
         } else {
-          throw new IOException(badLine);
+          throw new IOException(e);
         }
       } catch (InterruptedException e) {
         e.printStackTrace();
       }
     }",Buggy,"HBASE-3711 - amend patch to fix compilation error

git-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1086747 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for a compilation error. The diff shows a change in the catch block for `IllegalArgumentException`. The original code was throwing a `IOException` with the `BadTsvLineException` message, even when an `IllegalArgumentException` was caught. This is incorrect, as the exception message should correspond to the actual exception caught. The corrected code now throws an `IOException` with the message from the `IllegalArgumentException`. This correction addresses a bug where the wrong exception message was being propagated. The answer is **Buggy**."
ant,5957.json,79a591966485337a98ec87cfcda01218969b73da,"@@ -1,164 +1,164 @@
     public void execute() throws BuildException {
 
         if ( (sourceFileSets.size() == 0) && (sourceFileLists.size() == 0) ) { 
           throw new BuildException(""At least one <srcfileset> or <srcfilelist> element must be set"");
         }
 
         if ( (targetFileSets.size() == 0) && (targetFileLists.size() == 0) ) {
           throw new BuildException(""At least one <targetfileset> or <targetfilelist> element must be set"");
         }
 
         long now = (new Date()).getTime();
         /*
           If we're on Windows, we have to munge the time up to 2 secs to
           be able to check file modification times.
           (Windows has a max resolution of two secs for modification times)
         */
         if (Os.isFamily(""windows"")) {
             now += 2000;
         }
 
         //
         // Grab all the target files specified via filesets
         //
         Vector  allTargets         = new Vector();
         long oldestTargetTime = 0;
         File oldestTarget = null;
         Enumeration enumTargetSets = targetFileSets.elements();
         while (enumTargetSets.hasMoreElements()) {
                  
            FileSet targetFS          = (FileSet) enumTargetSets.nextElement();
            DirectoryScanner targetDS = targetFS.getDirectoryScanner(project);
            String[] targetFiles      = targetDS.getIncludedFiles();
                  
            for (int i = 0; i < targetFiles.length; i++) {
                     
               File dest = new File(targetFS.getDir(project), targetFiles[i]);
               allTargets.addElement(dest);
 
               if (dest.lastModified() > now) {
                  log(""Warning: ""+targetFiles[i]+"" modified in the future."", 
                      Project.MSG_WARN);
               }
 
               if (oldestTarget == null ||
                   dest.lastModified() < oldestTargetTime) {
                   oldestTargetTime = dest.lastModified();
                   oldestTarget = dest;
               }
            }
         }
 
         //
         // Grab all the target files specified via filelists
         //
         boolean upToDate            = true;
         Enumeration enumTargetLists = targetFileLists.elements();
         while (enumTargetLists.hasMoreElements()) {
                  
            FileList targetFL    = (FileList) enumTargetLists.nextElement();
            String[] targetFiles = targetFL.getFiles(project);
                  
            for (int i = 0; i < targetFiles.length; i++) {
                     
               File dest = new File(targetFL.getDir(project), targetFiles[i]);
               if (!dest.exists()) {
                  log(targetFiles[i]+ "" does not exist."", Project.MSG_VERBOSE);
                  upToDate = false;
                  continue;
               }
               else {
                  allTargets.addElement(dest);
               }
               if (dest.lastModified() > now) {
                  log(""Warning: ""+targetFiles[i]+"" modified in the future."", 
                      Project.MSG_WARN);
               }
+
               if (oldestTarget == null ||
                   dest.lastModified() < oldestTargetTime) {
                   oldestTargetTime = dest.lastModified();
                   oldestTarget = dest;
               }
            }
         }
         if (oldestTarget != null) {
             log(oldestTarget + "" is oldest target file"", Project.MSG_VERBOSE);
         } else { 
             // no target files, then we cannot remove any target files and
             // skip the following tests right away
             upToDate = false;
         }
 
         //
         // Check targets vs source files specified via filelists
         //
         if (upToDate) {
            Enumeration enumSourceLists = sourceFileLists.elements();
            while (upToDate && enumSourceLists.hasMoreElements()) {
           
               FileList sourceFL         = (FileList) enumSourceLists.nextElement();
               String[] sourceFiles      = sourceFL.getFiles(project);
 
-              int i = 0;
-              do {
+              for (int i=0; upToDate && i < sourceFiles.length; i++) {
                  File src = new File(sourceFL.getDir(project), sourceFiles[i]);
 
                  if (src.lastModified() > now) {
                     log(""Warning: ""+sourceFiles[i]+"" modified in the future."", 
                         Project.MSG_WARN);
                  }
 
                  if (!src.exists()) {
                     log(sourceFiles[i]+ "" does not exist."", Project.MSG_VERBOSE);
                     upToDate = false;
                     break;
                  }
 
                  if (src.lastModified() > oldestTargetTime) {
                     upToDate = false;
                     log(oldestTarget + "" is out of date with respect to "" +
                         sourceFiles[i], Project.MSG_VERBOSE);
                  }
-              } while (upToDate && (++i < sourceFiles.length) );
+              }
            }
         }
 
         //
         // Check targets vs source files specified via filesets
         //
         if (upToDate) {
            Enumeration enumSourceSets = sourceFileSets.elements();
            while (upToDate && enumSourceSets.hasMoreElements()) {
           
               FileSet sourceFS          = (FileSet) enumSourceSets.nextElement();
               DirectoryScanner sourceDS = sourceFS.getDirectoryScanner(project);
               String[] sourceFiles      = sourceDS.getIncludedFiles();
 
               for (int i=0; upToDate && i < sourceFiles.length; i++) {
                  File src = new File(sourceFS.getDir(project), sourceFiles[i]);
 
                  if (src.lastModified() > now) {
                     log(""Warning: ""+sourceFiles[i]+"" modified in the future."", 
                         Project.MSG_WARN);
                  }
 
                  if (src.lastModified() > oldestTargetTime) {
                     upToDate = false;
                     log(oldestTarget + "" is out of date with respect to "" +
                         sourceFiles[i], Project.MSG_VERBOSE);
                  }
               }
            }
         }
 
         if (!upToDate) {
            log(""Deleting all target files. "", Project.MSG_VERBOSE);
            for (Enumeration e = allTargets.elements(); e.hasMoreElements(); ) {
               File fileToRemove = (File)e.nextElement();
               log(""Deleting file "" + fileToRemove.getAbsolutePath(), Project.MSG_VERBOSE);
               fileToRemove.delete();
            }
         }
         
 
     } //-- execute",Buggy,"fix boundary checking problem similiar to the one fixed for bug report 4290

Actually this wouldn't throw an ArrayIndexOutOuBounds- but a
BuildException as FileList.getFiles barfs out on empty lists, but that
way it is more consistent.


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@271456 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for a boundary checking problem, similar to a previously fixed bug. The code diff modifies the loop that iterates through source files obtained from `FileList`. The original code used a `do...while` loop with manual incrementing of the index `i`. The loop condition also included `upToDate`. The modified code replaces this with a `for` loop, initializing `i` to 0, continuing as long as `upToDate` is true and `i` is less than the length of `sourceFiles`, and incrementing `i` in each iteration. This change ensures that the loop terminates correctly and avoids potential issues with the loop condition or index incrementing, which could lead to incorrect behavior or exceptions. The commit message and the code changes align, and the changes indicate a bug fix. The answer is **Buggy**."
ant,9265.json,12ae031068c8dc69dc744f1a4060aba054975801,"@@ -1,16 +1,18 @@
     public static IntrospectionHelper getHelper(Project p, Class c) {
-        IntrospectionHelper ih = (IntrospectionHelper) HELPERS.get(c);
-        if (ih == null) {
+        IntrospectionHelper ih = (IntrospectionHelper) HELPERS.get(c.getName());
+        // If a helper cannot be found, or if the helper is for another
+        // classloader, create a new IH
+        if (ih == null || ih.bean != c) {
             ih = new IntrospectionHelper(c);
             if (p != null) {
                 // #30162: do *not* cache this if there is no project, as we
                 // cannot guarantee that the cache will be cleared.
-                HELPERS.put(c, ih);
+                HELPERS.put(c.getName(), ih);
             }
         }
         if (p != null) {
             // Cleanup at end of project
             p.addBuildListener(ih);
         }
         return ih;
     }",Buggy,"Fix for OOME with <*ant*> and <typedef>
Bugzilla report 28283 and 33061

IH had a map of class->IH objects. The
class is the typedefed class and IH is the
attributes, elements etc of that class.
This works fine, except that the class is kept
until the build ends, this means that the classloader
for the class is also kept, a classloader contains
pointers to all the classes loaded by it - so a lot
of memory can be blocked.
When ant, or antcall is used and the called project
typedef the antcontrib, these will be new classloaders,
hence the memory being used up.

The fix is to use the name of the class, check if the IH
in the map is the same class, and if not replace that IH.




git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@465073 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message describes a fix for an OutOfMemoryError (OOME) related to typedefs and classloaders in Ant. The issue arises because `IntrospectionHelper` objects are cached using the class itself as the key, leading to classloader leaks when Ant or Antcall tasks are used with typedefs. The fix involves using the class name as the key instead and checking if the cached `IntrospectionHelper` is for the same class. If not, the cached `IntrospectionHelper` is replaced.

The diff shows that the `HELPERS` map now uses `c.getName()` as the key instead of `c`. It also adds a check `ih.bean != c` to ensure that the cached `IntrospectionHelper` is for the same class. If either the helper is not found or it's for a different class, a new `IntrospectionHelper` is created and cached. This aligns with the commit message's description of the fix.

The changes directly address the memory leak issue described in the commit message, indicating a bug fix. The answer is **Buggy**.
"
ant,9998.json,f40cbc60b34a952432e7abfb70181d0feabd4dd6,"@@ -1,13 +1,13 @@
     public String removeLeadingPath(File leading, File path) {
-        String l = normalize(leading.getAbsolutePath()).getAbsolutePath();
+        // if leading's path ends with a slash, it will be stripped by
+        // normalize - we always add one so we never think /foo was a
+        // parent directory of /foobar
+        String l = normalize(leading.getAbsolutePath()).getAbsolutePath()
+            + File.separator;
         String p = normalize(path.getAbsolutePath()).getAbsolutePath();
         if (p.startsWith(l)) {
-            String result = p.substring(l.length());
-            if (result.startsWith(File.separator)) {
-                result = result.substring(File.separator.length());
-            }
-            return result;
+            return p.substring(l.length());
         } else {
             return p;
         }
     }",Buggy,"Merge over a bug-fix needed to get jakarta-tomcat built by Gump.


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@272855 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix. The code modifies the `removeLeadingPath` method. The change ensures that the `leading` path always ends with a separator before comparing it with the `path`. This prevents incorrect identification of parent directories. The original code could strip a trailing slash from the `leading` path, leading to misidentification. The answer is **Buggy**."
ant,3709.json,d3f03ad754ffdd5d27796dc492ca2db38a7bf444,"@@ -1,95 +1,95 @@
         private void checkIncludePatterns() {
             Hashtable newroots = new Hashtable();
             // put in the newroots vector the include patterns without
             // wildcard tokens
             for (int icounter = 0; icounter < includes.length; icounter++) {
                 String newpattern =
                     SelectorUtils.rtrimWildcardTokens(includes[icounter]);
                 newroots.put(newpattern, includes[icounter]);
             }
             if (remotedir == null) {
                 try {
                     remotedir = ftp.printWorkingDirectory();
                 } catch (IOException e) {
                     throw new BuildException(""could not read current ftp directory"",
                         getLocation());
                 }
             }
             AntFTPFile baseFTPFile = new AntFTPRootFile(ftp, remotedir);
             rootPath = baseFTPFile.getAbsolutePath();
             // construct it
             if (newroots.containsKey("""")) {
                 // we are going to scan everything anyway
-                scandir(remotedir, """", true);
+                scandir(rootPath, """", true);
             } else {
                 // only scan directories that can include matched files or
                 // directories
                 Enumeration enum2 = newroots.keys();
 
                 while (enum2.hasMoreElements()) {
                     String currentelement = (String) enum2.nextElement();
                     String originalpattern = (String) newroots.get(currentelement);
                     AntFTPFile myfile = new AntFTPFile(baseFTPFile, currentelement);
                     boolean isOK = true;
                     boolean traversesSymlinks = false;
                     String path = null;
 
                     if (myfile.exists()) {
                         if (remoteSensitivityChecked
                             && remoteSystemCaseSensitive && isFollowSymlinks()) {
                             // cool case,
                             //we do not need to scan all the subdirs in the relative path
                             path = myfile.getFastRelativePath();
                         } else {
                             // may be on a case insensitive file system.  We want
                             // the results to show what's really on the disk, so
                             // we need to double check.
                             try {
                                 path = myfile.getRelativePath();
                                 traversesSymlinks = myfile.isTraverseSymlinks();
                             }  catch (IOException be) {
                                 throw new BuildException(be, getLocation());
                             } catch (BuildException be) {
                                 isOK = false;
 
                             }
                         }
                     } else {
                         isOK = false;
                     }
                     if (isOK) {
                         currentelement = path.replace(remoteFileSep.charAt(0), File.separatorChar);
                         if (!isFollowSymlinks()
                             && traversesSymlinks) {
                             continue;
                         }
 
                         if (myfile.isDirectory()) {
                             if (isIncluded(currentelement)
                                 && currentelement.length() > 0) {
                                 accountForIncludedDir(currentelement, myfile, true);
                             }  else {
                                 if (currentelement.length() > 0) {
                                     if (currentelement.charAt(currentelement
                                                               .length() - 1)
                                         != File.separatorChar) {
                                         currentelement =
                                             currentelement + File.separatorChar;
                                     }
                                 }
                                 scandir(myfile.getAbsolutePath(), currentelement, true);
                             }
                         } else {
                             if (isCaseSensitive
                                 && originalpattern.equals(currentelement)) {
                                 accountForIncludedFile(currentelement);
                             } else if (!isCaseSensitive
                                        && originalpattern
                                        .equalsIgnoreCase(currentelement)) {
                                 accountForIncludedFile(currentelement);
                             }
                         }
                     }
                 }
             }
         }",Buggy,"Merge from ANT_16_BRANCH
Fix problem with non absolute remote dirs
PR: 23833


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@275510 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message ""Fix problem with non absolute remote dirs"" suggests a bug fix related to how remote directories are handled, specifically when they are not absolute paths. The diff shows a change in the `checkIncludePatterns` method where `scandir` is called with `rootPath` instead of `remotedir`. The original code might have incorrectly scanned relative to the current working directory instead of the intended root path when dealing with non-absolute remote directories. This change ensures that the scan starts from the correct absolute path, addressing the bug described in the commit message. The answer is **Buggy**."
ant,8803.json,bb975e67e154a84b5dd47b5a5b24f853758dc7a5,"@@ -1,15 +1,15 @@
     protected void dieOnCircularReference(Stack<Object> stk, Project p)
         throws BuildException {
         if (isChecked()) {
             return;
         }
         if (isReference()) {
             super.dieOnCircularReference(stk, p);
         } else {
-            if (nested instanceof DataType) {
+            if (nested != null) {
                 pushAndInvokeCircularReferenceCheck((DataType) nested, stk,
                                                     p);
             }
             setChecked(true);
         }
     }",Buggy,"Fix the problem of instanceof test always return true.

This instanceof test will always return true because DataType is the superclass of variable nested's class ResourceComparator. The variable nested is not initialized, it would be better to do a null test rather than an instanceof test.
http://findbugs.sourceforge.net/bugDescriptions.html#BC_VACUOUS_INSTANCEOF
",Buggy,"The commit message indicates a fix for a problem where an `instanceof` test always returns true due to `nested` being a subclass of `DataType`. The message suggests replacing the `instanceof` check with a null check. The diff confirms this change, replacing `nested instanceof DataType` with `nested != null`. This resolves the issue of the always-true `instanceof` check, which is a bug. The answer is **Buggy**.
"
ant,3291.json,3549562783e517c3fbf00a6e914bc48d624847cc,"@@ -1,25 +1,25 @@
     public void execute() throws BuildException {
 
         // first off, make sure that we've got a from and to extension
         if (fromExtension == null || toExtension == null || srcDir == null) {
-            throw new BuildException(""srcDir, destDir, fromExtension and toExtension attributes must be set!"");
+            throw new BuildException(""srcDir, fromExtension and toExtension attributes must be set!"");
         }
 
         // scan source and dest dirs to build up rename list
         DirectoryScanner ds = getDirectoryScanner(srcDir);
 
         String[] files = ds.getIncludedFiles();
 
         Hashtable renameList = scanDir(srcDir, files);
 
         Enumeration e = renameList.keys();
         File fromFile = null;
         File toFile = null;
         while (e.hasMoreElements()) {
             fromFile = (File)e.nextElement();
             toFile = (File)renameList.get(fromFile);
             if (toFile.exists() && replace) toFile.delete();
             if (!fromFile.renameTo(toFile)) throw new BuildException(""Rename from: '"" + fromFile + ""' to '"" + toFile + ""' failed."");
         }
 
     }",Buggy,"fixed comments and error message
Submitted by: dIon Gillard <dion@multitask.com.au>


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@267640 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates fixes to comments and error messages. The diff modifies the error message thrown when required attributes are not set, removing `destDir` from the list of mandatory attributes. This suggests that `destDir` was previously incorrectly considered a required attribute, indicating a bug in the error reporting. The answer is **Buggy**."
ant,1040.json,cdf128015bad588566b90d913fba4b346460d82d,"@@ -1,45 +1,45 @@
     public void execute() throws BuildException {
         Method setProjectM = null;
         try {
             Class c = proxy.getClass();
             setProjectM = 
                 c.getMethod(""setProject"", new Class[] {Project.class});
             if (setProjectM != null) {
                 setProjectM.invoke(proxy, new Object[] {getProject()});
             }
         } catch (NoSuchMethodException e) {
             // ignore this if the class being used as a task does not have
             // a set project method.
         } catch (Exception ex) {
             log(""Error setting project in "" + proxy.getClass(), 
                 Project.MSG_ERR);
             throw new BuildException(ex);
         }
 
 
         Method executeM = null;
         try {
             Class c = proxy.getClass();
             executeM = c.getMethod(""execute"", new Class[0]);
             if (executeM == null) {
                 log(""No public execute() in "" + proxy.getClass(), 
                     Project.MSG_ERR);
                 throw new BuildException(""No public execute() in "" 
                     + proxy.getClass());
             }
             executeM.invoke(proxy, null);
             return; 
         } catch (java.lang.reflect.InvocationTargetException ie) {
-            log(""Error in "" + proxy.getClass(), Project.MSG_ERR);
+            log(""Error in "" + proxy.getClass(), Project.MSG_VERBOSE);
             Throwable t = ie.getTargetException();
             if (t instanceof BuildException) {
                 throw ((BuildException) t);
             } else {
                 throw new BuildException(t);
             }
         } catch (Exception ex) {
-            log(""Error in "" + proxy.getClass(), Project.MSG_ERR);
+            log(""Error in "" + proxy.getClass(), Project.MSG_VERBOSE);
             throw new BuildException(ex);
         }
 
     }",Buggy,"Fix for 20499:
When a proxied task throws a build exception, or other
exception, the taskadapter reports this at
error level. This is incorrect as the
intent of taskadapter is to transparently adapt
a task, the exception should be reported
at verbose level.


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@274649 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for issue 20499, where exceptions thrown by proxied tasks were incorrectly reported at the error level. The diff modifies the logging level from `Project.MSG_ERR` to `Project.MSG_VERBOSE` in the `catch` blocks that handle `InvocationTargetException` and other `Exception` types. This change aligns with the commit message, indicating that the fix involves reducing the severity of the logged message when a proxied task throws an exception. The answer is **Buggy**."
ant,6876.json,320e41bc89d8ac60180cbea1c4110dea68cebc33,"@@ -1,3 +1,9 @@
-    public void setTag(String p) {
-	this.tag = p;
-    }

+    public void setTag(String p) { 
+        // Check if not real tag => set it to null 
+        if (p != null) { 
+            if (p.trim().equals("""")) 
+                p = null; 
+        } 
+
+        this.tag = p; 
+    }",Buggy,"Fixed problem when an empty tag was supplied.

Submitted by:	Jean-Noel Gadreau <jngadreau@activcard.com>


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@267670 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for a problem when an empty tag was supplied. The diff shows that the `setTag` method now checks if the provided tag is empty after trimming whitespace. If it is, the tag is set to null. This prevents issues that might arise from processing empty tags. The answer is **Buggy**."
ant,5580.json,c807cf91c6fa47ef199cce3ae19a9d0e74fb4b18,"@@ -1,28 +1,28 @@
     private int removeEmptyDirectories(File dir, boolean removeIfEmpty,
                                        Set preservedEmptyDirectories) {
         int removedCount = 0;
-        if (!preservedEmptyDirectories.contains(dir) && dir.isDirectory()) {
+        if (dir.isDirectory()) {
             File[] children = dir.listFiles();
             for (int i = 0; i < children.length; ++i) {
                 File file = children[i];
                 // Test here again to avoid method call for non-directories!
-                if (!preservedEmptyDirectories.contains(file)
-                    && file.isDirectory()) {
+                if (file.isDirectory()) {
                     removedCount +=
                         removeEmptyDirectories(file, true,
                                                preservedEmptyDirectories);
                 }
             }
             if (children.length > 0) {
                 // This directory may have become empty...
                 // We need to re-query its children list!
                 children = dir.listFiles();
             }
-            if (children.length < 1 && removeIfEmpty) {
+            if (children.length < 1 && removeIfEmpty
+                && !preservedEmptyDirectories.contains(dir)) {
                 log(""Removing empty directory: "" + dir, Project.MSG_DEBUG);
                 dir.delete();
                 ++removedCount;
             }
         }
         return removedCount;
     }",Buggy,"properly handle non-recursive excludes (breaks one other test because of the same slowscan bug that I'm going to fix later)

git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@727978 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for handling non-recursive excludes. The code changes modify the logic within the `removeEmptyDirectories` method. Specifically, the condition `!preservedEmptyDirectories.contains(dir)` is removed at the beginning of the method, and added later within the `if (children.length < 1 && removeIfEmpty)` condition. This suggests that the original logic was incorrectly preventing the removal of empty directories in some non-recursive exclude scenarios, which is now corrected. The change addresses a specific issue related to directory exclusion, indicating a bug fix. The answer is **Buggy**.
"
ant,244.json,99684cfd77a3851d098824e9eb871cd6c087934f,"@@ -1,79 +1,79 @@
     private byte[] createCentralFileHeader(ZipEntry ze, ByteBuffer name, long lfhOffset,
                                            boolean needsZip64Extra) throws IOException {
         byte[] extra = ze.getCentralDirectoryExtra();
 
         // file comment length
         String comm = ze.getComment();
         if (comm == null) {
             comm = """";
         }
 
         ByteBuffer commentB = getEntryEncoding(ze).encode(comm);
         final int nameLen = name.limit() - name.position();
         final int commentLen = commentB.limit() - commentB.position();
         int len= CFH_FILENAME_OFFSET + nameLen + extra.length + commentLen;
         byte[] buf = new byte[len];
 
         System.arraycopy(CFH_SIG,  0, buf, CFH_SIG_OFFSET, WORD);
 
         // version made by
         // CheckStyle:MagicNumber OFF
         putShort((ze.getPlatform() << 8) | (!hasUsedZip64 ? DATA_DESCRIPTOR_MIN_VERSION : ZIP64_MIN_VERSION),
                 buf, CFH_VERSION_MADE_BY_OFFSET);
 
         final int zipMethod = ze.getMethod();
         final boolean encodable = zipEncoding.canEncode(ze.getName());
         putShort(versionNeededToExtract(zipMethod, needsZip64Extra), buf, CFH_VERSION_NEEDED_OFFSET);
         getGeneralPurposeBits(zipMethod, !encodable && fallbackToUTF8).encode(buf, CFH_GPB_OFFSET);
 
         // compression method
         putShort(zipMethod, buf, CFH_METHOD_OFFSET);
 
 
         // last mod. time and date
         ZipUtil.toDosTime(calendarInstance, ze.getTime(), buf, CFH_TIME_OFFSET);
 
         // CRC
         // compressed length
         // uncompressed length
         putLong(ze.getCrc(), buf, CFH_CRC_OFFSET);
         if (ze.getCompressedSize() >= ZIP64_MAGIC
                 || ze.getSize() >= ZIP64_MAGIC) {
             ZipLong.ZIP64_MAGIC.putLong(buf, CFH_COMPRESSED_SIZE_OFFSET);
             ZipLong.ZIP64_MAGIC.putLong(buf, CFH_ORIGINAL_SIZE_OFFSET);
         } else {
             putLong(ze.getCompressedSize(), buf, CFH_COMPRESSED_SIZE_OFFSET);
             putLong(ze.getSize(), buf, CFH_ORIGINAL_SIZE_OFFSET);
         }
 
         putShort(nameLen, buf, CFH_FILENAME_LENGTH_OFFSET);
 
         // extra field length
         putShort(extra.length, buf, CFH_EXTRA_LENGTH_OFFSET);
 
         putShort(commentLen, buf, CFH_COMMENT_LENGTH_OFFSET);
 
         // disk number start
         System.arraycopy(ZERO, 0, buf, CFH_DISK_NUMBER_OFFSET, SHORT);
 
         // internal file attributes
         putShort(ze.getInternalAttributes(), buf, CFH_INTERNAL_ATTRIBUTES_OFFSET);
 
         // external file attributes
         putLong(ze.getExternalAttributes(), buf, CFH_EXTERNAL_ATTRIBUTES_OFFSET);
 
         // relative offset of LFH
         putLong(Math.min(lfhOffset, ZIP64_MAGIC), buf, CFH_LFH_OFFSET);
 
         // file name
         System.arraycopy(name.array(), name.arrayOffset(), buf, CFH_FILENAME_OFFSET, nameLen);
 
         int extraStart = CFH_FILENAME_OFFSET + nameLen;
         System.arraycopy(extra, 0, buf, extraStart, extra.length);
 
-        int commentStart = extraStart + commentLen;
+        int commentStart = extraStart + extra.length;
 
         // file comment
         System.arraycopy(commentB.array(), commentB.arrayOffset(), buf, commentStart, commentLen);
         return buf;
     }",Buggy,"yet another potential AIOBException in zip package

Bug found and fix provided by Earl Hood
",Buggy,"The commit message indicates a fix for a potential `ArrayIndexOutOfBoundsException` (AIOOBException) in the zip package. The code diff modifies the `createCentralFileHeader` method, specifically adjusting the calculation of `commentStart`. The original code calculates `commentStart` as `extraStart + commentLen`, which is incorrect. It should be `extraStart + extra.length`. This correction suggests that the original calculation could lead to writing the comment data beyond the allocated buffer size, potentially causing an AIOOBException. The corrected code ensures that the comment data is written to the correct offset within the buffer, preventing the exception. The answer is **Buggy**."
ant,6044.json,e2a62c3179e855f7e102af8701aff81d6ba3c7e8,"@@ -1,56 +1,55 @@
         public int read(char[] cbuf, int off, int len)
             throws IOException {
 
             int amountRead = 0;
             while (pos < sourceFiles.size() || (needAddSeparator)) {
                 if (needAddSeparator) {
                     cbuf[off] = eolString.charAt(lastPos++);
                     if (lastPos >= eolString.length()) {
                         lastPos = 0;
                         needAddSeparator = false;
                         pos++;
                     }
                     len--;
                     off++;
                     amountRead++;
                     if (len == 0) {
                         return amountRead;
                     }
                     continue;
                 }
-
                 int nRead = getReader().read(cbuf, off, len);
                 if (nRead == -1 || nRead == 0) {
                     reader.close();
                     reader = null;
                     if (fixLastLine && isMissingEndOfLine()) {
                         needAddSeparator = true;
                         lastPos = 0;
                     } else {
                         pos++;
                     }
                 } else {
                     if (fixLastLine) {
                         for (int i = nRead;
                                  i > (nRead - lastChars.length);
                                  --i) {
                             if (i < 0) {
                                 break;
                             }
-                            addLastChar(cbuf[off + i]);
+                            addLastChar(cbuf[off + i - 1]);
                         }
                     }
                     len -= nRead;
                     off += nRead;
                     amountRead += nRead;
                     if (len == 0) {
                         return amountRead;
                     }
                 }
             }
             if (amountRead == 0) {
                 return -1;
             } else {
                 return amountRead;
             }
         }",Buggy,"Fix off-by-one error in concat with fixlastline=""yes""
PR: 25464
Obtained from: wang liang


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@275773 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for an off-by-one error when `fixlastline` is set to ""yes"". The diff modifies the loop that adds the last characters read to the `lastChars` array. The original code `addLastChar(cbuf[off + i]);` had an off-by-one error. It should be `addLastChar(cbuf[off + i - 1]);`. This correction aligns with the commit message, indicating a bug fix. The answer is **Buggy**."
ant,8293.json,feb03ce2d86cb52fbe8ce55b6b74dfe2536924b1,"@@ -1,5 +1,5 @@
-    protected Object clone() throws CloneNotSupportedException {
+    public Object clone() throws CloneNotSupportedException {
         Assertions that = (Assertions) super.clone();
         that.assertionList = (ArrayList) assertionList.clone();
         return that;
     }",Buggy,"added cloning support; bug #27218


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@276160 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates the addition of cloning support and references bug #27218. The diff shows a change to the `clone()` method, making it public and cloning the `assertionList`. This aligns with the commit message and suggests a bug fix related to cloning functionality. The answer is **Buggy**."
ant,4097.json,84d3e4a158b155664d401f0d8b57124504532b74,"@@ -1,3 +1,11 @@
     public void setGlib(String superGrammar) {
-        this.superGrammar = superGrammar;
+        String sg = null;
+        if (Os.isFamily(""dos"")) {
+            sg = superGrammar.replace('\\','/');
+        }
+        else
+        {
+            sg = superGrammar;
+        }
+        this.superGrammar = sg;
     }",Buggy,"All the tests in ANTLRTest.java involving a super-grammar file were failing on Windows.
This change fixes the problem.


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@274716 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates that tests involving a super-grammar file were failing on Windows. The diff introduces a fix that replaces backslashes with forward slashes in the superGrammar path when running on a DOS-based system (Windows). This is because Windows uses backslashes as path separators, while the code expects forward slashes. This change directly addresses the reported test failures on Windows, indicating a bug fix. The answer is **Buggy**."
ant,8414.json,08413ada31e6a9e9dbed3073158d17123d5028e0,"@@ -1,4 +1,6 @@
     private boolean hasPatterns(PatternSet ps) {
-        return ps.getIncludePatterns(getProject()).length > 0
-            || ps.getExcludePatterns(getProject()).length > 0;
+        String[] includePatterns = ps.getIncludePatterns(getProject());
+        String[] excludePatterns = ps.getExcludePatterns(getProject());
+        return (includePatterns != null && includePatterns.length > 0)
+            || (includePatterns != null && excludePatterns.length > 0);
     }",Buggy,"Fix Bug 42397: NPE in <path><files refid>

git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@540055 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for a NullPointerException (NPE) in the `<path><files refid>` Ant task. The diff modifies the `hasPatterns` method to check if the `includePatterns` and `excludePatterns` arrays returned by `ps.getIncludePatterns(getProject())` and `ps.getExcludePatterns(getProject())` are null before accessing their `length` property. This prevents an NPE if either of these methods returns null, which could happen if the pattern set is not properly configured or initialized. The added null checks directly address the reported NPE, indicating a bug fix. The answer is **Buggy**.
"
ant,7366.json,2a5857c384ef5a9e02b4264be44bf67f3a584d57,"@@ -1,48 +1,48 @@
     public PlanarImage executeDrawOperation() {
         log(""\tCreating Rectangle w="" + width + "" h="" + height + "" arcw=""
             + arcwidth + "" arch="" + archeight);
         BufferedImage bi = new BufferedImage(width, height, BufferedImage.TYPE_4BYTE_ABGR_PRE);
 
         Graphics2D graphics = (Graphics2D) bi.getGraphics();
 
         if (!stroke.equals(""transparent"")) {
             BasicStroke bStroke = new BasicStroke(stroke_width);
             graphics.setColor(ColorMapper.getColorByName(stroke));
             graphics.setStroke(bStroke);
 
             if ((arcwidth != 0) || (archeight != 0)) {
                 graphics.drawRoundRect(0, 0, width, height, arcwidth, archeight);
             } else {
                 graphics.drawRect(0, 0, width, height);
             }
         }
 
         if (!fill.equals(""transparent"")) {
             graphics.setColor(ColorMapper.getColorByName(fill));
             if ((arcwidth != 0) || (archeight != 0)) {
                 graphics.fillRoundRect(stroke_width, stroke_width,
                     width - (stroke_width * 2), height - (stroke_width * 2),
                     arcwidth, archeight);
             } else {
                 graphics.fillRect(stroke_width, stroke_width,
                     width - (stroke_width * 2), height - (stroke_width * 2));
             }
         }
 
 
         final int size = instructions.size();
         for (int i = 0; i < size; i++) {
             ImageOperation instr = ((ImageOperation) instructions.elementAt(i));
             if (instr instanceof DrawOperation) {
                 PlanarImage img = ((DrawOperation) instr).executeDrawOperation();
                 graphics.drawImage(img.getAsBufferedImage(), null, 0, 0);
             } else if (instr instanceof TransformOperation) {
-                graphics = (Graphics2D) bi.getGraphics();
                 PlanarImage image
                     = ((TransformOperation) instr)
                     .executeTransformOperation(PlanarImage.wrapRenderedImage(bi));
                 bi = image.getAsBufferedImage();
+                graphics = (Graphics2D) bi.getGraphics();
             }
         }
         return PlanarImage.wrapRenderedImage(bi);
     }",Buggy,"port image type bug fixes from Java8 refactoring in master
",Buggy,"The commit message indicates that the changes are bug fixes related to image types, ported from Java 8 refactoring. The diff shows a modification in the `executeDrawOperation` method, specifically within the loop that iterates through image operations. The original code obtained a new `Graphics2D` object from the `BufferedImage` *before* the `TransformOperation`, but did not re-assign it *after* the `TransformOperation`. The corrected code now obtains a new `Graphics2D` object *after* the `TransformOperation` is executed. This ensures that subsequent drawing operations are performed on the transformed image, fixing a potential issue where transformations were not being correctly applied. The answer is **Buggy**."
ant,9982.json,869b123c0437855adde003d242e4e084e8b35731,"@@ -1,30 +1,31 @@
     public File normalize(final String path) {
         Stack s = new Stack();
         String[] dissect = dissect(path);
         s.push(dissect[0]);
 
         StringTokenizer tok = new StringTokenizer(dissect[1], File.separator);
         while (tok.hasMoreTokens()) {
             String thisToken = tok.nextToken();
             if (""."".equals(thisToken)) {
                 continue;
             } else if ("".."".equals(thisToken)) {
                 if (s.size() < 2) {
-                    throw new BuildException(""Cannot resolve path "" + path);
+                    // Cannot resolve it, so skip it.
+                    return new File(path);
                 }
                 s.pop();
             } else { // plain component
                 s.push(thisToken);
             }
         }
         StringBuffer sb = new StringBuffer();
         for (int i = 0; i < s.size(); i++) {
             if (i > 1) {
                 // not before the filesystem root and not after it, since root
                 // already contains one
                 sb.append(File.separatorChar);
             }
             sb.append(s.elementAt(i));
         }
         return new File(sb.toString());
     }",Buggy,"#40281: ""Cannot resolve path"" error thrown gratuitously.


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@432379 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for a ""#40281: ""Cannot resolve path"" error thrown gratuitously."" The code diff modifies the `normalize` method, specifically addressing the case where "".."" (parent directory) tokens cause a `BuildException` when the stack size is less than 2. Instead of throwing an exception, the modified code now returns a `new File(path)`, effectively skipping the resolution. This change directly addresses the issue described in the commit message, indicating a bug fix where an error was being thrown unnecessarily. The answer is **Buggy**."
ant,7113.json,2fbb2e62ab0d3d323a75c6baf00a2e675c7fd6f5,"@@ -1,46 +1,55 @@
     protected void slowScan() {
         synchronized (slowScanLock) {
             if (haveSlowResults) {
                 return;
             }
             if (slowScanning) {
                 while (slowScanning) {
                     try {
                         slowScanLock.wait();
                     } catch (InterruptedException e) {
                     }
                 }
                 return;
             }
             slowScanning = true;
         }
         try {
             synchronized (this) {
 
+                // set in/excludes to reasonable defaults if needed:
+                boolean nullIncludes = (includes == null);
+                includes = nullIncludes ? new String[] {""**""} : includes;
+                boolean nullExcludes = (excludes == null);
+                excludes = nullExcludes ? new String[0] : excludes;
+
                 String[] excl = new String[dirsExcluded.size()];
                 dirsExcluded.copyInto(excl);
         
                 String[] notIncl = new String[dirsNotIncluded.size()];
                 dirsNotIncluded.copyInto(notIncl);
         
                 for (int i = 0; i < excl.length; i++) {
                     if (!couldHoldIncluded(excl[i])) {
                         scandir(new File(basedir, excl[i]),
                                 excl[i] + File.separator, false);
                     }
                 }
                 for (int i = 0; i < notIncl.length; i++) {
                     if (!couldHoldIncluded(notIncl[i])) {
                         scandir(new File(basedir, notIncl[i]),
                                 notIncl[i] + File.separator, false);
                     }
                 }
+                clearCaches();
+                includes = nullIncludes ? null : includes;
+                excludes = nullExcludes ? null : excludes;
             }
         } finally {
             synchronized (slowScanLock) {
                 haveSlowResults = true;
                 slowScanning = false;
                 slowScanLock.notifyAll();
             }
         }
     }",Buggy,"Not 100% sure this fixes the entire bug, but it eliminates NPEs for me.
PR: 34722


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@278219 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix, specifically addressing potential NullPointerExceptions (NPEs). The code diff shows modifications within the `slowScan` method. The changes include:

1.  **Handling null includes/excludes:** The code now checks if `includes` or `excludes` are null. If they are, it initializes them with default values (`""**""` for `includes` and an empty array for `excludes`). This prevents potential NPEs if these arrays are not initialized before being used.
2.  **Restoring original values:** After the scan, the code restores the `includes` and `excludes` arrays to null if they were originally null.
3.  `clearCaches()` is called.

The changes directly address the possibility of NPEs by ensuring that `includes` and `excludes` are not null before being used. This aligns with the commit message's claim of eliminating NPEs.

Therefore, the changes indicate a bug fix. The answer is **Buggy**."
ant,9433.json,fef3ea39f8bd474add292bb6437df6cbd22e1ba7,"@@ -1,47 +1,49 @@
     private static void buildJrePackages() {
         jrePackages = new Vector<String>();
         switch(javaVersionNumber) {
             case VERSION_1_9:
             case VERSION_1_8:
             case VERSION_1_7:
+                jrePackages.addElement(""jdk"");
+                // fall through
             case VERSION_1_6:
             case VERSION_1_5:
                 //In Java1.5, the apache stuff moved.
                 jrePackages.addElement(""com.sun.org.apache"");
                 //fall through.
             case VERSION_1_4:
                 if (javaVersionNumber == VERSION_1_4) {
                     jrePackages.addElement(""org.apache.crimson"");
                     jrePackages.addElement(""org.apache.xalan"");
                     jrePackages.addElement(""org.apache.xml"");
                     jrePackages.addElement(""org.apache.xpath"");
                 }
                 jrePackages.addElement(""org.ietf.jgss"");
                 jrePackages.addElement(""org.w3c.dom"");
                 jrePackages.addElement(""org.xml.sax"");
                 // fall through
             case VERSION_1_3:
                 jrePackages.addElement(""org.omg"");
                 jrePackages.addElement(""com.sun.corba"");
                 jrePackages.addElement(""com.sun.jndi"");
                 jrePackages.addElement(""com.sun.media"");
                 jrePackages.addElement(""com.sun.naming"");
                 jrePackages.addElement(""com.sun.org.omg"");
                 jrePackages.addElement(""com.sun.rmi"");
                 jrePackages.addElement(""sunw.io"");
                 jrePackages.addElement(""sunw.util"");
                 // fall through
             case VERSION_1_2:
                 jrePackages.addElement(""com.sun.java"");
                 jrePackages.addElement(""com.sun.image"");
                 // are there any here that we forgot?
                 // fall through
             case VERSION_1_1:
             default:
                 //things like sun.reflection, sun.misc, sun.net
                 jrePackages.addElement(""sun"");
                 jrePackages.addElement(""java"");
                 jrePackages.addElement(""javax"");
                 break;
         }
     }",Buggy,"fix Bug 59556 - support ""jdk"" package for Java 7+

Patch based on Chris Hegarty (Oracle) work.
""jdk"" package has been introduced in JDK7 but is not known from Ant.
""jdk.net.Sockets"" has been chosen as test class because it is available in JDK7, JDK8 and JDK9.
",Buggy,"The commit message indicates a fix for a bug where the ""jdk"" package was not supported for Java 7 and later versions. The diff adds ""jdk"" to the `jrePackages` list for Java versions 1.7, 1.8, and 1.9. This correction aligns with the commit message, suggesting a bug fix related to package recognition in Ant. The answer is **Buggy**."
lucene-solr,36661.json,d815a3608b2742dad7ed0c0b6233fdea1653c285,"@@ -1,33 +1,33 @@
   private final void count(List<MatchingDocs> matchingDocs) throws IOException {
     IntsRef scratch  = new IntsRef();
     for(MatchingDocs hits : matchingDocs) {
       OrdinalsReader.OrdinalsSegmentReader ords = ordinalsReader.getReader(hits.context);
       FixedBitSet bits = hits.bits;
     
       final int length = hits.bits.length();
       int doc = 0;
       while (doc < length && (doc = bits.nextSetBit(doc)) != -1) {
         ords.get(doc, scratch);
         for(int i=0;i<scratch.length;i++) {
-          ++counts[scratch.ints[i]];
+          counts[scratch.ints[scratch.offset+i]]++;
         }
         ++doc;
       }
     }
 
     // nocommit we could do this lazily instead:
 
     // Rollup any necessary dims:
     for(Map.Entry<String,FacetsConfig.DimConfig> ent : config.getDimConfigs().entrySet()) {
       String dim = ent.getKey();
       FacetsConfig.DimConfig ft = ent.getValue();
       if (ft.hierarchical && ft.multiValued == false) {
         int dimRootOrd = taxoReader.getOrdinal(new FacetLabel(dim));
         // It can be -1 if this field was declared in the
         // config but never indexed:
         if (dimRootOrd > 0) {
           counts[dimRootOrd] += rollup(children[dimRootOrd]);
         }
       }
     }
   }",Buggy,"LUCENE-5339: migrate some more tests; fix 'ignores IntsRef.offset bug' in TaxoFacetCounts; add FacetTestCase.getFacetCounts

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5339@1543506 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix related to `IntsRef.offset` in `TaxoFacetCounts`. The code diff shows a change in the line `counts[scratch.ints[i]]++;` to `counts[scratch.ints[scratch.offset+i]]++;`. This change correctly accounts for the offset within the `IntsRef` when accessing the `ints` array, which is crucial for handling cases where the `IntsRef` represents a sub-sequence of a larger array. The original code was likely ignoring the offset, leading to incorrect counts. The answer is **Buggy**.
"
lucene-solr,26469.json,4336de5e81d66e255abbface1bddf8c18e500a90,"@@ -1,16 +1,16 @@
   private void recordOffsetDiff(int inputLength, int outputLength) {
     if (inputLength == outputLength) {
       charCount += outputLength;
       return;
     }
     final int diff = inputLength - outputLength;
     final int cumuDiff = getLastCumulativeDiff();
     if (diff < 0) {
       for (int i = 1;  i <= -diff; ++i) {
         addOffCorrectMap(charCount + i, cumuDiff - i);
       }
     } else {
-      addOffCorrectMap(charCount + Math.min(1, outputLength), cumuDiff + diff);
+      addOffCorrectMap(charCount + outputLength, cumuDiff + diff);
     }
     charCount += outputLength;
   }",Buggy,"LUCENE-5547: fix bug in offset correction

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1580829 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix related to offset correction. The code diff modifies the `recordOffsetDiff` method, specifically the `addOffCorrectMap` call within the `else` block. The original code used `Math.min(1, outputLength)` which could lead to incorrect offset mapping when `outputLength` is 0. The corrected code uses `charCount + outputLength`, which seems more logically sound for calculating the correct offset. This change directly addresses the offset correction issue mentioned in the commit message. The answer is **Buggy**."
lucene-solr,19683.json,748cb61e824feefe8a6ef0fb2735af183c4afd5a,"@@ -1,9 +1,9 @@
   public IndexOutput createOutput(String name, IOContext context)
       throws IOException {
     if (SEGMENTS_GEN.equals(name)) {
-      return NULL_WRITER;
+      return new NullIndexOutput();
     }
     HdfsFileWriter writer = new HdfsFileWriter(getFileSystem(), new Path(
         hdfsDirPath, name));
     return new HdfsIndexOutput(writer);
   }",Buggy,"SOLR-4916: Fix bugs & usage of NullIndexOutput

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1502167 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates bug fixes related to `NullIndexOutput`. The code change replaces the static `NULL_WRITER` with a new instance of `NullIndexOutput` when creating the output for ""segments.gen"". This suggests that the previous usage of a static `NULL_WRITER` might have been problematic, potentially leading to shared state issues or incorrect behavior when multiple instances were needed. Creating a new instance each time likely resolves a bug related to the lifecycle or state management of the `NullIndexOutput`. The answer is **Buggy**."
lucene-solr,50349.json,b8feb92271c62b3bc1c24e2e3610ecd1cf646eab,"@@ -1,54 +1,60 @@
-  public double surfaceDistance(final GeoPoint p1, final GeoPoint p2) {
-    final double latA = p1.getLatitude();
-    final double lonA = p1.getLongitude();
-    final double latB = p2.getLatitude();
-    final double lonB = p2.getLongitude();
+  public double surfaceDistance(final GeoPoint pt1, final GeoPoint pt2) {
+    final double L = pt2.getLongitude() - pt1.getLongitude();
+    final double U1 = Math.atan((1.0-flattening) * Math.tan(pt1.getLatitude()));
+    final double U2 = Math.atan((1.0-flattening) * Math.tan(pt2.getLatitude()));
 
-    final double L = lonB - lonA;
-    final double oF = 1.0 - this.flattening;
-    final double U1 = Math.atan(oF * Math.tan(latA));
-    final double U2 = Math.atan(oF * Math.tan(latB));
-    final double sU1 = Math.sin(U1);
-    final double cU1 = Math.cos(U1);
-    final double sU2 = Math.sin(U2);
-    final double cU2 = Math.cos(U2);
+    final double sinU1 = Math.sin(U1);
+    final double cosU1 = Math.cos(U1);
+    final double sinU2 = Math.sin(U2);
+    final double cosU2 = Math.cos(U2);
 
-    double sigma, sinSigma, cosSigma;
-    double cos2Alpha, cos2SigmaM;
-    
+    final double dCosU1CosU2 = cosU1 * cosU2;
+    final double dCosU1SinU2 = cosU1 * sinU2;
+
+    final double dSinU1SinU2 = sinU1 * sinU2;
+    final double dSinU1CosU2 = sinU1 * cosU2;
+
+
     double lambda = L;
-    double iters = 100;
+    double lambdaP = Math.PI * 2.0;
+    int iterLimit = 0;
+    double cosSqAlpha;
+    double sinSigma;
+    double cos2SigmaM;
+    double cosSigma;
+    double sigma;
+    double sinAlpha;
+    double C;
+    double sinLambda, cosLambda;
 
     do {
-      final double sinLambda = Math.sin(lambda);
-      final double cosLambda = Math.cos(lambda);
-      sinSigma = Math.sqrt((cU2 * sinLambda) * (cU2 * sinLambda) + (cU1 * sU2 - sU1 * cU2 * cosLambda)
-          * (cU1 * sU2 - sU1 * cU2 * cosLambda));
-      if (Math.abs(sinSigma) < Vector.MINIMUM_RESOLUTION)
+      sinLambda = Math.sin(lambda);
+      cosLambda = Math.cos(lambda);
+      sinSigma = Math.sqrt((cosU2*sinLambda) * (cosU2*sinLambda) +
+                                    (dCosU1SinU2 - dSinU1CosU2 * cosLambda) * (dCosU1SinU2 - dSinU1CosU2 * cosLambda));
+
+      if (sinSigma==0.0) {
         return 0.0;
-
-      cosSigma = sU1 * sU2 + cU1 * cU2 * cosLambda;
+      }
+      cosSigma = dSinU1SinU2 + dCosU1CosU2 * cosLambda;
       sigma = Math.atan2(sinSigma, cosSigma);
-      final double sinAlpha = cU1 * cU2 * sinLambda / sinSigma;
-      cos2Alpha = 1.0 - sinAlpha * sinAlpha;
-      cos2SigmaM = cosSigma - 2.0 * sU1 * sU2 / cos2Alpha;
+      sinAlpha = dCosU1CosU2 * sinLambda / sinSigma;
+      cosSqAlpha = 1.0 - sinAlpha * sinAlpha;
+      cos2SigmaM = cosSigma - 2.0 * dSinU1SinU2 / cosSqAlpha;
 
-      final double c = this.flattening * 0.625 * cos2Alpha * (4.0 + this.flattening * (4.0 - 3.0 * cos2Alpha));
-      final double lambdaP = lambda;
-      lambda = L + (1.0 - c) * this.flattening * sinAlpha * (sigma + c * sinSigma * (cos2SigmaM + c * cosSigma *
-          (-1.0 + 2.0 * cos2SigmaM * cos2SigmaM)));
-      if (Math.abs(lambda - lambdaP) < Vector.MINIMUM_RESOLUTION)
-        break;
-    } while (--iters > 0);
+      if (Double.isNaN(cos2SigmaM))
+        cos2SigmaM = 0.0;  // equatorial line: cosSqAlpha=0
+      C = flattening / 16.0 * cosSqAlpha * (4.0 + flattening * (4.0 - 3.0 * cosSqAlpha));
+      lambdaP = lambda;
+      lambda = L + (1.0 - C) * flattening * sinAlpha *
+        (sigma + C * sinSigma * (cos2SigmaM + C * cosSigma * (-1.0 + 2.0 * cos2SigmaM *cos2SigmaM)));
+    } while (Math.abs(lambda-lambdaP) > Vector.MINIMUM_RESOLUTION && ++iterLimit < 40);
 
-    if (iters == 0)
-      return 0.0;
+    final double uSq = cosSqAlpha * this.squareRatio;
+    final double A = 1.0 + uSq / 16384.0 * (4096.0 + uSq * (-768.0 + uSq * (320.0 - 175.0 * uSq)));
+    final double B = uSq / 1024.0 * (256.0 + uSq * (-128.0 + uSq * (74.0 - 47.0 * uSq)));
+    final double deltaSigma = B * sinSigma * (cos2SigmaM + B / 4.0 * (cosSigma * (-1.0 + 2.0 * cos2SigmaM * cos2SigmaM)-
+                                        B / 6.0 * cos2SigmaM * (-3.0 + 4.0 * sinSigma * sinSigma) * (-3.0 + 4.0 * cos2SigmaM * cos2SigmaM)));
 
-    final double uSq = cos2Alpha * this.squareRatio;
-    final double A = 1.0 + uSq * 0.00006103515625 * (4096.0 + uSq * (-768.0 + uSq * (320.0 - 175.0 * uSq)));
-    final double B = uSq * 0.0009765625 * (256.0 + uSq * (-128.0 + uSq * (74.0 - 47.0 * uSq)));
-    final double deltaSigma = B * sinSigma * (cos2SigmaM + B * 0.25 * (cosSigma * (-1.0 + 2.0 * cos2SigmaM * cos2SigmaM) - B * 0.16666666666666666666667 * cos2SigmaM
-            * (-3.0 + 4.0 * sinSigma * sinSigma) * (-3.0 + 4.0 * cos2SigmaM * cos2SigmaM)));
-
-    return this.c * A * (sigma - deltaSigma);
+    return c * A * (sigma - deltaSigma);
   }",Buggy,"LUCENE-7139: fix bugs in geo3d's vincenty distance implementation
",Buggy,"The commit message indicates that the changes fix bugs in the geo3d's vincenty distance implementation. The diff shows a significant change in the `surfaceDistance` method, which calculates the distance between two GeoPoints. The changes include:
1.  Renaming variables for clarity (e.g., `p1` to `pt1`, `latA` to `pt1.getLatitude()`).
2.  Introducing intermediate variables to store calculations (e.g., `dCosU1CosU2`, `dSinU1SinU2`).
3.  Adding an iteration limit to the lambda calculation loop to prevent infinite loops.
4.  Handling the case where `sinSigma` is zero to avoid division by zero.
5.  Handling the case where `cos2SigmaM` is NaN.
6.  Modifying the calculations of `C`, `A`, `B`, and `deltaSigma`.

These changes suggest that the original implementation had issues with accuracy, convergence, and edge cases, which are now addressed in the corrected version. The commit message explicitly states that bugs are being fixed, and the code changes reflect this. The answer is **Buggy**."
lucene-solr,8422.json,dfdd1b7363afa9e56a47fec7b92960468edd7763,"@@ -1,37 +1,37 @@
   public List<ZkCoreNodeProps> getReplicaProps(String collection,
       String shardId, String thisNodeName, String coreName, String stateFilter) {
     CloudState cloudState = this.cloudState;
     if (cloudState == null) {
       return null;
     }
     Map<String,Slice> slices = cloudState.getSlices(collection);
     if (slices == null) {
       throw new ZooKeeperException(ErrorCode.BAD_REQUEST,
           ""Could not find collection in zk: "" + collection + "" ""
               + cloudState.getCollections());
     }
     
     Slice replicas = slices.get(shardId);
     if (replicas == null) {
       throw new ZooKeeperException(ErrorCode.BAD_REQUEST, ""Could not find shardId in zk: "" + shardId);
     }
     
     Map<String,ZkNodeProps> shardMap = replicas.getShards();
     List<ZkCoreNodeProps> nodes = new ArrayList<ZkCoreNodeProps>(shardMap.size());
     String filterNodeName = thisNodeName + ""_"" + coreName;
     for (Entry<String,ZkNodeProps> entry : shardMap.entrySet()) {
       ZkCoreNodeProps nodeProps = new ZkCoreNodeProps(entry.getValue());
-      String coreNodeName = nodeProps.getNodeName() + ""_"" + coreName;
+      String coreNodeName = nodeProps.getNodeName() + ""_"" + nodeProps.getCoreName();
       if (cloudState.liveNodesContain(nodeProps.getNodeName()) && !coreNodeName.equals(filterNodeName)) {
         if (stateFilter == null || stateFilter.equals(nodeProps.getState())) {
           nodes.add(nodeProps);
         }
       }
     }
     if (nodes.size() == 0) {
       // no replicas - go local
       return null;
     }
 
     return nodes;
   }",Buggy,"SOLR-3108: Error in SolrCloud's replica lookup code when replica's are hosted in same Solr instance (fix only)

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1242212 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix in SolrCloud's replica lookup code when replicas are hosted on the same Solr instance. The code diff modifies how the `coreNodeName` is constructed. Originally, it used the `coreName` parameter passed to the method. Now, it uses `nodeProps.getCoreName()`. This suggests that the original code was incorrectly using the method parameter instead of the core name associated with the node properties, which would lead to incorrect replica identification when multiple replicas were on the same node. This is a bug fix. The answer is **Buggy**."
lucene-solr,10932.json,56476fb8c70486c21324879f1bd1efc79df92ffb,"@@ -1,16 +1,24 @@
   public Map<String, Metric> getMetrics() {
     final Map<String, Metric> metrics = new HashMap<>();
 
     try {
       final ObjectName on = new ObjectName(""java.lang:type=OperatingSystem"");
       // verify that it exists
-      mBeanServer.getMBeanInfo(on);
+      MBeanInfo info = mBeanServer.getMBeanInfo(on);
+      // collect valid attributes
+      Set<String> attributes = new HashSet<>();
+      for (MBeanAttributeInfo ai : info.getAttributes()) {
+        attributes.add(ai.getName());
+      }
       for (String metric : METRICS) {
-        metrics.put(metric, new JmxAttributeGauge(mBeanServer, on, metric));
+        // verify that an attribute exists before attempting to add it
+        if (attributes.contains(metric)) {
+          metrics.put(metric, new JmxAttributeGauge(mBeanServer, on, metric));
+        }
       }
     } catch (JMException ignored) {
       log.debug(""Unable to load OperatingSystem MBean"", ignored);
     }
 
     return metrics;
   }",Buggy,"SOLR-9805 Fix assertion error on Windows where SystemLoadAverage is reported as -1.
Don't expose non-existent attributes.
",Buggy,"The commit message indicates a fix for an assertion error on Windows related to `SystemLoadAverage` being reported as -1. The code changes involve checking if a metric attribute exists in the MBean before attempting to add it. This prevents the code from trying to access non-existent attributes, which could lead to errors or unexpected behavior, especially in environments like Windows where certain attributes might not be available. The added check `if (attributes.contains(metric))` ensures that only valid attributes are added to the metrics map. This directly addresses the issue described in the commit message, indicating a bug fix. The answer is **Buggy**."
lucene-solr,28559.json,f55c8d1247fa9183f90bf222e827727ba0dee481,"@@ -1,11 +1,11 @@
     public DocsAndPositionsEnum reset(int[] postings, byte[] payloadBytes) {
       this.postings = postings;
       upto = 0;
       skipPositions = 0;
       startOffset = -1;
       endOffset = -1;
       docID = -1;
       payloadLength = 0;
-      payload.bytes = payloadBytes;
+      this.payloadBytes = payloadBytes;
       return this;
     }",Buggy,"fix bugs in DirectPF's lowFreq d-and-p-enum, set payload.bytes/offset/length in getPayload, also skip payload pointer correctly when scanning over deleted docs in nextDoc

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1364070 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates bug fixes related to `DirectPF`'s lowFreq d-and-p-enum, setting payload information in `getPayload`, and skipping payload pointers when scanning deleted documents. The provided diff shows a change in the `reset` method where `payload.bytes = payloadBytes;` is changed to `this.payloadBytes = payloadBytes;`. This suggests that the code was incorrectly assigning the `payloadBytes` to a field named `payload.bytes` instead of the correct field `this.payloadBytes`. This is a bug fix. The answer is **Buggy**."
lucene-solr,2958.json,785bebbcbd8f77ccc6d75acf3fb3d42ee29770fc,"@@ -1,4 +1,4 @@
     public String mapSafeElement(String name) {
       String lowerName = name.toLowerCase(Locale.ROOT);
-      return lowerName.equals(""br"") ? null : lowerName;
+      return (lowerName.equals(""br"") || lowerName.equals(""body"")) ? null : lowerName;
     }",Buggy,"SOLR-8981 remove ""don't test with java-9"" commands; fix bug introduced by TIKA-995 -- doubling of body elements in HTML tags; add copyright info for Jackcess.
",Buggy,"The commit message indicates a bug fix related to ""doubling of body elements in HTML tags,"" which was introduced by TIKA-995. The code diff modifies the `mapSafeElement` method to return `null` not only for ""br"" tags but also for ""body"" tags. This change directly addresses the issue of doubling body elements, as it prevents the method from processing ""body"" tags, thus fixing the bug. The answer is **Buggy**."
lucene-solr,21433.json,c0e5935900ef5f0c65b006f3071d2132143dda58,"@@ -1,24 +1,25 @@
   public void init(NamedList args) {
 
-    String patternParam = args.remove(PATTERN_PARAM).toString();
+    Object patternParam = args.remove(PATTERN_PARAM);
 
     if(patternParam == null) {
       throw new SolrException(ErrorCode.SERVER_ERROR, 
                               ""Missing required init parameter: "" + PATTERN_PARAM);
     }
+    
     try {
-      pattern = Pattern.compile(patternParam);      
+      pattern = Pattern.compile(patternParam.toString());      
     } catch (PatternSyntaxException e) {
       throw new SolrException(ErrorCode.SERVER_ERROR, 
                               ""Invalid regex: "" + patternParam, e);
     }                                
 
-    String replacementParam = args.remove(REPLACEMENT_PARAM).toString();
+    Object replacementParam = args.remove(REPLACEMENT_PARAM);
     if(replacementParam == null) {
       throw new SolrException(ErrorCode.SERVER_ERROR, 
                               ""Missing required init parameter: "" + REPLACEMENT_PARAM);
     }
-    replacement = Matcher.quoteReplacement(replacementParam);
+    replacement = Matcher.quoteReplacement(replacementParam.toString());
 
     super.init(args);
   }",Buggy,"fix a small bug around these null checks

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1357983 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for a small bug related to null checks. The diff shows that the code now retrieves parameters as Objects instead of Strings directly. It then checks if these Objects are null before converting them to Strings using `toString()`. This change addresses a potential `NullPointerException` if the parameters were not provided, thus fixing a bug. The answer is **Buggy**."
lucene-solr,48864.json,cd5b023219a4e7cd060e88799581704d15820edf,"@@ -1,11 +1,11 @@
     void mergeAt(int i) {
       assert stackSize > i + 1;
       final int l = runBase(i+1);
       final int pivot = runBase(i);
       final int h = runEnd(i);
       merge(l, pivot, h, pivot - l, h - pivot);
-      for (int j = 1; j <= i+1; ++j) {
+      for (int j = i + 1; j > 0; --j) {
         setRunEnd(j, runEnd(j-1));
       }
       --stackSize;
     }",Buggy,"LUCENE-4839: Fix bug in SorterTemplate.timSort().


git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1457315 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix in `SorterTemplate.timSort()`. The diff modifies the loop in `mergeAt()` function, changing the iteration direction from ascending (`j = 1; j <= i+1; ++j`) to descending (`j = i + 1; j > 0; --j`). This change likely corrects an error in how run ends are updated after a merge operation within the TimSort algorithm. This suggests a bug fix related to the merging process in TimSort. The answer is **Buggy**.
"
lucene-solr,38904.json,008bc74bebef96414f19118a267dbf982aba58b9,"@@ -1,30 +1,30 @@
     void add(int startOffset, int endOffset) {
       assert enumIdx == -1 : ""bad state"";
 
       // loop backwards since we expect a match at the end or close to it.  We expect O(1) not O(N).
       int pairIdx = numPairs - 1;
       for (; pairIdx >= 0; pairIdx--) {
         int iStartOffset = startOffsets[pairIdx];
         int iEndOffset = endOffsets[pairIdx];
         int cmp = Integer.compare(iStartOffset, startOffset);
         if (cmp == 0) {
           cmp = Integer.compare(iEndOffset, endOffset);
         }
         if (cmp == 0) {
           return; // we already have this offset-pair for this term
         } else if (cmp < 0) {
           break; //we will insert offsetPair to the right of pairIdx
         }
       }
       // pairIdx is now one position to the left of where we insert the new pair
       // shift right any pairs by one to make room
       final int shiftLen = numPairs - (pairIdx + 1);
       if (shiftLen > 0) {
-        System.arraycopy(startOffsets, pairIdx + 2, startOffsets, pairIdx + 3, shiftLen);
-        System.arraycopy(endOffsets, pairIdx + 2, endOffsets, pairIdx + 3, shiftLen);
+        System.arraycopy(startOffsets, pairIdx + 1, startOffsets, pairIdx + 2, shiftLen);
+        System.arraycopy(endOffsets, pairIdx + 1, endOffsets, pairIdx + 2, shiftLen);
       }
       // now we can place the offset pair
       startOffsets[pairIdx + 1] = startOffset;
       endOffsets[pairIdx + 1] = endOffset;
       numPairs++;
     }",Buggy,"LUCENE-8365: Fix ArrayIndexOutOfBoundsException in UnifiedHighlighter

This fixes a ""off by one"" error in the UnifiedHighlighter's code that
is only triggered when two nested SpanNearQueries contain the same term.

Signed-off-by: Simon Willnauer <simonw@apache.org>
Closes #408
",Buggy,"The commit message indicates a fix for an `ArrayIndexOutOfBoundsException` in the `UnifiedHighlighter` that occurs when nested `SpanNearQueries` contain the same term. The code diff modifies the `add` method, specifically the `System.arraycopy` calls. The original code had an off-by-one error in the source and destination indices of the `arraycopy` calls, potentially leading to the mentioned exception when shifting elements to insert a new offset pair. The corrected code adjusts these indices, resolving the issue. The answer is **Buggy**.
"
lucene-solr,2699.json,bee8d7ccb32bc23bd808f729493631b60a64bffb,"@@ -1,18 +1,9 @@
   public Comparator<Expression> comparator(final FacetSortDirection direction) {
     return (a, b) -> {
-      boolean aIsNull = a.getValue() == null;
-      boolean bIsNull = b.getValue() == null;
-
-      if (aIsNull && bIsNull) return 0;
-
-      if( direction == FacetSortDirection.ASCENDING ){ // nulls are last for ASC sort
-        return aIsNull ? 1
-          : bIsNull ? -1
-          : a.getValue().compareTo(b.getValue());
+      if( direction == FacetSortDirection.ASCENDING ){
+        return a.getValue().compareTo(b.getValue());
       } else {
-        return aIsNull ? -1
-          : bIsNull ? 1
-          : b.getValue().compareTo(a.getValue());
+        return b.getValue().compareTo(a.getValue());
       }
     };
   }",Buggy,"Revert ""SOLR-9981: Performance improvements and bug fixes for the Analytics component""

This reverts commit a5dce163eb09dcc0eb7f7eb81d692bf3d19964a3.
",Buggy,"The commit message indicates a revert of a previous commit that included bug fixes. The diff removes null checks in the comparator, which were likely added as part of the bug fixes in the original commit. Reverting these changes would reintroduce the original bug. The answer is **Buggy**."
lucene-solr,2699.json,a5dce163eb09dcc0eb7f7eb81d692bf3d19964a3,"@@ -1,9 +1,18 @@
   public Comparator<Expression> comparator(final FacetSortDirection direction) {
     return (a, b) -> {
-      if( direction == FacetSortDirection.ASCENDING ){
-        return a.getValue().compareTo(b.getValue());
+      boolean aIsNull = a.getValue() == null;
+      boolean bIsNull = b.getValue() == null;
+
+      if (aIsNull && bIsNull) return 0;
+
+      if( direction == FacetSortDirection.ASCENDING ){ // nulls are last for ASC sort
+        return aIsNull ? 1
+          : bIsNull ? -1
+          : a.getValue().compareTo(b.getValue());
       } else {
-        return b.getValue().compareTo(a.getValue());
+        return aIsNull ? -1
+          : bIsNull ? 1
+          : b.getValue().compareTo(a.getValue());
       }
     };
   }",Buggy,"SOLR-9981: Performance improvements and bug fixes for the Analytics component
",Buggy,"The commit message indicates performance improvements and bug fixes for the Analytics component. The diff introduces null checks for the values being compared in the comparator. This suggests that the original code might have thrown a NullPointerException when encountering null values, which is a bug. The added null checks ensure that null values are handled correctly during sorting, placing them either at the beginning or end depending on the sort direction. The answer is **Buggy**."
lucene-solr,5640.json,6e4924cbfc828506550fd27b0350e3f12c572746,"@@ -1,65 +1,65 @@
   private ResultSetValueSelector[] constructValueSelectors(ResultSetMetaData metadata) throws SQLException{
     ResultSetValueSelector[] valueSelectors = new ResultSetValueSelector[metadata.getColumnCount()];
     
     for(int columnIdx = 0; columnIdx < metadata.getColumnCount(); ++columnIdx){
       
       final int columnNumber = columnIdx + 1; // cause it starts at 1        
-      final String columnName = metadata.getColumnName(columnNumber);
+      final String columnName = metadata.getColumnLabel(columnNumber);
       String className = metadata.getColumnClassName(columnNumber);
       String typeName = metadata.getColumnTypeName(columnNumber);
             
       if(directSupportedTypes.contains(className)){
         valueSelectors[columnIdx] = new ResultSetValueSelector() {
           public Object selectValue(ResultSet resultSet) throws SQLException {
             Object obj = resultSet.getObject(columnNumber);
             if(resultSet.wasNull()){ return null; }
             return obj;
           }
           public String getColumnName() {
             return columnName;
           }
         };
       }
-      else if(Short.class.getName() == className){
+      else if(Short.class.getName().equals(className)) {
         valueSelectors[columnIdx] = new ResultSetValueSelector() {
           public Object selectValue(ResultSet resultSet) throws SQLException {
             Short obj = resultSet.getShort(columnNumber);
             if(resultSet.wasNull()){ return null; }
             return obj.longValue();
           }
           public String getColumnName() {
             return columnName;
           }
         };
       }
-      else if(Integer.class.getName() == className){
+      else if(Integer.class.getName().equals(className)) {
         valueSelectors[columnIdx] = new ResultSetValueSelector() {
           public Object selectValue(ResultSet resultSet) throws SQLException {
             Integer obj = resultSet.getInt(columnNumber);
             if(resultSet.wasNull()){ return null; }
             return obj.longValue();
           }
           public String getColumnName() {
             return columnName;
           }
         };
       }
-      else if(Float.class.getName() == className){
+      else if(Float.class.getName().equals(className)) {
         valueSelectors[columnIdx] = new ResultSetValueSelector() {
           public Object selectValue(ResultSet resultSet) throws SQLException {
             Float obj = resultSet.getFloat(columnNumber);
             if(resultSet.wasNull()){ return null; }
             return obj.doubleValue();
           }
           public String getColumnName() {
             return columnName;
           }
         };
       }
       else{
         throw new SQLException(String.format(Locale.ROOT, ""Unable to determine the valueSelector for column '%s' (col #%d) of java class '%s' and type '%s'"", columnName, columnNumber, className, typeName));
       }
     }
     
     return valueSelectors;
   }",Buggy,"Update dependencies and fix minor errors
",Buggy,"The commit message mentions ""fix minor errors"". The code changes involve replacing `==` with `.equals()` for comparing strings representing class names, and using `getColumnLabel` instead of `getColumnName`. Using `.equals()` for string comparison is crucial for correctness, as `==` compares object references, not content. The change from `getColumnName` to `getColumnLabel` might be to retrieve a more user-friendly or standardized name for the column. These changes suggest that the original code had potential bugs related to incorrect string comparisons and possibly inaccurate column name retrieval. The answer is **Buggy**."
lucene-solr,26464.json,96150badce8234cac00a23c2d5da55545e0be958,"@@ -1,16 +1,24 @@
-  private int readInputToBuffer() throws IOException {
-    final int len = input.read(tmpBuffer);
-    if (len == -1) {
-      inputFinished = true;
-      return 0;
+  private void readInputToBuffer() throws IOException {
+    while (true) {
+      // CharacterUtils.fill is supplementary char aware
+      final boolean hasRemainingChars = CharacterUtils.fill(tmpBuffer, input);
+
+      assert tmpBuffer.getOffset() == 0;
+      inputBuffer.append(tmpBuffer.getBuffer(), 0, tmpBuffer.getLength());
+
+      if (hasRemainingChars == false) {
+        inputFinished = true;
+        break;
+      }
+
+      final int lastCodePoint = Character.codePointBefore(tmpBuffer.getBuffer(), tmpBuffer.getLength());
+      if (normalizer.isInert(lastCodePoint)) {
+        // we require an inert char so that we can normalize content before and
+        // after this character independently
+        break;
+      }
     }
-    inputBuffer.append(tmpBuffer, 0, len);
 
     // if checkedInputBoundary was at the end of a buffer, we need to check that char again
     checkedInputBoundary = Math.max(checkedInputBoundary - 1, 0);
-    // this loop depends on 'isInert' (changes under normalization) but looks only at characters.
-    // so we treat all surrogates as non-inert for simplicity
-    if (normalizer.isInert(tmpBuffer[len - 1]) && !Character.isSurrogate(tmpBuffer[len-1])) {
-      return len;
-    } else return len + readInputToBuffer();
   }",Buggy,"LUCENE-7956: Fixed potential stack overflow error in ICUNormalizer2CharFilter.
",Buggy,"The commit message indicates a fix for a potential stack overflow error in `ICUNormalizer2CharFilter`. The diff shows a significant change in the `readInputToBuffer` method. The original code recursively called `readInputToBuffer` if the last character read was not inert and not a surrogate. This recursion could lead to a stack overflow if a long sequence of non-inert characters was encountered. The modified code uses a `while` loop to repeatedly read input into the buffer until the input is finished or an inert character is encountered. This iterative approach avoids the stack overflow issue. Therefore, the changes indicate a bug fix. The answer is **Buggy**."
lucene-solr,26464.json,63d67496cfae2cbe4611b294c18d285e89c8e254,"@@ -1,14 +1,16 @@
   private int readInputToBuffer() throws IOException {
     final int len = input.read(tmpBuffer);
     if (len == -1) {
       inputFinished = true;
       return 0;
     }
     inputBuffer.append(tmpBuffer, 0, len);
 
     // if checkedInputBoundary was at the end of a buffer, we need to check that char again
     checkedInputBoundary = Math.max(checkedInputBoundary - 1, 0);
-    if (normalizer.isInert(tmpBuffer[len - 1]) && !Character.isHighSurrogate(tmpBuffer[len-1])) {
+    // this loop depends on 'isInert' (changes under normalization) but looks only at characters.
+    // so we treat all surrogates as non-inert for simplicity
+    if (normalizer.isInert(tmpBuffer[len - 1]) && !Character.isSurrogate(tmpBuffer[len-1])) {
       return len;
     } else return len + readInputToBuffer();
   }",Buggy,"fix bug in buffering logic of this charfilter

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1586473 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix in the buffering logic. The code change modifies a condition that checks for inert characters at the end of the buffer. Specifically, it changes `Character.isHighSurrogate` to `Character.isSurrogate`. This suggests that the original code was not correctly handling surrogate pairs, which are used to represent characters outside the basic multilingual plane (BMP) in UTF-16. By using `Character.isSurrogate`, the code now correctly identifies both high and low surrogates, ensuring that the buffering logic works correctly with all Unicode characters. This is a bug fix because the previous code would have incorrectly handled some Unicode characters. The answer is **Buggy**."
lucene-solr,15099.json,07b37ff26becf214bae000dff08b9091d31327a8,"@@ -1,14 +1,12 @@
   private boolean predefinedPermissionAppliesToRequest(Permission predefinedPermission, AuthorizationContext context) {
-    if (context.getHandler() instanceof PermissionNameProvider) {
+    if (predefinedPermission.wellknownName == PermissionNameProvider.Name.ALL) {
+      return true; //'ALL' applies to everything!
+    } else if (! (context.getHandler() instanceof PermissionNameProvider)) {
+      return false; // We're not 'ALL', and the handler isn't associated with any other predefined permissions
+    } else {
       PermissionNameProvider handler = (PermissionNameProvider) context.getHandler();
       PermissionNameProvider.Name permissionName = handler.getPermissionName(context);
-      if (permissionName == null || !predefinedPermission.name.equals(permissionName.name)) {
-        return false;
-      }
-    } else {
-      //all is special. it can match any
-      if(predefinedPermission.wellknownName != PermissionNameProvider.Name.ALL) return false;
-    }
 
-    return true;
+      return permissionName != null && predefinedPermission.name.equals(permissionName.name);
+    }
   }",Buggy,"SOLR-13355: Obey 'ALL' for handlers with other predefined perms

Prior to this commit, RuleBasedAuthorizationPlugin would check for the
predefined 'ALL' permission only when the endpoint being hit wasn't
associated with another predefined-permission.

This resulted in some very unintuitive behavior. For example, the
permission {name:all, role:admin} would correctly prevent a
role:foo user from accessing /admin/info/properties, but would allow
write access to /admin/authorization because of the SECURITY_EDIT
predefined perm associated with that endpoint.

This commit fixes this bug so that the 'all' permission is always
consulted whether or not the endpoint is associated with other predefined
permissions.
",Buggy,"The commit message clearly states that it fixes a bug in how the 'ALL' permission is handled in `RuleBasedAuthorizationPlugin`. The previous logic incorrectly bypassed the 'ALL' permission check when an endpoint had other predefined permissions, leading to unintended access. The code changes in `predefinedPermissionAppliesToRequest` directly address this issue by ensuring that the 'ALL' permission is always checked, regardless of other predefined permissions. The logic is restructured to prioritize the 'ALL' permission check and simplify the conditional checks. The answer is **Buggy**.
"
lucene-solr,14965.json,0857bb60d3bb5bd6d8d851ce2e01edd9bde310ea,"@@ -1,20 +1,21 @@
     public void refresh(SolrQueryRequest req, SolrQueryResponse rsp, PayloadObj<String> payload) {
       String p = payload.get();
       if (p == null) {
         payload.addError(""Package null"");
         return;
       }
       PackageLoader.Package pkg = coreContainer.getPackageLoader().getPackage(p);
       if (pkg == null) {
         payload.addError(""No such package: "" + p);
         return;
       }
-
+      //first refresh my own
+      packageLoader.notifyListeners(p);
       for (String s : coreContainer.getPackageStoreAPI().shuffledNodes()) {
         Utils.executeGET(coreContainer.getUpdateShardHandler().getDefaultHttpClient(),
             coreContainer.getZkController().zkStateReader.getBaseUrlForNodeName(s).replace(""/solr"", ""/api"") + ""/cluster/package?wt=javabin&omitHeader=true&refreshPackage="" + p,
             Utils.JAVABINCONSUMER);
       }
 
 
     }",Buggy,"SOLR-13662, SOLR-13822: Fixing bug with refresh API, un-ignoring the test and new reference guide on Package Management
",Buggy,"The commit message indicates a bug fix related to the refresh API. The code changes involve refreshing a package. The added line `packageLoader.notifyListeners(p);` suggests that the package loader wasn't notifying listeners about the refresh, which could be the bug. The loop iterates through nodes and refreshes the package on each node. The addition of `packageLoader.notifyListeners(p);` before the loop ensures that the current node also refreshes. This looks like a bug fix. The answer is **Buggy**.
"
lucene-solr,19226.json,9e78be40c338005b75609a3b123778aea822bcf0,"@@ -1,7 +1,7 @@
   public void setNextReader(LeafReaderContext context) throws IOException {
-    if (globalDocValues instanceof MultiDocValues.MultiSortedDocValues) {
+    if (ordinalMap != null) {
       toGlobal = ordinalMap.getGlobalOrds(context.ord);
-      docValues = DocValues.getSorted(context.reader(), field);
     }
+    docValues = DocValues.getSorted(context.reader(), field);
     lastDocID = 0;
   }",Buggy,"SOLR-11598: Fix bug while setting and resetting string doc-values while exporting documents
",Buggy,"The commit message indicates a bug fix related to setting and resetting string doc-values during document export. The code diff modifies the `setNextReader` method. The original code checks if `globalDocValues` is an instance of `MultiDocValues.MultiSortedDocValues` before assigning `toGlobal`. The modified code checks if `ordinalMap` is not null before assigning `toGlobal`. Also, the assignment of `docValues` is moved outside of the conditional block. This change suggests that the original logic was flawed, possibly leading to incorrect or missing doc-values in certain scenarios, especially when `ordinalMap` was not properly handled. The fix ensures that `docValues` is always assigned, and the condition for assigning `toGlobal` is based on `ordinalMap` which is likely the correct condition for this logic. The answer is **Buggy**."
lucene-solr,38500.json,1d35bd0ea8beb635997c3cf131ded8ebe58d15a9,"@@ -1,9 +1,9 @@
   public void clearFieldsAfter(Calendar cal, int field) {
     int assertEra = -1;
     assert (assertEra = (((Calendar)cal.clone()).get(Calendar.ERA))) >= 0;//a trick to only get this if assert enabled
     //note: Calendar.ERA == 0;
-    for (int f = field+1; f <= Calendar.MILLISECOND; f++) {
+    for (int f = field + 1; f <= Calendar.MILLISECOND; f++) {
       cal.clear(f);
     }
-    assert ((Calendar)cal.clone()).get(Calendar.ERA) == assertEra : ""Calendar underflow"";
+    assert field + 1 == Calendar.ERA || ((Calendar)cal.clone()).get(Calendar.ERA) == assertEra : ""Calendar underflow"";
   }",Buggy,"LUCENE-7278: DRPT: fix bug in assert statement
",Buggy,"The commit message indicates a bug fix in an assert statement. The diff modifies the assert statement to account for a specific case where `field + 1` equals `Calendar.ERA`. This suggests that the original assert statement was too strict and triggered incorrectly in this scenario, indicating a bug in the assertion logic itself. The answer is **Buggy**."
lucene-solr,13814.json,d7f397056bcb52564bf5cbffa96152aa74c0fa1f,"@@ -1,28 +1,29 @@
   public Query parse() throws SyntaxError {
     Query q = super.parse();
     if (!(q instanceof BooleanQuery)) {
       return q;
     }
     BooleanQuery obq = (BooleanQuery)q;
     Collection<Query> should = new ArrayList<Query>();
     Collection<BooleanClause> prohibOrReq = new ArrayList<BooleanClause>();
     BooleanQuery newq = new BooleanQuery();
 
     for (BooleanClause clause : obq.getClauses()) {
       if(clause.isProhibited() || clause.isRequired()) {
         prohibOrReq.add(clause);
       } else {
         BooleanQuery bq = new BooleanQuery();
         bq.add(clause);
         should.add(bq);
       }
     }
     if (should.size() > 0) {
       DisjunctionMaxQuery dmq = new DisjunctionMaxQuery(should, tie);
       newq.add(dmq, BooleanClause.Occur.SHOULD);
     }
     for(BooleanClause c : prohibOrReq) {
       newq.add(c);
     }
+    newq.setBoost(obq.getBoost());
     return newq;
   }",Buggy,"SOLR-4785: Fixed bug with missing boost on toplevel query

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1486898 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix related to a missing boost on the toplevel query (SOLR-4785). The diff modifies the `parse()` method to ensure that the boost value from the original boolean query (`obq`) is applied to the new boolean query (`newq`). This suggests that the boost was previously being lost during the parsing process, which is a bug. The addition of `newq.setBoost(obq.getBoost());` directly addresses this issue. The answer is **Buggy**.
"
lucene-solr,20938.json,88eb9719fa0df2ed0477ec2bb9f20c8ab1644f85,"@@ -1,15 +1,15 @@
   private static boolean matchPath(String path, String name){
     List<String> pathSplit = StrUtils.splitSmart(path, '/');
     List<String> nameSplit = StrUtils.splitSmart(name, '/');
     for (int i = 0; i < nameSplit.size(); i++) {
       String s = nameSplit.get(i);
       String ps = pathSplit.size()>i ?  pathSplit.get(i) :null;
       if(ps == null) return false;
       if(s.equals(ps)) continue;
       if(""*"".equals(ps) && nameSplit.size()==i+1) return true;
       if(""**"".equals(ps)) return true;
       return false;
     }
-    return true;
+    return false;
 
   }",Buggy,"SOLR-6365 bug fix matching wrong  name when it is a shorter prefix of path

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1649996 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix related to path matching where a shorter prefix incorrectly matches a longer path. The code diff modifies the `matchPath` method. The original code returned `true` after iterating through all `nameSplit` elements, implying a match if all name segments matched the corresponding path segments. The corrected code now returns `false` after the loop, indicating a mismatch if the entire `name` is a prefix of `path` but not an exact match. This change addresses the bug described in the commit message. The answer is **Buggy**."
lucene-solr,46829.json,71c65184562499eba365d166fe3fabe0dbdc747b,"@@ -1,37 +1,37 @@
   public static final BinaryDocValues emptyBinary() {
     return new BinaryDocValues() {
       private int doc = -1;
       
       @Override
       public int advance(int target) {
         return doc = NO_MORE_DOCS;
       }
       
       @Override
       public boolean advanceExact(int target) throws IOException {
         doc = target;
-        return true;
+        return false;
       }
       
       @Override
       public int docID() {
         return doc;
       }
       
       @Override
       public int nextDoc() {
         return doc = NO_MORE_DOCS;
       }
       
       @Override
       public long cost() {
         return 0;
       }
 
       @Override
       public BytesRef binaryValue() {
         assert false;
         return null;
       }
     };
   }",Buggy,"LUCENE-7462: Fix buggy advanceExact impl of empty binary doc values.
",Buggy,"The commit message indicates a fix for a buggy `advanceExact` implementation for empty binary document values. The diff shows that the `advanceExact` method in the `emptyBinary` implementation of `BinaryDocValues` is changed to return `false` instead of `true`. This change aligns with the commit message, suggesting that the previous implementation had a bug where it incorrectly indicated a successful advance to the target document. The corrected implementation now returns `false`, which is the correct behavior for an empty `BinaryDocValues`. The answer is **Buggy**."
lucene-solr,21965.json,876573650b72adbf1ca005fe0f33607140c23841,"@@ -1,28 +1,28 @@
   public static SchemaField getAndCheckVersionField(IndexSchema schema) 
     throws SolrException {
-    final String errPrefix = VERSION_FIELD + ""field must exist in schema, using indexed=\""true\"" stored=\""true\"" and multiValued=\""false\"""";
+    final String errPrefix = VERSION_FIELD + "" field must exist in schema, using indexed=\""true\"" stored=\""true\"" and multiValued=\""false\"""";
     SchemaField sf = schema.getFieldOrNull(VERSION_FIELD);
 
     if (null == sf) {
       throw new SolrException
         (SolrException.ErrorCode.SERVER_ERROR, 
          errPrefix + "" ("" + VERSION_FIELD + "" does not exist)"");
     }
     if ( !sf.indexed() ) {
       throw new SolrException
         (SolrException.ErrorCode.SERVER_ERROR, 
          errPrefix + "" ("" + VERSION_FIELD + "" is not indexed"");
     }
     if ( !sf.stored() ) {
       throw new SolrException
         (SolrException.ErrorCode.SERVER_ERROR, 
          errPrefix + "" ("" + VERSION_FIELD + "" is not stored"");
     }
     if ( sf.multiValued() ) {
       throw new SolrException
         (SolrException.ErrorCode.SERVER_ERROR, 
          errPrefix + "" ("" + VERSION_FIELD + "" is multiValued"");
     }
     
     return sf;
   }",Buggy,"SOLR-5259: Fix typo in error message when _version_ field is missing

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1525620 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for a typo in an error message. The diff shows a change in the error message string, specifically correcting ""field must exist in schema, using"" to ""field must exist in schema, using"". This aligns with the commit message, indicating a bug fix related to a typographical error. The answer is **Buggy**."
lucene-solr,17330.json,be813bd0aefcf480f854a05c7880494da5e8c8bf,"@@ -1,21 +1,45 @@
   public void validateRouteValue(AddUpdateCommand cmd) throws SolrException {
 
     final Instant docTimestamp =
         parseRouteKey(cmd.getSolrInputDocument().getFieldValue(getRouteField()));
 
     // FUTURE: maybe in some cases the user would want to ignore/warn instead?
     if (docTimestamp.isAfter(Instant.now().plusMillis(getMaxFutureMs()))) {
       throw new SolrException(BAD_REQUEST,
           ""The document's time routed key of "" + docTimestamp + "" is too far in the future given "" +
               ROUTER_MAX_FUTURE + ""="" + getMaxFutureMs());
     }
 
     // Although this is also checked later, we need to check it here too to handle the case in Dimensional Routed
     // aliases where one can legally have zero collections for a newly encountered category and thus the loop later
     // can't catch this.
-    Instant startTime = parseRouteKey(start);
+
+    // SOLR-13760 - we need to fix the date math to a specific instant when the first document arrives.
+    // If we don't do this DRA's with a time dimension have variable start times across the other dimensions
+    // and logic gets much to complicated, and depends too much on queries to zookeeper. This keeps life simpler.
+    // I have to admit I'm not terribly fond of the mutation during a validate method however.
+    Instant startTime;
+    try {
+      startTime = Instant.parse(start);
+    } catch (DateTimeParseException e) {
+      startTime = DateMathParser.parseMath(new Date(), start).toInstant();
+      SolrCore core = cmd.getReq().getCore();
+      ZkStateReader zkStateReader = core.getCoreContainer().getZkController().zkStateReader;
+      Aliases aliases = zkStateReader.getAliases();
+      Map<String, String> props = new HashMap<>(aliases.getCollectionAliasProperties(aliasName));
+      start = DateTimeFormatter.ISO_INSTANT.format(startTime);
+      props.put(ROUTER_START, start);
+
+      // This could race, but it only occurs when the alias is first used and the values produced
+      // should all be identical and who wins won't matter (baring cases of Date Math involving seconds,
+      // which is pretty far fetched). Putting this in a separate thread to ensure that any failed
+      // races don't cause documents to get rejected.
+      core.runAsync(() -> zkStateReader.aliasesManager.applyModificationAndExportToZk(
+          (a) -> aliases.cloneWithCollectionAliasProperties(aliasName, props)));
+
+    }
     if (docTimestamp.isBefore(startTime)) {
       throw new SolrException(BAD_REQUEST, ""The document couldn't be routed because "" + docTimestamp +
           "" is before the start time for this alias "" +start+"")"");
     }
   }",Buggy,"SOLR-13760 - restore viability of date math in TRA start property (#879)

* SOLR-13760 - restore viability of date math in TRA start property by 
fixing the start date for time routed aliases
upon the receipt of the first document to avoid problems
with date math calculations required by DRA's
",Buggy,"The commit message indicates a bug fix related to date math calculations in time-routed aliases (TRA) within Solr. The code changes modify how the start date for these aliases is handled, specifically addressing issues where date math calculations were inconsistent due to variable start times across different dimensions. The code now fixes the start date to a specific instant upon the receipt of the first document. This involves parsing the start date, potentially using `DateMathParser` if it's a date math expression, and then updating the alias properties in ZooKeeper with the resolved start date in ISO_INSTANT format. The commit message and code changes are highly relevant, indicating a bug fix for incorrect date math calculations. The answer is **Buggy**."
lucene-solr,11507.json,75e69c5198c02e6635eed274b03ea759ef1c4818,"@@ -1,19 +1,19 @@
     public void collect(int doc) throws IOException{
       int valuesDocID = leafOutcomeValue.docID();
       if (valuesDocID < doc) {
-        valuesDocID = leafOutcomeValue.advance(valuesDocID);
+        valuesDocID = leafOutcomeValue.advance(doc);
       }
       int outcome;
       if (valuesDocID == doc) {
         outcome = (int) leafOutcomeValue.longValue();
       } else {
         outcome = 0;
       }
 
       outcome = trainingParams.positiveLabel == outcome? 1 : 0;
       if (outcome == 1) {
         positiveDocsSet.set(context.docBase + doc);
       }
       docsSet.set(context.docBase+doc);
 
     }",Buggy,"SOLR-9549: Fix bug in advancing docValues
",Buggy,"The commit message indicates a bug fix related to advancing docValues. The code diff shows that `leafOutcomeValue.advance(valuesDocID)` is changed to `leafOutcomeValue.advance(doc)`. This suggests that the previous code was advancing to the wrong document ID, potentially skipping over some documents. The change ensures that the advancement is done to the correct document ID, which aligns with the commit message indicating a bug fix. The answer is **Buggy**."
lucene-solr,6041.json,e4d4e582a0049de34990fcff3df5fb220f14ee4b,"@@ -1,5 +1,13 @@
   public void open() throws IOException {
-    Map<String, List<Tuple>> lets = streamContext.getLets();
-    List<Tuple> tuples = lets.get(name);
-    tupleIterator = tuples.iterator();
+    Map<String, Object> lets = streamContext.getLets();
+    Object o = lets.get(name);
+    List l = null;
+    if(o instanceof List) {
+        l = (List)o;
+      if(l.get(0) instanceof Tuple) {
+        tupleIterator = l.iterator();
+      } else {
+        throw new IOException(""Get was not passed a list of tuples:""+o.getClass());
+      }
+    }
   }",Buggy,"SOLR-10559: Fixed compilation error
",Buggy,"The commit message indicates a fix for a compilation error. The code changes involve modifying the type of the `lets` variable from `Map<String, List<Tuple>>` to `Map<String, Object>`. It then checks if the object retrieved from the map is an instance of `List` and if the first element of the list is an instance of `Tuple` before assigning the iterator. If the object is not a list or the list does not contain tuples, an `IOException` is thrown. This change addresses a type safety issue that likely caused a compilation error. The code now handles cases where the `lets` map contains objects other than lists of tuples, preventing runtime errors and ensuring type correctness. The answer is **Buggy**."
lucene-solr,48392.json,05d62a357711d1e4e850a5d2fb7336bf0a7acf24,"@@ -1,7 +1,9 @@
   public static double haversinSortKey(double lat1, double lon1, double lat2, double lon2) {
     double x1 = lat1 * TO_RADIANS;
     double x2 = lat2 * TO_RADIANS;
     double h1 = 1 - cos(x1 - x2);
     double h2 = 1 - cos((lon1 - lon2) * TO_RADIANS);
-    return h1 + cos(x1) * cos(x2) * h2;
+    double h = h1 + cos(x1) * cos(x2) * h2;
+    // clobber crazy precision so subsequent rounding does not create ties.
+    return Double.longBitsToDouble(Double.doubleToRawLongBits(h) & 0xFFFFFFFFFFFFFFF8L);
   }",Buggy,"LUCENE-7185: fix tie-breaker sort bug
",Buggy,The commit message indicates a fix for a tie-breaker sort bug. The code change introduces a manipulation of the double value `h` to reduce precision before returning it. This is done to prevent ties during sorting due to floating-point precision issues. The change directly addresses the bug described in the commit message. The answer is **Buggy**.
lucene-solr,50327.json,97a5295f075d37b1a31c5e77e85f7a9934cae096,"@@ -1,5 +1,5 @@
   public double getLongitude() {
     if (Math.abs(x) < MINIMUM_RESOLUTION && Math.abs(y) < MINIMUM_RESOLUTION)
       return 0.0;
-    return Math.atan2(y,z);
+    return Math.atan2(y,x);
   }",Buggy,"LUCENE-6487: Geo3D with WGS84 patch from Karl: fix bug in GeoPoint.getLongitude with test
from https://reviews.apache.org/r/34744/diff/raw/

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene6487@1682357 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix in `GeoPoint.getLongitude`. The diff shows that the `Math.atan2` function's arguments have been changed from `y, z` to `y, x`. This suggests that the original code was using the wrong coordinates to calculate the longitude, which is a bug. The answer is **Buggy**."
lucene-solr,22328.json,942750c33fc97c7f021c4831b61cb617f5cccf24,"@@ -1,69 +1,69 @@
     private long readWord(final int position) {
         if(position < 0) {
             throw new ArrayIndexOutOfBoundsException(position);
         }
 
         // First bit of the word
-        final long firstBitIndex = (position * wordLength);
+        final long firstBitIndex = ((long)position) * ((long)wordLength);
         final int firstByteIndex = (bytePadding + (int)(firstBitIndex / BITS_PER_BYTE));
         final int firstByteSkipBits = (int)(firstBitIndex % BITS_PER_BYTE);
 
         // Last bit of the word
         final long lastBitIndex = (firstBitIndex + wordLength - 1);
         final int lastByteIndex = (bytePadding + (int)(lastBitIndex / BITS_PER_BYTE));
         final int lastByteBitsToConsume;
 
         final int bitsAfterByteBoundary = (int)((lastBitIndex + 1) % BITS_PER_BYTE);
         // If the word terminates at the end of the last byte, consume the whole
         // last byte.
         if(bitsAfterByteBoundary == 0) {
             lastByteBitsToConsume = BITS_PER_BYTE;
         } else {
             // Otherwise, only consume what is necessary.
             lastByteBitsToConsume = bitsAfterByteBoundary;
         }
 
         if(lastByteIndex >= bytes.length) {
             throw new ArrayIndexOutOfBoundsException(""Word out of bounds of backing array."");
         }
 
         // Accumulator
         long value = 0;
 
         // --------------------------------------------------------------------
         // First byte
         final int bitsRemainingInFirstByte = (BITS_PER_BYTE - firstByteSkipBits);
         final int bitsToConsumeInFirstByte = Math.min(bitsRemainingInFirstByte, wordLength);
         long firstByte = (long)bytes[firstByteIndex];
 
         // Mask off the bits to skip in the first byte.
         final long firstByteMask = ((1L << bitsRemainingInFirstByte) - 1L);
         firstByte &= firstByteMask;
         // Right-align relevant bits of first byte.
         firstByte >>>= (bitsRemainingInFirstByte - bitsToConsumeInFirstByte);
 
         value |= firstByte;
 
         // If the first byte contains the whole word, short-circuit.
         if(firstByteIndex == lastByteIndex) {
             return value;
         }
 
         // --------------------------------------------------------------------
         // Middle bytes
         final int middleByteCount = (lastByteIndex - firstByteIndex - 1);
         for(int i=0; i<middleByteCount; i++) {
             final long middleByte = (bytes[firstByteIndex + i + 1] & BYTE_MASK);
             // Push middle byte onto accumulator.
             value <<= BITS_PER_BYTE;
             value |= middleByte;
         }
 
         // --------------------------------------------------------------------
         // Last byte
         long lastByte = (bytes[lastByteIndex] & BYTE_MASK);
         lastByte >>= (BITS_PER_BYTE - lastByteBitsToConsume);
         value <<= lastByteBitsToConsume;
         value |= lastByte;
         return value;
     }",Buggy,"SOLR-7954: Fixed an integer overflow bug in the HyperLogLog code used by the 'cardinality' option of stats.field to prevent ArrayIndexOutOfBoundsException in a distributed search when a large precision is selected and a large number of values exist in each shard

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1697969 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for an integer overflow bug in the HyperLogLog code, which could lead to an ArrayIndexOutOfBoundsException during distributed searches with large precision and numerous values per shard. The diff modifies the `readWord` method, specifically casting `position` to `long` in the calculation of `firstBitIndex`. This change suggests that `position * wordLength` could exceed the maximum value of an `int`, causing an overflow and leading to an incorrect `firstBitIndex`. By casting `position` to `long`, the calculation is performed using `long` arithmetic, preventing the overflow. The rest of the code remains largely the same, indicating that the primary issue was the integer overflow. The answer is **Buggy**."
lucene-solr,21543.json,3c436362810e80ce036d8785eb03abbda2d10014,"@@ -1,42 +1,42 @@
   private static List<TypeMapping> parseTypeMappings(NamedList args) {
     List<TypeMapping> typeMappings = new ArrayList<TypeMapping>();
     List<Object> typeMappingsParams = args.getAll(TYPE_MAPPING_PARAM);
     for (Object typeMappingObj : typeMappingsParams) {
       if (null == typeMappingObj) {
         throw new SolrException(SERVER_ERROR, ""'"" + TYPE_MAPPING_PARAM + ""' init param cannot be null"");
       }
       if ( ! (typeMappingObj instanceof NamedList) ) {
         throw new SolrException(SERVER_ERROR, ""'"" + TYPE_MAPPING_PARAM + ""' init param must be a <lst>"");
       }
       NamedList typeMappingNamedList = (NamedList)typeMappingObj;
 
       Object fieldTypeObj = typeMappingNamedList.remove(FIELD_TYPE_PARAM);
       if (null == fieldTypeObj) {
         throw new SolrException(SERVER_ERROR,
             ""Each '"" + TYPE_MAPPING_PARAM + ""' <lst/> must contain a '"" + FIELD_TYPE_PARAM + ""' <str>"");
       }
       if ( ! (fieldTypeObj instanceof CharSequence)) {
         throw new SolrException(SERVER_ERROR, ""'"" + FIELD_TYPE_PARAM + ""' init param must be a <str>"");
       }
       if (null != typeMappingNamedList.get(FIELD_TYPE_PARAM)) {
         throw new SolrException(SERVER_ERROR,
-            ""Each '"" + TYPE_MAPPING_PARAM + ""' <lst/> must contain a '"" + FIELD_TYPE_PARAM + ""' <str>"");
+            ""Each '"" + TYPE_MAPPING_PARAM + ""' <lst/> may contain only one '"" + FIELD_TYPE_PARAM + ""' <str>"");
       }
       String fieldType = fieldTypeObj.toString();
 
       Collection<String> valueClasses
           = FieldMutatingUpdateProcessorFactory.oneOrMany(typeMappingNamedList, VALUE_CLASS_PARAM);
       if (valueClasses.isEmpty()) {
         throw new SolrException(SERVER_ERROR, 
             ""Each '"" + TYPE_MAPPING_PARAM + ""' <lst/> must contain at least one '"" + VALUE_CLASS_PARAM + ""' <str>"");
       }
       typeMappings.add(new TypeMapping(fieldType, valueClasses));
 
       if (0 != typeMappingNamedList.size()) {
         throw new SolrException(SERVER_ERROR, 
             ""Unexpected '"" + TYPE_MAPPING_PARAM + ""' init sub-param(s): '"" + typeMappingNamedList.toString() + ""'"");
       }
       args.remove(TYPE_MAPPING_PARAM);
     }
     return typeMappings;
   }",Buggy,"SOLR-4894: fix error message

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1503275 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message ""SOLR-4894: fix error message"" indicates a bug fix related to an error message. The diff modifies the error message thrown when a NamedList contains more than one FIELD_TYPE_PARAM. The original message incorrectly stated that the FIELD_TYPE_PARAM was missing, while the corrected message accurately states that only one FIELD_TYPE_PARAM is allowed. Therefore, the changes indicate a bug fix. The answer is **Buggy**."
lucene-solr,39255.json,33d18a0c599a5bc294f9503a6b8fa3e326f589a7,"@@ -1,114 +1,116 @@
   private void  setInternalDependencyProperties() {
     log(""Loading module dependencies from: "" + moduleDependenciesPropertiesFile, verboseLevel);
     Properties moduleDependencies = new Properties();
     try (InputStream inputStream = new FileInputStream(moduleDependenciesPropertiesFile);
          Reader reader = new InputStreamReader(inputStream, StandardCharsets.UTF_8)) {
       moduleDependencies.load(reader);
     } catch (FileNotFoundException e) {
       throw new BuildException(""Properties file does not exist: "" + moduleDependenciesPropertiesFile.getPath());
     } catch (IOException e) {
       throw new BuildException(""Exception reading properties file "" + moduleDependenciesPropertiesFile.getPath(), e);
     }
     Map<String,SortedSet<String>> testScopeDependencies = new HashMap<>();
     Map<String, String> testScopePropertyKeys = new HashMap<>();
     for (Map.Entry entry : moduleDependencies.entrySet()) {
       String newPropertyKey = (String)entry.getKey();
       StringBuilder newPropertyValue = new StringBuilder();
       String value = (String)entry.getValue();
       Matcher matcher = MODULE_DEPENDENCIES_COORDINATE_KEY_PATTERN.matcher(newPropertyKey);
       if ( ! matcher.matches()) {
         throw new BuildException(""Malformed module dependencies property key: '"" + newPropertyKey + ""'"");
       }
       String antProjectName = matcher.group(1);
       boolean isTest = null != matcher.group(2);
       String artifactName = antProjectToArtifactName(antProjectName);
       newPropertyKey = artifactName + (isTest ? "".internal.test"" : "".internal"") + "".dependencies""; // Add "".internal""
       if (isTest) {
         testScopePropertyKeys.put(artifactName, newPropertyKey);
       }
       if (null == value || value.isEmpty()) {
         allProperties.setProperty(newPropertyKey, """");
         Map<String,SortedSet<String>> scopedDependencies
             = isTest ? testScopeDependencies : internalCompileScopeDependencies;
         scopedDependencies.put(artifactName, new TreeSet<String>());
       } else {
         // Lucene analysis modules' build dirs do not include hyphens, but Solr contribs' build dirs do
         String origModuleDir = antProjectName.replace(""analyzers-"", ""analysis/"");
+        // Exclude the module's own build output, in addition to UNWANTED_INTERNAL_DEPENDENCIES
         Pattern unwantedInternalDependencies = Pattern.compile
-            (""(?:lucene/build/|solr/build/(?:contrib/)?)"" + origModuleDir + ""|"" + UNWANTED_INTERNAL_DEPENDENCIES);
+            (""(?:lucene/build/|solr/build/(?:contrib/)?)"" + origModuleDir + ""/"" // require dir separator 
+             + ""|"" + UNWANTED_INTERNAL_DEPENDENCIES);
         SortedSet<String> sortedDeps = new TreeSet<>();
         for (String dependency : value.split("","")) {
           matcher = SHARED_EXTERNAL_DEPENDENCIES_PATTERN.matcher(dependency);
           if (matcher.find()) {
             String otherArtifactName = matcher.group(1);
             boolean isTestScope = null != matcher.group(2) && matcher.group(2).length() > 0;
             otherArtifactName = otherArtifactName.replace('/', '-');
             otherArtifactName = otherArtifactName.replace(""lucene-analysis"", ""lucene-analyzers"");
             otherArtifactName = otherArtifactName.replace(""solr-contrib-solr-"", ""solr-"");
             otherArtifactName = otherArtifactName.replace(""solr-contrib-"", ""solr-"");
             if ( ! otherArtifactName.equals(artifactName)) {
               Map<String,Set<String>> sharedDeps
                   = isTest ? interModuleExternalTestScopeDependencies : interModuleExternalCompileScopeDependencies;
               Set<String> sharedSet = sharedDeps.get(artifactName);
               if (null == sharedSet) {
                 sharedSet = new HashSet<>();
                 sharedDeps.put(artifactName, sharedSet);
               }
               if (isTestScope) {
                 otherArtifactName += "":test"";
               }
               sharedSet.add(otherArtifactName);
             }
           }
           matcher = unwantedInternalDependencies.matcher(dependency);
           if (matcher.find()) {
             continue;  // skip external (/(test-)lib/), and non-jar and unwanted (self) internal deps
           }
           String artifactId = dependencyToArtifactId(newPropertyKey, dependency);
           String groupId = ""org.apache."" + artifactId.substring(0, artifactId.indexOf('-'));
           String coordinate = groupId + ':' + artifactId;
           sortedDeps.add(coordinate);
         }
         if (isTest) {  // Don't set test-scope properties until all compile-scope deps have been seen
           testScopeDependencies.put(artifactName, sortedDeps);
         } else {
           internalCompileScopeDependencies.put(artifactName, sortedDeps);
           for (String dependency : sortedDeps) {
             int splitPos = dependency.indexOf(':');
             String groupId = dependency.substring(0, splitPos);
             String artifactId = dependency.substring(splitPos + 1);
             appendDependencyXml(newPropertyValue, groupId, artifactId, ""    "", null, false, false, null, null);
           }
           if (newPropertyValue.length() > 0) {
             newPropertyValue.setLength(newPropertyValue.length() - 1); // drop trailing newline
           }
           allProperties.setProperty(newPropertyKey, newPropertyValue.toString());
         }
       }
     }
     // Now that all compile-scope dependencies have been seen, include only those test-scope
     // dependencies that are not also compile-scope dependencies.
     for (Map.Entry<String,SortedSet<String>> entry : testScopeDependencies.entrySet()) {
       String module = entry.getKey();
       SortedSet<String> testDeps = entry.getValue();
       SortedSet<String> compileDeps = internalCompileScopeDependencies.get(module);
       if (null == compileDeps) {
         throw new BuildException(""Can't find compile scope dependencies for module "" + module);
       }
       StringBuilder newPropertyValue = new StringBuilder();
       for (String dependency : testDeps) {
         // modules with separate compile-scope and test-scope POMs need their compile-scope deps
         // included in their test-scope deps.
         if (modulesWithSeparateCompileAndTestPOMs.contains(module) || ! compileDeps.contains(dependency)) {
           int splitPos = dependency.indexOf(':');
           String groupId = dependency.substring(0, splitPos);
           String artifactId = dependency.substring(splitPos + 1);
           appendDependencyXml(newPropertyValue, groupId, artifactId, ""    "", null, true, false, null, null);
         }
       }
       if (newPropertyValue.length() > 0) {
         newPropertyValue.setLength(newPropertyValue.length() - 1); // drop trailing newline
       }
       allProperties.setProperty(testScopePropertyKeys.get(module), newPropertyValue.toString());
     }
   }",Buggy,"LUCENE-6607: Fix spatial3d module's Maven config - include dependency interpolation sites, make packaging jar instead of pom, don't skip deploy phase, etc.; and fix GetMavenDependenciesTask to exclude a module's build artifacts only if the build dir fully matches, rather than a prefix (this bug caused lucene-spatial's test dependency on the lucene-spatial3d jar to be left out of the generated POM, because lucene/build/spatial3d matched the regex for lucene-spatial's build output dir: 'lucene/build/spatial', i.e. with no dir separator)

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1690842 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix in `GetMavenDependenciesTask`, specifically related to excluding a module's build artifacts. The bug caused incorrect dependency exclusion due to a prefix-based directory matching logic. The diff modifies the regex pattern `unwantedInternalDependencies` to require a directory separator `/` at the end of the module's build directory path. This ensures that only exact matches of the build directory are excluded, resolving the bug where a parent directory was incorrectly matched. The answer is **Buggy**."
lucene-solr,46217.json,4cfc012bfa66d31b129495b258b5537663f37a10,"@@ -1,4 +1,7 @@
   public IndexWriterConfig setCodec(Codec codec) {
+    if (codec == null) {
+      throw new NullPointerException();
+    }
     this.codec = codec;
     return this;
   }",Buggy,"fix test bug (and fix IWC to fail immediately if you do this)

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene4547@1439519 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix in a test and a modification to IndexWriterConfig to fail immediately if a null codec is set. The diff adds a null check for the codec parameter in the setCodec method, throwing a NullPointerException if it's null. This prevents potential issues later on if a null codec is used, aligning with the commit message's intention to fail immediately. The answer is **Buggy**."
lucene-solr,49999.json,a14972d7ae2357ab7150909332a04c9a1a94474c,"@@ -1,3 +1,3 @@
     int docId() {
-      return doc > 1 ? NumericDocValues.NO_MORE_DOCS : doc;
+      return doc > 0 ? NumericDocValues.NO_MORE_DOCS : doc;
     }",Buggy,"LUCENE-8055: MemoryIndex.MemoryDocValuesIterator returns 2 documents instead of 1

Fixes a bug if there is a DV field in the MemoryIndex the
`MemoryIndex.MemoryDocValuesIterator` will return 2 documents instead of 1.
",Buggy,"The commit message indicates a bug in `MemoryIndex.MemoryDocValuesIterator` where it returns two documents instead of one. The diff modifies the `docId()` method, changing the condition from `doc > 1` to `doc > 0`. This change ensures that `NO_MORE_DOCS` is returned when `doc` is greater than 0, effectively preventing the iterator from returning an extra document. The answer is **Buggy**."
lucene-solr,5155.json,ebd130b7e2720a4a5cfc3c542461f61932caadf0,"@@ -1,44 +1,54 @@
   public Tuple read() throws IOException {
 
     if(finished) {
       Map<String,Object> m = new HashMap<>();
       m.put(""EOF"", true);
       return new Tuple(m);
     } else {
       finished = true;
       Map<String, Object> values = new HashMap<>();
 
-      String plot= stringParams.get(""type"");
-      StreamEvaluator xvalues = evaluatorParams.get(""x"");
-      StreamEvaluator yvalues = evaluatorParams.get(""y"");
+      // add all string based params
+      // these could come from the context, or they will just be treated as straight strings
+      for(Entry<String,String> param : stringParams.entrySet()){
+        if(streamContext.getLets().containsKey(param.getValue())){
+          values.put(param.getKey(), streamContext.getLets().get(param.getValue()));
+        }
+        else{
+          values.put(param.getKey(), param.getValue());
+        }
+      }
 
-      List<Number> y = (List<Number>)yvalues.evaluateOverContext();
-      List<Number> x = null;
+      // add all evaluators
+      for(Entry<String,StreamEvaluator> param : evaluatorParams.entrySet()){
+        values.put(param.getKey(), param.getValue().evaluateOverContext());
+      }
 
-      if(xvalues == null) {
+      List<Number> y = (List<Number>)values.get(""y"");
+      List<Number> x = (List<Number>)values.get(""x"");
+
+      if(x == null) {
         //x is null so add a sequence
         x = new ArrayList();
         for(int i=0; i<y.size(); i++) {
           x.add(i+1);
         }
-      } else {
-        x = (List<Number>) xvalues.evaluateOverContext();
       }
 
       List<List<Number>> xy = new ArrayList();
       for(int i=0; i<x.size(); i++) {
         List<Number> pair = new ArrayList();
         pair.add(x.get(i));
         pair.add(y.get(i));
         xy.add(pair);
       }
 
-      values.put(""plot"", plot);
+      values.put(""plot"", values.get(""type""));
       values.put(""data"", xy);
 
       Tuple tup = new Tuple(values);
       tup.fieldLabels = fieldLabels;
       tup.fieldNames = fieldNames;
       return tup;
     }
   }",Buggy,"SOLR-10802: Fix problem with variable assignment
",Buggy,"The commit message indicates a fix related to variable assignment. The code changes involve refactoring how parameters are handled. The original code directly accessed `stringParams` and `evaluatorParams`. The modified code iterates through these parameters, retrieves values from the `streamContext` if available, and populates the `values` map. The x and y values are then retrieved from this map. Finally, the ""plot"" value is also retrieved from the `values` map using the key ""type"". This suggests that the original code might have had issues with how parameters were being accessed or assigned, potentially leading to incorrect behavior. The changes ensure that parameters are consistently handled and that values from the context are prioritized. The answer is **Buggy**."
lucene-solr,7768.json,e81dd4e870d2a9b27e1f4366e92daa6dba054da8,"@@ -1,5 +1,6 @@
     public boolean isNodeAlive(String node) {
-      if (zkClientClusterStateProvider != null && zkClientClusterStateProvider.getLiveNodes().contains(node))
-        return true;
+      if (zkClientClusterStateProvider != null) {
+        return zkClientClusterStateProvider.getLiveNodes().contains(node);
+      }
       return true;
     }",Buggy,"SOLR-12977: fixed bug
",Buggy,"The commit message indicates a bug fix. The code change adds a conditional block around the `zkClientClusterStateProvider.getLiveNodes().contains(node)` call. Previously, if `zkClientClusterStateProvider` was null, the method would return true without checking the live nodes. Now, it only checks the live nodes if `zkClientClusterStateProvider` is not null. This change likely fixes a bug where the method incorrectly reported nodes as alive when the `zkClientClusterStateProvider` was not initialized or available. The answer is **Buggy**."
lucene-solr,13947.json,9cfba4a728e38a7e6c59c60a377420abc769be46,"@@ -1,39 +1,45 @@
   public static QParser getParser(String qstr, String parserName, boolean allowLocalParams, SolrQueryRequest req) throws SyntaxError {
     // SolrParams localParams = QueryParsing.getLocalParams(qstr, req.getParams());
     if (parserName == null) {
       parserName = QParserPlugin.DEFAULT_QTYPE;//""lucene""
     }
     String stringIncludingLocalParams = qstr;
     ModifiableSolrParams localParams = null;
     SolrParams globalParams = req.getParams();
     boolean valFollowedParams = true;
     int localParamsEnd = -1;
 
     if (allowLocalParams && qstr != null && qstr.startsWith(QueryParsing.LOCALPARAM_START)) {
       localParams = new ModifiableSolrParams();
       localParamsEnd = QueryParsing.parseLocalParams(qstr, 0, localParams, globalParams);
 
       String val = localParams.get(QueryParsing.V);
       if (val != null) {
         // val was directly specified in localParams via v=<something> or v=$arg
         valFollowedParams = false;
         //TODO if remainder of query string after '}' is non-empty, then what? Throw error? Fall back to lucene QParser?
       } else {
         // use the remainder of the string as the value
         valFollowedParams = true;
         val = qstr.substring(localParamsEnd);
         localParams.set(QueryParsing.V, val);
       }
 
       parserName = localParams.get(QueryParsing.TYPE,parserName);
-      qstr = localParams.get(""v"");
+      qstr = localParams.get(QueryParsing.V);
     }
 
     QParserPlugin qplug = req.getCore().getQueryPlugin(parserName);
+    if (qplug == null) {
+      // there should a way to include parameter for which parsing failed
+      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,
+          ""invalid query parser '"" + parserName + (stringIncludingLocalParams == null?
+              ""'"": ""' for query '"" + stringIncludingLocalParams + ""'""));
+    }
     QParser parser =  qplug.createParser(qstr, localParams, req.getParams(), req);
 
     parser.stringIncludingLocalParams = stringIncludingLocalParams;
     parser.valFollowedParams = valFollowedParams;
     parser.localParamsEnd = localParamsEnd;
     return parser;
   }",Buggy,"SOLR-13187: Fix NPE when invalid qParser is specified

* When non-existent qParser is specified return 400 error code
* SOLR-13197: Fix NPE when createQParser is called in StatsField
",Buggy,"The commit message indicates a fix for a NullPointerException (NPE) that occurs when an invalid `qParser` is specified. The diff adds a check to ensure that the `QParserPlugin` exists before attempting to create a `QParser`. If the plugin is not found, a `SolrException` with a `BAD_REQUEST` error code is thrown, providing a more informative error message. This prevents the NPE and provides better error handling for invalid `qParser` specifications. The answer is **Buggy**.
"
lucene-solr,47147.json,2d422a995af8132e3d39d5ef576216a3105c457a,"@@ -1,9 +1,14 @@
   public void close() throws IOException {
     try (final OutputStream o = os) {
       // We want to make sure that os.flush() was running before close:
       // BufferedOutputStream may ignore IOExceptions while flushing on close().
-      // TODO: this is no longer an issue in Java 8:
-      // http://hg.openjdk.java.net/jdk8/tl/jdk/rev/759aa847dcaf
-      o.flush();
+      // We keep this also in Java 8, although it claims to be fixed there,
+      // because there are more bugs around this! See:
+      // # https://bugs.openjdk.java.net/browse/JDK-7015589
+      // # https://bugs.openjdk.java.net/browse/JDK-8054565
+      if (!flushedOnClose) {
+        flushedOnClose = true; // set this BEFORE calling flush!
+        o.flush();
+      }
     }
   }",Buggy,"LUCENE-6152: Fix double close bug in OutputStreamIndexOutput

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1648724 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for a ""double close bug"". The code change introduces a `flushedOnClose` flag to ensure that the output stream's `flush()` method is called only once during the `close()` operation. This prevents potential issues related to flushing on close, especially considering known bugs in Java versions. The addition of the `flushedOnClose` flag and the conditional `o.flush()` call directly address the reported bug. The answer is **Buggy**.
"
lucene-solr,5763.json,8dddd88d3054596b7afb536429b42792145fdffe,"@@ -1,51 +1,49 @@
   protected void constructStreams() throws IOException {
 
     try {
 
       ZkStateReader zkStateReader = cloudSolrClient.getZkStateReader();
       ClusterState clusterState = zkStateReader.getClusterState();
 
       //System.out.println(""Connected to zk an got cluster state."");
 
       Collection<Slice> slices = clusterState.getActiveSlices(this.collection);
 
       if(slices == null) {
-
-        String colLower = this.collection.toLowerCase(Locale.getDefault());
         //Try case insensitive match
         for(String col : clusterState.getCollections()) {
-          if(col.toLowerCase(Locale.getDefault()).equals(colLower)) {
+          if(col.equalsIgnoreCase(this.collection)) {
             slices = clusterState.getActiveSlices(col);
             break;
           }
         }
 
         if(slices == null) {
           throw new Exception(""Collection not found:"" + this.collection);
         }
       }
 
       params.put(""distrib"",""false""); // We are the aggregator.
 
       for(Slice slice : slices) {
         Collection<Replica> replicas = slice.getReplicas();
         List<Replica> shuffler = new ArrayList();
         for(Replica replica : replicas) {
           shuffler.add(replica);
         }
 
         Collections.shuffle(shuffler, new Random());
         Replica rep = shuffler.get(0);
         ZkCoreNodeProps zkProps = new ZkCoreNodeProps(rep);
         String url = zkProps.getCoreUrl();
         SolrStream solrStream = new SolrStream(url, params);
         if(streamContext != null) {
           solrStream.setStreamContext(streamContext);
         }
         solrStream.setFieldMappings(this.fieldMappings);
         solrStreams.add(solrStream);
       }
     } catch (Exception e) {
       throw new IOException(e);
     }
   }",Buggy,"Fix more locale bugs in SQL handler ans streams
",Buggy,"The commit message indicates fixes for locale bugs. The code diff changes the way collection names are compared in a case-insensitive manner. It replaces `toLowerCase(Locale.getDefault()).equals()` with `equalsIgnoreCase()`. This change addresses potential issues where the default locale might not be appropriate for comparing collection names, which could lead to incorrect collection selection. This is a bug fix. The answer is **Buggy**."
lucene-solr,21252.json,ad453aeeda705e120fb53ba5e8c607cb0a13d85d,"@@ -1,10 +1,10 @@
   private void ensureLog() {
     if (tlog == null) {
-      String newLogName = String.format(Locale.ENGLISH, ""%s.%019d"", TLOG_NAME, id);
+      String newLogName = String.format(Locale.ENGLISH, LOG_FILENAME_PATTERN, TLOG_NAME, id);
       try {
         tlog = new TransactionLog(new File(tlogDir, newLogName), globalStrings);
       } catch (IOException e) {
         throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, ""Can't open new tlog!"", e);
       }
     }
   }",Buggy,"SOLR-3206: fixed the test, will follow up with a root problem fix too.

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1297927 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for a test, and the code change involves modifying the way a new log name is generated using `String.format`. Specifically, it replaces a hardcoded format string with a constant `LOG_FILENAME_PATTERN`. While this change itself doesn't scream ""bug fix"" in the sense of correcting a logical error, the commit message explicitly states that it's fixing a test. This suggests that the original log naming scheme was causing the test to fail, implying a bug in the test setup or environment. Therefore, the change is related to fixing a problem, even if it's primarily test-related. The answer is **Buggy**."
lucene-solr,23008.json,da832d4f3aa2e87cf1947ef4373977778ea6d2e0,"@@ -1,30 +1,30 @@
       public Map.Entry<String, String> next() {
         if (!hasNext()) {
           throw new NoSuchElementException();
         }
 
         if (headerValue == null || !headerValue.hasMoreElements()) {
           headerName = headerNameIt.nextElement();
           headerValue = request.getHeaders(headerName);
         }
 
         String key = headerName;
         String val = headerValue.nextElement();
 
-        return new Map.Entry<>() {
+        return new Map.Entry<String, String>() {
           @Override
           public String getKey() {
             return key;
           }
 
           @Override
           public String getValue() {
             return val;
           }
 
           @Override
           public String setValue(String value) {
             throw new UnsupportedOperationException();
           }
         };
       }",Buggy,"SOLR-13434: Fixes problem on Java 8 build
",Buggy,"The commit message indicates a fix for a problem on Java 8 builds. The code changes involve the anonymous class definition for `Map.Entry`. Specifically, the addition of `<String, String>` to `new Map.Entry<>()` suggests a fix related to generic type inference, which might behave differently in Java 8 compared to other versions. This implies a bug fix related to compilation or runtime behavior specific to Java 8. The answer is **Buggy**."
lucene-solr,22004.json,9374fcc6e91c841772fd6d26a7599abcd01dba38,"@@ -1,8 +1,8 @@
   public Query rewrite(IndexReader reader) throws IOException {
     Query rewritten = in.rewrite(reader);
     if (rewritten != in) {
-      return new DeleteByQueryWrapper(in, schema);
+      return new DeleteByQueryWrapper(rewritten, schema);
     } else {
       return this;
     }
   }",Buggy,"LUCENE-5666: fix rewrite bug

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5666@1594418 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix related to query rewriting. The diff shows that the `rewrite` method now correctly wraps the rewritten query (`rewritten`) instead of the original query (`in`) in a `DeleteByQueryWrapper`. This ensures that the delete operation is performed on the rewritten, optimized query, which is the intended behavior. The original code had a bug where it was wrapping the original query, leading to incorrect deletion results. The answer is **Buggy**."
lucene-solr,12659.json,30de6c512ae51391159b28e0250a9482f12690c7,"@@ -1,23 +1,24 @@
   public LeafCollector getLeafCollector(AtomicReaderContext context) throws IOException {
     if (SortingMergePolicy.isSorted(context.reader(), sort)) {
       // segment is sorted, can early-terminate
       return new FilterLeafCollector(super.getLeafCollector(context)) {
+        private int numCollected;
 
         @Override
         public void collect(int doc) throws IOException {
           super.collect(doc);
           if (++numCollected >= numDocsToCollect) {
             throw new CollectionTerminatedException();
           }
         }
 
         @Override
         public boolean acceptsDocsOutOfOrder() {
           return false;
         }
 
       };
     } else {
       return super.getLeafCollector(context);
     }
   }",Buggy,"LUCENE-5623: fix bug in earlyterminatingcollector, fix test to be reproducible and more evil

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1588953 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix in `EarlyTerminatingCollector`. The diff introduces a counter `numCollected` within the `FilterLeafCollector` to track the number of collected documents. It increments this counter in the `collect` method and throws a `CollectionTerminatedException` when `numCollected` reaches `numDocsToCollect`. This mechanism allows early termination of collection when the desired number of documents has been collected, which likely addresses a bug where the collection process was not terminating as expected. The answer is **Buggy**."
jna,352.json,af1ef9330f4ef7d4f3618d5c4edc8e49b2b4862a,"@@ -1,5 +1,5 @@
-    public static final String getPrivateProfileString(final String appName, final String keyName, final String defaultValue, final String fileName) {
+    public static final String getPrivateProfileString(final String lpAppName, final String lpKeyName, final String lpDefault, final String lpFileName) {
         final char buffer[] = new char[1024];
         Kernel32.INSTANCE.GetPrivateProfileString(appName, keyName, defaultValue, buffer, new DWORD(buffer.length), fileName);
         return Native.toString(buffer);
     }",Buggy,"fix javadoc errors/warnings
",NotBuggy,"The commit message indicates a fix for Javadoc errors/warnings. The diff shows a change in parameter names from `appName`, `keyName`, `defaultValue`, `fileName` to `lpAppName`, `lpKeyName`, `lpDefault`, `lpFileName`. This change does not affect the functionality of the code. It's likely done to align the parameter names with the naming conventions used in the underlying native API (Kernel32.INSTANCE.GetPrivateProfileString). This change improves the documentation and readability of the code, but it doesn't fix a bug in the code's logic or error handling. The answer is **NotBuggy**."
jna,3664.json,fce337b823bcc6130313cd71740b053abf681368,"@@ -1,7 +1,7 @@
     public void read() {
         boolean returnWide = original instanceof WString[];
         for (int si=0;si < original.length;si++) {
             String s = getPointer(si * Pointer.SIZE).getString(0, wide);
-            original[si] = returnWide ? new WString(s) : s; 
+            original[si] = returnWide ? new WString(s) : (Object)s; 
         }
     }",Buggy,"Fix 1.4 compile error

git-svn-id: https://svn.java.net/svn/jna~svn/trunk@294 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80
",Buggy,"The commit message indicates a fix for a compilation error related to Java version 1.4. The code change involves casting `s` to `Object` in the line `original[si] = returnWide ? new WString(s) : (Object)s;`. This suggests that without the cast, there was a type mismatch issue that prevented the code from compiling correctly in Java 1.4. This is a bug fix. The answer is **Buggy**."
jna,3945.json,563931750bb0f5af89da6d55030bdfd4c8854906,"@@ -1,3 +1,3 @@
     public int hashCode() {
-        return callFlags + options.hashCode() + peer.hashCode();
+        return callFlags + options.hashCode() + super.hashCode();
     }",Buggy,"fix compiler error

git-svn-id: https://svn.java.net/svn/jna~svn/trunk@1143 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80
",Buggy,"The commit message indicates a fix for a compiler error. The diff shows a change in the `hashCode()` method, replacing `peer.hashCode()` with `super.hashCode()`. This suggests that the original code was attempting to call `hashCode()` on a field named `peer`, which likely caused a compiler error because `peer` either didn't have a `hashCode()` method or was not accessible in that context. Replacing it with `super.hashCode()` resolves the error by calling the `hashCode()` method of the superclass. This correction directly addresses a compilation issue, indicating a bug fix. The answer is **Buggy**.
"
jna,4035.json,40f00b1dcba4c24b37452d05cb3118bdbc1480f3,"@@ -1,29 +1,31 @@
     public void setValue(long value) {
         long truncated = value;
         switch (size) {
         case 1:
             truncated = (byte) value;
             this.value = new Byte((byte) value);
             break;
         case 2:
             truncated = (short) value;
             this.value = new Short((short) value);
             break;
         case 4:
             truncated = (int) value;
             this.value = new Integer((int) value);
             break;
         case 8:
             this.value = new Long(value);
             break;
         default:
             throw new IllegalArgumentException(""Unsupported size: "" + size);
         }
-        long mask = (-1L >> size * 8) << (size * 8);
-        if ((value < 0 && truncated != value)
-            || (value >= 0 && (mask & value) != 0)) {
-            throw new IllegalArgumentException(""Argument (0x""
-                + Long.toHexString(value) + "") exceeds native capacity (""
-                + size + "" bytes)"");
+        if (size < 8) {
+            long mask = ~((1L << (size*8)) - 1);
+            if ((value < 0 && truncated != value)
+                    || (value >= 0 && (mask & value) != 0)) {
+                throw new IllegalArgumentException(""Argument value 0x""
+                        + Long.toHexString(value) + "" exceeds native capacity (""
+                        + size + "" bytes) mask=0x"" + Long.toHexString(mask));
+            }
         }
     }",Buggy,"Fix bug checking IntegerType limits

git-svn-id: https://svn.java.net/svn/jna~svn/trunk@391 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80
",Buggy,The commit message indicates a bug fix related to checking IntegerType limits. The code diff modifies the `setValue` method to correctly validate the input `value` against the specified `size`. The original code used a bitwise right shift `(-1L >> size * 8)` which has undefined behavior when `size * 8` is 64. The corrected code calculates the mask using `~((1L << (size*8)) - 1)` which is more robust. The addition of the `size < 8` condition ensures that the mask calculation is only performed for sizes less than 8 bytes. The exception message is also updated to include the mask value. This indicates a bug fix. The answer is **Buggy**.
spring-framework,28571.json,582864802e49f0cc47575be00194e4194d7b1dc5,"@@ -1,8 +1,11 @@
 	public ModelMap getModel() {
 		if (useDefaultModel()) {
 			return this.defaultModel;
 		}
 		else {
-			return (this.redirectModel != null) ? this.redirectModel : new ModelMap();
+			if (this.redirectModel == null) {
+				this.redirectModel = new ModelMap();
+			}
+			return this.redirectModel;
 		}
 	}",Buggy,"Ensure RedirectModel is initialized

This commit fixes an old bug in ModelAndViewContainer where getModel
returns a new ModelMap instance that isn't saved and re-used.

Issue: SPR-14045
",Buggy,"The commit message indicates a bug fix in `ModelAndViewContainer` related to the `getModel` method and the initialization of `redirectModel`. The diff shows that the code now ensures `redirectModel` is initialized before being returned. Previously, a new `ModelMap` instance was created and returned without being saved, leading to the bug. The corrected code initializes `redirectModel` if it's null and then returns it, ensuring that the same model is reused across requests. The answer is **Buggy**."
spring-framework,5833.json,ee95f171c8a934080421e802fa6868843af1c45f,"@@ -1,4 +1,4 @@
 	public PropertySources getAppliedPropertySources() throws IllegalStateException {
-		Assert.state(this.appliedPropertySources != null, ""PropertySources have not get been applied"");
+		Assert.state(this.appliedPropertySources != null, ""PropertySources have not yet been applied"");
 		return this.appliedPropertySources;
 	}",Buggy,"Update PropertySourcesPlaceholderConfigurer.java

Fix the error message when PropertySources are not applied",Buggy,"The commit message indicates a fix to an error message. The diff changes ""PropertySources have not get been applied"" to ""PropertySources have not yet been applied"". This is a minor correction to improve the clarity and correctness of the error message. The change addresses a textual issue, which can be considered a bug fix in the sense of improving user experience and providing accurate information. The answer is **Buggy**."
spring-framework,20130.json,7e799295e55f5e5984a1040a016169ea0883bc2d,"@@ -1,10 +1,10 @@
 	public HttpInputMessage beforeBodyRead(HttpInputMessage request, MethodParameter parameter,
-			Type targetType, Class<? extends HttpMessageConverter<?>> converterType) {
+			Type targetType, Class<? extends HttpMessageConverter<?>> converterType) throws IOException {
 
 		for (RequestBodyAdvice advice : getMatchingAdvice(parameter, RequestBodyAdvice.class)) {
 			if (advice.supports(parameter, targetType, converterType)) {
 				request = advice.beforeBodyRead(request, parameter, targetType, converterType);
 			}
 		}
 		return request;
 	}",Buggy,"Fix compile error
",Buggy,"The commit message indicates a fix for a compile error. The diff shows a change in the `beforeBodyRead` method signature, specifically the addition of `throws IOException`. This suggests that the original code was missing an exception declaration, leading to a compile-time error. The addition of `throws IOException` resolves this issue, indicating a bug fix related to exception handling. The answer is **Buggy**."
spring-framework,24540.json,f084b632864685d8e65d0cdfa719a813824fc59f,"@@ -1,31 +1,36 @@
 	private static String decodeHeaderFieldParam(String input) {
 		Assert.notNull(input, ""Input String should not be null"");
 		int firstQuoteIndex = input.indexOf('\'');
 		int secondQuoteIndex = input.indexOf('\'', firstQuoteIndex + 1);
 		// US_ASCII
 		if (firstQuoteIndex == -1 || secondQuoteIndex == -1) {
 			return input;
 		}
 		Charset charset = Charset.forName(input.substring(0, firstQuoteIndex));
 		Assert.isTrue(UTF_8.equals(charset) || ISO_8859_1.equals(charset),
 				""Charset should be UTF-8 or ISO-8859-1"");
 		byte[] value = input.substring(secondQuoteIndex + 1, input.length()).getBytes(charset);
 		ByteArrayOutputStream bos = new ByteArrayOutputStream();
 		int index = 0;
 		while (index < value.length) {
 			byte b = value[index];
 			if (isRFC5987AttrChar(b)) {
 				bos.write((char) b);
 				index++;
 			}
-			else if (b == '%') {
-				char[] array = { (char)value[index + 1], (char)value[index + 2]};
-				bos.write(Integer.parseInt(String.valueOf(array), 16));
+			else if (b == '%' && index < value.length - 2) {
+				char[] array = new char[]{(char) value[index + 1], (char) value[index + 2]};
+				try {
+					bos.write(Integer.parseInt(String.valueOf(array), 16));
+				}
+				catch (NumberFormatException ex) {
+					throw new IllegalArgumentException(INVALID_HEADER_FIELD_PARAMETER_FORMAT, ex);
+				}
 				index+=3;
 			}
 			else {
-				throw new IllegalArgumentException(""Invalid header field parameter format (as defined in RFC 5987)"");
+				throw new IllegalArgumentException(INVALID_HEADER_FIELD_PARAMETER_FORMAT);
 			}
 		}
 		return new String(bos.toByteArray(), charset);
 	}",Buggy,"Fix ""array index out of bounds"" problem reported by LGTM.com
",Buggy,"The commit message indicates a fix for an ""array index out of bounds"" problem. The code diff shows a modification within the `decodeHeaderFieldParam` method, specifically in the handling of percent-encoded characters (`%`). The original code directly accessed `value[index + 1]` and `value[index + 2]` without checking if `index + 2` was within the bounds of the `value` array. This could lead to an `ArrayIndexOutOfBoundsException` if a percent sign appeared near the end of the array.

The corrected code adds a check `index < value.length - 2` before accessing `value[index + 1]` and `value[index + 2]`. This prevents the out-of-bounds access. Additionally, a try-catch block is added to handle `NumberFormatException` which might occur during `Integer.parseInt`.

The changes directly address the potential `ArrayIndexOutOfBoundsException` and the commit message accurately describes the fix.

The answer is **Buggy**.
"
sonarqube,18348.json,9b57d0b379935e053ef050b0379c65fc32810bc1,"@@ -1,3 +1,3 @@
   public boolean isEnabled() {
-    return getRule().isEnabled();
+    return getRule()!=null && getRule().isEnabled();
   }",Buggy,"Fix bug on deprecated profiles
",Buggy,"The commit message indicates a bug fix related to deprecated profiles. The code change adds a null check for `getRule()` before calling `isEnabled()`. This prevents a potential `NullPointerException` if `getRule()` returns null, which would likely cause the profile to not function correctly. This is a bug fix. The answer is **Buggy**."
sonarqube,4866.json,56963334491068e70db0695c7fc36d9c40bfca9a,"@@ -1,6 +1,7 @@
   private void checkNoOtherMetricWithTargetKey(DbSession dbSession, MetricDto metricInDb, MetricDto template) {
-    MetricDto metricWithTargetKey = dbClient.metricDao().selectNullableByKey(dbSession, template.getKey());
+    String targetKey = template.getKey();
+    MetricDto metricWithTargetKey = dbClient.metricDao().selectNullableByKey(dbSession, targetKey);
     if (isMetricFoundInDb(metricWithTargetKey) && !metricInDb.getId().equals(metricWithTargetKey.getId())) {
-      throw new ServerException(HttpURLConnection.HTTP_CONFLICT, ""A me metric exists with the key: "" + metricInDb.getKey());
+      throw new ServerException(HttpURLConnection.HTTP_CONFLICT, String.format(""The key '%s' is already used by an existing metric."", targetKey));
     }
   }",Buggy,"SONAR-6572 WS metrics/update fix error message when updating with an existing key
",Buggy,The commit message indicates a fix for an error message when updating metrics with an existing key. The diff changes the error message to include the key that is already in use. This clarifies the error message and helps the user understand the conflict. The original message was not informative enough. The answer is **Buggy**.
sonarqube,4786.json,7e1ea5d43a0fd5231baddfc22bc4c76209622968,"@@ -1,14 +1,12 @@
   public void define(WebService.NewController controller) {
     WebService.NewAction action = controller.createAction(""uninstall"")
       .setPost(true)
       .setDescription(""Uninstalls the plugin specified by its key."" +
         ""<br/>"" +
-        ""Plugin information is retrieved from Update Center."" +
-        ""<br/>"" +
-        ""Requires user to be authenticated with Administer System permissions"")
+        ""Requires user to be authenticated with Administer System permissions."")
       .setHandler(this);
 
     action.createParam(PARAM_KEY)
       .setDescription(""The key identifying the plugin to uninstall"")
       .setRequired(true);
   }",Buggy,"SONAR-6380 fix error in action description
",Buggy,"The commit message indicates a fix to an error in the action description. The diff shows a change in the description of the ""uninstall"" action, specifically removing the line ""Plugin information is retrieved from Update Center."" and adjusting the punctuation. This aligns with the commit message, suggesting a correction of an inaccurate or misleading description. This is a bug fix. The answer is **Buggy**."
sonarqube,15877.json,51f8fa984e4a2c11ededba97e6b2c0a2fe29bb02,"@@ -1,8 +1,8 @@
   public String toString() {
     return Objects.toStringHelper(this)
       .add(""key"", qpKey)
       .add(""name"", qpName)
       .add(""language"", languageKey)
-      .add(""rulesUpdatedAt"", rulesUpdatedAt)
+      .add(""rulesUpdatedAt"", rulesUpdatedAt.getTime())
       .toString();
   }",Buggy,"fix some coverage flaws
",Buggy,"The commit message indicates a fix for ""coverage flaws."" The diff changes `rulesUpdatedAt` to `rulesUpdatedAt.getTime()` in the `toString()` method. This suggests that `rulesUpdatedAt` is likely a `Date` or similar object, and the original code was not properly converting it to a string representation, potentially leading to incomplete or incorrect information being included in the string. This change ensures that the timestamp is represented as a numerical value, which is more suitable for inclusion in the string and improves test coverage by ensuring that the timestamp is properly handled. The answer is **Buggy**."
sonarqube,20277.json,5e5fc1731d0e99ab4a457fb9eeee121f342d1f33,"@@ -1,7 +1,7 @@
   static String[] getListFromProperty(Map<String, String> properties, String key) {
     String propValue = properties.get(key);
     if (propValue != null) {
-      return DefaultConfiguration.parseAsCsv(ProjectDefinition.SOURCES_PROPERTY, propValue);
+      return DefaultConfiguration.parseAsCsv(key, propValue);
     }
     return new String[0];
   }",Buggy,"SONAR-10122 fix misleading error message for multivalue properties

when parsing fails in ProjectReactor
it concerns only sonar.sources, sonar.tests and sonar.modules
",Buggy,"The commit message indicates a fix for a misleading error message related to multi-value properties in Project Reactor, specifically `sonar.sources`, `sonar.tests`, and `sonar.modules`. The diff shows a change in the `getListFromProperty` method where the first argument to `DefaultConfiguration.parseAsCsv` is changed from `ProjectDefinition.SOURCES_PROPERTY` to the `key` parameter. This suggests that the error message was misleading because it was always referencing `ProjectDefinition.SOURCES_PROPERTY` even when parsing other properties like `sonar.tests` or `sonar.modules`. By changing the argument to `key`, the error message will now correctly reflect the property that failed to parse. This is a bug fix. The answer is **Buggy**.
"
sonarqube,1092.json,e1ecfa7d65c33df5ee054e57e2b5d696da038154,"@@ -1,12 +1,10 @@
   public TokenQueue chunk(Reader reader) {
-    CodeReaderConfiguration codeReaderConfiguration = new CodeReaderConfiguration();
-    codeReaderConfiguration.setBufferCapacity(BUFFER_CAPACITY);
-    CodeReader code = new CodeReader(reader, codeReaderConfiguration);
+    CodeReader code = new CodeReader(reader);
     TokenQueue queue = new TokenQueue();
     try {
       channelDispatcher.consume(code, queue);
       return queue;
     } catch (Exception e) {
       throw new DuplicationsException(""Unable to lex source code at line : "" + code.getLinePosition() + "" and column : "" + code.getColumnPosition(), e);
     }
   }",Buggy,"SONAR-2923 and SONAR-2632: Fixed some side-effect compilations error in other modules
",Buggy,"The commit message indicates that the change fixes compilation errors that were side effects in other modules. The code diff simplifies the `chunk` method by removing the configuration of `CodeReader` and directly instantiating it with the reader. The configuration was setting the buffer capacity, which is now likely using a default value. The try-catch block remains, handling exceptions during the tokenizing process. This change could be related to fixing a bug if the previous configuration was causing issues in other modules during compilation. The removal of the configuration suggests that the previous configuration was causing side effects, which aligns with the commit message. The answer is **Buggy**."
voldemort,8473.json,a8fa1f7a2beb35aa81fc82167eb061901506d6ac,"@@ -1,21 +1,17 @@
     public static long copyLarge(Reader input, Writer output, long limit) throws IOException {
         char[] buffer = new char[DEFAULT_BUFFER_SIZE];
         long count = 0;
         int n = 0;
         long remaining = limit;
         while(remaining > 0) {
-            if(remaining > DEFAULT_BUFFER_SIZE) {
-                n = input.read(buffer);
-            } else {
-                char[] remainingbuffer = new char[(int) remaining];
-                n = input.read(remainingbuffer);
-            }
+            n = (remaining > DEFAULT_BUFFER_SIZE) ? input.read(buffer)
+                                                 : input.read(buffer, 0, (int) remaining);
             if(n == -1) {
                 break;
             }
             output.write(buffer, 0, n);
             count += n;
             remaining -= n;
         }
         return count;
     }",Buggy,"fix bug in last read
",Buggy,"The commit message indicates a bug fix related to the last read operation. The code diff modifies the `copyLarge` method, specifically how it handles reading from the input `Reader` when a limit is specified. The original code had a conditional block that allocated a new buffer `remainingbuffer` of size `remaining` when `remaining` was less than `DEFAULT_BUFFER_SIZE`. However, it used `input.read(remainingbuffer)` which reads into the newly created buffer, but the subsequent `output.write` still used the original `buffer`, leading to incorrect data being written in the last read. The corrected code uses `input.read(buffer, 0, (int) remaining)` to read directly into the original `buffer` with the correct offset and length, ensuring the correct data is written. This resolves the bug where the last part of the input was not being copied correctly. The answer is **Buggy**."
voldemort,10824.json,6980fd513d4c6545813e2294f5ac10afaa3bcbb6,"@@ -1,26 +1,29 @@
     public HintedHandoffStrategy updateHintedHandoffStrategy(StoreDefinition storeDef,
                                                              Cluster cluster) {
         if(HintedHandoffStrategyType.CONSISTENT_STRATEGY.toDisplay()
                                                         .compareTo(storeDef.getHintedHandoffStrategyType()
                                                                            .toDisplay()) == 0) {
             Integer hintPrefListSize = storeDef.getHintPrefListSize();
+
+            // Default value for hint pref list size = replication factor
             if(null == hintPrefListSize) {
-                if(cluster.getNumberOfNodes() > 6)
-                    hintPrefListSize = cluster.getNumberOfNodes() / 2;
+                if(cluster.getNumberOfNodes() == storeDef.getReplicationFactor())
+                    hintPrefListSize = storeDef.getReplicationFactor() - 1;
                 else
-                    hintPrefListSize = cluster.getNumberOfNodes();
+                    hintPrefListSize = storeDef.getReplicationFactor();
             }
+
             return new ConsistentHandoffStrategy(cluster,
                                                  hintPrefListSize,
                                                  enableZoneRouting,
                                                  clientZoneId);
         } else if(HintedHandoffStrategyType.TO_ALL_STRATEGY.toDisplay()
                                                            .compareTo(storeDef.getHintedHandoffStrategyType()
                                                                               .toDisplay()) == 0) {
             return new HandoffToAllStrategy(cluster, enableZoneRouting, clientZoneId);
         } else {
             throw new VoldemortException(""HintedHandoffStrategyType:""
                                          + storeDef.getHintedHandoffStrategyType()
                                          + "" not handled by "" + this.getClass());
         }
     }",Buggy,"Fixed another bug in Consistent strategy + Updated stores.xml so as to get EndToEnd Test running
",Buggy,"The commit message indicates a bug fix in the Consistent strategy. The code changes involve the logic for determining the `hintPrefListSize` when it's not explicitly defined in the `storeDef`. The original code used `cluster.getNumberOfNodes()` to determine the size, with a condition based on whether the number of nodes was greater than 6. The updated code now checks if the number of nodes is equal to the replication factor. If they are equal, it sets `hintPrefListSize` to `replicationFactor - 1`; otherwise, it sets it to `replicationFactor`. This change suggests a bug fix related to how the `hintPrefListSize` was being calculated in the Consistent strategy, especially when the number of nodes matched the replication factor. The answer is **Buggy**."
voldemort,9755.json,97bfa6751eba8afb13bfc801afa47d2b6bd9af1d,"@@ -1,3 +1,3 @@
     public double get99thWaitTimeMs() {
-        return this.histogramWaitMs.getQuantile(0.99);
+        return (double) (this.histogramWaitNs.getQuantile(0.99)) / Time.NS_PER_MS;
     }",Buggy,"Fixed bug and verified monitoring feature of q99th wait time
",Buggy,"The commit message indicates a bug fix related to the 99th percentile wait time calculation. The diff shows that the code was modified to calculate the 99th percentile wait time in milliseconds by converting the value obtained from `histogramWaitNs` (which is presumably in nanoseconds) to milliseconds. This conversion is done by dividing the nanosecond value by `Time.NS_PER_MS`. This change suggests that the original code was either returning the wait time in the wrong unit (nanoseconds instead of milliseconds) or was not correctly calculating the 99th percentile wait time. Therefore, the change is a bug fix. The answer is **Buggy**."
voldemort,9005.json,c07b777533cdbdfe4042258dbf44f2d9d51e3bc0,"@@ -1,286 +1,283 @@
     public void run() {
         Object message = messageEvent.getMessage();
         if(message instanceof CoordinatorStoreClientRequest) {
             CoordinatorStoreClientRequest storeClientRequestObject = (CoordinatorStoreClientRequest) message;
             this.requestObject = storeClientRequestObject.getRequestObject();
             this.storeClient = storeClientRequestObject.getStoreClient();
 
             // This shouldn't ideally happen.
             if(this.requestObject != null) {
 
                 switch(requestObject.getOperationType()) {
                     case VoldemortOpCode.GET_METADATA_OP_CODE:
                         if(logger.isDebugEnabled()) {
                             logger.debug(""GET Metadata request received."");
                         }
 
                         try {
 
                             String queryStoreName = ByteUtils.getString(this.requestObject.getKey()
                                                                                           .get(),
                                                                         ""UTF-8"");
                             StoreDefinition storeDef = StoreDefinitionUtils.getStoreDefinitionWithName(this.coordinatorMetadata.getStoresDefs(),
                                                                                                        queryStoreName);
                             String serializerInfoXml = RestUtils.constructSerializerInfoXml(storeDef);
                             GetMetadataResponseSender metadataResponseSender = new GetMetadataResponseSender(messageEvent,
                                                                                                              serializerInfoXml.getBytes());
 
                             metadataResponseSender.sendResponse(this.coordinatorPerfStats,
                                                                 true,
                                                                 this.requestObject.getRequestOriginTimeInMs());
                             if(logger.isDebugEnabled()) {
                                 logger.debug(""GET Metadata successful !"");
                             }
                         } catch(Exception e) {
                             /*
                              * We might get InsufficientOperationalNodes
                              * exception due to a timeout, thus creating
                              * confusion in the root cause. Hence explicitly
                              * check for timeout.
                              */
                             if(System.currentTimeMillis() >= (this.requestObject.getRequestOriginTimeInMs() + this.requestObject.getRoutingTimeoutInMs())) {
                                 RestErrorHandler.writeErrorResponse(this.messageEvent,
                                                                     REQUEST_TIMEOUT,
                                                                     ""GET METADATA request timed out: ""
                                                                             + e.getMessage());
+                            } else {
+                                getErrorHandler.handleExceptions(messageEvent, e);
                             }
-
-                            getErrorHandler.handleExceptions(messageEvent, e);
                         }
                         break;
 
                     case VoldemortOpCode.GET_OP_CODE:
                         if(logger.isDebugEnabled()) {
                             logger.debug(""GET request received."");
                         }
 
                         try {
                             boolean keyExists = false;
                             List<Versioned<byte[]>> versionedValues = this.storeClient.getWithCustomTimeout(this.requestObject);
                             if(versionedValues == null || versionedValues.size() == 0) {
                                 if(this.requestObject.getValue() != null) {
                                     if(versionedValues == null) {
                                         versionedValues = new ArrayList<Versioned<byte[]>>();
                                     }
                                     versionedValues.add(this.requestObject.getValue());
                                     keyExists = true;
 
                                 }
                             } else {
                                 keyExists = true;
                             }
 
                             if(keyExists) {
                                 GetResponseSender responseConstructor = new GetResponseSender(messageEvent,
                                                                                               requestObject.getKey(),
                                                                                               versionedValues,
                                                                                               this.storeClient.getStoreName());
                                 responseConstructor.sendResponse(this.coordinatorPerfStats,
                                                                  true,
                                                                  this.requestObject.getRequestOriginTimeInMs());
                                 if(logger.isDebugEnabled()) {
                                     logger.debug(""GET successful !"");
                                 }
 
                             } else {
                                 RestErrorHandler.writeErrorResponse(this.messageEvent,
                                                                     NOT_FOUND,
                                                                     ""Requested Key does not exist"");
                             }
 
                         } catch(Exception e) {
                             /*
                              * We might get InsufficientOperationalNodes
                              * exception due to a timeout, thus creating
                              * confusion in the root cause. Hence explicitly
                              * check for timeout.
                              */
                             if(System.currentTimeMillis() >= (this.requestObject.getRequestOriginTimeInMs() + this.requestObject.getRoutingTimeoutInMs())) {
                                 RestErrorHandler.writeErrorResponse(this.messageEvent,
                                                                     REQUEST_TIMEOUT,
                                                                     ""GET request timed out: ""
                                                                             + e.getMessage());
+                            } else {
+                                getErrorHandler.handleExceptions(messageEvent, e);
                             }
-
-                            getErrorHandler.handleExceptions(messageEvent, e);
                         }
                         break;
 
                     case VoldemortOpCode.GET_ALL_OP_CODE:
                         if(logger.isDebugEnabled()) {
                             logger.debug(""GET ALL request received."");
                         }
 
                         try {
                             Map<ByteArray, List<Versioned<byte[]>>> versionedResponses = this.storeClient.getAllWithCustomTimeout(this.requestObject);
                             if(versionedResponses == null
                                || versionedResponses.values().size() == 0) {
                                 logger.error(""Error when doing getall. Keys do not exist."");
 
                                 RestErrorHandler.writeErrorResponse(this.messageEvent,
                                                                     NOT_FOUND,
                                                                     ""Error when doing getall. Keys do not exist."");
                             } else {
                                 GetAllResponseSender responseConstructor = new GetAllResponseSender(messageEvent,
                                                                                                     versionedResponses,
                                                                                                     this.storeClient.getStoreName());
                                 responseConstructor.sendResponse(this.coordinatorPerfStats,
                                                                  true,
                                                                  this.requestObject.getRequestOriginTimeInMs());
 
                                 if(logger.isDebugEnabled()) {
                                     logger.debug(""GET ALL successful !"");
                                 }
 
                             }
 
                         } catch(Exception e) {
                             /*
                              * We might get InsufficientOperationalNodes
                              * exception due to a timeout, thus creating
                              * confusion in the root cause. Hence explicitly
                              * check for timeout.
                              */
                             if(System.currentTimeMillis() >= (this.requestObject.getRequestOriginTimeInMs() + this.requestObject.getRoutingTimeoutInMs())) {
                                 RestErrorHandler.writeErrorResponse(this.messageEvent,
                                                                     REQUEST_TIMEOUT,
                                                                     ""GET ALL request timed out: ""
                                                                             + e.getMessage());
+                            } else {
+                                getErrorHandler.handleExceptions(messageEvent, e);
                             }
-
-                            getErrorHandler.handleExceptions(messageEvent, e);
                         }
                         break;
 
                     // TODO: Implement this in the next pass
                     case VoldemortOpCode.GET_VERSION_OP_CODE:
 
                         if(logger.isDebugEnabled()) {
                             logger.debug(""Incoming get version request"");
                         }
 
                         try {
 
                             if(logger.isDebugEnabled()) {
                                 logger.debug(""GET versions request successful !"");
                             }
 
                         } catch(Exception e) {
                             /*
                              * We might get InsufficientOperationalNodes
                              * exception due to a timeout, thus creating
                              * confusion in the root cause. Hence explicitly
                              * check for timeout.
                              */
                             if(System.currentTimeMillis() >= (this.requestObject.getRequestOriginTimeInMs() + this.requestObject.getRoutingTimeoutInMs())) {
                                 RestErrorHandler.writeErrorResponse(this.messageEvent,
                                                                     REQUEST_TIMEOUT,
                                                                     ""GET VERSION request timed out: ""
                                                                             + e.getMessage());
+                            } else {
+                                getVersionErrorHandler.handleExceptions(messageEvent, e);
                             }
-
-                            getVersionErrorHandler.handleExceptions(messageEvent, e);
                         }
                         break;
 
                     case VoldemortOpCode.PUT_OP_CODE:
                         if(logger.isDebugEnabled()) {
                             logger.debug(""PUT request received."");
                         }
 
                         try {
                             VectorClock successfulPutVC = null;
 
                             if(this.requestObject.getValue() != null) {
                                 successfulPutVC = ((VectorClock) this.storeClient.putVersionedWithCustomTimeout(this.requestObject)).clone();
                             } else {
                                 successfulPutVC = ((VectorClock) this.storeClient.putWithCustomTimeout(this.requestObject)).clone();
                             }
 
                             PutResponseSender responseConstructor = new PutResponseSender(messageEvent,
                                                                                           successfulPutVC,
                                                                                           this.storeClient.getStoreName(),
                                                                                           this.requestObject.getKey());
                             responseConstructor.sendResponse(this.coordinatorPerfStats,
                                                              true,
                                                              this.requestObject.getRequestOriginTimeInMs());
 
                             if(logger.isDebugEnabled()) {
                                 logger.debug(""PUT successful !"");
                             }
 
                         } catch(Exception e) {
                             /*
                              * We might get InsufficientOperationalNodes
                              * exception due to a timeout, thus creating
                              * confusion in the root cause. Hence explicitly
                              * check for timeout.
                              */
                             if(System.currentTimeMillis() >= (this.requestObject.getRequestOriginTimeInMs() + this.requestObject.getRoutingTimeoutInMs())) {
                                 RestErrorHandler.writeErrorResponse(this.messageEvent,
                                                                     REQUEST_TIMEOUT,
                                                                     ""PUT request timed out: ""
                                                                             + e.getMessage());
+                            } else {
+                                putErrorHandler.handleExceptions(messageEvent, e);
                             }
-
-                            putErrorHandler.handleExceptions(messageEvent, e);
                         }
 
                         break;
 
                     case VoldemortOpCode.DELETE_OP_CODE:
                         if(logger.isDebugEnabled()) {
                             logger.debug(""Incoming delete request"");
                         }
 
                         try {
                             boolean isDeleted = this.storeClient.deleteWithCustomTimeout(this.requestObject);
                             if(isDeleted) {
                                 DeleteResponseSender responseConstructor = new DeleteResponseSender(messageEvent,
                                                                                                     this.storeClient.getStoreName(),
                                                                                                     this.requestObject.getKey());
                                 responseConstructor.sendResponse(this.coordinatorPerfStats,
                                                                  true,
                                                                  this.requestObject.getRequestOriginTimeInMs());
 
                                 if(logger.isDebugEnabled()) {
                                     logger.debug(""DELETE request successful !"");
                                 }
 
                             } else {
                                 logger.error(""Requested Key with the specified version does not exist"");
                                 RestErrorHandler.writeErrorResponse(this.messageEvent,
                                                                     NOT_FOUND,
                                                                     ""Requested Key with the specified version does not exist"");
                             }
 
                         } catch(Exception e) {
 
                             /*
                              * We might get InsufficientOperationalNodes
                              * exception due to a timeout, thus creating
                              * confusion in the root cause. Hence explicitly
                              * check for timeout.
                              */
                             if(System.currentTimeMillis() >= (this.requestObject.getRequestOriginTimeInMs() + this.requestObject.getRoutingTimeoutInMs())) {
                                 RestErrorHandler.writeErrorResponse(this.messageEvent,
                                                                     REQUEST_TIMEOUT,
                                                                     ""DELETE request timed out: ""
                                                                             + e.getMessage());
+                            } else {
+                                deleteErrorHandler.handleExceptions(messageEvent, e);
                             }
-
-                            deleteErrorHandler.handleExceptions(messageEvent, e);
                         }
                         break;
 
                     default:
                         System.err.println(""Illegal operation."");
                         return;
-
                 }
 
             }
-
         }
-
     }",Buggy,"fix duplicate error message bug and add large value size test for coordinator
",Buggy,"The commit message indicates a fix for a duplicate error message bug. The diff introduces a change in the exception handling logic within the `run` method. Specifically, within each case of the switch statement (GET_METADATA_OP_CODE, GET_OP_CODE, GET_ALL_OP_CODE, GET_VERSION_OP_CODE, PUT_OP_CODE, DELETE_OP_CODE), the code now checks if a timeout has occurred before delegating to the respective error handler. If a timeout has occurred, a specific ""REQUEST_TIMEOUT"" error response is sent. Otherwise, the exception is handled by the corresponding error handler (getErrorHandler, getVersionErrorHandler, putErrorHandler, deleteErrorHandler). This change prevents the generic error handler from being invoked after a timeout has already been detected and handled, which could lead to duplicate or misleading error messages. The change aligns with the commit message, indicating a bug fix. The answer is **Buggy**."
voldemort,7694.json,d1b041d2e3ef3148bfcfae07d6e2609a5aa0ac45,"@@ -1,33 +1,33 @@
     public VAdminProto.UpdateMetadataPairResponse handleUpdateMetadataPair(VAdminProto.UpdateMetadataPairRequest request) {
         VAdminProto.UpdateMetadataPairResponse.Builder response = VAdminProto.UpdateMetadataPairResponse.newBuilder();
         try {
             ByteArray clusterKey = ProtoUtils.decodeBytes(request.getClusterKey());
             ByteArray storesKey = ProtoUtils.decodeBytes(request.getStoresKey());
             String clusterKeyString = ByteUtils.getString(clusterKey.get(), ""UTF-8"");
             String storesKeyString = ByteUtils.getString(storesKey.get(), ""UTF-8"");
 
             if(MetadataStore.METADATA_KEYS.contains(clusterKeyString)
                && MetadataStore.METADATA_KEYS.contains(storesKeyString)) {
 
                 Versioned<byte[]> clusterVersionedValue = ProtoUtils.decodeVersioned(request.getClusterValue());
                 Versioned<byte[]> storesVersionedValue = ProtoUtils.decodeVersioned(request.getStoresValue());
 
                 metadataStore.writeLock.lock();
                 try {
                     logger.info(""Updating metadata for keys '"" + clusterKeyString + ""'"" + "" and '""
                                 + storesKeyString + ""'"");
                     metadataStore.put(clusterKey, clusterVersionedValue, null);
-                    metadataStore.put(storesKey, storesVersionedValue, null);
+                    metadataStore.updateStoreDefinitions(storesVersionedValue);
                     logger.info(""Successfully updated metadata for keys '"" + clusterKeyString + ""'""
                                 + "" and '"" + storesKeyString + ""'"");
                 } finally {
                     metadataStore.writeLock.unlock();
                 }
             }
         } catch(VoldemortException e) {
             response.setError(ProtoUtils.encodeError(errorCodeMapper, e));
             logger.error(""handleUpdateMetadataPair failed for request("" + request.toString() + "")"",
                          e);
         }
         return response.build();
     }",Buggy,"Bug fix in RebalanceRebootstrapConsistencyTest and AdminServiceRequestHandler. Adding wrapper for updateRemoteStoreDefList in AdminClient
",Buggy,"The commit message indicates a bug fix related to `RebalanceRebootstrapConsistencyTest` and `AdminServiceRequestHandler`, and mentions adding a wrapper for `updateRemoteStoreDefList` in `AdminClient`.

The diff shows a change in `AdminServiceRequestHandler.handleUpdateMetadataPair`. Instead of directly putting the `storesVersionedValue` into the `metadataStore` using `metadataStore.put(storesKey, storesVersionedValue, null);`, it now calls `metadataStore.updateStoreDefinitions(storesVersionedValue);`. This suggests that the previous direct put operation was incorrect or insufficient for handling store definitions, and the new `updateStoreDefinitions` method provides the correct logic for updating store definitions. This indicates a bug fix.

Therefore, the answer is **Buggy**.
"
voldemort,7656.json,1f057d7e3c68dc4d67387a5485ac6793d3feb8b7,"@@ -1,31 +1,33 @@
     public VAdminProto.RebalanceStateChangeResponse handleRebalanceStateChange(VAdminProto.RebalanceStateChangeRequest request) {
-
         VAdminProto.RebalanceStateChangeResponse.Builder response = VAdminProto.RebalanceStateChangeResponse.newBuilder();
 
-        try {
-            // Retrieve all values first
-            List<RebalancePartitionsInfo> rebalancePartitionsInfo = Lists.newArrayList();
-            for(RebalancePartitionInfoMap map: request.getRebalancePartitionInfoListList()) {
-                rebalancePartitionsInfo.add(ProtoUtils.decodeRebalancePartitionInfoMap(map));
+        synchronized(rebalancer) {
+            try {
+                // Retrieve all values first
+                List<RebalancePartitionsInfo> rebalancePartitionsInfo = Lists.newArrayList();
+                for(RebalancePartitionInfoMap map: request.getRebalancePartitionInfoListList()) {
+                    rebalancePartitionsInfo.add(ProtoUtils.decodeRebalancePartitionInfoMap(map));
+                }
+
+                Cluster cluster = new ClusterMapper().readCluster(new StringReader(request.getClusterString()));
+
+                boolean swapRO = request.getSwapRo();
+                boolean changeClusterMetadata = request.getChangeClusterMetadata();
+                boolean changeRebalanceState = request.getChangeRebalanceState();
+                boolean rollback = request.getRollback();
+
+                rebalancer.rebalanceStateChange(cluster,
+                                                rebalancePartitionsInfo,
+                                                swapRO,
+                                                changeClusterMetadata,
+                                                changeRebalanceState,
+                                                rollback);
+            } catch(VoldemortException e) {
+                response.setError(ProtoUtils.encodeError(errorCodeMapper, e));
+                logger.error(""handleRebalanceStateChange failed for request("" + request.toString()
+                             + "")"", e);
             }
-
-            Cluster cluster = new ClusterMapper().readCluster(new StringReader(request.getClusterString()));
-
-            boolean swapRO = request.getSwapRo();
-            boolean changeClusterMetadata = request.getChangeClusterMetadata();
-            boolean changeRebalanceState = request.getChangeRebalanceState();
-            boolean rollback = request.getRollback();
-
-            rebalancer.rebalanceStateChange(cluster,
-                                            rebalancePartitionsInfo,
-                                            swapRO,
-                                            changeClusterMetadata,
-                                            changeRebalanceState,
-                                            rollback);
-        } catch(VoldemortException e) {
-            response.setError(ProtoUtils.encodeError(errorCodeMapper, e));
-            logger.error(""handleRebalanceStateChange failed for request("" + request.toString()
-                         + "")"", e);
         }
+
         return response.build();
     }",Buggy,"fix two concurrency bugs during the termination of donor-based rebalancing
",Buggy,"The commit message indicates the fix of two concurrency bugs. The diff introduces a `synchronized` block around the core logic of the `handleRebalanceStateChange` method, synchronizing on the `rebalancer` object. This synchronization aims to prevent concurrent access to the `rebalancer`'s state during the rebalancing process, which could lead to data corruption or inconsistent state transitions. The try-catch block is moved inside the synchronized block. This change directly addresses potential concurrency issues by ensuring that only one thread can execute the rebalancing logic at a time, thus fixing the concurrency bugs mentioned in the commit message. The answer is **Buggy**."
voldemort,6967.json,9f2e07c6e18984df4d2428dd51accd37534e9a68,"@@ -1,11 +1,12 @@
     public void stop() {
         logger.info(""Stopping "" + getType().getDisplayName());
         synchronized(this) {
             if(!isStarted()) {
                 logger.info(""The service is already stopped, ignoring duplicate attempt."");
+                return;
             }
 
             stopInner();
             isStarted.set(false);
         }
     }",Buggy,"Fixed the re-introduction of BindExceptions upon start up of some recently added tests:

All of the following tests directly used the method ServerTestUtils.startVoldemortServer
  test/unit/voldemort/client/ClientRegistryTest.java
  test/unit/voldemort/client/AdminFetchTest.java
  test/unit/voldemort/client/EndToEndRebootstrapTest.java
  test/unit/voldemort/store/system/AsyncMetadataVersionManagerTest.java
  test/unit/voldemort/store/system/SystemStoreTest.java

This method of starting Voldemort servers is susceptible to
BindException errors. Unless something very fancy is being done, the
method ServerTestUtils.startVoldemortCluster should be used to start a
cluster within a test.

test/unit/voldemort/client/ClientRegistryTest.java extended from
TestCase. This is a ""Junit3"" idiom that we should not use in new
tests. The Junit4 approach to tests relies solely on annotation.
",Buggy,"The commit message indicates a fix for BindExceptions occurring during the startup of certain tests. The diff shows a change in the `stop()` method of a service, adding a check to prevent stopping an already stopped service. This change likely addresses a scenario where the service is stopped multiple times, potentially leading to resource conflicts and BindExceptions during subsequent startups. The added `return` statement avoids redundant operations and potential errors. The answer is **Buggy**.
"
voldemort,8213.json,46a5b4d7e916e3fe0a9a77659599d74709f18c2d,"@@ -1,55 +1,56 @@
     private void write(DataOutputStream output, Object object, Object type) throws IOException {
         try {
             if(type instanceof Map) {
                 if(object != null && !(object instanceof Map))
                     throw new SerializationException(""Expected Map, but got "" + object.getClass()
                                                      + "": "" + object);
                 writeMap(output, (Map<String, Object>) object, (Map<String, Object>) type);
             } else if(type instanceof List) {
                 if(object != null && !(object instanceof List))
                     throw new SerializationException(""Expected List but got "" + object.getClass()
                                                      + "": "" + object);
                 writeList(output, (List<Object>) object, (List<Object>) type);
             } else if(type instanceof JsonTypes) {
                 JsonTypes jsonType = (JsonTypes) type;
                 switch(jsonType) {
                     case STRING:
                         writeString(output, (String) object);
                         break;
                     case INT8:
                         writeInt8(output, (Byte) object);
                         break;
                     case INT16:
                         writeInt16(output, coerceToShort(object));
                         break;
                     case INT32:
                         writeInt32(output, coerceToInteger(object));
                         break;
                     case INT64:
                         writeInt64(output, coerceToLong(object));
                         break;
                     case FLOAT32:
                         writeFloat32(output, coerceToFloat(object));
                         break;
                     case FLOAT64:
                         writeFloat64(output, coerceToDouble(object));
                         break;
                     case DATE:
                         writeDate(output, coerceToDate(object));
                         break;
                     case BYTES:
                         writeBytes(output, (byte[]) object);
                         break;
                     case BOOLEAN:
                         writeBoolean(output, (Boolean) object);
+                        break;
                     default:
                         throw new SerializationException(""Unknown type: "" + type);
                 }
             }
         } catch(ClassCastException e) {
             // simpler than doing every test
             throw new SerializationException(""Expected type "" + type
                                              + "" but got object of incompatible type ""
                                              + object.getClass().getName() + ""."");
         }
     }",Buggy,"Fix serialization bug with writing boolean values--missing break in case statement.
",Buggy,"The commit message indicates a fix for a serialization bug related to writing boolean values, specifically mentioning a missing `break` statement in a case statement. The provided code diff shows a `switch` statement handling different `JsonTypes`. The `BOOLEAN` case is now followed by a `break;` statement. This confirms the bug fix described in the commit message, as the missing `break` would have caused the code to fall through to the `default` case after writing a boolean value, leading to incorrect behavior. The answer is **Buggy**."
voldemort,9151.json,ff128a8ad5de83bc55b8cec80d7775e61816e4e6,"@@ -1,16 +1,21 @@
     public List<Versioned<T>> resolveConflicts(List<Versioned<T>> items) {
+        List<ClockEntry> maxClock = null;
         if(items.size() <= 1) {
             return items;
         } else {
             Versioned<T> max = items.get(0);
             long maxTime = ((VectorClock) items.get(0).getVersion()).getTimestamp();
+            maxClock = ((VectorClock) items.get(0).getVersion()).getEntries();
             for(Versioned<T> versioned: items) {
                 VectorClock clock = (VectorClock) versioned.getVersion();
                 if(clock.getTimestamp() > maxTime) {
                     max = versioned;
                     maxTime = ((VectorClock) versioned.getVersion()).getTimestamp();
                 }
+                maxClock = VectorClock.maxClockList(maxClock, clock.getEntries());
             }
-            return Collections.singletonList(max);
+            Versioned<T> maxTimeClockVersioned = new Versioned<T>(max.getValue(),
+                                                                  new VectorClock(maxClock, maxTime));
+            return Collections.singletonList(maxTimeClockVersioned);
         }
     }",Buggy,"TimeBasedInconsistency resolover bug fix
",Buggy,"The commit message indicates a bug fix related to ""TimeBasedInconsistency"" in a resolver. The diff shows changes in the `resolveConflicts` method, specifically how the maximum version is determined and how a new `Versioned` object is created. The original code only considered the timestamp for determining the maximum, but the corrected code also merges the clock entries using `VectorClock.maxClockList`. This suggests that the previous implementation might have incorrectly resolved conflicts when multiple versions had the same timestamp but different clock entries. The creation of `maxTimeClockVersioned` with the merged clock and the maximum timestamp further supports this. The answer is **Buggy**."
voldemort,11028.json,c3773f409dffb3761aaeba6811d3ed6836263987,"@@ -1,15 +1,15 @@
     public static void validateUserStoreNamesOnNode(AdminClient adminClient,
                                                  Integer nodeId,
                                                  List<String> storeNames) {
         List<StoreDefinition> storeDefList = adminClient.metadataMgmtOps.getRemoteStoreDefList(nodeId)
                                                                         .getValue();
-        List<String> storeNameList = Lists.newArrayList();
+        Map<String, Boolean> existingStoreNames = new HashMap<String, Boolean>();
         for(StoreDefinition storeDef: storeDefList) {
-            storeNameList.add(storeDef.getName());
+          existingStoreNames.put(storeDef.getName(), true);
         }
         for(String storeName: storeNames) {
-            if(!storeNameList.contains(storeName)) {
+            if(!Boolean.TRUE.equals(existingStoreNames.get(storeName))) {
                 Utils.croak(""Store "" + storeName + "" does not exist!"");
             }
         }
     }",Buggy,"Fix a vadmin error
",Buggy,"The code change replaces a list with a map to check if a store name exists. This change is likely to improve the performance of the `validateUserStoreNamesOnNode` method, as checking for the existence of an element in a map is typically faster than checking in a list. The original code used `storeNameList.contains(storeName)` which has O(n) complexity, while the new code uses `existingStoreNames.get(storeName)` which has O(1) complexity. The change also fixes a bug where `storeNameList.contains(storeName)` returns false if the store name is null, while `existingStoreNames.get(storeName)` returns null if the store name is not present in the map, and `Boolean.TRUE.equals(null)` returns false. The answer is **Buggy**."
voldemort,755.json,0548406c86c8ed3af48c0a59586f8dd03d3aefdd,"@@ -1,11 +1,10 @@
             public void nodeUnavailable(Node node) {
                 if(logger.isInfoEnabled())
-                    logger.info(""Node "" + node
-                                + "" has been marked as unavailable, destroying socket pool"");
+                    logger.info(node + "" has been marked as unavailable, destroying socket pool"");
 
                 // Kill the socket pool for this node...
                 SocketDestination destination = new SocketDestination(node.getHost(),
                                                                       node.getSocketPort(),
                                                                       config.getRequestFormatType());
                 socketPool.close(destination);
             }",Buggy,"Fixed inconsistent error message.
",Buggy,"The commit message indicates a fix for an inconsistent error message. The diff modifies a log message in the `nodeUnavailable` method. Specifically, it removes the string concatenation ""Node "" + node and directly logs the node object. This change likely aims to provide a more consistent and informative error message by relying on the `toString()` implementation of the `Node` object. The change directly addresses the error message, indicating a bug fix. The answer is **Buggy**."
voldemort,151.json,a36d1fe39e88e79875bef39bc0ce10419d8cbcf2,"@@ -1,8 +1,8 @@
-    private AdminClient createAdminClient(String url, boolean fetchAllStoresXml) {
+    private AdminClient createAdminClient(String url, boolean fetchAllStoresXml, int connectionTimeoutSec, int socketTimeoutSec) {
         ClientConfig config = new ClientConfig().setBootstrapUrls(url)
-                .setConnectionTimeout(15,TimeUnit.SECONDS)
+                .setConnectionTimeout(connectionTimeoutSec ,TimeUnit.SECONDS)
                 .setFetchAllStoresXmlInBootstrap(fetchAllStoresXml);
 
-        AdminClientConfig adminConfig = new AdminClientConfig().setAdminSocketTimeoutSec(60);
+        AdminClientConfig adminConfig = new AdminClientConfig().setAdminSocketTimeoutSec(socketTimeoutSec);
         return new AdminClient(adminConfig, config);
     }",Buggy,"Made admin connection/socket timeout configurable in BnP.

Also changed the default socket timeout to 180 seconds.

This fixes the following problem: when a node is unreachable and
completely shut down, requests to it will time out, which takes
60 seconds. When BnP notices this, it will reach one of the live
nodes in the cluster and ask it to deal with the failure. The
live node will try to talk to the dead node, which will also
take 60 seconds to time out. By the time the live node decides
that the dead node is unreachable, and responds to the BnP job,
the BnP job will have already timed out. Then, the BnP job will
think that the HandleFailedFetchRequest could not complete
successfully (even though it did in fact complete successfully)
and BnP HA will be aborted.

The solution is that the BnP job's socket timeout must be greater
than the server's default connection timeout.

This was not an issue before when we had insanely long time outs,
but those time outs have been reduced considerably in commit
34debd34c5896b6c2a01b1012e89dd1a3a0a0242. This is likely when we
regressed on the handling of this failure mode.
",Buggy,"The commit message describes a problem where BnP HA was being aborted due to timeouts when a node was unreachable. The solution involves making the admin connection/socket timeout configurable and increasing the default socket timeout. The diff shows that the `createAdminClient` method is modified to accept `connectionTimeoutSec` and `socketTimeoutSec` as parameters, and these parameters are used to configure the `ClientConfig` and `AdminClientConfig` respectively. This change directly addresses the timeout issue described in the commit message. The answer is **Buggy**.
"
voldemort,132.json,da4a1bf5015df49a9e8ba867a90d4dab7f0ba194,"@@ -1,29 +1,29 @@
-    public static void main(String[] args) {
+    public static void main(String[] args) throws Exception {
         // Validate arguments
         if (args.length < 1) {
             logger.error(""Please provide a job config file name as the argument to this script."");
             System.exit(1);
         }
         String fileName = args[0];
 
         // Load config
         logger.info(""Extracting config properties out of: "" + fileName);
         Props props = null;
         try {
             props = new Props(null, fileName);
         } catch (IOException e) {
             logger.error(""Exception while reading config file!"", e);
             System.exit(1);
         }
 
         // Run job
         VoldemortBuildAndPushJob job = new VoldemortBuildAndPushJob(""shell-job"", props);
         try {
             job.run();
         } catch (Exception e) {
             logger.error(""Exception while running BnP job!"", e);
             System.exit(1);
         }
         logger.info(""BnP job finished successfully (:"");
         System.exit(0);
     }",Buggy,"Fixed a compilation error introduced by last commit. (#468)

",Buggy,"The commit message indicates a fix for a compilation error introduced by the last commit. The diff adds `throws Exception` to the `main` method signature. This suggests that the previous commit introduced code that could throw a checked exception, and this change is necessary to handle it, resolving the compilation error. The answer is **Buggy**."
voldemort,9849.json,41cf6a7204e1b9821833bf9f6022f61abd10f007,"@@ -1,17 +1,17 @@
     private VectorClock readVersion(String key) {
         try {
             File versionFile = new File(getVersionDirectory(), key);
             if(!versionFile.exists()) {
                 // bootstrap file save default clock as version.
-                VectorClock clock = new VectorClock();
+                VectorClock clock = new VectorClock(0);
                 writeVersion(key, clock);
                 return clock;
             } else {
                 // read the version file and return version.
                 String hexCode = FileUtils.readFileToString(versionFile, ""UTF-8"");
                 return new VectorClock(Hex.decodeHex(hexCode.toCharArray()));
             }
         } catch(Exception e) {
             throw new VoldemortException(""Failed to read Version for Key:"" + key, e);
         }
     }",Buggy,"fixed a bug that throws exceptions for some tests
",Buggy,"The code initializes a `VectorClock` with a default constructor `VectorClock()` and then writes it to a file if the version file doesn't exist. The change replaces `VectorClock()` with `VectorClock(0)`. This suggests that the default constructor might have been causing issues, potentially leading to exceptions or incorrect behavior. The commit message confirms that the change fixes a bug that throws exceptions for some tests. Therefore, the change likely addresses a bug related to the initial state of the `VectorClock`. The answer is **Buggy**.
"
cassandra,15770.json,3740f815c21254bd625ad1cbe8d47aa657727a83,"@@ -1,26 +1,29 @@
     public boolean maybeWaitForArchiving(String name)
     {
         Future<?> f = archivePending.remove(name);
         if (f == null)
             return true; // archiving disabled
 
         try
         {
             f.get();
         }
         catch (InterruptedException e)
         {
             throw new AssertionError(e);
         }
         catch (ExecutionException e)
         {
-            if (e.getCause() instanceof IOException)
+            if (e.getCause() instanceof RuntimeException)
             {
-                logger.error(""Looks like the archiving of file {} failed earlier, cassandra is going to ignore this segment for now."", name);
-                return false;
+                if (e.getCause().getCause() instanceof IOException)
+                {
+                    logger.error(""Looks like the archiving of file {} failed earlier, cassandra is going to ignore this segment for now."", name, e.getCause().getCause());
+                    return false;
+                }
             }
             throw new RuntimeException(e);
         }
 
         return true;
     }",Buggy,"Fix bugs in commit log archiving startup behavior

patch by Ariel Weisberg; reviewed by Branimir Lambov for CASSANDRA-10593
",Buggy,"The commit message indicates a bug fix related to commit log archiving startup behavior. The code changes involve modifications to the `maybeWaitForArchiving` method. Specifically, the code now checks if the `ExecutionException`'s cause is a `RuntimeException`, and then checks if the `RuntimeException`'s cause is an `IOException`. If this nested condition is met, it logs an error message and returns `false`. Additionally, the logger now includes the nested cause in the error message. This suggests that the original code was not correctly handling exceptions during the archiving process, potentially leading to incorrect behavior or failure to properly archive commit logs. The updated code provides more specific error handling for `IOException`s that occur during archiving, which aligns with the commit message indicating a bug fix. The answer is **Buggy**."
cassandra,5689.json,02030dd658e6ca0cdb8987fd72e0454066e1b6d6,"@@ -1,4 +1,7 @@
     public boolean contains(T position)
     {
-        return Range.contains(left, right, position) || left.equals(position);
+        // Range.contains doesnt work correctly if left == right because for
+        // Range that means a wrapping range that select the whole ring. So we
+        // must explicitely handle this case
+        return left.equals(position) || (!left.equals(right) && Range.contains(left, right, position));
     }",Buggy,"Fix LCS bug with sstables containing only 1 row

patch by slebresne; reviewed by jbellis for CASSANDRA-4411
",Buggy,"The commit message indicates a bug fix in the LCS (likely Leveled Compaction Strategy) related to sstables containing only one row. The code diff modifies the `contains` method. The original implementation used `Range.contains(left, right, position) || left.equals(position)`. The corrected version adds a check `!left.equals(right)` before calling `Range.contains`. The comment explains that `Range.contains` doesn't work correctly if `left == right`, because for Range that means a wrapping range that selects the whole ring. This indicates a logical error in the original code's handling of single-row sstables, which is now fixed. The answer is **Buggy**.
"
cassandra,13729.json,967a2cfe179548835d5e8c1640889420ce0d40ce,"@@ -1,13 +1,13 @@
     private void updateCrc()
     {
-        if (crcPosition == buffer.position() | crcUpdateDisabled)
+        if (crcPosition == buffer.position() || crcUpdateDisabled)
             return;
 
         assert crcPosition >= 0 && crcPosition < buffer.position();
 
         ByteBuffer unprocessed = buffer.duplicate();
         unprocessed.position(crcPosition)
                    .limit(buffer.position());
 
         crc.update(unprocessed);
     }",Buggy,"8630: fixed coverity defects
",Buggy,"The commit message indicates a fix for Coverity defects. The code change replaces a bitwise OR operator `|` with a logical OR operator `||` in the `updateCrc` method's if condition. This suggests that the original code might have had unintended side effects due to the bitwise OR, potentially leading to incorrect CRC updates or other unexpected behavior. The change to a logical OR likely corrects this, ensuring that the condition is evaluated as intended. The answer is **Buggy**.
"
cassandra,14898.json,50c1987265f8ddaf5032a7ace07b2462aba4f09b,"@@ -1,38 +1,38 @@
     static Set<List<String>> getCompactionBuckets(List<String> files, long min)
     {
-    	Map<List<String>, Long> buckets = new NonBlockingHashMap<List<String>, Long>();
+    	Map<List<String>, Long> buckets = new ConcurrentHashMap<List<String>, Long>();
     	for(String fname : files)
     	{
     		File f = new File(fname);
     		long size = f.length();
 
     		boolean bFound = false;
             // look for a bucket containing similar-sized files:
             // group in the same bucket if it's w/in 50% of the average for this bucket,
             // or this file and the bucket are all considered ""small"" (less than `min`)
             for (List<String> bucket : buckets.keySet())
     		{
                 long averageSize = buckets.get(bucket);
                 if ((size > averageSize/2 && size < 3*averageSize/2)
                     || ( size < min && averageSize < min))
     			{
                     // remove and re-add because adding changes the hash
                     buckets.remove(bucket);
     				averageSize = (averageSize + size) / 2 ;
                     bucket.add(fname);
                     buckets.put(bucket, averageSize);
     				bFound = true;
     				break;
     			}
     		}
             // no similar bucket found; put it in a new one
     		if(!bFound)
     		{
                 ArrayList<String> bucket = new ArrayList<String>();
                 bucket.add(fname);
                 buckets.put(bucket, size);
     		}
     	}
 
         return buckets.keySet();
     }",Buggy,"fix getCompactionBuckets -- something is broken in NonBlockingHashMap when removing and re-adding the same collection mid-iteration; it ends up with multiple references to that collection.  going back to ConcurrentHashMap fixes the problem.  (any additional overhead from CHM is negligible here.)  patch by jbellis; reviewed by Eric Evans for #57

git-svn-id: https://svn.apache.org/repos/asf/incubator/cassandra/trunk@764004 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug in `getCompactionBuckets` related to the use of `NonBlockingHashMap`. The issue arises when removing and re-adding the same collection mid-iteration, leading to multiple references to that collection. The fix involves replacing `NonBlockingHashMap` with `ConcurrentHashMap`. The diff confirms this change. Therefore, the changes indicate a bug fix. The answer is **Buggy**."
cassandra,6678.json,7b083a4e083442cb6e6bf34735753c6103d88654,"@@ -1,8 +1,6 @@
     public static void rescheduleFailedTasks()
     {
-        for (SnapshotDeletingTask task : failedTasks)
-        {
-            failedTasks.remove(task);
+        Runnable task;
+        while ( null != (task = failedTasks.poll()))
             ScheduledExecutors.nonPeriodicTasks.submit(task);
-        }
     }",Buggy,"Fix coverity errors with SSTableDeletingTask and SnapshotDeletingtask

Patch by jmckenzie; reviewed by stefania for CASSANDRA-10222
",Buggy,"The commit message indicates a fix for Coverity errors related to `SSTableDeletingTask` and `SnapshotDeletingTask`. The code diff modifies the `rescheduleFailedTasks` method. Instead of iterating through the `failedTasks` collection and removing elements during iteration (which can lead to `ConcurrentModificationException` or similar issues), the code now uses a `while` loop with `failedTasks.poll()`. This safely retrieves and removes tasks from the `failedTasks` collection until it's empty. This change addresses a potential bug related to concurrent modification of the `failedTasks` collection. The answer is **Buggy**."
cassandra,9236.json,2a656e6bd5e1f5ac8161b65a211dd8bed74b7def,"@@ -1,10 +1,10 @@
     private MerkleTree getMerkleTree(Token t)
     {
         for (Range<Token> range : merkleTrees.keySet())
         {
             if (range.contains(t))
                 return merkleTrees.get(range);
         }
 
-        return null;
+        throw new AssertionError(""Expected tree for token "" + t);
     }",Buggy,"Fixed coverity defects
Reviewed by Marcus Olsson for CASSANDRA-5220
",Buggy,"The commit message indicates a fix for Coverity defects. The diff modifies the `getMerkleTree` method. Previously, if no Merkle tree was found for a given token, the method returned `null`. Now, it throws an `AssertionError` with a descriptive message. This change suggests that the original code assumed a Merkle tree would always be found for a token, and the `null` return was likely causing issues elsewhere. Replacing the `null` return with an `AssertionError` indicates a bug fix related to unexpected or invalid state. The answer is **Buggy**.
"
cassandra,20480.json,6991556e431a51575744248a4c484270c4f918c9,"@@ -1,4 +1,4 @@
     public static void truncate()
     {
-        ALL.forEach(table -> getSchemaCFS(table).truncateBlocking());
+        ALL.reverse().forEach(table -> getSchemaCFS(table).truncateBlocking());
     }",Buggy,"Fix startup problems due to schema tables not completely flushed

patch by Stefania Alborghetti; reviewed by Aleksey Yeschenko for CASSANDRA-12213.
",Buggy,"The commit message indicates a fix for startup problems related to schema tables not being completely flushed. The code change reverses the order in which schema tables are truncated. This suggests that the original order of truncation was causing dependencies or conflicts, leading to incomplete flushing and startup issues. Reversing the order likely resolves these dependencies, ensuring a complete flush. The answer is **Buggy**.
"
cassandra,17609.json,0f977c597a6b70984de96bcc49474acdb12ad2ea,"@@ -1,24 +1,24 @@
     public int compare(Composite c1, Composite c2)
     {
         // This method assumes that simple composites never have an EOC != NONE. This assumption
         // stands in particular on the fact that a Composites.EMPTY never has a non-NONE EOC. If
         // this ever change, we'll need to update this.
 
         if (isByteOrderComparable)
         {
             // toByteBuffer is always cheap for simple types, and we keep virtual method calls to a minimum:
             // hasRemaining will always be inlined, as will most of the call-stack for BBU.compareUnsigned
             ByteBuffer b1 = c1.toByteBuffer();
             ByteBuffer b2 = c2.toByteBuffer();
             if (!b1.hasRemaining() || !b2.hasRemaining())
                 return b1.hasRemaining() ? 1 : (b2.hasRemaining() ? -1 : 0);
             return ByteBufferUtil.compareUnsigned(b1, b2);
         }
 
         boolean c1isEmpty = c1.isEmpty();
         boolean c2isEmpty = c2.isEmpty();
         if (c1isEmpty || c2isEmpty)
-            return c1isEmpty ? 1 : (c2isEmpty ? -1 : 0);
+            return !c1isEmpty ? 1 : (!c2isEmpty ? -1 : 0);
 
         return type.compare(c1.get(0), c2.get(0));
     }",Buggy,"Fix CellName comparison bugs

Patch by tjake; reviewed by bes for CASSANDRA-7227
",Buggy,"The commit message indicates a fix for ""CellName comparison bugs."" The diff modifies the comparison logic when `c1` or `c2` are empty. The original code `return c1isEmpty ? 1 : (c2isEmpty ? -1 : 0);` returns 1 if `c1` is empty, -1 if `c2` is empty, and 0 if both are empty. The corrected code `return !c1isEmpty ? 1 : (!c2isEmpty ? -1 : 0);` returns 1 if `c1` is *not* empty, -1 if `c2` is *not* empty, and 0 if both are empty. This is a logical error in the original code that has been corrected. The answer is **Buggy**."
cassandra,16419.json,653dcc63e0f6b0c5b3c3592beb1e0dd3bc1ee0cf,"@@ -1,4 +1,4 @@
     public Long compose(ByteBuffer bytes)
     {
-        return ByteBufferUtil.toLong(bytes);
+        return CounterContext.instance().total(bytes);
     }",Buggy,"fix merge problem with CHANGES.txt, use right call in AbstractCommutativeType.compose()

git-svn-id: https://svn.apache.org/repos/asf/cassandra/trunk@1082155 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix related to `AbstractCommutativeType.compose()`. The diff shows that the original implementation used `ByteBufferUtil.toLong(bytes)` to convert a ByteBuffer to a Long, but it has been replaced with `CounterContext.instance().total(bytes)`. This suggests that the original method was not correctly handling the ByteBuffer in the context of a counter, and the change fixes this issue by using the appropriate method to extract the total count from the ByteBuffer. This is a bug fix. The answer is **Buggy**.
"
jgit,9755.json,d2600693bd5fb8bda20bae41467132668caa1e14,"@@ -1,34 +1,40 @@
 	private static RebaseTodoLine parseLine(byte[] buf, int tokenBegin,
 			int lineEnd) {
 		RebaseTodoLine.Action action = null;
 		AbbreviatedObjectId commit = null;
 
 		int nextSpace = RawParseUtils.next(buf, tokenBegin, ' ');
 		int tokenCount = 0;
-		while (tokenCount < 3 && nextSpace < lineEnd) {
+		while (tokenCount < 3 && nextSpace <= lineEnd) {
 			switch (tokenCount) {
 			case 0:
 				String actionToken = new String(buf, tokenBegin,
 						nextSpace - tokenBegin - 1, UTF_8);
 				tokenBegin = nextSpace;
 				action = RebaseTodoLine.Action.parse(actionToken);
 				if (action == null)
 					return null; // parsing failed
 				break;
 			case 1:
 				nextSpace = RawParseUtils.next(buf, tokenBegin, ' ');
-				String commitToken = new String(buf, tokenBegin,
-						nextSpace - tokenBegin - 1, UTF_8);
+				String commitToken;
+				if (nextSpace > lineEnd + 1) {
+					commitToken = new String(buf, tokenBegin,
+							lineEnd - tokenBegin + 1, UTF_8);
+				} else {
+					commitToken = new String(buf, tokenBegin,
+							nextSpace - tokenBegin - 1, UTF_8);
+				}
 				tokenBegin = nextSpace;
 				commit = AbbreviatedObjectId.fromString(commitToken);
 				break;
 			case 2:
 				return new RebaseTodoLine(action, commit,
 						RawParseUtils.decode(buf, tokenBegin, 1 + lineEnd));
 			}
 			tokenCount++;
 		}
 		if (tokenCount == 2)
 			return new RebaseTodoLine(action, commit, """"); //$NON-NLS-1$
 		return null;
 	}",Buggy,"Fix off-by-one error in RebaseTodoFile when reading a todo file

Commit messages of length 1 were not read. 'lineEnd' is the offset
of the last character in the line before the terminating LF or CR-LF,
and 'nextSpace' is actually the offset of the character _after_ the
next space. With a one-character commit message, nextSpace == lineEnd.

The code also assumes the commit message to be optional, but actually
failed in that case because it read beyond the line ending. Fix that,
too.

Add a test case for reading a todo file.

Bug: 546245
Change-Id: I368d63615930ea2398a6230e756442fd88870654
Signed-off-by: Thomas Wolf <thomas.wolf@paranor.ch>",Buggy,"The commit message indicates a fix for an off-by-one error in `RebaseTodoFile` when reading a todo file, specifically addressing issues with commit messages of length 1 and the assumption that the commit message is optional. The diff modifies the `parseLine` method to correctly handle these cases. The condition `nextSpace < lineEnd` is changed to `nextSpace <= lineEnd` to include the last character. Additionally, a condition `nextSpace > lineEnd + 1` is added to handle cases where the commit message is empty. These changes directly address the reported bug. The answer is **Buggy**."
jgit,9755.json,4feace2b9ecb90fe591c4317403f6d8e6309287e,"@@ -1,32 +1,34 @@
 	private static RebaseTodoLine parseLine(byte[] buf, int tokenBegin,
 			int lineEnd) {
 		RebaseTodoLine.Action action = null;
 		AbbreviatedObjectId commit = null;
 
 		int nextSpace = RawParseUtils.next(buf, tokenBegin, ' ');
 		int tokenCount = 0;
 		while (tokenCount < 3 && nextSpace < lineEnd) {
 			switch (tokenCount) {
 			case 0:
 				String actionToken = new String(buf, tokenBegin, nextSpace
 						- tokenBegin - 1);
 				tokenBegin = nextSpace;
 				action = RebaseTodoLine.Action.parse(actionToken);
 				if (action == null)
 					return null; // parsing failed
 				break;
 			case 1:
 				nextSpace = RawParseUtils.next(buf, tokenBegin, ' ');
 				String commitToken = new String(buf, tokenBegin, nextSpace
 						- tokenBegin - 1);
 				tokenBegin = nextSpace;
 				commit = AbbreviatedObjectId.fromString(commitToken);
 				break;
 			case 2:
 				return new RebaseTodoLine(action, commit, RawParseUtils.decode(
 						buf, tokenBegin, 1 + lineEnd));
 			}
 			tokenCount++;
 		}
+		if (tokenCount == 2)
+			return new RebaseTodoLine(action, commit, """"); //$NON-NLS-1$
 		return null;
 	}",Buggy,"Fix parsing Rebase todo lines when commit message is missing

Bug: 422253
Change-Id: I9739b16c91d2df31a481360a712d3479a4eeee2e
Signed-off-by: Stefan Lay <stefan.lay@sap.com>",Buggy,"The commit message indicates a fix for parsing rebase todo lines when the commit message is missing. The diff modifies the `parseLine` method to handle cases where the commit message is absent in a rebase todo line. Specifically, it adds a condition `if (tokenCount == 2)` to create a `RebaseTodoLine` object with an empty string as the commit message when only the action and commit ID are present. This addresses the bug where the parser would fail when encountering such lines. The answer is **Buggy**."
jgit,168.json,846ef78a02edceb99940d7aa92dcd2462a85c602,"@@ -1,24 +1,24 @@
 	private RefTree rebuild(RefDatabase refdb) throws IOException {
 		RefTree tree = RefTree.newEmptyTree();
 		List<org.eclipse.jgit.internal.storage.reftree.Command> cmds
 			= new ArrayList<>();
 
 		Ref head = refdb.exactRef(HEAD);
 		if (head != null) {
 			cmds.add(new org.eclipse.jgit.internal.storage.reftree.Command(
 					null,
 					head));
 		}
 
 		for (Ref r : refdb.getRefs(RefDatabase.ALL).values()) {
-			if (r.getName().equals(txnCommitted)
+			if (r.getName().equals(txnCommitted) || r.getName().equals(HEAD)
 					|| r.getName().startsWith(txnNamespace)) {
 				continue;
 			}
 			cmds.add(new org.eclipse.jgit.internal.storage.reftree.Command(
 					null,
 					db.peel(r)));
 		}
 		tree.apply(cmds);
 		return tree;
 	}",Buggy,"Fix RebuildRefTree trying to add HEAD twice to RefTree

14dfa70520 fixed the problem that HEAD wasn't added to the reftree when
rebuilding the reftree in an empty repository where HEAD isn't yet
resolvable. Since non-resolvable refs are filtered out by
RefDatabase.getRefs(ALL) we have to add HEAD to the reftree explicitly
in this special case.

This fix resulted in another bug: rebuilding the reftree in a repository
which has a resolvable HEAD failed with a DirCacheNameConflictException
in RefTree.apply(). If HEAD is resolvable RefDatabase.getRefs(ALL) does
not filter out HEAD. This results in two identical CREATE commands for
HEAD which RefTree.apply() refuses to execute.

Fix this by no longer creating a duplicate CREATE command for HEAD.

See: I46cbc2611b9ae683ef7319dc46af277925dfaee5
Change-Id: I58dd6bcdef88820aa7de29761d43e2edfa18fcbe
Signed-off-by: Matthias Sohn <matthias.sohn@sap.com>",Buggy,"The commit message clearly states that the change fixes a bug where rebuilding the reftree in a repository with a resolvable HEAD resulted in a `DirCacheNameConflictException`. This was due to duplicate CREATE commands for HEAD. The diff modifies the loop that iterates through refs obtained from `refdb.getRefs(RefDatabase.ALL)`. It adds a condition `|| r.getName().equals(HEAD)` to the `if` statement, preventing the duplicate addition of the HEAD ref to the command list when HEAD is resolvable. This directly addresses the bug described in the commit message. The answer is **Buggy**."
jgit,8367.json,ec97912762754ee88f1af5ed80e993c545778242,"@@ -1,26 +1,26 @@
 	public int match(final RawCharSequence rcs) {
 		final int needleLen = needle.length;
 		final byte first = needle[0];
 
 		final byte[] text = rcs.buffer;
 		int matchPos = rcs.startPtr;
 		final int maxPos = rcs.endPtr - needleLen;
 
-		OUTER: for (; matchPos < maxPos; matchPos++) {
+		OUTER: for (; matchPos <= maxPos; matchPos++) {
 			if (neq(first, text[matchPos])) {
-				while (++matchPos < maxPos && neq(first, text[matchPos])) {
+				while (++matchPos <= maxPos && neq(first, text[matchPos])) {
 					/* skip */
 				}
-				if (matchPos == maxPos)
+				if (matchPos > maxPos)
 					return -1;
 			}
 
-			int si = ++matchPos;
+			int si = matchPos + 1;
 			for (int j = 1; j < needleLen; j++, si++) {
 				if (neq(needle[j], text[si]))
 					continue OUTER;
 			}
-			return matchPos - 1;
+			return matchPos;
 		}
 		return -1;
 	}",Buggy,"Fix multiple bugs in RawSubStringPattern used by MessageRevFilter

* Match at end of input was not handled correctly.
* When more than one character matched but not all, the next character
  was not considered as a match start (e.g. pattern ""abab"" didn't match
  input ""abaabab"").

Bug: 409144
Change-Id: Ia44682c618bfbb927f5567c194227421d222a160
Signed-off-by: Matthias Sohn <matthias.sohn@sap.com>",Buggy,"The commit message indicates multiple bug fixes in `RawSubStringPattern`, specifically addressing issues with matching at the end of the input and handling overlapping matches. The diff modifies the loop condition from `matchPos < maxPos` to `matchPos <= maxPos`, which allows the pattern to match at the very end of the input. Additionally, the return value is changed from `matchPos - 1` to `matchPos`. These changes directly address the bugs described in the commit message. The answer is **Buggy**."
jgit,6829.json,5e44bfa3ad462e1220426492c53606c6a643a970,"@@ -1,6 +1,10 @@
 	private boolean isNoNewlineAtEndOfFile(FileHeader fh) {
-		HunkHeader lastHunk = fh.getHunks().get(fh.getHunks().size() - 1);
+		List<? extends HunkHeader> hunks = fh.getHunks();
+		if (hunks == null || hunks.isEmpty()) {
+			return false;
+		}
+		HunkHeader lastHunk = hunks.get(hunks.size() - 1);
 		RawText lhrt = new RawText(lastHunk.getBuffer());
-		return lhrt.getString(lhrt.size() - 1).equals(
-				""\""); //$NON-NLS-1$
+		return lhrt.getString(lhrt.size() - 1)
+				.equals(""\""); //$NON-NLS-1$
 	}",Buggy,"Fix ApplyCommand which doesn't work if patch adds empty file

Bug: 548219
Change-Id: Ibb32132a38e54508a24489322da58ddfd80a1d9a
Signed-off-by: Anton Khodos <khodosanton@gmail.com>
Signed-off-by: Matthias Sohn <matthias.sohn@sap.com>",Buggy,"The commit message indicates a fix for a bug where ApplyCommand fails when a patch adds an empty file. The diff adds a check to handle cases where the file header has no hunks. If the list of hunks is null or empty, the method now returns false, preventing an error. This directly addresses the bug described in the commit message. The answer is **Buggy**."
jgit,9865.json,3e2ec7e5e81a489a326d872aa0670119ff2c1152,"@@ -1,8 +1,8 @@
 	public int compareTo(LfsPointer o) {
 		int x = getOid().compareTo(o.getOid());
 		if (x != 0) {
 			return x;
 		}
 
-		return (int) (getSize() - o.getSize());
+		return Long.compare(getSize(), o.getSize());
 	}",Buggy,"Fix BadComparable error flagged by error prone

Running recent error prone version complaining on that code:

LfsPointer.java:171: error: [BadComparable] Possible sign flip from
narrowing conversion
		return (int) (getSize() - o.getSize());
		       ^
    (see https://errorprone.info/bugpattern/BadComparable)
  Did you mean 'return Long.compare(getSize(), o.getSize());'?

Bug: 562756
Change-Id: I0522f1025319a9290c448a064fbafdb4b16d1d59
Signed-off-by: David Ostrovsky <david@ostrovsky.org>
",Buggy,"The commit message indicates a fix for a `BadComparable` error, which arises from a potential sign flip during a narrowing conversion when comparing `getSize()` values. The original code casts the difference of two `getSize()` values (which are presumably `long`) to an `int`, which can lead to incorrect comparison results if the difference is outside the range of `int`. The corrected code uses `Long.compare()` to avoid this issue, providing a correct and robust comparison. The answer is **Buggy**.
"
jgit,7605.json,56ee8117802a672e80011ee07ea6b253a5b7d0a9,"@@ -1,13 +1,17 @@
 	public boolean isPathSuffix(final byte[] p, final int pLen) {
 		final AbstractTreeIterator t = currentHead;
 		final byte[] c = t.path;
 		final int cLen = t.pathLen;
-		int ci;
 
-		for (ci = 1; ci < cLen && ci < pLen; ci++) {
-			if (c[cLen-ci] != p[pLen-ci])
+		for (int i = 1; i <= pLen; i++) {
+			// Pattern longer than current path
+			if (i > cLen)
+				return false;
+			// Current path doesn't match pattern
+			if (c[cLen - i] != p[pLen - i])
 				return false;
 		}
 
+		// Whole pattern tested -> matches
 		return true;
 	}",Buggy,"Fix bugs in TreeWalk#isPathSuffix used by PathSuffixFilter

* It didn't check the first character in the pattern due to a off-by-one
  error. Spotted by James Roper.
* It returned true even when pattern was longer than current path, e.g.
  it returned that "".txt"" is suffix of ""txt"".

Bug: 411999
Change-Id: I9fbcd68a11fb57cc49956b70c387a47271a0424f
Signed-off-by: Robin Stocker <robin@nibor.org>
",Buggy,The commit message clearly states that it fixes bugs in `TreeWalk#isPathSuffix`. The diff shows a change in the loop condition and the addition of a check to ensure the pattern is not longer than the current path. The original code had an off-by-one error and incorrectly returned true when the pattern was longer than the current path. The corrected code addresses these issues by adjusting the loop and adding a length check. The answer is **Buggy**.
jgit,7675.json,5b55498b16267102b021a47b3a7c0bdbfae63e71,"@@ -1,5 +1,3 @@
 	protected byte[] idSubmodule(final Entry e) {
-		if (repository == null)
-			return idSubmodule(getDirectory(), e);
-		return super.idSubmodule(e);
+		return idSubmodule(getDirectory(), e);
 	}",Buggy,"Fix FileTreeIterator.idSubmodule(Entry)

FileTreeIterator was calling by mistake
WorkingTreeIterator.idSubmodule(Entry). Instead it should always compute
idSubmodule on its own.

Change-Id: Id1b988aded06939b1d7edd2671e34bf756896c0e
",Buggy,"The commit message indicates a bug fix in `FileTreeIterator.idSubmodule(Entry)`. The diff shows that the original code was mistakenly calling `WorkingTreeIterator.idSubmodule(Entry)` in some cases, which was incorrect. The corrected code now always computes `idSubmodule` on its own, as intended. This resolves the bug where the wrong method was being called, leading to incorrect behavior. The answer is **Buggy**.
"
jgit,4217.json,e60b9e1879f8774e1afe07be4224605045f49eec,"@@ -1,8 +1,9 @@
 	private long getEffectiveRacyThreshold() {
 		long timestampResolution = fileStoreAttributeCache
 				.getFsTimestampResolution().toNanos();
 		long minRacyInterval = fileStoreAttributeCache.getMinimalRacyInterval()
 				.toNanos();
-		// add a 30% safety margin
-		return Math.max(timestampResolution, minRacyInterval) * 13 / 10;
+		long max = Math.max(timestampResolution, minRacyInterval);
+		// safety margin: factor 2.5 below 100ms otherwise 1.25
+		return max < 100_000_000L ? max * 5 / 2 : max * 5 / 4;
 	}",Buggy,"FileSnapshot: fix bug with timestamp thresholding

Increase the safety factor to 2.5x for extra safety if max of measured
timestamp resolution and measured minimal racy threshold is < 100ms, use
1.25 otherwise since for large filesystem resolution values the
influence of finite resolution of the system clock should be negligible.

Before, not yet using the newly introduced minRacyThreshold measurement,
the threshold was 1.1x FS resolution, and we could issue the
following sequence of events,

  start
  create-file
  read-file (currentTime)
  end

which had the following timestamps:

  create-file 1564589081998
  start 1564589082002
  read 1564589082003
  end 1564589082004

In this case, the difference between create-file and read is 5ms,
which exceeded the 4ms FS resolution, even though the events together
took just 2ms of runtime.

Reproduce with:
  bazel test --runs_per_test=100 \
    //org.eclipse.jgit.test:org_eclipse_jgit_internal_storage_file_FileSnapshotTest

The file system timestamp resolution is 4ms in this case.

This code assumes that the kernel and the JVM use the same clock that
is synchronized with the file system clock. This seems plausible,
given the resolution of System.currentTimeMillis() and the latency for
a gettimeofday system call (typically ~1us), but it would be good to
justify this with specifications.

Also cover a source of flakiness: if the test runs under extreme load,
then we could have

  start
  create-file
  <long delay>
  read
  end

which would register as an unmodified file. Avoid this by skipping the
test if end-start is too big.

[msohn]:
- downported from master to stable-5.1
- skip test if resolution is below 10ms
- adjust safety factor to 1.25 for resolutions above 100ms

Change-Id: I87d2cf035e01c44b7ba8364c410a860aa8e312ef
Signed-off-by: Han-Wen Nienhuys <hanwen@google.com>
Signed-off-by: Matthias Sohn <matthias.sohn@sap.com>
",Buggy,"The commit message indicates a bug fix related to timestamp thresholding in `FileSnapshot`. The explanation details a scenario where the file system resolution was insufficient, leading to incorrect file modification detection. The provided diff modifies the `getEffectiveRacyThreshold` method, increasing the safety factor for timestamp comparison. Specifically, it changes the safety margin calculation, using a factor of 2.5 when the maximum of the measured timestamp resolution and the measured minimal racy threshold is less than 100ms, and 1.25 otherwise. This adjustment aims to prevent false negatives in file modification detection due to limitations in timestamp resolution. The commit message clearly states the bug and the diff directly addresses it. The answer is **Buggy**."
jgit,5424.json,35b01dac4c81542b195169e3b7365c12a165232c,"@@ -1,45 +1,41 @@
 	public void fillTo(final int highMark) throws MissingObjectException,
 			IncorrectObjectTypeException, IOException {
 		if (walker == null || size > highMark)
 			return;
 
-		Generator p = walker.pending;
-		RevCommit c = p.next();
+		RevCommit c = walker.next();
 		if (c == null) {
-			walker.pending = EndGenerator.INSTANCE;
 			walker = null;
 			return;
 		}
 		enter(size, (E) c);
 		add((E) c);
-		p = walker.pending;
 
 		while (size <= highMark) {
 			int index = size;
 			Block s = contents;
 			while (index >> s.shift >= BLOCK_SIZE) {
 				s = new Block(s.shift + BLOCK_SHIFT);
 				s.contents[0] = contents;
 				contents = s;
 			}
 			while (s.shift > 0) {
 				final int i = index >> s.shift;
 				index -= i << s.shift;
 				if (s.contents[i] == null)
 					s.contents[i] = new Block(s.shift - BLOCK_SHIFT);
 				s = (Block) s.contents[i];
 			}
 
 			final Object[] dst = s.contents;
 			while (size <= highMark && index < BLOCK_SIZE) {
-				c = p.next();
+				c = walker.next();
 				if (c == null) {
-					walker.pending = EndGenerator.INSTANCE;
 					walker = null;
 					return;
 				}
 				enter(size++, (E) c);
 				dst[index++] = c;
 			}
 		}
 	}",Buggy,"Fix RevCommitList to work with subclasses of RevWalk

Bug: 321502
Change-Id: Ic4bc49a0da90234271aea7c0a4e344a1c3620cfc
Signed-off-by: Matthias Sohn <matthias.sohn@sap.com>",Buggy,"The commit message indicates a fix for `RevCommitList` to work with subclasses of `RevWalk`. The code diff removes the `Generator p` and uses `walker.next()` directly. Also, the lines `walker.pending = EndGenerator.INSTANCE;` are removed. This suggests that the previous implementation had issues with how it handled the `RevWalk`'s pending commits, especially when subclasses were involved. The change simplifies the logic and directly uses the `walker` to get the next commit. This indicates a bug fix related to how `RevCommitList` interacted with `RevWalk` and its subclasses. The answer is **Buggy**."
jgit,5825.json,b07db609089749ed49a7f0b1fb3841a8f74110c2,"@@ -1,13 +1,16 @@
 	static int count(String s, char c, boolean ignoreFirstLast) {
 		int start = 0;
 		int count = 0;
-		while (true) {
+		int length = s.length();
+		while (start < length) {
 			start = s.indexOf(c, start);
-			if (start == -1)
+			if (start == -1) {
 				break;
-			if (!ignoreFirstLast || (start != 0 && start != s.length()))
+			}
+			if (!ignoreFirstLast || (start != 0 && start != length - 1)) {
 				count++;
+			}
 			start++;
 		}
 		return count;
 	}",Buggy,"Fix off-by-one error in Strings.count()

Change-Id: I0667b1624827d1cf0cc1b81f86c7bb44eafd68a7
Signed-off-by: Thomas Wolf <thomas.wolf@paranor.ch>",Buggy,"The commit message indicates a fix for an off-by-one error in the `Strings.count()` method. The code change introduces a `length` variable to store the string's length and modifies the conditional statement `start != s.length()` to `start != length - 1`. This adjustment likely corrects a scenario where the last character was incorrectly considered when `ignoreFirstLast` was true, thus fixing the off-by-one error. The answer is **Buggy**."
jgit,6619.json,51a5cc7f1a1033664ee2fb760ed217b665b12b34,"@@ -1,6 +1,6 @@
 	private static int findForwardLine(IntList lines, int idx, int ptr) {
 		final int end = lines.size() - 2;
-		while (idx < end && lines.get(idx + 2) <= ptr)
+		while (idx < end && lines.get(idx + 2) < ptr)
 			idx++;
 		return idx;
 	}",Buggy,"Fix diff when first text is the start of the other

The problem occurred when the first text ends in the middle
of the last line of the other text and the first text has no
end of line.

Bug: 344975
Change-Id: I1f0dd9f8062f2148a7c1341c9122202e082ad19d
Signed-off-by: Robin Rosenberg <robin.rosenberg@dewire.com>
",Buggy,"The commit message indicates a bug fix related to finding differences between texts, specifically when one text is a prefix of the other and lacks a newline character at the end. The diff modifies the `findForwardLine` method by changing the loop condition from `lines.get(idx + 2) <= ptr` to `lines.get(idx + 2) < ptr`. This suggests that the original code might have incorrectly advanced the index `idx` in certain scenarios, potentially leading to an inaccurate diff calculation. The change ensures that the index is only incremented if the value at `lines.get(idx + 2)` is strictly less than `ptr`, which likely addresses the bug described in the commit message. The answer is **Buggy**."
jgit,6447.json,e0e52cb0110a908959f7df13c5e66bf911bad74f,"@@ -1,6 +1,6 @@
 	private boolean result(Candidate n) throws IOException {
 		n.beginResult(revPool);
 		outCandidate = n;
 		outRegion = n.regionList;
-		return true;
+		return outRegion != null;
 	}",Buggy,"Fix NPE in BlameGenerator.getSourceStart()

Bug: 499543
Change-Id: I99f6ebb1c3ceea20e8ca093acbe824c9f0362d45
Signed-off-by: Matthias Sohn <matthias.sohn@sap.com>",Buggy,"The commit message indicates a fix for a NullPointerException (NPE) in `BlameGenerator.getSourceStart()`. The diff modifies the `result` method to return `outRegion != null` instead of `true`. This suggests that `outRegion` could be null in some cases, and the original code didn't handle this, potentially leading to an NPE when `outRegion` was later used. The change ensures that the method returns `true` only if `outRegion` is not null, thus preventing the potential NPE. The answer is **Buggy**.
"
jgit,6994.json,0505657d6a7dd4575a64ddfb5c0928870fe5843b,"@@ -1,9 +1,9 @@
 	public CommitCommand setAll(boolean all) {
 		checkCallable();
-		if (!only.isEmpty())
+		if (all && !only.isEmpty())
 			throw new JGitInternalException(MessageFormat.format(
 					JGitText.get().illegalCombinationOfArguments, ""--all"", //$NON-NLS-1$
 					""--only"")); //$NON-NLS-1$
 		this.all = all;
 		return this;
 	}",Buggy,"commit command: allow to specify path(s) argument(s)

This fixes the command below:

jgit commit a -m ""added file a""

which currently fails with:

org.eclipse.jgit.api.errors.JGitInternalException: The combination of
arguments --all and --only is not allowed

Bug: 484973
Change-Id: I37a4ccd68101a66520ef99110f7aa0cbdcc8beba
Signed-off-by: Andrey Loskutov <loskutov@gmx.de>
",Buggy,"The commit message indicates a fix for a bug where using both `--all` and `--only` arguments in the `commit` command resulted in a `JGitInternalException`. The diff modifies the `setAll` method to throw an exception only when `all` is true and `only` is not empty, effectively allowing the command to proceed when only paths are specified. This resolves the reported bug. The answer is **Buggy**.
"
jgit,7416.json,e56d50a94bfe60663bd589da61e07b9461f2c5f7,"@@ -1,22 +1,22 @@
 	private static String composeSquashMessage(boolean isSquash,
 			RevCommit commitToPick, String currSquashMessage, int count) {
 		StringBuilder sb = new StringBuilder();
 		String ordinal = getOrdinal(count);
 		sb.setLength(0);
 		sb.append(""# This is a combination of "").append(count)
 				.append("" commits.\n"");
+		// Add the previous message without header (i.e first line)
+		sb.append(currSquashMessage.substring(currSquashMessage.indexOf(""\n"") + 1));
+		sb.append(""\n"");
 		if (isSquash) {
 			sb.append(""# This is the "").append(count).append(ordinal)
 					.append("" commit message:\n"");
 			sb.append(commitToPick.getFullMessage());
 		} else {
 			sb.append(""# The "").append(count).append(ordinal)
 					.append("" commit message will be skipped:\n# "");
 			sb.append(commitToPick.getFullMessage().replaceAll(""([\n\r])"",
 					""$1# ""));
 		}
-		// Add the previous message without header (i.e first line)
-		sb.append(""\n"");
-		sb.append(currSquashMessage.substring(currSquashMessage.indexOf(""\n"") + 1));
 		return sb.toString();
 	}",Buggy,"Interactive rebase: Fix order of commit messages on squash

Bug: 431214
Change-Id: I295bfdc5751545b046d7fe7efc3f8b39ab4f5415
Signed-off-by: Stefan Lay <stefan.lay@sap.com>
Signed-off-by: Robin Rosennberg <robin.rosenberg@dewire.com>
",Buggy,"The commit message indicates a fix for the order of commit messages during a squash operation in an interactive rebase. The diff modifies the `composeSquashMessage` method, specifically reordering the appending of the current squash message and the new commit message. The original code appended the current squash message *after* the new commit message, which would result in an incorrect order. The corrected code appends the current squash message *before* the new commit message, fixing the order. This clearly indicates a bug fix. The answer is **Buggy**.
"
jgit,7416.json,8339a07e8314d6a40e15252bcc736a46c0aca0ea,"@@ -1,22 +1,22 @@
 	private static String composeSquashMessage(boolean isSquash,
 			RevCommit commitToPick, String currSquashMessage, int count) {
 		StringBuilder sb = new StringBuilder();
 		String ordinal = getOrdinal(count);
 		sb.setLength(0);
 		sb.append(""# This is a combination of "").append(count)
 				.append("" commits.\n"");
 		if (isSquash) {
 			sb.append(""# This is the "").append(count).append(ordinal)
 					.append("" commit message:\n"");
 			sb.append(commitToPick.getFullMessage());
 		} else {
 			sb.append(""# The "").append(count).append(ordinal)
 					.append("" commit message will be skipped:\n# "");
-			sb.append(commitToPick.getFullMessage().replaceAll(""([\n\r]+)"",
+			sb.append(commitToPick.getFullMessage().replaceAll(""([\n\r])"",
 					""$1# ""));
 		}
 		// Add the previous message without header (i.e first line)
 		sb.append(""\n"");
 		sb.append(currSquashMessage.substring(currSquashMessage.indexOf(""\n"") + 1));
 		return sb.toString();
 	}",Buggy,"Fix FIXUP error for blank lines in interactive rebase

Empty lines of discarded commit messages were added to the commit
message because they were not commented out properly.

Bug: 422246
Change-Id: I263e8a6b30a3392d8b4f09c0695505068a0a485d
Signed-off-by: Stefan Lay <stefan.lay@sap.com>",Buggy,"The commit message indicates a fix for a bug where empty lines in discarded commit messages were incorrectly added to the commit message during interactive rebase. The diff modifies the `composeSquashMessage` method to properly comment out these empty lines. Specifically, the `replaceAll` method now correctly replaces newline characters (`\n` or `\r`) with `$1# `, ensuring that empty lines are commented out. This aligns with the bug description in the commit message. The answer is **Buggy**.
"
jgit,9377.json,325cb35ccd6108eaf9e6c3ec6343ca6988f10a21,"@@ -1,6 +1,8 @@
 	private ObjectId idFor(int objType, byte[] raw) {
 		if (skipList != null) {
-			return new ObjectInserter.Formatter().idFor(objType, raw);
+			try (ObjectInserter.Formatter fmt = new ObjectInserter.Formatter()) {
+				return fmt.idFor(objType, raw);
+			}
 		}
 		return null;
 	}",Buggy,"[infer] Fix resource leak in ObjectChecker

Bug: 509385
Change-Id: I6b6ff5b721d959eb0708003a40c8f97d6826ac46
Signed-off-by: Matthias Sohn <matthias.sohn@sap.com>",Buggy,"The commit message indicates a fix for a resource leak in `ObjectChecker`. The diff introduces a try-with-resources block around the `ObjectInserter.Formatter` instantiation. This ensures that the `Formatter` is properly closed and its resources are released after use, preventing a potential resource leak. The change aligns with the commit message, indicating a bug fix. The answer is **Buggy**."
weka,20283.json,8215c432375cb2ff1bbda2825426fa0535b77834,"@@ -1,25 +1,13 @@
-  public EventSetDescriptor [] getEventSetDescriptors() {
+  public EventSetDescriptor[] getEventSetDescriptors() {
     try {
-      EventSetDescriptor [] esds = 
-      { new EventSetDescriptor(DataSource.class, 
-                               ""dataSet"", 
-                               DataSourceListener.class, 
-                               ""acceptDataSet""),
-        new EventSetDescriptor(DataSource.class, 
-                               ""instance"", 
-                               InstanceListener.class, 
-                               ""acceptInstance""),
-        new EventSetDescriptor(TrainingSetProducer.class, 
-                               ""trainingSet"", 
-                               TrainingSetListener.class, 
-                               ""acceptTrainingSet""),
-        new EventSetDescriptor(TestSetProducer.class, 
-                               ""testSet"", 
-                               TestSetListener.class, 
-                               ""acceptTestSet"")  };
+      EventSetDescriptor[] esds = {
+        new EventSetDescriptor(DataSource.class, ""dataSet"",
+          DataSourceListener.class, ""acceptDataSet""),
+        new EventSetDescriptor(DataSource.class, ""instance"",
+          InstanceListener.class, ""acceptInstance""), };
       return esds;
     } catch (Exception ex) {
       ex.printStackTrace();
     }
     return null;
   }",Buggy,"Fixed a bug in the specification of event types produced by Appender and in the routine that determines whether a particular event type can be generated at a given point in time.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@10150 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to event types produced by Appender and the routine that determines whether a particular event type can be generated at a given point in time. The diff removes `TrainingSetProducer` and `TestSetProducer` event set descriptors. This suggests that the original implementation incorrectly included these event types, and the fix corrects this by removing them. The try-catch block remains, but the core change is the removal of the incorrect event types. The answer is **Buggy**."
weka,26539.json,88c70a2f184a0b7b2d27a48ebc04083d0d800049,"@@ -1,3 +1,6 @@
   public static List<String> getAllMetricNames() {
-    return Evaluation.getAllEvaluationMetricNames();
+    List<String> metrics = getBuiltInMetricNames();
+    metrics.addAll(getPluginMetricNames());
+
+    return metrics;
   }",Buggy,"Fixed a bug in the getAllMetricNames() method.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@10919 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix in the `getAllMetricNames()` method. The diff shows that the method's implementation has been changed from simply returning the result of `Evaluation.getAllEvaluationMetricNames()` to combining the results of `getBuiltInMetricNames()` and `getPluginMetricNames()`. This suggests that the original implementation was incomplete or incorrect, and the new implementation fixes this issue by including metrics from both built-in and plugin sources. The answer is **Buggy**."
weka,20282.json,289721c0795b80e42b2664ffaa47dfab08f7ceac,"@@ -1,6 +1,7 @@
   private String statusMessagePrefix() {
     return getCustomName() + ""$"" + hashCode() + ""|""
-    + ((m_Filter instanceof OptionHandler) 
+    + ((m_Filter instanceof OptionHandler &&
+        Utils.joinOptions(((OptionHandler)m_Filter).getOptions()).length() > 0) 
         ? Utils.joinOptions(((OptionHandler)m_Filter).getOptions()) + ""|""
             : """");
   }",Buggy,"Fixed a minor status logging bug.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@4797 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix for a minor status logging bug. The code change modifies the `statusMessagePrefix` method to include options of the filter only if the filter is an `OptionHandler` and has options. This prevents the method from attempting to access options when they don't exist, which could lead to an error or incorrect status message. The addition of `Utils.joinOptions(((OptionHandler)m_Filter).getOptions()).length() > 0` acts as a guard. The answer is **Buggy**."
weka,19858.json,06022e54131102f3817c490551c2babc818ef2e2,"@@ -1,47 +1,49 @@
   protected void newFlow() {
     m_newFlowBut.setEnabled(false);
 
     String user = m_viewer.getUser();
     String password = m_viewer.getPassword();
     String uRL = m_viewer.getURL();
     String query = m_viewer.getQuery();
 
     if (query == null) {
       query = """";
     }
 
     try {
       DatabaseLoader dbl = new DatabaseLoader();
       dbl.setUser(user);
       dbl.setPassword(password);
       dbl.setUrl(uRL);
       dbl.setQuery(query);
 
       BeanContextSupport bc = new BeanContextSupport();
       bc.setDesignTime(true);
 
       Loader loaderComp = new Loader();
       bc.add(loaderComp);
       loaderComp.setLoader(dbl);
 
       KnowledgeFlowApp singleton = KnowledgeFlowApp.getSingleton();
       m_mainPerspective.addTab(""DBSource"");
-      /*
-       * BeanInstance beanI = new
-       * BeanInstance(m_mainPerspective.getBeanLayout(m_mainPerspective
-       * .getNumTabs() - 1), loaderComp, 50, 50, m_mainPerspective.getNumTabs()
-       * - 1);
-       */
-      Vector<Object> beans = BeanInstance.getBeanInstances(m_mainPerspective
-        .getNumTabs() - 1);
-      Vector<BeanConnection> connections = BeanConnection
-        .getConnections(m_mainPerspective.getNumTabs() - 1);
-      singleton.integrateFlow(beans, connections, true, false);
+
+      // The process of creating a BeanInstance integrates will result
+      // in it integrating itself into the flow in the specified tab
+      new BeanInstance(m_mainPerspective.getBeanLayout(m_mainPerspective
+        .getNumTabs() - 1), loaderComp, 50, 50,
+        m_mainPerspective.getNumTabs()
+        - 1);
+
+      // Vector<Object> beans = BeanInstance.getBeanInstances(m_mainPerspective
+      // .getNumTabs() - 1);
+      // Vector<BeanConnection> connections = BeanConnection
+      // .getConnections(m_mainPerspective.getNumTabs() - 1);
+      // singleton.integrateFlow(beans, connections, true, false);
       singleton.setActivePerspective(0); // switch back to the main perspective
 
       m_newFlowBut.setEnabled(true);
 
     } catch (Exception ex) {
       ex.printStackTrace();
     }
   }",Buggy,"Fixed a bug where the code that creates the DatabaseLoader component when the new flow button is pressed got accidently commented out.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@11286 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message states that code for creating the DatabaseLoader component was accidentally commented out. The diff shows that a block of code related to creating a `BeanInstance` was uncommented, and a similar block of code using `BeanInstance.getBeanInstances` and `BeanConnection.getConnections` was commented out. This aligns with the commit message's description of fixing an accidentally commented-out section of code. The answer is **Buggy**."
weka,5643.json,a6289b3f0cf2a2301bce6c1a9f33e7f522996b43,"@@ -1,24 +1,28 @@
   protected Instance mergeInstances(Instance source, Instance dest) {
 
     Instances outputFormat = outputFormatPeek();
     double[] vals = new double[outputFormat.numAttributes()];
     for(int i = 0; i < vals.length; i++) {
       if ((i != outputFormat.classIndex()) && (m_SelectedCols.isInRange(i))) {
         if (source != null) {
           vals[i] = source.value(i);
         } else {
           vals[i] = Utils.missingValue();
         }
       } else {
         vals[i] = dest.value(i);
       }
     }
     Instance inst = null;
     if (dest instanceof SparseInstance) {
       inst = new SparseInstance(dest.weight(), vals);
     } else {
       inst = new DenseInstance(dest.weight(), vals);
     }
-    inst.setDataset(dest.dataset());
+    // inst.setDataset(dest.dataset());
+    // push() sets the dataset to the output format, however, if
+    // a preview transformation is being done then push() does not
+    // get called, so set the output format correctly here.
+    inst.setDataset(outputFormat);
     return inst;
   }",Buggy,"Fixed a bug that affected the structure (dataset) assigned to a transformed instance when doing a preview transformation

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@13412 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to the dataset assigned to a transformed instance during a preview transformation. The code diff shows that the line `inst.setDataset(dest.dataset());` is commented out and replaced with `inst.setDataset(outputFormat);`. The comment explains that `push()` usually sets the dataset to the output format, but this doesn't happen during preview transformations. Therefore, the dataset is explicitly set to `outputFormat` in this case. This change directly addresses the bug described in the commit message. The answer is **Buggy**."
weka,31430.json,ac9fe6291a2243cdf039b445dd9190343c4c6d51,"@@ -1,18 +1,13 @@
   public final Matrix transpose() {
 
     int nr = m_Elements.length, nc = m_Elements[0].length;
-    Matrix b;
-    try {
-      b = (Matrix)clone();
-    } catch (CloneNotSupportedException ex) {
-      b = new Matrix(nr, nc);
-    }
+    Matrix b = new Matrix(nc, nr);
 
     for(int i = 0;i < nc; i++) {
       for(int j = 0; j < nr; j++) {
 	b.m_Elements[i][j] = m_Elements[j][i];
       }
     }
 
     return b;
   }",Buggy,"Fixed the matrix transpose problem introduced with last checkin.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@773 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message clearly states that it fixes a matrix transpose problem introduced in the last check-in. The code diff shows that the transpose function was modified. The original code attempted to clone the matrix, but this is unnecessary for a transpose operation. The corrected code directly creates a new matrix with dimensions swapped (nc, nr) and populates it with the transposed elements. This confirms that the changes are indeed fixing a bug related to the matrix transpose operation. The answer is **Buggy**."
weka,30779.json,a24ded419dad69fad9e5bdfca523043e633de4a1,"@@ -1,32 +1,34 @@
     public Instance nextElement(Instances dataset) {
       Instance	result;
       
       result = null;
       
       if (isIncremental()) {
 	// is there still an instance in the buffer?
 	if (m_IncrementalBuffer != null) {
 	  result              = m_IncrementalBuffer;
 	  m_IncrementalBuffer = null;
 	}
 	else {
 	  try {
 	    result = m_Loader.getNextInstance(dataset);
 	  }
 	  catch (Exception e) {
 	    e.printStackTrace();
 	    result = null;
 	  }
 	}
       }
       else {
 	if (m_BatchCounter < m_BatchBuffer.numInstances()) {
 	  result = m_BatchBuffer.instance(m_BatchCounter);
 	  m_BatchCounter++;
 	}
       }
 
-      result.setDataset(dataset);
+      if (result != null) {
+        result.setDataset(dataset);
+      }
       
       return result;
     }",Buggy,"Fixed a bug where a null pointer could get dereferenced in the nextElement() method of DataSource.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@6417 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix for a null pointer dereference in the `nextElement()` method of the `DataSource` class. The diff adds a null check before calling `result.setDataset(dataset)`. This prevents a `NullPointerException` if `result` is null, which could occur if `m_IncrementalBuffer` is null and `m_Loader.getNextInstance(dataset)` throws an exception or returns null, or if `m_BatchCounter` exceeds the number of instances in `m_BatchBuffer`. The added null check directly addresses the potential null pointer dereference described in the commit message. The answer is **Buggy**."
weka,14864.json,8ed966e4e1e65cf3ca1691aac3767c74ad03ae50,"@@ -1,214 +1,211 @@
   public int[] search (ASEvaluation ASEval, Instances data)
     throws Exception {
     m_totalEvals = 0;
     if (!(ASEval instanceof SubsetEvaluator)) {
       throw  new Exception(ASEval.getClass().getName() 
 			   + "" is not a "" 
 			   + ""Subset evaluator!"");
     }
 
     if (ASEval instanceof UnsupervisedSubsetEvaluator) {
       m_hasClass = false;
-    }
-    else {
+    } else {
       m_hasClass = true;
       m_classIndex = data.classIndex();
     }
 
     SubsetEvaluator ASEvaluator = (SubsetEvaluator)ASEval;
     m_numAttribs = data.numAttributes();
     int i, j;
     int best_size = 0;
     int size = 0;
     int done;
     int sd = m_searchDirection;
     BitSet best_group, temp_group;
     int stale;
     double best_merit;
     double merit;
     boolean z;
     boolean added;
     Link2 tl;
     Hashtable lookup = new Hashtable(m_cacheSize * m_numAttribs);
     int insertCount = 0;
     int cacheHits = 0;
     LinkedList2 bfList = new LinkedList2(m_maxStale);
     best_merit = -Double.MAX_VALUE;
     stale = 0;
     best_group = new BitSet(m_numAttribs);
 
     m_startRange.setUpper(m_numAttribs-1);
     if (!(getStartSet().equals(""""))) {
       m_starting = m_startRange.getSelection();
     }
     // If a starting subset has been supplied, then initialise the bitset
     if (m_starting != null) {
       for (i = 0; i < m_starting.length; i++) {
 	if ((m_starting[i]) != m_classIndex) {
 	  best_group.set(m_starting[i]);
 	}
       }
 
       best_size = m_starting.length;
       m_totalEvals++;
-    }
-    else {
+    } else {
       if (m_searchDirection == SELECTION_BACKWARD) {
 	setStartSet(""1-last"");
 	m_starting = new int[m_numAttribs];
 
 	// init initial subset to all attributes
 	for (i = 0, j = 0; i < m_numAttribs; i++) {
 	  if (i != m_classIndex) {
 	    best_group.set(i);
 	    m_starting[j++] = i;
 	  }
 	}
 
 	best_size = m_numAttribs - 1;
 	m_totalEvals++;
       }
     }
 
     // evaluate the initial subset
     best_merit = ASEvaluator.evaluateSubset(best_group);
     // add the initial group to the list and the hash table
     Object [] best = new Object[1];
     best[0] = best_group.clone();
     bfList.addToList(best, best_merit);
     BitSet tt = (BitSet)best_group.clone();
     String hashC = tt.toString();
-    lookup.put(hashC, """");
+    lookup.put(hashC, new Double(best_merit));
 
     while (stale < m_maxStale) {
       added = false;
 
       if (m_searchDirection == SELECTION_BIDIRECTIONAL) {
 	// bi-directional search
-	  done = 2;
-	  sd = SELECTION_FORWARD;
-	}
-      else {
+        done = 2;
+        sd = SELECTION_FORWARD;
+      } else {
 	done = 1;
       }
 
       // finished search?
       if (bfList.size() == 0) {
 	stale = m_maxStale;
 	break;
       }
 
       // copy the attribute set at the head of the list
       tl = bfList.getLinkAt(0);
       temp_group = (BitSet)(tl.getData()[0]);
       temp_group = (BitSet)temp_group.clone();
       // remove the head of the list
       bfList.removeLinkAt(0);
       // count the number of bits set (attributes)
       int kk;
 
       for (kk = 0, size = 0; kk < m_numAttribs; kk++) {
 	if (temp_group.get(kk)) {
 	  size++;
 	}
       }
 
       do {
 	for (i = 0; i < m_numAttribs; i++) {
 	  if (sd == SELECTION_FORWARD) {
 	    z = ((i != m_classIndex) && (!temp_group.get(i)));
-	  }
-	  else {
+	  } else {
 	    z = ((i != m_classIndex) && (temp_group.get(i)));
 	  }
-
+          
 	  if (z) {
 	    // set the bit (attribute to add/delete)
 	    if (sd == SELECTION_FORWARD) {
 	      temp_group.set(i);
 	      size++;
-	    }
-	    else {
+	    } else {
 	      temp_group.clear(i);
 	      size--;
 	    }
 
 	    /* if this subset has been seen before, then it is already 
 	       in the list (or has been fully expanded) */
 	    tt = (BitSet)temp_group.clone();
 	    hashC = tt.toString();
+	    
 	    if (lookup.containsKey(hashC) == false) {
 	      merit = ASEvaluator.evaluateSubset(temp_group);
 	      m_totalEvals++;
-
-	      if (m_debug) {
-		System.out.print(""Group: "");
-		printGroup(tt, m_numAttribs);
-		System.out.println(""Merit: "" + merit);
-	      }
-
-	      // is this better than the best?
-	      if (sd == SELECTION_FORWARD) {
-		z = ((merit - best_merit) > 0.00001);
-	      }
-	      else {
-		if (merit == best_merit) {
-		  z = (size < best_size);
-		} else {
-		  z = (merit >  best_merit);
-		} 
-	      }
-
-	      if (z) {
-		added = true;
-		stale = 0;
-		best_merit = merit;
-		//		best_size = (size + best_size);
-		best_size = size;
-		best_group = (BitSet)(temp_group.clone());
-	      }
-
+	      
+	      // insert this one in the hashtable
 	      if (insertCount > m_cacheSize * m_numAttribs) {
 		lookup = new Hashtable(m_cacheSize * m_numAttribs);
 		insertCount = 0;
 	      }
-	      // insert this one in the list and in the hash table
-	      Object [] add = new Object[1];
-	      add[0] = tt.clone();
-	      bfList.addToList(add, merit);
 	      hashC = tt.toString();
-	      lookup.put(hashC, """");
-	      insertCount++;
+    	      lookup.put(hashC, new Double(merit));
+    	      insertCount++;
 	    } else {
-	      cacheHits++;
+	      merit = ((Double)lookup.get(hashC)).doubleValue();
+	      cacheHits++;  
+	    }
+	    
+	    // insert this one in the list
+	    Object[] add = new Object[1];
+	    add[0] = tt.clone();
+	    bfList.addToList(add, merit);
+	    
+	    if (m_debug) {
+	      System.out.print(""Group: "");
+	      printGroup(tt, m_numAttribs);
+	      System.out.println(""Merit: "" + merit);
+	    }
+
+	    // is this better than the best?
+	    if (sd == SELECTION_FORWARD) {
+	      z = ((merit - best_merit) > 0.00001);
+	    } else {
+	      if (merit == best_merit) {
+		z = (size < best_size);
+	      } else {
+		z = (merit >  best_merit);
+	      } 
+	    }
+
+	    if (z) {
+	      added = true;
+	      stale = 0;
+	      best_merit = merit;
+	      //		best_size = (size + best_size);
+	      best_size = size;
+	      best_group = (BitSet)(temp_group.clone());
 	    }
 
 	    // unset this addition(deletion)
 	    if (sd == SELECTION_FORWARD) {
 	      temp_group.clear(i);
 	      size--;
-	    }
-	    else {
+	    } else {
 	      temp_group.set(i);
 	      size++;
 	    }
 	  }
 	}
 
 	if (done == 2) {
 	  sd = SELECTION_BACKWARD;
 	}
 
 	done--;
       } while (done > 0);
 
       /* if we haven't added a new attribute subset then full expansion 
 	 of this node hasen't resulted in anything better */
       if (!added) {
 	stale++;
       }
     }
 
     m_bestMerit = best_merit;
     return  attributeList(best_group);
   }",Buggy,"Added Martin Guetlein's bug fix for merit caching


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@4162 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to merit caching. The diff introduces a change in how merit values are stored and retrieved from the `lookup` hashtable. Previously, the code only stored a placeholder value ("""") in the hashtable, and the merit was recalculated every time a subset was encountered. The corrected code stores the actual merit value (as a Double object) in the hashtable and retrieves it when the subset is encountered again. This ensures that the cached merit value is used, which is essential for the correct functioning of the merit caching mechanism. The answer is **Buggy**."
weka,14864.json,89300e74cf24e09a676e8b2c132e4381f57905ac,"@@ -1,208 +1,216 @@
   public int[] search (ASEvaluation ASEval, Instances data)
     throws Exception {
     m_totalEvals = 0;
     if (!(ASEval instanceof SubsetEvaluator)) {
       throw  new Exception(ASEval.getClass().getName() 
 			   + "" is not a "" 
 			   + ""Subset evaluator!"");
     }
 
     if (ASEval instanceof UnsupervisedSubsetEvaluator) {
       m_hasClass = false;
     }
     else {
       m_hasClass = true;
       m_classIndex = data.classIndex();
     }
 
     SubsetEvaluator ASEvaluator = (SubsetEvaluator)ASEval;
     m_numAttribs = data.numAttributes();
     int i, j;
     int best_size = 0;
     int size = 0;
     int done;
     int sd = m_searchDirection;
     int evals = 0;
     BitSet best_group, temp_group;
     int stale;
     double best_merit;
     boolean ok = true;
     double merit;
     boolean z;
     boolean added;
     Link2 tl;
     Hashtable lookup = new Hashtable(m_cacheSize * m_numAttribs);
     int insertCount = 0;
     int cacheHits = 0;
     LinkedList2 bfList = new LinkedList2(m_maxStale);
     best_merit = -Double.MAX_VALUE;
     stale = 0;
     best_group = new BitSet(m_numAttribs);
 
     m_startRange.setUpper(m_numAttribs-1);
     if (!(getStartSet().equals(""""))) {
       m_starting = m_startRange.getSelection();
     }
     // If a starting subset has been supplied, then initialise the bitset
     if (m_starting != null) {
       for (i = 0; i < m_starting.length; i++) {
 	if ((m_starting[i]) != m_classIndex) {
 	  best_group.set(m_starting[i]);
 	}
       }
 
       best_size = m_starting.length;
       m_totalEvals++;
     }
     else {
       if (m_searchDirection == SELECTION_BACKWARD) {
 	setStartSet(""1-last"");
 	m_starting = new int[m_numAttribs];
 
 	// init initial subset to all attributes
 	for (i = 0, j = 0; i < m_numAttribs; i++) {
 	  if (i != m_classIndex) {
 	    best_group.set(i);
 	    m_starting[j++] = i;
 	  }
 	}
 
 	best_size = m_numAttribs - 1;
 	m_totalEvals++;
       }
     }
 
     // evaluate the initial subset
     best_merit = ASEvaluator.evaluateSubset(best_group);
     // add the initial group to the list and the hash table
     Object [] best = new Object[1];
     best[0] = best_group.clone();
     bfList.addToList(best, best_merit);
     BitSet tt = (BitSet)best_group.clone();
     String hashC = tt.toString();
     lookup.put(hashC, """");
 
     while (stale < m_maxStale) {
       added = false;
 
       if (m_searchDirection == SELECTION_BIDIRECTIONAL) {
 	// bi-directional search
 	  done = 2;
 	  sd = SELECTION_FORWARD;
 	}
       else {
 	done = 1;
       }
 
       // finished search?
       if (bfList.size() == 0) {
 	stale = m_maxStale;
 	break;
       }
 
       // copy the attribute set at the head of the list
       tl = bfList.getLinkAt(0);
       temp_group = (BitSet)(tl.getData()[0]);
       temp_group = (BitSet)temp_group.clone();
       // remove the head of the list
       bfList.removeLinkAt(0);
       // count the number of bits set (attributes)
       int kk;
 
       for (kk = 0, size = 0; kk < m_numAttribs; kk++) {
 	if (temp_group.get(kk)) {
 	  size++;
 	}
       }
 
       do {
 	for (i = 0; i < m_numAttribs; i++) {
 	  if (sd == SELECTION_FORWARD) {
 	    z = ((i != m_classIndex) && (!temp_group.get(i)));
 	  }
 	  else {
 	    z = ((i != m_classIndex) && (temp_group.get(i)));
 	  }
 
 	  if (z) {
 	    // set the bit (attribute to add/delete)
 	    if (sd == SELECTION_FORWARD) {
 	      temp_group.set(i);
+	      size++;
 	    }
 	    else {
 	      temp_group.clear(i);
+	      size--;
 	    }
 
 	    /* if this subset has been seen before, then it is already 
 	       in the list (or has been fully expanded) */
 	    tt = (BitSet)temp_group.clone();
 	    hashC = tt.toString();
 	    if (lookup.containsKey(hashC) == false) {
 	      merit = ASEvaluator.evaluateSubset(temp_group);
 	      m_totalEvals++;
 
 	      if (m_debug) {
 		System.out.print(""Group: "");
 		printGroup(tt, m_numAttribs);
 		System.out.println(""Merit: "" + merit);
 	      }
 
 	      // is this better than the best?
 	      if (sd == SELECTION_FORWARD) {
 		z = ((merit - best_merit) > 0.00001);
 	      }
 	      else {
-		z = ((merit >= best_merit) && ((size) < best_size));
+		if (merit == best_merit) {
+		  z = (size < best_size);
+		} else {
+		  z = (merit >  best_merit);
+		} 
 	      }
 
 	      if (z) {
 		added = true;
 		stale = 0;
 		best_merit = merit;
 		//		best_size = (size + best_size);
 		best_size = size;
 		best_group = (BitSet)(temp_group.clone());
 	      }
 
 	      if (insertCount > m_cacheSize * m_numAttribs) {
 		lookup = new Hashtable(m_cacheSize * m_numAttribs);
 		insertCount = 0;
 	      }
 	      // insert this one in the list and in the hash table
 	      Object [] add = new Object[1];
 	      add[0] = tt.clone();
 	      bfList.addToList(add, merit);
 	      hashC = tt.toString();
 	      lookup.put(hashC, """");
 	      insertCount++;
 	    } else {
 	      cacheHits++;
 	    }
 
 	    // unset this addition(deletion)
 	    if (sd == SELECTION_FORWARD) {
 	      temp_group.clear(i);
+	      size--;
 	    }
 	    else {
 	      temp_group.set(i);
+	      size++;
 	    }
 	  }
 	}
 
 	if (done == 2) {
 	  sd = SELECTION_BACKWARD;
 	}
 
 	done--;
       } while (done > 0);
 
       /* if we haven't added a new attribute subset then full expansion 
 	 of this node hasen't resulted in anything better */
       if (!added) {
 	stale++;
       }
     }
 
     m_bestMerit = best_merit;
     return  attributeList(best_group);
   }",Buggy,"Fixed bug in backward mode


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@2250 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message ""Fixed bug in backward mode"" indicates a bug fix. The diff focuses on the backward search direction (SELECTION_BACKWARD) within the `search` method. Specifically, the condition for determining if a subset is better than the current best subset in backward mode has been modified. The original code `z = ((merit >= best_merit) && ((size) < best_size));` has been replaced with a more robust comparison that prioritizes a higher merit and then a smaller size: `if (merit == best_merit) { z = (size < best_size); } else { z = (merit >  best_merit); }`. Furthermore, there are also size increments and decrements added when setting and clearing bits in the forward and backward selection. This change addresses a potential issue where the algorithm might not correctly identify the best subset in backward mode, especially when multiple subsets have the same merit. The answer is **Buggy**."
weka,14864.json,316eaac24eb968e7441b31b4e54c4ba10926a75a,"@@ -1,191 +1,189 @@
   public int[] search (int[] startSet, ASEvaluation ASEval, Instances data)
     throws Exception
   {
     if (!(ASEval instanceof SubsetEvaluator)) {
       throw  new Exception(ASEval.getClass().getName() 
 			   + "" is not a "" 
 			   + ""Subset evaluator!"");
     }
 
     if (startSet != null) {
       m_starting = startSet;
     }
 
     if (ASEval instanceof UnsupervisedSubsetEvaluator) {
       m_hasClass = false;
     }
     else {
       m_hasClass = true;
       m_classIndex = data.classIndex();
     }
 
     SubsetEvaluator ASEvaluator = (SubsetEvaluator)ASEval;
     m_numAttribs = data.numAttributes();
     int i, j;
     int best_size = 0;
     int size = 0;
     int done;
     int sd = m_searchDirection;
     int evals = 0;
     BitSet best_group, temp_group;
     int stale;
     double best_merit;
     boolean ok = true;
     double merit;
     boolean z;
     boolean added;
     Link2 tl;
     Hashtable lookup = new Hashtable((int)(200.0*m_numAttribs*1.5));
     LinkedList2 bfList = new LinkedList2(m_maxStale);
     best_merit = -Double.MAX_VALUE;
     stale = 0;
     best_group = new BitSet(m_numAttribs);
 
     // If a starting subset has been supplied, then initialise the bitset
     if (m_starting != null) {
       for (i = 0; i < m_starting.length; i++) {
 	if ((m_starting[i]) != m_classIndex) {
 	  best_group.set(m_starting[i]);
 	}
       }
 
-      // evaluate the initial set
-      best_merit = ASEvaluator.evaluateSubset(best_group);
       best_size = m_starting.length;
       m_totalEvals++;
     }
     else {if (m_searchDirection == -1) {
       m_starting = new int[m_numAttribs];
 
       // init initial subset to all attributes
       for (i = 0, j = 0; i < m_numAttribs; i++) {
 	if (i != m_classIndex) {
 	  best_group.set(i);
 	  m_starting[j++] = i;
 	}
       }
 
-      // evaluate the initial set
-      best_merit = ASEvaluator.evaluateSubset(best_group);
       best_size = m_numAttribs - 1;
       m_totalEvals++;
     }
     }
 
+    // evaluate the initial subset
+    best_merit = ASEvaluator.evaluateSubset(best_group);
     // add the initial group to the list and the hash table
     bfList.addToList(best_group, best_merit);
     BitSet tt = (BitSet)best_group.clone();
     lookup.put(tt, """");
 
     while (stale < m_maxStale) {
       added = false;
 
       if (m_searchDirection == 0) // bi-directional search
 	{
 	  done = 2;
 	  sd = 1;
 	}
       else {
 	done = 1;
       }
 
       // finished search?
       if (bfList.size() == 0) {
 	stale = m_maxStale;
 	break;
       }
 
       // copy the attribute set at the head of the list
       tl = bfList.getLinkAt(0);
       temp_group = (BitSet)(tl.getGroup().clone());
       // remove the head of the list
       bfList.removeLinkAt(0);
       // count the number of bits set (attributes)
       int kk;
 
       for (kk = 0, size = 0; kk < m_numAttribs; kk++) {
 	if (temp_group.get(kk)) {
 	  size++;
 	}
       }
 
       do {
 	for (i = 0; i < m_numAttribs; i++) {
 	  if (sd == 1) {
 	    z = ((i != m_classIndex) && (!temp_group.get(i)));
 	  }
 	  else {
 	    z = ((i != m_classIndex) && (temp_group.get(i)));
 	  }
 
 	  if (z) {
 	    // set the bit (attribute to add/delete)
 	    if (sd == 1) {
 	      temp_group.set(i);
 	    }
 	    else {
 	      temp_group.clear(i);
 	    }
 
 	    /* if this subset has been seen before, then it is already 
 	       in the list (or has been fully expanded) */
 	    tt = (BitSet)temp_group.clone();
 
 	    if (lookup.containsKey(tt) == false) {
 	      merit = ASEvaluator.evaluateSubset(temp_group);
 	      m_totalEvals++;
 
 	      if (m_debug) {
 		System.out.print(""Group: "");
 		printGroup(tt, m_numAttribs);
 		System.out.println(""Merit: "" + merit);
 	      }
 
 	      // is this better than the best?
 	      if (sd == 1) {
 		z = ((merit - best_merit) > 0.00001);
 	      }
 	      else {
 		z = ((merit >= best_merit) && ((size + sd) < best_size));
 	      }
 
 	      if (z) {
 		added = true;
 		stale = 0;
 		best_merit = merit;
 		best_size = (size + best_size);
 		best_group = (BitSet)(temp_group.clone());
 	      }
 
 	      // insert this one in the list and in the hash table
 	      bfList.addToList(tt, merit);
 	      lookup.put(tt, """");
 	    }
 
 	    // unset this addition(deletion)
 	    if (sd == 1) {
 	      temp_group.clear(i);
 	    }
 	    else {
 	      temp_group.set(i);
 	    }
 	  }
 	}
 
 	if (done == 2) {
 	  sd = -1;
 	}
 
 	done--;
       } while (done > 0);
 
       /* if we haven't added a new attribute subset then full expansion 
 	 of this node hasen't resulted in anything better */
       if (!added) {
 	stale++;
       }
     }
 
     m_bestMerit = best_merit;
     return  attributeList(best_group);
   }",Buggy,"Fixed small bug in evaluation of an empty subset.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@169 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix for a bug related to the evaluation of an empty subset. The code changes involve initializing the `best_merit` variable by evaluating the initial subset *before* entering the main loop. This ensures that even if the initial subset is empty, its merit is considered. Without this initialization, the `best_merit` would remain at its initial value of `-Double.MAX_VALUE`, potentially leading to incorrect selection of the best subset. The answer is **Buggy**."
weka,20981.json,6c77ccd674985be37160a0788865e371bf1cb5e1,"@@ -1,49 +1,53 @@
   private void saveLayout(int tabIndex, boolean showDialog) {
     // m_loadB.setEnabled(false);
     // m_saveB.setEnabled(false);
     java.awt.Color bckC = getBackground();
 
     File sFile = m_mainKFPerspective.getFlowFile(tabIndex);
     int returnVal = JFileChooser.APPROVE_OPTION;
+    boolean shownDialog = false;
 
     if (showDialog || sFile.getName().equals(""-NONE-"")) {
       returnVal = m_FileChooser.showSaveDialog(this);
+      shownDialog = true;
     }
 
     if (returnVal == JFileChooser.APPROVE_OPTION) {
       // temporarily remove this panel as a property changle listener from
       // each bean
 
       Vector beans = BeanInstance.getBeanInstances(tabIndex);
       detachFromLayout(beans);
 
-      // determine filename
-      sFile = m_FileChooser.getSelectedFile();
+      // determine filename (if necessary)
+      if (shownDialog) {
+        sFile = m_FileChooser.getSelectedFile();
+      }
 
       // add extension if necessary
       if (m_FileChooser.getFileFilter() == m_KfFilter) {
         if (!sFile.getName().toLowerCase().endsWith(FILE_EXTENSION)) {
           sFile = new File(sFile.getParent(), sFile.getName() + FILE_EXTENSION);
         }
       } else if (m_FileChooser.getFileFilter() == m_KOMLFilter) {
         if (!sFile.getName().toLowerCase().endsWith(KOML.FILE_EXTENSION + ""kf"")) {
           sFile = new File(sFile.getParent(), sFile.getName()
               + KOML.FILE_EXTENSION + ""kf"");
         }
       } else if (m_FileChooser.getFileFilter() == m_XStreamFilter) {
         if (!sFile.getName().toLowerCase()
             .endsWith(XStream.FILE_EXTENSION + ""kf"")) {
           sFile = new File(sFile.getParent(), sFile.getName()
               + XStream.FILE_EXTENSION + ""kf"");
         }
       } else if (m_FileChooser.getFileFilter() == m_XMLFilter) {
         if (!sFile.getName().toLowerCase().endsWith(FILE_EXTENSION_XML)) {
           sFile = new File(sFile.getParent(), sFile.getName()
               + FILE_EXTENSION_XML);
         }
       }
 
       saveLayout(sFile, m_mainKFPerspective.getCurrentTabIndex(), false);
       m_mainKFPerspective.setFlowFile(tabIndex, sFile);
     }
   }",Buggy,"Fixed a bug where a tab would get saved with the filename associated with another tab

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@9860 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug where a tab's filename could be associated with the wrong tab. The diff introduces a `shownDialog` boolean to track whether the file chooser dialog was displayed. The filename is only updated if the dialog was shown. This ensures that the filename is only updated when the user explicitly selects a new file, preventing the bug where the filename gets associated with the wrong tab. The answer is **Buggy**."
weka,37320.json,24265153c621e05ff04b8fd1e06a4dfd6a63950a,"@@ -1,12 +1,13 @@
   public void addStepOutputListener(StepOutputListener listener,
     String outputConnectionName) {
     List<StepOutputListener> listenersForConnectionType =
       m_outputListeners.get(outputConnectionName);
     if (listenersForConnectionType == null) {
       listenersForConnectionType = new ArrayList<StepOutputListener>();
+      m_outputListeners.put(outputConnectionName, listenersForConnectionType);
     }
 
     if (!listenersForConnectionType.contains(listener)) {
       listenersForConnectionType.add(listener);
     }
   }",Buggy,"Fixed a bug in the step output listener framework

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@12498 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix in the step output listener framework. The diff shows that the code now explicitly puts the new `listenersForConnectionType` into the `m_outputListeners` map when it's initially created. Without this, the listener would not be notified. The answer is **Buggy**."
weka,21662.json,ac1cf6e094d4d41b6cff6e286f9abb0cdd58e95b,"@@ -1,9 +1,9 @@
   public void addObject(String name, Object o) {
     String nameCopy = name;
     int i = 0;
-    while (m_Results.containsKey(nameCopy)) {
+    while (m_Objs.containsKey(nameCopy)) {
       nameCopy = name + ""_"" + i++;
     }
 
     m_Objs.put(nameCopy, o);
   }",Buggy,"Fixed bug, introduced in the last change, that affected accessing of objects stored against results

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@13734 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to accessing objects stored against results, introduced in the last change. The code diff modifies the `addObject` method. The while loop condition `m_Results.containsKey(nameCopy)` is changed to `m_Objs.containsKey(nameCopy)`. This suggests that the original code was incorrectly checking for the existence of the object in `m_Results` instead of `m_Objs`, leading to potential issues when adding new objects. This change fixes the bug by ensuring that the correct map (`m_Objs`) is checked for existing keys, aligning with the commit message. The answer is **Buggy**."
weka,32460.json,c3cf13ff2162159996a765cb4fcfb11c86359d6e,"@@ -1,23 +1,23 @@
   private static void initStemmers() {
     Vector<String> classnames;
     int i;
 
     if (m_Stemmers != null) {
       return;
     }
 
     m_Stemmers = new Vector<String>();
 
     if (!m_Present) {
       return;
     }
 
     classnames = GenericObjectEditor.getClassnames(SNOWBALL_PROGRAM);
     // try dynamic discovery if not in props file
     if (classnames.size() == 0) {
       classnames = ClassDiscovery.find(SNOWBALL_PROGRAM, PACKAGE_EXT);
-      for (i = 0; i < classnames.size(); i++) {
-        m_Stemmers.add(getStemmerName(classnames.get(i).toString()));
-      }
+    }
+    for (i = 0; i < classnames.size(); i++) {
+      m_Stemmers.add(getStemmerName(classnames.get(i).toString()));
     }
   }",Buggy,"Fixed a bug that prevented snowball classes from being found when static entries in GenericObjectEditor.props are being used (instead of dynamic class discovery)

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@13430 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to finding snowball classes when using static entries in `GenericObjectEditor.props`. The code diff shows that the `ClassDiscovery.find` method was previously only called when the `classnames` list was empty, implying that dynamic discovery was only attempted if static entries were not found. The change moves the loop that adds stemmer names to `m_Stemmers` outside the `if` block, ensuring that stemmers are added regardless of whether they were found through static entries or dynamic discovery. This suggests that the original code failed to add stemmers found through static entries, which aligns with the bug described in the commit message. Therefore, the changes indicate a bug fix. The answer is **Buggy**."
weka,24188.json,639ea63bec73dc387b27664947c01effdfc97c4c,"@@ -1,153 +1,153 @@
   public String toStringMatrix() {
     StringBuffer    result;
     String[][]      cells;
     int             i;
     int             j;
     int             n;
     int             k;
     int             size;
     String          line;
     int             indexBase;
     int             indexSecond;
     StringBuffer    head;
     StringBuffer    body;
     StringBuffer    foot;
     int[]           startMeans;
     int[]           startSigs;
     int             maxLength;
 
     result     = new StringBuffer();
     head       = new StringBuffer();
     body       = new StringBuffer();
     foot       = new StringBuffer();
     cells      = toArray();
     startMeans = new int[getColCount()];
     startSigs  = new int[getColCount() - 1];
     maxLength  = 0;
 
     // pad numbers
     for (n = 1; n < cells[0].length; n++) {
       size = getColSize(cells, n, true, true);
       for (i = 1; i < cells.length - 1; i++)
         cells[i][n] = padString(cells[i][n], size, true);
     }
 
     // index of base column in array
     indexBase = 1;
     if (getShowStdDev())
       indexBase++;
 
     // index of second column in array
     indexSecond = indexBase + 1;
     if (getShowStdDev())
       indexSecond++;
 
     // output data (without ""(v/ /*)"")
     j = 0;
     k = 0;
     for (i = 1; i < cells.length - 1; i++) {
       line = """";
       
       for (n = 0; n < cells[0].length; n++) {
         // record starts
         if (i == 1) {
           if (isMean(n)) {
             startMeans[j] = line.length();
             j++;
           }
 
           if (isSignificance(n)) {
             startSigs[k] = line.length();
             k++;
           }
         }
         
         if (n == 0) {
           line += padString(cells[i][n], getRowNameWidth());
-          line += padString(""("" + Utils.doubleToString(getCount(i-1), 0) + "")"", 
+          line += padString(""("" + Utils.doubleToString(getCount(getDisplayRow(i-1)), 0) + "")"", 
                         getCountWidth(), true);
         }
         else {
           // additional space before means
           if (isMean(n))
             line += ""  "";
 
           // print cell
           if (getShowStdDev()) {
             if (isMean(n - 1)) {
               if (!cells[i][n].trim().equals(""""))              
                 line += ""("" + cells[i][n] + "")"";
               else
                 line += "" "" + cells[i][n] + "" "";
             }
             else
               line += "" "" + cells[i][n];
           }
           else {
             line += "" "" + cells[i][n];
           }
         }
 
         // add separator after base column
         if (n == indexBase)
           line += "" |"";
       }
 
       // record overall length
       if (i == 1)
         maxLength = line.length();
       
       body.append(line + ""\n"");
     }
 
     // column names
     line = padString(cells[0][0], startMeans[0]);
     i    = -1;
     for (n = 1; n < cells[0].length; n++) {
       if (isMean(n)) {
         i++;
 
         if (i == 0)
           line = padString(line, startMeans[i] - getCountWidth());
         else if (i == 1)
           line = padString(line, startMeans[i] - "" |"".length());
         else if (i > 1)
           line = padString(line, startMeans[i]);
         
         if (i == 1)
           line += "" |"";
         
         line += "" "" + cells[0][n];
       }
     }
     line = padString(line, maxLength);
     head.append(line + ""\n"");
     head.append(line.replaceAll(""."", ""-"") + ""\n"");
     body.append(line.replaceAll(""."", ""-"") + ""\n"");
 
     // output wins/losses/ties
     if (getColCount() > 1) {
       line = padString(cells[cells.length - 1][0], startMeans[1]-2, true) + "" |"";
       i    = 0;
       for (n = 1; n < cells[cells.length - 1].length; n++) {
         if (isSignificance(n)) {
           line = padString(
                   line, startSigs[i] + 1 - cells[cells.length - 1][n].length());
           line += "" "" + cells[cells.length - 1][n];
           i++;
         }
       }
       line = padString(line, maxLength);
     }
     else {
       line = padString(cells[cells.length - 1][0], line.length() - 2) + "" |"";
     }
     foot.append(line + ""\n"");
     
     // assemble output
     result.append(head.toString());
     result.append(body.toString());
     result.append(foot.toString());
 
     return result.toString();
   }",Buggy,"Fixed bug in ResultMatrixPlainText, which meant that displayed counts could be out of line with corresponding dataset and results. Changed TTesters to not throw an exception when missing values are encountered.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@2816 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix in `ResultMatrixPlainText` related to misaligned displayed counts with the dataset and results. The diff modifies the `toStringMatrix` method, specifically changing `getCount(i-1)` to `getCount(getDisplayRow(i-1))`. This suggests that the original code was using an incorrect index to retrieve the count, leading to the misalignment issue. The change ensures that the correct row index is used to fetch the count, thereby fixing the bug. The answer is **Buggy**."
weka,17500.json,d109992d4145875b41832b4a599e8cc7b7e5d4a2,"@@ -1,71 +1,74 @@
   protected void setNumeric() {
     m_isNumeric = true;
     /*      m_maxC = mxC;
 	    m_minC = mnC; */
 
     double min=Double.POSITIVE_INFINITY;
     double max=Double.NEGATIVE_INFINITY;
     double value;
 
     for (int i=0;i<m_Instances.numInstances();i++) {
       if (!m_Instances.instance(i).isMissing(m_cIndex)) {
 	value = m_Instances.instance(i).value(m_cIndex);
 	if (value < min) {
 	  min = value;
 	}
 	if (value > max) {
 	  max = value;
 	}
       }
     }
      
+    // handle case where all values are missing
+    if (min == Double.POSITIVE_INFINITY) min = max = 0.0;
+
     m_minC = min; m_maxC = max;
 
     int whole = (int)Math.abs(m_maxC);
     double decimal = Math.abs(m_maxC) - whole;
     int nondecimal;
     nondecimal = (whole > 0) 
       ? (int)(Math.log(whole) / Math.log(10))
       : 1;
     
     m_precisionC = (decimal > 0) 
       ? (int)Math.abs(((Math.log(Math.abs(m_maxC)) / 
 				      Math.log(10))))+2
       : 1;
     if (m_precisionC > VisualizeUtils.MAX_PRECISION) {
       m_precisionC = 1;
     }
 
     String maxStringC = Utils.doubleToString(m_maxC,
 					     nondecimal+1+m_precisionC
 					     ,m_precisionC);
     if (m_labelMetrics != null) {
       m_HorizontalPad = m_labelMetrics.stringWidth(maxStringC);
     }
 
     whole = (int)Math.abs(m_minC);
     decimal = Math.abs(m_minC) - whole;
     nondecimal = (whole > 0) 
       ? (int)(Math.log(whole) / Math.log(10))
       : 1;
     
      m_precisionC = (decimal > 0) 
        ? (int)Math.abs(((Math.log(Math.abs(m_minC)) / 
 				      Math.log(10))))+2
       : 1;
      if (m_precisionC > VisualizeUtils.MAX_PRECISION) {
        m_precisionC = 1;
      }
     
      maxStringC = Utils.doubleToString(m_minC,
 				       nondecimal+1+m_precisionC
 				       ,m_precisionC);
      if (m_labelMetrics != null) {
        if (m_labelMetrics.stringWidth(maxStringC) > m_HorizontalPad) {
 	 m_HorizontalPad = m_labelMetrics.stringWidth(maxStringC);
        }
      }
 
     setOn(true);
     this.repaint();
   }",Buggy,"Fixed bug caused when all numeric values are missing.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@1219 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix for a bug that occurs when all numeric values are missing. The diff introduces a check to handle the case where `min` remains `Double.POSITIVE_INFINITY` after iterating through all instances, meaning all values were missing. In this scenario, both `min` and `max` are set to 0.0 to avoid issues in subsequent calculations. This clearly addresses a bug where missing numeric values were not properly handled. The answer is **Buggy**."
weka,31181.json,feacc664ba9b84371e98b74802bc9f6a36edcff4,"@@ -1,19 +1,19 @@
   public String[] getOptions() {
 
     Vector<String> result = new Vector<String>();
 
     result.add(""-R"");
     result.add(getAttributeIndices());
 
     if (getInvertSelection()) {
       result.add(""-V"");
     }
 
-    result.add(""F"");
+    result.add(""-F"");
     result.add("""" + getFilterSpec());
 
-    result.add(""D"");
+    result.add(""-D"");
     result.add("""" + getDistanceSpec());
 
     return result.toArray(new String[result.size()]);
   }",Buggy,"Small bug fix in getOptions().

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@11186 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a small bug fix in the `getOptions()` method. The diff shows that the code was missing hyphens in front of the options ""-F"" and ""-D"". This correction ensures that the options are correctly parsed when the program is run with these options. The answer is **Buggy**."
weka,36114.json,f20281f569296add23cdf098a2ef5d7c1c9101a3,"@@ -1,31 +1,31 @@
   public static boolean hasInterface(Class intf, Class cls) {
     Class[]       intfs;
     int           i;
     boolean       result;
     Class         currentclass;
     
     result       = false;
     currentclass = cls;
     do {
       // check all the interfaces, this class implements
       intfs = currentclass.getInterfaces();
       for (i = 0; i < intfs.length; i++) {
         if (intfs[i].equals(intf)) {
           result = true;
           break;
         }
       }
 
       // get parent class
       if (!result) {
         currentclass = currentclass.getSuperclass();
         
-        // topmost class reached?
-        if (currentclass.equals(Object.class))
+        // topmost class reached or no superclass?
+        if ( (currentclass == null) || (currentclass.equals(Object.class)) )
           break;
       }
     } 
     while (!result);
       
     return result;
   }",Buggy,"fixed bug with interfaces (return null as superclass)


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@2976 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to interfaces, specifically when returning the superclass. The diff modifies the `hasInterface` method to handle cases where `currentclass.getSuperclass()` returns `null`, which can occur when the class being inspected is an interface. The original code only checked if the `currentclass` was `Object.class`, potentially leading to an error or incorrect behavior when an interface was encountered. The modified code now includes a check for `currentclass == null` to address this issue. The answer is **Buggy**."
weka,36431.json,a91eb3b6273ee2b25218e3fd6a1520a0f11b6e6e,"@@ -1,17 +1,18 @@
   public void stepInit() throws WekaException {
     m_isReset = true;
+    m_streamingData = null;
 
     // see if the specified downstream steps are connected
     m_validTrueStep =
       getStepManager().getOutgoingConnectedStepWithName(
         environmentSubstitute(m_customNameOfTrueStep)) != null;
     m_validFalseStep =
       getStepManager().getOutgoingConnectedStepWithName(
         environmentSubstitute(m_customNameOfFalseStep)) != null;
 
     m_incomingStructure = null;
 
     if (m_expressionString == null || m_expressionString.length() == 0) {
       throw new WekaException(""No expression defined!"");
     }
   }",Buggy,"Fixed a small bug in FlowByExpression - object holding streaming data output did not get set to null in reset() method

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@13301 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix in the `FlowByExpression` class, specifically that the `m_streamingData` object was not being set to `null` in the `reset()` method. The diff shows that `m_streamingData = null;` has been added to the `stepInit()` method. Although the method name is `stepInit` instead of `reset`, the commit message clearly states the intent to fix this bug. Therefore, the change aligns with the commit message and indicates a bug fix. The answer is **Buggy**."
weka,29966.json,a5e2d1ef0f62ebac46d095c840ea7ff4a9145b48,"@@ -1,172 +1,192 @@
   public static void main(String[] args) {
     try {
       if (args.length == 0 || args[0].equalsIgnoreCase(""-h"") ||
           args[0].equalsIgnoreCase(""-help"")) {
-        System.err.println(""Usage:\n\tweka.Run [-no-scan | -no-load] <scheme name [scheme options]>"");
+        System.err.println(""Usage:\n\tweka.Run [-no-scan] [-no-load] <scheme name [scheme options]>"");
         System.exit(1);
       }
+      boolean noScan = false;
+      boolean noLoad = false;
       if (args[0].equals(""-list-packages"")) {
         weka.core.WekaPackageManager.loadPackages(true);
         System.exit(0);
-      } else if (!args[0].equals(""-no-load"")) {
+      } else if (args[0].equals(""-no-load"")) {
+        noLoad = true;
+        if (args.length > 1) {
+          if (args[1].equals(""-no-scan"")) {
+            noScan = true;
+          }
+        }
+      } else if (args[0].equals(""-no-scan"")) {
+        noScan = true;
+        if (args.length > 1) {
+          if (args[1].equals(""-no-load"")) {
+            noLoad = true;
+          }
+        }
+      }
+      
+      if (!noLoad) {
         weka.core.WekaPackageManager.loadPackages(false);
       }
       
+      int schemeIndex = 0;
+      if (noLoad && noScan) {
+        schemeIndex = 2;
+      } else if (noLoad || noScan) {
+        schemeIndex = 1;
+      }
+      
       String schemeToRun = null;
       String[] options = null;
-      if (args[0].equals(""-no-scan"") || args[0].equals(""-no-load"")) {
-        if (args.length < 2) {
-          System.err.println(""No scheme name given."");
-          System.exit(1);
-        }
-        schemeToRun = args[1];
-        options = new String[args.length - 2];
-        if (options.length > 0) {
-          System.arraycopy(args, 2, options, 0, options.length);
-        }
-      } else {
-        // scan packages for matches
-        schemeToRun = args[0];
-        options = new String[args.length - 1];
-        if (options.length > 0) {
-          System.arraycopy(args, 1, options, 0, options.length);
-        }
-
-        ArrayList<String> matches = weka.core.ClassDiscovery.find(args[0]);
+      
+      if (schemeIndex >= args.length) {
+        System.err.println(""No scheme name given."");
+        System.exit(1);
+      }
+      schemeToRun = args[schemeIndex];
+      options = new String[args.length - schemeIndex - 1];
+      if (options.length > 0) {
+        System.arraycopy(args, schemeIndex + 1, options, 0, options.length);
+      }
+      
+           
+      if (!noScan) {     
+        ArrayList<String> matches = weka.core.ClassDiscovery.find(schemeToRun);
         ArrayList<String> prunedMatches = new ArrayList<String>();
         // prune list for anything that isn't a runnable scheme      
         for (int i = 0; i < matches.size(); i++) {
           try {
             Object scheme = java.beans.Beans.instantiate((new Run()).getClass().getClassLoader(),
                 matches.get(i));          
             if (scheme instanceof weka.classifiers.Classifier ||
                 scheme instanceof weka.clusterers.Clusterer ||
                 scheme instanceof weka.associations.Associator ||
                 scheme instanceof weka.attributeSelection.ASEvaluation ||
                 scheme instanceof weka.filters.Filter) {
               prunedMatches.add(matches.get(i));
             }
           } catch (Exception ex) {
             // ignore any classes that we can't instantiate due to no no-arg constructor
           }
         }
 
         if (prunedMatches.size() == 0) {
-          System.err.println(""Can't find scheme "" + args[0] + "", or it is not runnable."");
+          System.err.println(""Can't find scheme "" + schemeToRun + "", or it is not runnable."");
           System.exit(1);
         } else if (prunedMatches.size() > 1) {
           java.io.BufferedReader br = 
             new java.io.BufferedReader(new java.io.InputStreamReader(System.in));
           boolean done = false;
           while (!done) {
             System.out.println(""Select a scheme to run, or <return> to exit:"");
             for (int i = 0; i < prunedMatches.size(); i++) {
               System.out.println(""\t"" + (i+1) + "") "" + prunedMatches.get(i));
             }
             System.out.print(""\nEnter a number > "");
             String choice = null;
             int schemeNumber = 0;
             try {
               choice = br.readLine();
               if (choice.equals("""")) {
                 System.exit(0);
               } else {
                 schemeNumber = Integer.parseInt(choice);
                 schemeNumber--;
                 if (schemeNumber >= 0 && schemeNumber < prunedMatches.size()) {
                   schemeToRun = prunedMatches.get(schemeNumber);
                   done = true;
                 }
               }
             } catch (java.io.IOException ex) {
               // ignore
             }
           }
         } else {
           schemeToRun = prunedMatches.get(0);
         }
       }
 
       Object scheme = null;
       try {
         scheme = java.beans.Beans.instantiate((new Run()).getClass().getClassLoader(),
             schemeToRun);
       } catch (Exception ex) {
         System.err.println(schemeToRun + "" is not runnable!"");
         System.exit(1);
       }
       // now see which interfaces/classes this scheme implements/extends
       ArrayList<SchemeType> types = new ArrayList<SchemeType>();      
       if (scheme instanceof weka.classifiers.Classifier) {
         types.add(SchemeType.CLASSIFIER);
       }
       if (scheme instanceof weka.clusterers.Clusterer) {
         types.add(SchemeType.CLUSTERER);
       }
       if (scheme instanceof weka.associations.Associator) {
         types.add(SchemeType.ASSOCIATOR);
       }
       if (scheme instanceof weka.attributeSelection.ASEvaluation) {
         types.add(SchemeType.ATTRIBUTE_SELECTION);
       }
       if (scheme instanceof weka.filters.Filter) {
         types.add(SchemeType.FILTER);
       }
       
       SchemeType selectedType = null;
       if (types.size() == 0) {
         System.err.println("""" + schemeToRun + "" is not runnable!"");
         System.exit(1);
       }
       if (types.size() == 1) {
         selectedType = types.get(0);
       } else {
         java.io.BufferedReader br = 
           new java.io.BufferedReader(new java.io.InputStreamReader(System.in));
         boolean done = false;
         while (!done) {
           System.out.println("""" + schemeToRun + "" can be executed as any of the following:"");
           for (int i = 0; i < types.size(); i++) {
             System.out.println(""\t"" + (i+1) + "") "" + types.get(i));
           }
           System.out.print(""\nEnter a number > "");
           String choice = null;
           int typeNumber = 0;
           try {
             choice = br.readLine();
             if (choice.equals("""")) {
               System.exit(0);
             } else {
               typeNumber = Integer.parseInt(choice);
               typeNumber--;
               if (typeNumber >= 0 && typeNumber < types.size()) {
                 selectedType = types.get(typeNumber);
                 done = true;
               }
             }
           } catch (java.io.IOException ex) {
             // ignore
           }
         }
       }
             
       if (selectedType == SchemeType.CLASSIFIER) {
         weka.classifiers.AbstractClassifier.runClassifier((weka.classifiers.Classifier)scheme, options);
       } else if (selectedType == SchemeType.CLUSTERER) {
         weka.clusterers.AbstractClusterer.runClusterer((weka.clusterers.Clusterer)scheme, options);
       } else if (selectedType == SchemeType.ATTRIBUTE_SELECTION) {
         weka.attributeSelection.ASEvaluation.runEvaluator((weka.attributeSelection.ASEvaluation)scheme, options);
       } else if (selectedType == SchemeType.ASSOCIATOR) {
         weka.associations.AbstractAssociator.runAssociator((weka.associations.Associator)scheme, options);
       } else if (selectedType == SchemeType.FILTER) {
         weka.filters.Filter.runFilter((weka.filters.Filter)scheme, options);
       }
     } 
     catch (Exception e) {
       if (    ((e.getMessage() != null) && (e.getMessage().indexOf(""General options"") == -1))
 	   || (e.getMessage() == null) )
 	e.printStackTrace();
       else
 	System.err.println(e.getMessage());
     }
   }",Buggy,"Fixed a bug that prevented the -no-scan and -no-load options to be used at the same time.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@6676 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,The commit message indicates a bug fix related to using the `-no-scan` and `-no-load` options together. The diff modifies the argument parsing logic in the `main` method to correctly handle these options. The original code likely had a flaw that prevented these options from being used simultaneously. The updated code introduces boolean flags `noScan` and `noLoad` to track the presence of these options and adjusts the `schemeIndex` accordingly. The usage message is also updated to reflect the correct syntax. This clearly indicates a bug fix. The answer is **Buggy**.
weka,6813.json,017098ae02f2d6bdf3fb5d25b0d3ccedd8c46b7e,"@@ -1,28 +1,35 @@
   protected static Vector instanceToVector(Instance toProcess, int classIndex) {
     if (toProcess instanceof SparseInstance) {
       int classModifier = classIndex >= 0 ? 1 : 0;
+      if (classModifier > 0) {
+        double classValue = toProcess.classValue();
+        if (classValue == 0) {
+          classModifier = 0; // class is sparse
+        }
+      }
       SparseInstance toProcessSparse = ((SparseInstance) toProcess);
       int[] indices = new int[toProcessSparse.numValues() - classModifier];
       double[] values = new double[toProcessSparse.numValues() - classModifier];
       int index = 0;
       for (int i = 0; i < toProcessSparse.numValues(); i++) {
         if (toProcessSparse.index(i) != classIndex) {
           indices[index] = toProcessSparse.index(i);
           values[index++] = toProcessSparse.valueSparse(i);
         }
       }
-      return Vectors.sparse(toProcess.numValues(), indices, values);
+      return Vectors.sparse(
+        toProcess.numAttributes() - (classIndex >= 0 ? 1 : 0), indices, values);
     } else {
       if (classIndex < 0) {
         return Vectors.dense(toProcess.toDoubleArray());
       }
       double[] independent = new double[toProcess.numAttributes() - 1];
       int index = 0;
       for (int i = 0; i < toProcess.numAttributes(); i++) {
         if (i != classIndex) {
           independent[index++] = toProcess.value(i);
         }
       }
       return Vectors.dense(independent);
     }
   }",Buggy,"Fixed a bug in sparse instance to sparse Vector conversion.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@14868 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix in the conversion of sparse instances to sparse vectors. The code diff shows a change in how the `instanceToVector` method handles sparse instances, specifically addressing the `classModifier` and the size of the sparse vector. The original code calculates the size of the sparse vector incorrectly, potentially leading to an `ArrayIndexOutOfBoundsException` or other unexpected behavior. The updated code includes a check to determine if the class value is sparse and adjusts the `classModifier` accordingly. Additionally, the `Vectors.sparse` method now uses `toProcess.numAttributes() - (classIndex >= 0 ? 1 : 0)` to determine the size of the vector, which is the correct number of attributes. This correction aligns with the commit message, indicating a bug fix. The answer is **Buggy**.
"
weka,1604.json,a3134adec9386c387ea691ca59e46a35f35f858d,"@@ -1,175 +1,179 @@
   public void buildClassifier(Instances data) throws Exception {
     // can classifier handle the data?
     getCapabilities().testWithFail(data);
 
     m_errorsFromR = new ArrayList<String>();
 
     if (m_modelHash == null) {
       m_modelHash = """" + hashCode();
     }
 
     data = new Instances(data);
     data.deleteWithMissingClass();
 
     if (data.numInstances() == 0 || data.numAttributes() == 1) {
       if (data.numInstances() == 0) {
         System.err
           .println(""No instances with non-missing class - using ZeroR model"");
       } else {
         System.err.println(""Only the class attribute is present in ""
           + ""the data - using ZeroR model"");
       }
       m_zeroR = new ZeroR();
       m_zeroR.buildClassifier(data);
       return;
     }
 
     // remove useless attributes
     m_removeUseless = new RemoveUseless();
     m_removeUseless.setInputFormat(data);
     data = Filter.useFilter(data, m_removeUseless);
 
     if (!m_dontReplaceMissingValues) {
       m_missingFilter = new ReplaceMissingValues();
       m_missingFilter.setInputFormat(data);
       data = Filter.useFilter(data, m_missingFilter);
     }
 
     data = handleZeroFrequencyNominalValues(data);
 
     m_serializedModel = null;
 
     if (!m_initialized) {
       init();
 
       if (!m_mlrAvailable) {
         throw new Exception(
           ""MLR is not available for some reason - can't continue!"");
       }
     } else {
       // unload and then reload MLR to try and clear any errors/inconsistent
       // state
       reloadMLR();
     }
 
     m_baseLearnerLibraryAvailable = false;
     loadBaseLearnerLibrary();
     if (!m_baseLearnerLibraryAvailable) {
       throw new Exception(""Library ""
         + MLRClassifier.TAGS_LEARNER[m_rLearner].getIDStr() + "" for learner ""
         + MLRClassifier.TAGS_LEARNER[m_rLearner].getReadable()
         + "" is not available for some reason - can't continue!"");
     }
 
     RSession eng = null;
     eng = RSession.acquireSession(this);
     eng.setLog(this, m_logger);
     eng.clearConsoleBuffer(this);
 
+    // Change to a temporary directory where we have write access
+    // in case an mlr scheme tries to write a local file
+    eng.parseAndEval(this, ""setwd(tempdir())"");
+
     // Set seed for random number generator in R in a data-dependent manner
     eng.parseAndEval(this,
       ""set.seed("" + data.getRandomNumberGenerator(getSeed()).nextInt() + "")"");
 
     // clean up any previous model
     // suffix model identifier with hashcode of this object
     eng.parseAndEval(this, ""remove(weka_r_model"" + m_modelHash + "")"");
 
     // transfer training data into a data frame in R
     Object[] result = RUtils.instancesToDataFrame(eng, this, data, ""mlr_data"");
     m_cleanedAttNames = (String[])result[0];
     m_cleanedAttValues = (String[][])result[1];
 
     try {
       String mlrIdentifier =
         MLRClassifier.TAGS_LEARNER[m_rLearner].getReadable();
 
       if (data.classAttribute().isNumeric()
         && mlrIdentifier.startsWith(""classif"")) {
         throw new Exception(""Training instances has a numeric class but ""
           + ""selected R learner is a classifier!"");
       } else if (data.classAttribute().isNominal()
         && mlrIdentifier.startsWith(""regr"")) {
         throw new Exception(""Training instances has a nominal class but ""
           + ""selected R learner is a regressor!"");
       }
 
       m_errorsFromR.clear();
       // make classification/regression task
       String taskString = null;
       if (data.classAttribute().isNominal()) {
         taskString = ""task <- makeClassifTask(data = mlr_data, target = \""""
           + m_cleanedAttNames[data.classIndex()] + ""\"")"";
       } else {
         taskString = ""task <- makeRegrTask(data = mlr_data, target = \""""
           + m_cleanedAttNames[data.classIndex()] + ""\"")"";
       }
 
       if (m_Debug) {
         System.err.println(""Prediction task: "" + taskString);
       }
 
       eng.parseAndEval(this, taskString);
       // eng.parseAndEval(this, ""print(task)"");
       checkForErrors();
 
       m_schemeProducesProbs = schemeProducesProbabilities(mlrIdentifier, eng);
 
       String probs =
         (data.classAttribute().isNominal() && m_schemeProducesProbs)
           ? "", predict.type = \""prob\"""" : """";
       String learnString = null;
       if (m_schemeOptions != null && m_schemeOptions.length() > 0) {
         learnString = ""l <- makeLearner(\"""" + mlrIdentifier + ""\"""" + probs
           + "", "" + m_schemeOptions + "")"";
       } else {
         learnString =
           ""l <- makeLearner(\"""" + mlrIdentifier + ""\"""" + probs + "")"";
       }
 
       if (m_Debug) {
         System.err.println(""Make a learner object: "" + learnString);
       }
 
       eng.parseAndEval(this, learnString);
       checkForErrors();
 
       // eng.parseAndEval(this, ""print(l)"");
 
       // train model
       eng.parseAndEval(this,
         ""weka_r_model"" + m_modelHash + "" <- train(l, task)"");
 
       checkForErrors();
 
       // get the model for serialization
       REXP serializedRModel = eng.parseAndEval(this,
         ""serialize(weka_r_model"" + m_modelHash + "", NULL)"");
 
       checkForErrors();
 
       m_modelText = new StringBuffer();
 
       // get the textual representation
       eng.parseAndEval(this,
         ""print(getLearnerModel(weka_r_model"" + m_modelHash + ""))"");
       m_modelText.append(eng.getConsoleBuffer(this));
 
       // now try and serialize the model
       XStream xs = new XStream();
       String xml = xs.toXML(serializedRModel);
       if (xml != null && xml.length() > 0) {
         m_serializedModel = new StringBuffer();
         m_serializedModel.append(xml);
       }
     } catch (Exception ex) {
       ex.printStackTrace();
       // remove R training data frame after completion
       eng.parseAndEval(this, ""remove(mlr_data)"");
       RSession.releaseSession(this);
 
       throw new Exception(ex.getMessage());
     }
 
     eng.parseAndEval(this, ""remove(mlr_data)"");
     RSession.releaseSession(this);
   }",Buggy,"Fixed problem for learning algorithms such as xgboost that create files. Now MLRClassifier creates and uses a temporary directory.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@14368 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix for a problem where learning algorithms (like xgboost) create files. The diff shows that the code now changes to a temporary directory using `eng.parseAndEval(this, ""setwd(tempdir())"")` before training the model. This change addresses the issue of algorithms creating files in potentially inappropriate locations by redirecting them to a temporary directory. The answer is **Buggy**."
weka,23659.json,04f417158535105b2ada4d2cb15709a24d2924e2,"@@ -1,125 +1,125 @@
   public String toStringMatrix() {
     StringBuffer    result;
     String[][]      cells;
     int             i;
     int             j;
     int             n;
     int             size;
 
     result  = new StringBuffer();
     cells   = toArray();
 
     result.append(  ""\\begin{table}[thb]\n\\caption{\\label{labelname}""
                   + ""Table Caption}\n"");
     if (!getShowStdDev())
       result.append(""\\footnotesize\n"");
     else
       result.append(""\\scriptsize\n"");
 
     // output the column alignment characters
     // one for the dataset name and one for the comparison column
     if (!getShowStdDev()) {
       result.append(  ""{\\centering \\begin{tabular}{""
                     + ""l""                     // dataset
                     + """"                      // separator
                     + ""r""                     // mean
                     );
     } else {
       // dataset, mean, std dev
       result.append(  ""{\\centering \\begin{tabular}{"" 
                     + ""l""                     // dataset
                     + """"                      // separator
                     + ""r""                     // mean
                     + ""@{\\hspace{0cm}}""      // separator
                     + ""c""                     // +/-
                     + ""@{\\hspace{0cm}}""      // separator
                     + ""r""                     // stddev
                     );
     }
 
     for (j = 1; j < getColCount(); j++) {
       if (getColHidden(j))
         continue;
       if (!getShowStdDev())
         result.append(  ""r""                   // mean
                       + ""@{\\hspace{0.1cm}}""  // separator
                       + ""c""                   // significance
                       );
       else 
         result.append(  ""r""                   // mean
                       + ""@{\\hspace{0cm}}""    // separator
                       + ""c""                   // +/-
                       + ""@{\\hspace{0cm}}""    // separator
                       + ""r""                   // stddev
                       + ""@{\\hspace{0.1cm}}""  // separator
                       + ""c""                   // significance
                       );
     }
     result.append(""}\n\\\\\n\\hline\n"");
     if (!getShowStdDev())
-      result.append(""Dataset & "" + getColName(0));
+      result.append(""Dataset & "" + cells[0][1]);
     else
-      result.append(""Dataset & \\multicolumn{3}{c}{"" + getColName(0) + ""}"");
+      result.append(""Dataset & \\multicolumn{3}{c}{"" + cells[0][1] + ""}"");
 
     // now do the column names (numbers)
-    for (j = 1; j < getColCount(); j++) {
-      if (getColHidden(j))
+    for (j = 2; j < cells[0].length; j++) {
+      if (!isMean(j))
         continue;
       if (!getShowStdDev())
-        result.append(""& "" + getColName(j) + "" & "");
+        result.append(""& "" + cells[0][j] + "" & "");
       else
-        result.append(""& \\multicolumn{4}{c}{"" + getColName(j) + ""} "");
+        result.append(""& \\multicolumn{4}{c}{"" + cells[0][j] + ""} "");
     }
     result.append(""\\\\\n\\hline\n"");
 
     // change ""_"" to ""-"" in names
     for (i = 1; i < cells.length; i++)
       cells[i][0] = cells[i][0].replace('_', '-');
 
     // pad numbers
     for (n = 1; n < cells[0].length; n++) {
       size = getColSize(cells, n);
       for (i = 1; i < cells.length; i++)
         cells[i][n] = padString(cells[i][n], size, true);
     }
 
     // output data (w/o wins/ties/losses)
     for (i = 1; i < cells.length - 1; i++) {
       for (n = 0; n < cells[0].length; n++) {
         if (n == 0) {
           result.append(padString(cells[i][n], getRowNameWidth()));
         }
         else {
           if (getShowStdDev()) {
             if (isMean(n - 1)) {
               if (!cells[i][n].trim().equals(""""))
                 result.append("" & $\\pm$ & "");
               else
                 result.append("" &       & "");
             }
             else
               result.append("" & "");
           }
           else {
             result.append("" & "");
           }
           result.append(cells[i][n]);
         }
       }
       
       result.append(""\\\\\n"");
     }
 
     result.append(""\\hline\n\\multicolumn{"" + cells[0].length + ""}{c}{$\\circ$, $\\bullet$""
 		  +"" statistically significant improvement or degradation}""
 		  +""\\\\\n\\end{tabular} "");
     if (!getShowStdDev())     
       result.append(""\\footnotesize "");
     else
       result.append(""\\scriptsize "");
     
     result.append(""\\par}\n\\end{table}""
 		  +""\n"");
      
     return result.toString();
   }",Buggy,"fixed bug with column names (weren't in the correct order)


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@2393 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to the order of column names. The diff replaces `getColName(0)` with `cells[0][1]` and `getColName(j)` with `cells[0][j]`. This suggests that the code was previously using an incorrect method to retrieve column names, leading to them being displayed in the wrong order. The change directly addresses the issue described in the commit message. The answer is **Buggy**."
weka,36869.json,1a86d86af7ad235af95da7469c4252a5adf77d13,"@@ -1,27 +1,29 @@
   protected void checkPendingStreaming() throws WekaException {
     try {
       m_streamingFilter.batchFinished();
       Instances structureCopy =
         m_streamingFilter.getOutputFormat().stringFreeStructure();
       while (m_streamingFilter.numPendingOutput() > 0) {
         getStepManager().throughputUpdateStart();
         Instance filteredI = m_streamingFilter.output();
         if (m_stringAttsPresent) {
           for (int i = 0; i < filteredI.numAttributes(); i++) {
-            String val = filteredI.stringValue(i);
-            structureCopy.attribute(i).setStringValue(val);
-            filteredI.setValue(i, 0);
+            if (filteredI.attribute(i).isString() && ! filteredI.isMissing(i)) {
+              String val = filteredI.stringValue(i);
+              structureCopy.attribute(i).setStringValue(val);
+              filteredI.setValue(i, 0);
+            }
           }
           filteredI.setDataset(structureCopy);
         }
         m_incrementalData
           .setPayloadElement(StepManager.CON_INSTANCE, filteredI);
         if (!isStopRequested()) {
           getStepManager().outputData(m_incrementalData);
         }
         getStepManager().throughputUpdateEnd();
       }
     } catch (Exception ex) {
       throw new WekaException(ex);
     }
   }",Buggy,"Fixed a small bug in processing of string attributes

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@13321 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to string attributes. The code diff includes a check `if (filteredI.attribute(i).isString() && ! filteredI.isMissing(i))`. This condition ensures that the code only attempts to process string values for attributes that are actually strings and are not missing. Without this check, the code might have tried to call `stringValue(i)` on non-string attributes or missing values, leading to errors. This suggests a bug fix related to handling string attributes correctly. The answer is **Buggy**.
"
weka,29259.json,a01e554f4dee170f439264d96f61f10c13bb316c,"@@ -1,58 +1,58 @@
   public double[] pairwiseCoupling(double[][] n, double[][] r) {
 
     // Initialize p and u array
     double[] p = new double[r.length];
     for (int i =0; i < p.length; i++) {
       p[i] = 1.0 / (double)p.length;
     }
     double[][] u = new double[r.length][r.length];
     for (int i = 0; i < r.length; i++) {
       for (int j = i + 1; j < r.length; j++) {
 	u[i][j] = 0.5;
       }
     }
 
     // firstSum doesn't change
     double[] firstSum = new double[p.length];
     for (int i = 0; i < p.length; i++) {
       for (int j = i + 1; j < p.length; j++) {
 	firstSum[i] += n[i][j] * r[i][j];
 	firstSum[j] += n[i][j] * (1 - r[i][j]);
       }
     }
 
     // Iterate until convergence
     boolean changed;
     do {
       changed = false;
       double[] secondSum = new double[p.length];
       for (int i = 0; i < p.length; i++) {
 	for (int j = i + 1; j < p.length; j++) {
 	  secondSum[i] += n[i][j] * u[i][j];
 	  secondSum[j] += n[i][j] * (1 - u[i][j]);
 	}
       }
       for (int i = 0; i < p.length; i++) {
-	if (firstSum[i] == 0) {
+	if ((firstSum[i] == 0) || (secondSum[i] == 0)) {
 	  if (p[i] > 0) {
 	    changed = true;
 	  }
 	  p[i] = 0;
 	} else {
 	  double factor = firstSum[i] / secondSum[i];
 	  double pOld = p[i];
 	  p[i] *= factor;
 	  if (Math.abs(pOld - p[i]) > 1.0e-3) {
 	    changed = true;
 	  }
 	}
       }
       Utils.normalize(p);
       for (int i = 0; i < r.length; i++) {
 	for (int j = i + 1; j < r.length; j++) {
 	  u[i][j] = p[i] / (p[i] + p[j]);
 	}
       }
     } while (changed);
     return p;
   }",Buggy,"Fixed division by zero problem in pairwise coupling


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@1430 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,The commit message indicates a fix for a division by zero error. The code diff adds a check `(firstSum[i] == 0) || (secondSum[i] == 0)` before the division `firstSum[i] / secondSum[i]`. This prevents the division by zero when `secondSum[i]` is zero. The answer is **Buggy**.
weka,5830.json,4e28ba24b34efd31ec6954aa8312a90cd500ce2a,"@@ -1,12 +1,12 @@
   public void stepInit() throws WekaException {
     if ((m_encodedForecaster == null || m_encodedForecaster.equals(""-NONE-"")) &&
-      m_fileName == null || isEmpty(m_fileName.toString())) {
+      (m_fileName == null || isEmpty(m_fileName.toString()))) {
       throw new WekaException(""No forecaster specified!"");
     }
 
     m_isReset = true;
     m_isStreaming = false;
     m_overlayData = null;
     m_bufferedPrimeData = null;
     m_streamingData = new Data(StepManager.CON_INSTANCE);
   }",Buggy,"Fixed a bug that prevented initialization to complete successfully when the model being used is serialized into the metadata for the step

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@13349 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to initialization when the model is serialized into metadata. The diff modifies a conditional statement in the `stepInit` method. The original condition `(m_encodedForecaster == null || m_encodedForecaster.equals(""-NONE-"")) && m_fileName == null || isEmpty(m_fileName.toString())` could lead to incorrect evaluation due to operator precedence. The corrected condition `(m_encodedForecaster == null || m_encodedForecaster.equals(""-NONE-"")) && (m_fileName == null || isEmpty(m_fileName.toString()))` uses parentheses to ensure the intended logic, preventing the exception from being thrown prematurely. This correction directly addresses the bug described in the commit message. The answer is **Buggy**."
weka,25919.json,92304dbdd06bfcf597e16639d7f8b1656d4269d9,"@@ -1,3 +1,4 @@
   public void setComputeAttributeImportance(boolean computeAttributeImportance) {
     m_computeAttributeImportance = computeAttributeImportance;
+    ((RandomTree)m_Classifier).setComputeImpurityDecreases(computeAttributeImportance);
   }",Buggy,"Fixed another small bug in option setting

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@13190 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to option setting. The diff shows that setting `m_computeAttributeImportance` now also calls `setComputeImpurityDecreases` on the underlying classifier, which is cast to `RandomTree`. This suggests that the previous implementation was incomplete, and the impurity decreases weren't being updated when the attribute importance option was set, which is a bug. The answer is **Buggy**."
weka,10363.json,e68621d5b0b175c4b155cb383a8c007a83d10875,"@@ -1,8 +1,8 @@
   public String globalInfo() {
-    return ""Iteratively fits a regression model by attempting to minimize absolute error, based on""
+    return ""Iteratively fits a regression model by attempting to minimize absolute error, using""
             + ""a base learner that minimizes weighted squared error.\n\n""
             + ""Weights are bounded from below by 1.0 / Utils.SMALL.\n\n""
             + ""Resamples data based on weights if base learner is not a WeightedInstancesHandler.\n\n""
       +""For more information see:\n\n""
       + getTechnicalInformation().toString();
   }",Buggy,"Updated Javadoc in IterativeAbsoluteErrorRegression. Fixed bug in globalInfo().

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@12364 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix in `globalInfo()`. The diff shows a change in the Javadoc description of the `globalInfo()` method. The original description said the model minimizes absolute error ""based on"" a base learner, while the updated description says it minimizes absolute error ""using"" a base learner. This change in wording suggests a clarification or correction of the method's functionality description, which aligns with a bug fix. The answer is **Buggy**."
weka,12909.json,1e5d54bb479e458b94b035c9d201dae6a4aec513,"@@ -1,45 +1,48 @@
   public void updateDefaultList() {
     
     ((ModelList.SortedListModel) m_ModelList.getModel()).clear();
     
     String ensemblePackageString = getPackageName();
     
     int index = m_DefaultFilesComboBox.getSelectedIndex();
     
     Vector classifiers = null;
     
     LibrarySerialization serialization;
     try {
       
       serialization = new LibrarySerialization();
       
       String defaultFileString = ensemblePackageString
       + m_DefaultFileNames[index].trim() + "".model.xml"";
       
       //System.out.println(defaultFileString);
-      
-      InputStream is = ClassLoader.getSystemResourceAsStream(defaultFileString);
+    
+      ClassLoader thisLoader = getClass().getClassLoader();
+      // InputStream is = ClassLoader.getSystemResourceAsStream(defaultFileString);
+      InputStream is = thisLoader.getResourceAsStream(defaultFileString);
       
       if (is == null) {
 	File f = new File(defaultFileString);
 	if (f.exists()) {
 	  System.out.println(""file existed: "" + f.getPath());
 	} else {
 	  System.out.println(""file didn't exist: "" + f.getPath());
 	}
 	
       }
       
-      classifiers = (Vector) serialization.read(ClassLoader.getSystemResourceAsStream(defaultFileString));
+      // classifiers = (Vector) serialization.read(ClassLoader.getSystemResourceAsStream(defaultFileString));
+      classifiers = (Vector) serialization.read(thisLoader.getResourceAsStream(defaultFileString));
       
       for (Iterator it = classifiers.iterator(); it.hasNext();) {
 	EnsembleLibraryModel model = m_ListModelsPanel.getLibrary().createModel((Classifier) it.next());
 	model.testOptions();
 	((ModelList.SortedListModel) m_ModelList.getModel()).add(model);
       }
       
     } catch (Exception e) {
       // TODO Auto-generated catch block
       e.printStackTrace();
     }
   }",Buggy,"Fixed a bug that was preventing a property file from being loaded correctly

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@14340 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix for a bug that prevented a property file from being loaded correctly. The code diff shows that the code was changed to use the class's classloader instead of the system classloader to load the resource. The code also includes some debugging statements to print whether the file exists or not. This change suggests that the original code was not able to find the resource using the system classloader, and using the class's classloader fixed the issue. The answer is **Buggy**."
weka,21385.json,1f61d35aae476dd90e8166897219ea0c8128e978,"@@ -1,173 +1,173 @@
   public void actionPerformed(ActionEvent e) {
     
     //JMenuItem m = (JMenuItem)e.getSource();
     
     if (e.getActionCommand() == null) {
       if (m_scaling == 0) {
 	repaint();
       }
       else {
 	animateScaling(m_nViewPos, m_nViewSize, m_scaling);
       }
     }
     else if (e.getActionCommand().equals(""Fit to Screen"")) {
       
       Dimension np = new Dimension();
       Dimension ns = new Dimension();
 
       getScreenFit(np, ns);
 
       animateScaling(np, ns, 10);
       
     }
     else if (e.getActionCommand().equals(""Center on Top Node"")) {
       
       int tpx = (int)(m_topNode.getCenter() * m_viewSize.width);   //calculate
       //the top nodes postion but don't adjust for where 
       int tpy = (int)(m_topNode.getTop() * m_viewSize.height);     //view is
       
       
       
       Dimension np = new Dimension(getSize().width / 2 - tpx, 
 				   getSize().width / 6 - tpy);
       
       animateScaling(np, m_viewSize, 10);
       
     }
     else if (e.getActionCommand().equals(""Auto Scale"")) {
       autoScale();  //this will figure the best scale value 
       //keep the focus on the middle of the screen and call animate
     }
     else if (e.getActionCommand().equals(""Visualize The Node"")) {
       //send the node data to the visualizer 
       if (m_focusNode >= 0) {
 	Instances inst;
 	if ((inst = m_nodes[m_focusNode].m_node.getInstances()) != null) {
 	  VisualizePanel pan = new VisualizePanel();
 	  pan.setInstances(inst);
 	  JFrame nf = new JFrame();
 	  nf.setSize(400, 300);
 	  nf.getContentPane().add(pan);
 	  nf.setVisible(true);
 	}
 	else {
 	  JOptionPane.showMessageDialog(this, ""Sorry, there is no "" + 
-					""availble Instances data for "" +
+					""available Instances data for "" +
 					""this Node."", ""Sorry!"",
 					JOptionPane.WARNING_MESSAGE); 
 	}
       }
       else {
 	JOptionPane.showMessageDialog(this, ""Error, there is no "" + 
 				      ""selected Node to perform "" +
 				      ""this operation on."", ""Error!"",
 				      JOptionPane.ERROR_MESSAGE); 
       }
     }
     else if (e.getActionCommand().equals(""Create Child Nodes"")) {
       if (m_focusNode >= 0) {
 	if (m_listener != null) {
 	  //then send message to the listener
 	  m_listener.userCommand(new TreeDisplayEvent
 	    (TreeDisplayEvent.ADD_CHILDREN, 
 	     m_nodes[m_focusNode].m_node.getRefer()));
 	}
 	else {
 	  JOptionPane.showMessageDialog(this, ""Sorry, there is no "" + 
 					""available Decision Tree to "" +
 					""perform this operation on."",
 					""Sorry!"", 
 					JOptionPane.WARNING_MESSAGE);
 	}
       }
       else {
 	JOptionPane.showMessageDialog(this, ""Error, there is no "" +
 				      ""selected Node to perform this "" +
 				      ""operation on."", ""Error!"",
 				      JOptionPane.ERROR_MESSAGE);
       }
     }
     else if (e.getActionCommand().equals(""Remove Child Nodes"")) {
       if (m_focusNode >= 0) {
 	if (m_listener != null) {
 	  //then send message to the listener
 	  m_listener.userCommand(new 
 	    TreeDisplayEvent(TreeDisplayEvent.REMOVE_CHILDREN, 
 			     m_nodes[m_focusNode].m_node.getRefer()));
 	}
 	else {
 	  JOptionPane.showMessageDialog(this, ""Sorry, there is no "" + 
 					""available Decsion Tree to "" +
 					""perform this operation on."",
 					""Sorry!"", 
 					JOptionPane.WARNING_MESSAGE);
 	}
       }
       else {
 	JOptionPane.showMessageDialog(this, ""Error, there is no "" +
 				      ""selected Node to perform this "" +
 				      ""operation on."", ""Error!"",
 				      JOptionPane.ERROR_MESSAGE);
       }
     }
     else if (e.getActionCommand().equals(""classify_child"")) {
       if (m_focusNode >= 0) {
 	if (m_listener != null) {
 	  //then send message to the listener
 	  m_listener.userCommand(new TreeDisplayEvent
 	    (TreeDisplayEvent.CLASSIFY_CHILD, 
 	     m_nodes[m_focusNode].m_node.getRefer()));
 	}
 	else {
 	  JOptionPane.showMessageDialog(this, ""Sorry, there is no "" + 
 					""available Decsion Tree to "" +
 					""perform this operation on."",
 					""Sorry!"", 
 					JOptionPane.WARNING_MESSAGE);
 	}
       }
       else {
 	JOptionPane.showMessageDialog(this, ""Error, there is no "" +
 				      ""selected Node to perform this "" +
 				      ""operation on."", ""Error!"",
 				      JOptionPane.ERROR_MESSAGE);
       }
     }
     else if (e.getActionCommand().equals(""send_instances"")) {
       if (m_focusNode >= 0) {
 	if (m_listener != null) {
 	  //then send message to the listener
 	  m_listener.userCommand(new TreeDisplayEvent
 	    (TreeDisplayEvent.SEND_INSTANCES, 
 	     m_nodes[m_focusNode].m_node.getRefer()));
 	}
 	else {
 	  JOptionPane.showMessageDialog(this, ""Sorry, there is no "" + 
 					""available Decsion Tree to "" +
 					""perform this operation on."",
 					""Sorry!"", 
 					JOptionPane.WARNING_MESSAGE);
 	}
       }
       else {
 	JOptionPane.showMessageDialog(this, ""Error, there is no "" +
 				      ""selected Node to perform this "" +
 				      ""operation on."", ""Error!"",
 				      JOptionPane.ERROR_MESSAGE);
       }
     }
     else if (e.getActionCommand().equals(""Accept The Tree"")) {
       if (m_listener != null) {
 	//then send message to the listener saying that the tree is done
 	m_listener.userCommand(new TreeDisplayEvent(TreeDisplayEvent.ACCEPT,
 						  null));
       }
       else {
 	JOptionPane.showMessageDialog(this, ""Sorry, there is no "" +
 				      ""available Decision Tree to "" +
 				      ""perform this operation on."",
 				      ""Sorry!"", 
 				      JOptionPane.WARNING_MESSAGE);
       }
     }
   }",Buggy,"fixed typo in error message


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@3733 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix for a typo in an error message. The diff shows a change from ""availble"" to ""available"" within a JOptionPane.showMessageDialog call. This correction directly addresses the typo mentioned in the commit message. The answer is **Buggy**."
weka,26536.json,8a6133c51b18360a896a4e959c47ff6c31ed9425,"@@ -1,16 +1,12 @@
   protected void initializeWithPluginMetrics() {
     m_pluginMetrics.clear();
     List<AbstractEvaluationMetric> pluginMetrics = m_eval.getPluginMetrics();
     if (pluginMetrics != null && pluginMetrics.size() > 0) {
       for (AbstractEvaluationMetric m : pluginMetrics) {
-        if (m instanceof InformationRetrievalEvaluationMetric) {
-          List<String> statNames = m.getStatisticNames();
-          for (String s : statNames) {
-            m_pluginMetrics.put(s.toLowerCase(), m);
-          }
-        } else {
-          m_pluginMetrics.put(m.getMetricName().toLowerCase(), m);
+        List<String> statNames = m.getStatisticNames();
+        for (String s : statNames) {
+          m_pluginMetrics.put(s.toLowerCase(), m);
         }
       }
     }
   }",Buggy,"Fixed a small bug in the initializeWithPluginMetrics() method.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@10890 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix in the `initializeWithPluginMetrics()` method. The diff removes the conditional check `if (m instanceof InformationRetrievalEvaluationMetric)`. This suggests that the original code was incorrectly filtering out some metrics, leading to incomplete initialization. By removing the conditional, all plugin metrics are now processed, which fixes the bug. The answer is **Buggy**."
weka,21345.json,f05e160a2290644b91f34152ee153ba21fa688bd,"@@ -1,9 +1,21 @@
     public void setValueAt(Object aValue,
 			   int rowIndex,
 			   int columnIndex) {
 
       //      double value = ((Double) aValue).doubleValue();
       //      m_matrix.setElement(rowIndex, columnIndex, value);
-      m_matrix.setCell(rowIndex, columnIndex, aValue);
+      // try to parse it as a double first
+      Double val;
+      try {
+        val = new Double(((String)aValue));
+        double value = val.doubleValue();
+      } catch (Exception ex) {
+        val = null;
+      }
+      if (val == null) {
+        m_matrix.setCell(rowIndex, columnIndex, aValue);
+      } else {
+        m_matrix.setCell(rowIndex, columnIndex, val);
+      }
       fireTableCellUpdated(rowIndex, columnIndex);
     }",Buggy,"Fixed problem with setting cost values


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@3268 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix related to setting cost values. The code diff shows that the `setValueAt` method in a matrix class has been modified. The original code directly set the cell value using `m_matrix.setCell(rowIndex, columnIndex, aValue)`. The updated code attempts to parse the input `aValue` as a Double. If successful, the double value is used to set the cell; otherwise, the original `aValue` is used. This change suggests that the previous implementation had issues when the input was not a Double, leading to incorrect or unexpected behavior. The added try-catch block handles potential exceptions during the parsing process, indicating an attempt to fix a bug or improve error handling. The answer is **Buggy**."
weka,32633.json,5bf47271c53b258212fcf284bc65101b75cd4309,"@@ -1,45 +1,45 @@
   protected static void loadPackageDirectory(File directory, boolean verbose)
       throws Exception {
     File[] contents = directory.listFiles();
 
     // make sure that jar files and lib directory get processed first
     for (int i = 0; i < contents.length; i++) {
       if (contents[i].isFile() && contents[i].getPath().endsWith("".jar"")) {
         if (verbose) {
           System.out.println(""[Weka] loading "" + contents[i].getPath());
         }
         ClassloaderUtil.addFile(contents[i].getPath());
       } else if (contents[i].isDirectory()
           && contents[i].getName().equalsIgnoreCase(""lib"")) {
         // add any jar files in the lib directory to the classpath
         loadPackageDirectory(contents[i], verbose);
       }
     }
 
     // now any auxilliary files
     for (int i = 0; i < contents.length; i++) {
       if (contents[i].isFile() && contents[i].getPath().endsWith(""Beans.props"")) {
         // KnowledgeFlow plugin -- add the Beans.props file to the list of
         // bean plugin props
 
         KnowledgeFlowApp.addToPluginBeanProps(contents[i]);
         KnowledgeFlowApp.disposeSingleton();
 
       } else if (contents[i].isFile()
           && contents[i].getPath().endsWith(""Explorer.props"")) {
         // Explorer tabs plugin
         // process the keys in the properties file and append/add values
         processExplorerProps(contents[i]);
       } else if (contents[i].isFile()
           && contents[i].getPath().endsWith(""GUIEditors.props"")) {
         // Editor for a particular component
         processGUIEditorsProps(contents[i]);
       } else if (contents[i].isFile()
           && contents[i].getPath().endsWith(""GenericPropertiesCreator.props"")) {
         processGenericPropertiesCreatorProps(contents[i]);
       } else if (contents[i].isFile()
-          && contents[i].getParent().endsWith(""PluginManager.props"")) {
+          && contents[i].getPath().endsWith(""PluginManager.props"")) {
         processPluginManagerProps(contents[i]);
       }
     }
   }",Buggy,"Fixed a bug in the processing of PluginManager.props files.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@9314 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to processing `PluginManager.props` files. The diff shows that the condition `contents[i].getParent().endsWith(""PluginManager.props"")` has been changed to `contents[i].getPath().endsWith(""PluginManager.props"")`. The original condition checks if the *parent directory* name ends with ""PluginManager.props"", which is incorrect. The corrected condition checks if the *file path* ends with ""PluginManager.props"", which is the intended behavior. This change directly addresses the bug described in the commit message. The answer is **Buggy**."
weka,30120.json,4223a444b35556add50503527e26b24f82ea6784,"@@ -1,6 +1,6 @@
   public Instances trainCV(int numFolds, int numFold, Random random) {
 
-    Instances train = trainCV(numFold, numFold);
+    Instances train = trainCV(numFolds, numFold);
     train.randomize(random);
     return train;
   }",Buggy,"Fixed a bug that I just introduced with my new code.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@1850 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,The commit message indicates a bug fix. The code change involves correcting the first argument passed to the `trainCV` method. It seems the incorrect `numFold` was being passed instead of the intended `numFolds`. This is a clear indication of a bug fix. The answer is **Buggy**.
weka,20457.json,0fb7be37516d2f841ec1d8dc7949967049dd41f9,"@@ -1,5 +1,10 @@
   public void setText(String text) {
     m_currentContents = text;
-    m_combo.setSelectedItem(m_currentContents);
+    java.awt.Component theEditor = m_combo.getEditor().getEditorComponent();
+    if (theEditor instanceof JTextField) {
+      ((JTextField)theEditor).setText(text);
+    } else {
+      m_combo.setSelectedItem(m_currentContents);
+    }
     m_support.firePropertyChange("""", null, null);
   }",Buggy,"Fixed a bug in setting the value of the field.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@8116 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to setting the value of a field. The diff shows that the code now checks if the editor component of a combo box is a JTextField. If it is, the text is set directly on the JTextField; otherwise, the selected item of the combo box is set. This suggests that the original code might have failed to update the text field in certain cases, leading to an inconsistency between the combo box's selected item and the displayed text. This change ensures that the text field is always updated correctly, which aligns with the commit message indicating a bug fix. The answer is **Buggy**."
weka,36417.json,5887d72aa9c3dc9d14f09c98ea60b54b3ea4a75c,"@@ -1,31 +1,40 @@
   public void processSecondary(Integer setNum, Integer maxSetNum, Data data,
     PairedDataHelper<weka.classifiers.Classifier> helper) throws WekaException {
 
     // trained classifier for this set number
     weka.classifiers.Classifier classifier =
       helper.getIndexedPrimaryResult(setNum);
 
     // test data
     Instances testSplit = data.getPrimaryPayload();
 
+    if (m_trainedClassifierHeader != null
+      && !testSplit.equalHeaders(m_trainedClassifierHeader)) {
+      if (!(m_trainedClassifier instanceof InputMappedClassifier)) {
+        throw new WekaException(
+          ""Structure of incoming data does not match ""
+            + ""that of the trained classifier"");
+      }
+    }
+
     // paired training data
     Instances trainingSplit =
       helper.getIndexedValueFromNamedStore(""trainingSplits"", setNum);
 
     getStepManager().logBasic(
       ""Dispatching model for set "" + setNum + "" out of "" + maxSetNum
         + "" to output"");
 
     Data batchClassifier =
       new Data(StepManager.CON_BATCH_CLASSIFIER, classifier);
     batchClassifier.setPayloadElement(StepManager.CON_AUX_DATA_TRAININGSET,
       trainingSplit);
     batchClassifier.setPayloadElement(StepManager.CON_AUX_DATA_TESTSET,
       testSplit);
     batchClassifier.setPayloadElement(StepManager.CON_AUX_DATA_SET_NUM, setNum);
     batchClassifier.setPayloadElement(StepManager.CON_AUX_DATA_MAX_SET_NUM,
       maxSetNum);
     batchClassifier
       .setPayloadElement(StepManager.CON_AUX_DATA_LABEL, getName());
     getStepManager().outputData(batchClassifier);
   }",Buggy,"Fixed a bug where the Classifier step was not checking for compatible structure structure between training and test sets in the case where the classifier is not an InputMappedClassifier

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@14678 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to the Classifier step, specifically when the classifier is not an InputMappedClassifier. The code diff adds a check to ensure that the structure of the incoming test data matches the structure of the data used to train the classifier. If the structures are incompatible and the classifier is not an InputMappedClassifier, a WekaException is thrown. This confirms the bug fix described in the commit message. The answer is **Buggy**."
weka,36360.json,a415ad7bb7e75ba3cda722d4c62bd27c01db961e,"@@ -1,78 +1,84 @@
   protected void createOffscreenPlot(Data data) {
     List<Instances> offscreenPlotData = new ArrayList<Instances>();
     Instances predictedI = data.getPrimaryPayload();
+    boolean colorSpecified = false;
 
-    if (predictedI.classIndex() >= 0 && predictedI.classAttribute().isNominal()) {
+    String additional = m_additionalOptions;
+    if (m_additionalOptions.length() > 0) {
+      additional = environmentSubstitute(additional);
+    }
+
+    if (!additional.contains(""-color"")
+      && m_offscreenRendererName.contains(""Weka Chart Renderer"")) {
+      // for WekaOffscreenChartRenderer only
+      if (additional.length() > 0) {
+        additional += "","";
+      }
+      if (predictedI.classIndex() >= 0) {
+        additional += ""-color="" + predictedI.classAttribute().name();
+      } else {
+        additional += ""-color=/last"";
+      }
+    } else {
+      colorSpecified = true;
+    }
+
+    if (predictedI.classIndex() >= 0 && predictedI.classAttribute().isNominal()
+      && !colorSpecified) {
       // set up multiple series - one for each class
       Instances[] classes = new Instances[predictedI.numClasses()];
       for (int i = 0; i < predictedI.numClasses(); i++) {
         classes[i] = new Instances(predictedI, 0);
         classes[i].setRelationName(predictedI.classAttribute().value(i));
       }
       for (int i = 0; i < predictedI.numInstances(); i++) {
         Instance current = predictedI.instance(i);
         classes[(int) current.classValue()].add((Instance) current.copy());
       }
       for (Instances classe : classes) {
         offscreenPlotData.add(classe);
       }
     } else {
       offscreenPlotData.add(new Instances(predictedI));
     }
 
     List<String> options = new ArrayList<String>();
-    String additional = m_additionalOptions;
-    if (m_additionalOptions.length() > 0) {
-      additional = environmentSubstitute(additional);
-    }
-
-    if (additional.contains(""-color"")) {
-      // for WekaOffscreenChartRenderer only
-      if (additional.length() > 0) {
-        additional += "","";
-      }
-      if (predictedI.classIndex() >= 0) {
-        additional += ""-color="" + predictedI.classAttribute().name();
-      } else {
-        additional += ""-color=/last"";
-      }
-    }
 
     String[] optionsParts = additional.split("","");
     for (String p : optionsParts) {
       options.add(p.trim());
     }
 
     // only need the x-axis (used to specify the attribute to plot)
     String xAxis = m_xAxis;
     xAxis = environmentSubstitute(xAxis);
 
     String width = m_width;
     String height = m_height;
     int defWidth = 500;
     int defHeight = 400;
     width = environmentSubstitute(width);
     height = environmentSubstitute(height);
 
     defWidth = Integer.parseInt(width);
     defHeight = Integer.parseInt(height);
 
     try {
       getStepManager().logDetailed(""Creating image"");
       BufferedImage osi =
         m_offscreenRenderer.renderHistogram(defWidth, defHeight,
           offscreenPlotData, xAxis, options);
 
       Data imageData = new Data(StepManager.CON_IMAGE, osi);
       String relationName = predictedI.relationName();
       if (relationName.length() > 10) {
         relationName = relationName.substring(0, 10);
       }
       imageData.setPayloadElement(StepManager.CON_AUX_DATA_TEXT_TITLE,
         relationName + "":"" + m_xAxis);
       getStepManager().outputData(imageData);
     } catch (Exception e1) {
       e1.printStackTrace();
     }
 
   }",Buggy,"Fixed a few bugs in offscreen rendering that affected stacked histograms

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@13327 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates that the changes fix bugs in offscreen rendering, specifically affecting stacked histograms. The diff modifies the `createOffscreenPlot` method. The code adds logic to handle color specification for the ""Weka Chart Renderer"" and ensures that the color attribute is correctly set based on the class index. The code also handles cases where the color is not specified in the additional options. The changes suggest that there were issues with how colors were being handled in the offscreen rendering, especially for stacked histograms, and this commit aims to correct those issues. The answer is **Buggy**."
weka,9851.json,64d54abda520da9b2297ed56047ce86beda812b4,"@@ -1,113 +1,117 @@
   protected void makeTree(FastVector BestFirstElements, BFTree root,
       Instances train, Instances test, FastVector modelError, int[][] sortedIndices,
       double[][] weights, double[][][] dists, double[] classProbs, double totalWeight,
       double[] branchProps, int minNumObj, boolean useHeuristic, boolean useGini, boolean useErrorRate)
   throws Exception {
 
     if (BestFirstElements.size()==0) return;
 
     ///////////////////////////////////////////////////////////////////////
     // All information about the node to split (first BestFirst object in
     // BestFirstElements)
     FastVector firstElement = (FastVector)BestFirstElements.elementAt(0);
 
     // node to split
     //BFTree nodeToSplit = (BFTree)firstElement.elementAt(0);
 
     // split attribute
     Attribute att = (Attribute)firstElement.elementAt(1);
 
     // info of split value or split string
     double splitValue = Double.NaN;
     String splitStr = null;
     if (att.isNumeric())
       splitValue = ((Double)firstElement.elementAt(2)).doubleValue();
     else {
       splitStr=((String)firstElement.elementAt(2)).toString();
     }
 
     // the best gini gain or information of this node
     double gain = ((Double)firstElement.elementAt(3)).doubleValue();
     ///////////////////////////////////////////////////////////////////////
 
     if (totalWeight < 2*minNumObj || branchProps[0]==0
 	|| branchProps[1]==0) {
       // remove the first element
       BestFirstElements.removeElementAt(0);
       makeLeaf(train);
+      if (BestFirstElements.size() == 0) {
+        return;
+      }
+
       BFTree nextSplitNode = (BFTree)
       ((FastVector)BestFirstElements.elementAt(0)).elementAt(0);
       nextSplitNode.makeTree(BestFirstElements, root, train, test, modelError,
 	  nextSplitNode.m_SortedIndices, nextSplitNode.m_Weights,
 	  nextSplitNode.m_Dists, nextSplitNode.m_ClassProbs,
 	  nextSplitNode.m_TotalWeight, nextSplitNode.m_Props, minNumObj,
 	  useHeuristic, useGini, useErrorRate);
       return;
 
     }
 
     // If gini gain or information gain is 0, make all nodes in the BestFirstElements leaf nodes
     // because these node sorted descendingly according to gini gain or information gain.
     // (namely, gini gain or information gain of all nodes in BestFirstEelements is 0).
     if (gain==0) {
       for (int i=0; i<BestFirstElements.size(); i++) {
 	FastVector element = (FastVector)BestFirstElements.elementAt(i);
 	BFTree node = (BFTree)element.elementAt(0);
 	node.makeLeaf(train);
       }
       BestFirstElements.removeAllElements();
     }
 
     // gini gain or information gain is not 0
     else {
       // remove the first element
       BestFirstElements.removeElementAt(0);
       m_Attribute = att;
       if (att.isNumeric()) m_SplitValue = splitValue;
       else m_SplitString = splitStr;
 
       int[][][] subsetIndices = new int[2][train.numAttributes()][0];
       double[][][] subsetWeights = new double[2][train.numAttributes()][0];
 
       splitData(subsetIndices, subsetWeights, m_Attribute,
 	  m_SplitValue, m_SplitString,
 	  sortedIndices, weights, train);
 
       // if split will generate node(s) which has total weights less than m_minNumObj,
       // do not split
       int attIndex = att.index();
       if (subsetIndices[0][attIndex].length<minNumObj ||
 	  subsetIndices[1][attIndex].length<minNumObj) {
 	makeLeaf(train);
       }
 
       // split the node and cauculate error rate of this temporary tree
       else {
 	m_isLeaf = false;
 	m_Attribute = att;
 
 	makeSuccessors(BestFirstElements,train,subsetIndices,
 	    subsetWeights,dists, m_Attribute, useHeuristic, useGini);
 	for (int i=0; i<2; i++){
 	  m_Successors[i].makeLeaf(train);
 	}
 
 	Evaluation eval = new Evaluation(test);
 	eval.evaluateModel(root, test);
 	double error;
 	if (useErrorRate) error = eval.errorRate();
 	else error = eval.rootMeanSquaredError();
 	modelError.addElement(new Double(error));
       }
 
       if (BestFirstElements.size()!=0) {
 	FastVector nextSplitElement = (FastVector)BestFirstElements.elementAt(0);
 	BFTree nextSplitNode = (BFTree)nextSplitElement.elementAt(0);
 	nextSplitNode.makeTree(BestFirstElements, root, train, test, modelError,
 	    nextSplitNode.m_SortedIndices, nextSplitNode.m_Weights,
 	    nextSplitNode.m_Dists, nextSplitNode.m_ClassProbs,
 	    nextSplitNode.m_TotalWeight, nextSplitNode.m_Props, minNumObj,
 	    useHeuristic, useGini,useErrorRate);
       }
     }
   }",Buggy,"Fixed null pntr exception due to the fact that BFTree's recursion depended on a bug in FastVector


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@4237 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix for a null pointer exception caused by a bug in FastVector affecting BFTree's recursion. The code diff adds a null check `if (BestFirstElements.size() == 0)` and returns if the size is zero. This prevents a potential null pointer exception when trying to access elements from an empty `BestFirstElements` vector, which aligns with the commit message's description of fixing a null pointer exception. The answer is **Buggy**."
weka,28572.json,563360413e272237a4f3fe3033b8e50dfb132e27,"@@ -1,12 +1,18 @@
 	void replaceAtt(int nTargetNode, String sName, FastVector values) {
 		Attribute newAtt = new Attribute(sName, values);
 		if (m_Instances.classIndex() == nTargetNode) {
 			m_Instances.setClassIndex(-1);
-			m_Instances.insertAttributeAt(newAtt, nTargetNode);
+			/*m_Instances.insertAttributeAt(newAtt, nTargetNode);
 			m_Instances.deleteAttributeAt(nTargetNode + 1);
+			m_Instances.setClassIndex(nTargetNode); */
+			
+			m_Instances.deleteAttributeAt(nTargetNode);
+			m_Instances.insertAttributeAt(newAtt, nTargetNode);
 			m_Instances.setClassIndex(nTargetNode);
 		} else {
-			m_Instances.insertAttributeAt(newAtt, nTargetNode);
-			m_Instances.deleteAttributeAt(nTargetNode + 1);
+			/*m_Instances.insertAttributeAt(newAtt, nTargetNode);
+			m_Instances.deleteAttributeAt(nTargetNode + 1); */
+		        m_Instances.deleteAttributeAt(nTargetNode);
+		        m_Instances.insertAttributeAt(newAtt, nTargetNode);
 		}
 	} // replaceAtt",Buggy,"Fixed a bug (caused indirectly by the change that disallows attributes with the same name in a set of instances) whereby an exception was generated by actions that mutated the values at a node.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@7834 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to actions that mutate values at a node, which caused an exception. The code diff shows changes in the `replaceAtt` method, specifically how attributes are replaced within the `m_Instances` object. The original code used `insertAttributeAt` followed by `deleteAttributeAt(nTargetNode + 1)`, while the corrected code uses `deleteAttributeAt(nTargetNode)` followed by `insertAttributeAt(newAtt, nTargetNode)`. The commented-out code suggests an attempt to fix the issue by inserting at the target node and deleting the subsequent node, but this approach was replaced by deleting the target node first and then inserting the new attribute at the same position. This change addresses a bug where mutating values at a node caused an exception, indicating a bug fix. The answer is **Buggy**."
weka,20177.json,b5bafa7c1b1d4a69d3060a9be3b02c4b280127b9,"@@ -1,10 +1,11 @@
   public static void addPlugin(String interfaceName, String name, 
       String concreteType) {
     if (PLUGINS.get(interfaceName) == null) {
       Map<String, String> pluginsOfInterfaceType = 
         new TreeMap<String, String>();
       pluginsOfInterfaceType.put(name, concreteType);
+      PLUGINS.put(interfaceName, pluginsOfInterfaceType);      
     } else {
       PLUGINS.get(interfaceName).put(name, concreteType);
     }
   }",Buggy,"Fixed a bug in the plugin registering mechanism.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@7629 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix in the plugin registering mechanism. The diff shows that if no plugins for a given interface exist, a new `TreeMap` is created and the plugin is added, but the map itself was not being added to the `PLUGINS` map. The added line `PLUGINS.put(interfaceName, pluginsOfInterfaceType);` fixes this, ensuring the new map is stored, thus fixing the bug. The answer is **Buggy**.
"
weka,21148.json,bad512e53bc1d955e4847e14296af551c9f28459,"@@ -1,50 +1,50 @@
   public void acceptDataSet(DataSetEvent e) {
 
     m_busy = true;
     if (m_log != null) {
       m_log.statusMessage(statusMessagePrefix() + ""Processing batch..."");
     }
 
     try {
       makeOutputStructure(new Instances(e.getDataSet(), 0));
     } catch (Exception ex) {
       String msg = statusMessagePrefix()
         + ""ERROR: unable to create output instances structure."";
       if (m_log != null) {
         m_log.statusMessage(msg);
         m_log.logMessage(""[SubstringLabeler] "" + ex.getMessage());
       }
       stop();
 
       ex.printStackTrace();
       m_busy = false;
       return;
     }
 
     Instances toProcess = e.getDataSet();
 
     for (int i = 0; i < toProcess.numInstances(); i++) {
       Instance current = toProcess.instance(i);
       Instance result = null;
       try {
         result = m_matches.makeOutputInstance(current, true);
       } catch (Exception ex) {
         ex.printStackTrace();
       }
 
       if (result != null) {
         // m_outputStructure.add(result);
         m_matches.getOutputStructure().add(result);
       }
     }
 
     if (m_log != null) {
       m_log.statusMessage(statusMessagePrefix() + ""Finished."");
     }
 
     // notify listeners
-    DataSetEvent d = new DataSetEvent(this, m_matches.getInputStructure());
+    DataSetEvent d = new DataSetEvent(this, m_matches.getOutputStructure());
     notifyDataListeners(d);
 
     m_busy = false;
   }",Buggy,"Fixed a bug in batch mode

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@11956 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix in batch mode. The diff modifies the `acceptDataSet` method. Specifically, the `DataSetEvent` constructor now uses `m_matches.getOutputStructure()` instead of `m_matches.getInputStructure()`. This suggests that the original code was incorrectly passing the input structure where the output structure was expected, which is a bug. The answer is **Buggy**."
weka,36953.json,aeff9067d2f9056edfc89df194af976e07e4bd40,"@@ -1,105 +1,106 @@
   public void processIncoming(Data data) throws WekaException {
     Object modelToSave = null;
     Instances modelHeader = null;
     Integer setNum = null;
     Integer maxSetNum = null;
 
     if (data.getConnectionName().equals(StepManager.CON_INCREMENTAL_CLASSIFIER)) {
       if (m_incrementalHeader == null
         && !getStepManager().isStreamFinished(data)) {
         m_incrementalHeader =
           ((Instance) data
             .getPayloadElement(StepManager.CON_AUX_DATA_TEST_INSTANCE))
             .dataset();
       }
       if (getStepManager().isStreamFinished(data)
         || (m_incrementalSaveSchedule > 0
           && m_counter % m_incrementalSaveSchedule == 0 && m_counter > 0)) {
         modelToSave =
           (weka.classifiers.Classifier) data
             .getPayloadElement(StepManager.CON_INCREMENTAL_CLASSIFIER);
+        modelHeader = m_incrementalHeader;
       }
     } else {
       modelToSave = data.getPayloadElement(data.getConnectionName());
       modelHeader =
         (Instances) data
           .getPayloadElement(StepManager.CON_AUX_DATA_TRAININGSET);
       setNum =
         (Integer) data.getPayloadElement(StepManager.CON_AUX_DATA_SET_NUM);
       maxSetNum =
         (Integer) data.getPayloadElement(StepManager.CON_AUX_DATA_MAX_SET_NUM);
       if (modelHeader == null) {
         modelHeader =
           (Instances) data.getPayloadElement(StepManager.CON_AUX_DATA_TESTSET);
       }
     }
 
     if (modelToSave != null) {
       if (modelToSave instanceof UpdateableBatchProcessor) {
         try {
           // make sure model cleans up before saving
           ((UpdateableBatchProcessor) modelToSave).batchFinished();
         } catch (Exception ex) {
           throw new WekaException(ex);
         }
       }
 
       if (modelHeader != null) {
         modelHeader = new Instances(modelHeader, 0);
       }
 
       getStepManager().processing();
       String prefix = getStepManager().environmentSubstitute(m_filenamePrefix);
       String relationName =
         m_includeRelationName && modelHeader != null ? modelHeader
           .relationName() : """";
       String setSpec =
         maxSetNum != null && setNum != null ? ""_"" + setNum + ""_"" + maxSetNum
           + ""_"" : """";
 
       String modelName = modelToSave.getClass().getCanonicalName();
       modelName =
         modelName.substring(modelName.lastIndexOf(""."") + 1, modelName.length());
       String filename = """" + prefix + relationName + setSpec + modelName;
       filename = sanitizeFilename(filename);
 
       String dirName =
         getStepManager().environmentSubstitute(m_directory.toString());
       File tempFile = new File(dirName);
       filename = tempFile.getAbsolutePath() + File.separator + filename;
 
       getStepManager().logBasic(
         ""Saving model "" + modelToSave.getClass().getCanonicalName() + "" to ""
           + filename + "".model"");
       getStepManager().statusMessage(
         ""Saving model: "" + modelToSave.getClass().getCanonicalName());
 
       ObjectOutputStream oos = null;
       try {
         oos =
           new ObjectOutputStream(new BufferedOutputStream(new FileOutputStream(
             new File(filename + "".model""))));
         oos.writeObject(modelToSave);
         if (modelHeader != null) {
           oos.writeObject(modelHeader);
         }
         oos.close();
       } catch (Exception ex) {
         throw new WekaException(ex);
       } finally {
         if (data.getConnectionName() != StepManager.CON_INCREMENTAL_CLASSIFIER
           || getStepManager().isStreamFinished(data)) {
           getStepManager().finished();
         }
         if (oos != null) {
           try {
             oos.close();
           } catch (Exception ex) {
             throw new WekaException(ex);
           }
         }
       }
     }
 
     m_counter++;
   }",Buggy,"Fixed a bug where the training data header was not getting saved for incremental models

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@13175 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message states ""Fixed a bug where the training data header was not getting saved for incremental models"". The diff adds the line `modelHeader = m_incrementalHeader;` inside the `if` block that checks if the connection name is `StepManager.CON_INCREMENTAL_CLASSIFIER` and if the model should be saved. This line assigns the incremental header to the `modelHeader` variable, which is then used to save the header along with the model. This confirms that the bug was indeed related to the training data header not being saved for incremental models, and the added line fixes this issue. The answer is **Buggy**."
weka,9988.json,ff62ff2731fed53f3db748855b97c8c08a893dca,"@@ -1,19 +1,19 @@
   protected synchronized void trimZeroDistances(Neighborhood n) {    
-    int index = 0;
+    int index = n.m_neighbors.numInstances();
     for (int i = 0; i < n.m_neighbors.numInstances(); i++) {
       if (n.m_distances[i] > 0) {
         index = i;
         break;
       }
     }
     
     if (index > 0) {
       // trim zero distances
       for (int i = 0; i < index; i++) {
         n.m_neighbors.remove(0);
       }
       double[] newDist = new double[n.m_distances.length - index];
       System.arraycopy(n.m_distances, index, newDist, 0, newDist.length);
       n.m_distances = newDist;
     }
   }",Buggy,"Fixed a bug in trimZeroDistances()

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@9723 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix in the `trimZeroDistances()` method. The code diff shows that the initial value of `index` was changed from 0 to `n.m_neighbors.numInstances()`. Also, the logic to find the first non-zero distance is modified. The original code would break out of the loop after finding the first non-zero distance, but the modified code initializes `index` to the number of instances and then iterates through the distances. If a distance is greater than 0, `index` is updated with the current index `i`. If no distance is greater than 0, `index` will remain `n.m_neighbors.numInstances()`. The subsequent `if (index > 0)` block trims zero distances. The change suggests that the original code might not have correctly handled cases where all distances were zero or negative, or where the first few distances were zero. The updated code appears to address this issue, indicating a bug fix.
The answer is **Buggy**."
weka,18815.json,0cd35ea8a7783c8ca9d16a71fc888af2d82c3e90,"@@ -1,69 +1,74 @@
   public static void main(String[] args) {
+
+    weka.core.logging.Logger.log(weka.core.logging.Logger.Level.INFO,
+      ""Logging started"");
+    WekaPackageManager.loadPackages(false, true, true);
+
     try {
       LookAndFeel.setLookAndFeel(WorkbenchDefaults.APP_ID,
         WorkbenchDefaults.APP_ID + "".lookAndFeel"", WorkbenchDefaults.LAF);
     } catch (IOException ex) {
       ex.printStackTrace();
     }
     weka.gui.GenericObjectEditor.determineClasses();
 
     try {
       if (System.getProperty(""os.name"").contains(""Mac"")) {
         System.setProperty(""apple.laf.useScreenMenuBar"", ""true"");
       }
       m_workbench = new WorkbenchApp();
       final javax.swing.JFrame jf =
         new javax.swing.JFrame(""Weka "" + m_workbench.getApplicationName());
       jf.getContentPane().setLayout(new java.awt.BorderLayout());
 
       Image icon =
         Toolkit.getDefaultToolkit().getImage(
           WorkbenchApp.class.getClassLoader().getResource(
             ""weka/gui/weka_icon_new_48.png""));
       jf.setIconImage(icon);
 
       jf.getContentPane().add(m_workbench, BorderLayout.CENTER);
       jf.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE);
       jf.pack();
       m_workbench.showMenuBar(jf);
       jf.setSize(1024, 768);
       jf.setVisible(true);
 
       if (args.length == 1) {
         System.err.println(""Loading instances from "" + args[0]);
         AbstractFileLoader loader = ConverterUtils.getLoaderForFile(args[0]);
         loader.setFile(new File(args[0]));
         m_workbench.getPerspectiveManager().getMainPerspective()
           .setInstances(loader.getDataSet());
       }
 
       Thread memMonitor = new Thread() {
         @Override
         public void run() {
           while (true) {
             // try {
             // System.out.println(""Before sleeping."");
             // Thread.sleep(10);
 
             if (m_Memory.isOutOfMemory()) {
               // clean up
               jf.dispose();
               m_workbench = null;
               System.gc();
 
               // display error
               System.err.println(""\ndisplayed message:"");
               m_Memory.showOutOfMemory();
               System.err.println(""\nexiting"");
               System.exit(-1);
             }
           }
         }
       };
 
       memMonitor.setPriority(Thread.MAX_PRIORITY);
       memMonitor.start();
     } catch (Exception ex) {
       ex.printStackTrace();
     }
   }",Buggy,"Fixed a weird class loading bug that affected loading RPlugin

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@15118 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix for a class loading bug affecting RPlugin. The diff adds logging at the start of the main method and loads packages using `WekaPackageManager.loadPackages`. This suggests the fix addresses issues related to package loading or initialization, which could be the root cause of the RPlugin loading problem. The logging likely helps diagnose the issue. The answer is **Buggy**."
weka,30927.json,95a3f1d8e4e0087ec7760ebed797b134337b1feb,"@@ -1,31 +1,32 @@
   protected static String makeOptionStr(AbstractFileLoader loader) {
     StringBuffer result;
     Option option;
 
     result = new StringBuffer(""\nUsage:\n"");
     result.append(""\t"" + loader.getClass().getName().replaceAll("".*\\."", """"));
-    if (loader instanceof OptionHandler) {
-      result.append("" [options]"");
-    }
     result.append("" <"");
     String[] ext = loader.getFileExtensions();
     for (int i = 0; i < ext.length; i++) {
       if (i > 0) {
         result.append("" | "");
       }
       result.append(""file"" + ext[i]);
     }
-    result.append("">\n"");
+    result.append("">"");
+    if (loader instanceof OptionHandler) {
+      result.append("" [options]"");
+    }
+    result.append(""\n"");
 
     if (loader instanceof OptionHandler) {
       result.append(""\nOptions:\n\n"");
       Enumeration<Option> enm = ((OptionHandler) loader).listOptions();
       while (enm.hasMoreElements()) {
         option = enm.nextElement();
         result.append(option.synopsis() + ""\n"");
         result.append(option.description() + ""\n"");
       }
     }
 
     return result.toString();
   }",Buggy,"Fixed output bug in command-line help: file name needs to be given *before* options.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@12103 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix for an output bug in the command-line help, specifically related to the order of file name and options. The diff modifies the `makeOptionStr` method to change the order in which the file name and options are displayed in the usage string. The file name is now displayed before the options, aligning with the commit message's description of the bug fix. The change directly addresses the described bug. The answer is **Buggy**."
weka,12790.json,bfb54a334c1bdae9044c51ab3cd1319734e8cab4,"@@ -1,98 +1,100 @@
     public void buildClusterer(Instances data) throws Exception
   {
     reset();
     meanInstance = new DenseInstance(data.numAttributes());
     for (int i = 0; i < data.numAttributes(); i++)
       meanInstance.setValue(i, data.meanOrMode(i));
     numInstances = data.numInstances();
 
     kMeans.setDistanceFunction(distanceFunction);
     kMeans.setMaxIterations(maxIterations);
     //    kMeans.setInitializeUsingKMeansPlusPlusMethod(initializeWithKMeansPlusPlus);
-    kMeans.setInitializationMethod(new weka.core.SelectedTag(SimpleKMeans.KMEANS_PLUS_PLUS, SimpleKMeans.TAGS_SELECTION));
+    if (initializeWithKMeansPlusPlus) {
+      kMeans.setInitializationMethod(new weka.core.SelectedTag(SimpleKMeans.KMEANS_PLUS_PLUS, SimpleKMeans.TAGS_SELECTION));
+    }
 
     /**
      * step 1: iterate over all restarts and possible k values, record CH-scores
      */
     Random r = new Random(m_Seed);
     double meanCHs[] = new double[maxNumClusters + 1 - minNumClusters];
     double maxCHs[] = new double[maxNumClusters + 1 - minNumClusters];
     int maxSeed[] = new int[maxNumClusters + 1 - minNumClusters];
 
     for (int i = 0; i < restarts; i++)
       {
         if (printDebug)
           System.out.println(""cascade> restarts: "" + (i + 1) + "" / "" + restarts);
 
         for (int k = minNumClusters; k <= maxNumClusters; k++)
           {
             if (printDebug)
               System.out.print(""cascade>  k:"" + k + "" "");
 
             int seed = r.nextInt();
             kMeans.setSeed(seed);
             kMeans.setNumClusters(k);
             kMeans.buildClusterer(data);
             double ch = getCalinskiHarabasz();
 
             int index = k - minNumClusters;
             meanCHs[index] = (meanCHs[index] * i + ch) / (double) (i + 1);
             if (i == 0 || ch > maxCHs[index])
               {
                 maxCHs[index] = ch;
                 maxSeed[index] = seed;
               }
 
             if (printDebug)
               System.out.println("" CH:"" + df.format(ch) + ""  W:""
                                  + df.format(kMeans.getSquaredError() / (double) (numInstances - kMeans.getNumClusters()))
                                  + "" (unweighted:"" + df.format(kMeans.getSquaredError()) + "")  B:""
                                  + df.format(getSquaredErrorBetweenClusters() / (double) (kMeans.getNumClusters() - 1))
                                  + "" (unweighted:"" + df.format(getSquaredErrorBetweenClusters()) + "") "");
           }
       }
     if (printDebug)
       {
         String s = ""cascade> max CH: [ "";
         for (int i = 0; i < maxSeed.length; i++)
           s += df.format(maxCHs[i]) + "" "";
         System.out.println(s + ""]"");
       }
     String s = ""cascade> mean CH: [ "";
     for (int i = 0; i < maxSeed.length; i++)
       s += df.format(meanCHs[i]) + "" "";
 
     finalMeanCH = s + ""]"";
     //    System.out.println(s + ""]"");
 
     /**
      * step 2: select k with best mean CH-score; select seed for max CH score for this k
      */
     int bestK = -1;
     double maxCH = -1;
     for (int k = minNumClusters; k <= maxNumClusters; k++)
       {
         int index = k - minNumClusters;
         if (bestK == -1 || meanCHs[index] > maxCH)
           {
             maxCH = meanCHs[index];
             bestK = k;
           }
       }
     if (manuallySelectNumClusters)
       {
         int selectedK = selectKManually(meanCHs, bestK);
         if (selectedK != -1)
           bestK = selectedK;
       }
     int bestSeed = maxSeed[bestK - minNumClusters];
 
     finalBestK = bestK;
     finalBestSeed = bestSeed;
     //    System.out.println(""cascade> k (yields highest mean CH): "" + bestK);
     //    System.out.println(""cascade> seed (highest CH for k="" + bestK + "") : "" + bestSeed);
 
     kMeans.setSeed(bestSeed);
     kMeans.setNumClusters(bestK);
     kMeans.buildClusterer(data);
   }",Buggy,"Fixed a small bug introduced with the last change.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@10457 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to the previous change. The diff adds a conditional statement that checks the value of `initializeWithKMeansPlusPlus`. If this variable is true, then the initialization method of kMeans is set to KMEANS_PLUS_PLUS. This suggests that the previous change might have unintentionally removed or altered the initialization method, leading to incorrect clustering results. The current change restores the intended behavior based on the `initializeWithKMeansPlusPlus` flag. The answer is **Buggy**."
weka,31032.json,4ab412e6138508ec825027ffab22b044aedd6a09,"@@ -1,9 +1,11 @@
   private void initTokenizer(StreamTokenizer tokenizer) {
     tokenizer.resetSyntax();         
     tokenizer.whitespaceChars(0, (' '-1));    
     tokenizer.wordChars(' ','\u00FF');
     tokenizer.whitespaceChars(',',',');
     tokenizer.whitespaceChars('\t','\t');
     tokenizer.commentChar('%');
+    tokenizer.quoteChar('""');
+    tokenizer.quoteChar('\'');
     tokenizer.eolIsSignificant(true);
   }",Buggy,"Fixed bug introduced in r4914, where handling of quoted strings got deleted.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@5150 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to handling quoted strings, which was introduced in a previous revision. The diff shows the reintroduction of `tokenizer.quoteChar('""')` and `tokenizer.quoteChar('\'')`, which are responsible for defining the quote characters for the `StreamTokenizer`. This confirms that the change addresses the bug described in the commit message. The answer is **Buggy**."
weka,28754.json,4718fad5f7d375b9878d9d345d7cefc62c223c70,"@@ -1,25 +1,26 @@
   protected synchronized void completedClassifier(int iteration,
       boolean success) {
-    m_completed++;
 
     if (!success) {
       m_failed++;
       if (m_Debug) {
         System.err.println(""Iteration "" + iteration + "" failed!"");
       }
+    } else {
+      m_completed++;
     }
 
     if (m_completed + m_failed == m_Classifiers.length) {
       if (m_failed > 0) {
         if (m_Debug) {
           System.err.println(""Problem building classifiers - some iterations failed."");
         }
       }
 
       // have to shut the pool down or program executes as a server
       // and when running from the command line does not return to the
       // prompt
       m_executorPool.shutdown();
       block(false);
     }
   }",Buggy,"Fixed a bug that prevented the sum of completed and failed classifiers from equaling the total number of classifiers when failures occur. This bug only affected the case when one or more failures occurred.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@6266 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to the calculation of completed and failed classifiers, specifically when failures occur. The diff shows that the `m_completed++` statement was moved from the beginning of the method to inside the `else` block, which is executed only when the classifier completes successfully. This change ensures that the `m_completed` counter is incremented only for successful classifiers, correcting the previous logic where it was incremented regardless of success. This aligns with the commit message's description of the bug. The answer is **Buggy**."
weka,36601.json,2b4d1e40b90f0675df910618594260c237945fec,"@@ -1,3 +1,3 @@
-  public boolean getStepMustRunSingleThreaded() {
+  public boolean stepMustRunSingleThreaded() {
     return getStepManager().getStepMustRunSingleThreaded();
   }",Buggy,"Fixed a bug that could cause Knowledge Flow files to fail to load. This was inadvertently introduced with the single threaded executor addition

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@13700 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to loading Knowledge Flow files, which was introduced with a single-threaded executor addition. The diff shows a change in the method name from `getStepMustRunSingleThreaded()` to `stepMustRunSingleThreaded()`. While this change itself doesn't directly reveal the nature of the bug, the commit message strongly suggests that this renaming is part of the fix. The change in method name likely reflects a correction in how the single-threaded execution is handled, thus resolving the file loading issue. The answer is **Buggy**."
weka,1770.json,81174005aadc513eaeddade0fcb41f7c02f7cf21,"@@ -1,106 +1,107 @@
   protected void makeTree(Instances data, int totalInstances,
     int[][] sortedIndices, double[][] weights, double[] classProbs,
     double totalWeight, double minNumObj, boolean useHeuristic)
     throws Exception {
 
     // if no instances have reached this node (normally won't happen)
     if (totalWeight == 0) {
       m_Attribute = null;
       m_ClassValue = Utils.missingValue();
       m_Distribution = new double[data.numClasses()];
       return;
     }
 
     m_totalTrainInstances = totalInstances;
     m_isLeaf = true;
+    m_Successors = null;
 
     m_ClassProbs = new double[classProbs.length];
     m_Distribution = new double[classProbs.length];
     System.arraycopy(classProbs, 0, m_ClassProbs, 0, classProbs.length);
     System.arraycopy(classProbs, 0, m_Distribution, 0, classProbs.length);
     if (Utils.sum(m_ClassProbs) != 0) {
       Utils.normalize(m_ClassProbs);
     }
 
     // Compute class distributions and value of splitting
     // criterion for each attribute
     double[][][] dists = new double[data.numAttributes()][0][0];
     double[][] props = new double[data.numAttributes()][0];
     double[][] totalSubsetWeights = new double[data.numAttributes()][2];
     double[] splits = new double[data.numAttributes()];
     String[] splitString = new String[data.numAttributes()];
     double[] giniGains = new double[data.numAttributes()];
 
     // for each attribute find split information
     for (int i = 0; i < data.numAttributes(); i++) {
       Attribute att = data.attribute(i);
       if (i == data.classIndex()) {
         continue;
       }
       if (att.isNumeric()) {
         // numeric attribute
         splits[i] = numericDistribution(props, dists, att, sortedIndices[i],
           weights[i], totalSubsetWeights, giniGains, data);
       } else {
         // nominal attribute
         splitString[i] = nominalDistribution(props, dists, att,
           sortedIndices[i], weights[i], totalSubsetWeights, giniGains, data,
           useHeuristic);
       }
     }
 
     // Find best attribute (split with maximum Gini gain)
     int attIndex = Utils.maxIndex(giniGains);
     m_Attribute = data.attribute(attIndex);
 
     m_train = new Instances(data, sortedIndices[attIndex].length);
     for (int i = 0; i < sortedIndices[attIndex].length; i++) {
       Instance inst = data.instance(sortedIndices[attIndex][i]);
       Instance instCopy = (Instance) inst.copy();
       instCopy.setWeight(weights[attIndex][i]);
       m_train.add(instCopy);
     }
 
     // Check if node does not contain enough instances, or if it can not be
     // split,
     // or if it is pure. If does, make leaf.
     if (totalWeight < 2 * minNumObj || giniGains[attIndex] == 0
       || props[attIndex][0] == 0 || props[attIndex][1] == 0) {
       makeLeaf(data);
     }
 
     else {
       m_Props = props[attIndex];
       int[][][] subsetIndices = new int[2][data.numAttributes()][0];
       double[][][] subsetWeights = new double[2][data.numAttributes()][0];
 
       // numeric split
       if (m_Attribute.isNumeric()) {
         m_SplitValue = splits[attIndex];
       } else {
         m_SplitString = splitString[attIndex];
       }
 
       splitData(subsetIndices, subsetWeights, m_Attribute, m_SplitValue,
         m_SplitString, sortedIndices, weights, data);
 
       // If split of the node results in a node with less than minimal number of
       // isntances,
       // make the node leaf node.
       if (subsetIndices[0][attIndex].length < minNumObj
         || subsetIndices[1][attIndex].length < minNumObj) {
         makeLeaf(data);
         return;
       }
 
       // Otherwise, split the node.
       m_isLeaf = false;
       m_Successors = new SimpleCart[2];
       for (int i = 0; i < 2; i++) {
         m_Successors[i] = new SimpleCart();
         m_Successors[i].makeTree(data, m_totalTrainInstances, subsetIndices[i],
           subsetWeights[i], dists[attIndex][i],
           totalSubsetWeights[attIndex][i], minNumObj, useHeuristic);
       }
     }
   }",Buggy,"Fixed problem where unprune() step caused root node to not be labeled a leaf when it really is a leaf. This only caused problems when unpruned tree consists of a single node that is a leaf node.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@10490 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix for a scenario where the `unprune()` step incorrectly prevented a root node from being labeled as a leaf node when it should have been. This issue specifically occurred when the unpruned tree consisted of a single node that was a leaf.

The diff shows that `m_Successors` is set to null and `m_isLeaf` is set to true at the beginning of the `makeTree` method. This ensures that if the tree consists of only one node, it will be correctly labeled as a leaf.

The changes directly address the problem described in the commit message. The initialization of `m_Successors` and `m_isLeaf` ensures that the single-node tree is correctly identified as a leaf node.

Therefore, the answer is **Buggy**."
weka,17814.json,34abf3851dd64f8c49bdd8875debb6ec45310dc5,"@@ -1,27 +1,33 @@
   public void setInstancesFromFileQ() {
     
 //     if (m_IOThread == null) {
       int returnVal = m_FileChooser.showOpenDialog(this);
       if (returnVal == JFileChooser.APPROVE_OPTION) {
 	File selected = m_FileChooser.getSelectedFile();
 	
 	try
 	{
 	java.io.Reader r = new java.io.BufferedReader(
 				new java.io.FileReader(selected));
 	Instances i = new Instances(r);
 	setInstances(i);
 	
 	//dataFileLabel.setText(selected.getName());
-	dataFileLabel.setText(i.relationName());
+	String relationName = i.relationName();
+	String truncatedN = relationName;
+	if (relationName.length() > 25) {
+	  truncatedN = relationName.substring(0, 25) + ""..."";
+	}
+	dataFileLabel.setText(truncatedN);
+	dataFileLabel.setToolTipText(relationName);
 	} catch (Exception e)
 	{
 		JOptionPane.showMessageDialog(this,""Can't load at this time,\n""
 				    + ""currently busy with other IO"",
 				    ""Load Instances"",
 				    JOptionPane.WARNING_MESSAGE);
 		    e.printStackTrace();
 	
 	}
       }
   }",Buggy,"Fixed a bug that caused a widget layout problem when loading a dataset with a long relation name.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@6482 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix for a widget layout issue caused by long relation names. The diff shows that the code now truncates the relation name displayed in the `dataFileLabel` if it exceeds 25 characters, adding ""..."" to the end. It also sets the full relation name as a tooltip for the label. This change directly addresses the layout problem described in the commit message. The answer is **Buggy**."
weka,24248.json,3a2501745298eb32d9cb3350d07eb15e93716bb9,"@@ -1,13 +1,19 @@
   public void calculateDerived() {
 
     mean = Double.NaN;
     stdDev = Double.NaN;
     if (count > 0) {
       mean = sum / count;
+      stdDev = Double.POSITIVE_INFINITY;
       if (count > 1) {
 	stdDev = sumSq - (sum * sum) / count;
 	stdDev /= (count - 1);
+        if (stdDev < 0) {
+          System.err.println(""Warning: stdDev value = "" + stdDev 
+                             + "" -- rounded to zero."");
+          stdDev = 0;
+        }
 	stdDev = Math.sqrt(stdDev);
       }
     }
   }",Buggy,"Fixed some NaN problems with Std Devs.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@714 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates that the commit fixes NaN problems with standard deviations. The code changes in `calculateDerived` method show that the `stdDev` is initialized to `Double.POSITIVE_INFINITY` and there is a check to ensure that `stdDev` is not negative. If it is negative, a warning message is printed, and `stdDev` is set to 0. This indicates a bug fix related to handling potential negative values for `stdDev` which could lead to NaN values after taking the square root. The answer is **Buggy**."
weka,23181.json,2e12f8aa73f943180eb6c082e92698d3281baf35,"@@ -1,8 +1,23 @@
   protected void updateCapabilitiesFilter(Capabilities filter) {
     if (filter == null) {
       m_AssociatorEditor.setCapabilitiesFilter(new Capabilities(null));
       return;
     }
     
     m_AssociatorEditor.setCapabilitiesFilter(filter);
+    
+    m_StartBut.setEnabled(true);
+    // Check capabilities
+    Capabilities currentFilter = m_AssociatorEditor.getCapabilitiesFilter();
+    Associator associator = (Associator) m_AssociatorEditor.getValue();
+    Capabilities currentSchemeCapabilities =  null;
+    if (associator != null && currentFilter != null && 
+        (associator instanceof CapabilitiesHandler)) {
+      currentSchemeCapabilities = ((CapabilitiesHandler)associator).getCapabilities();
+      
+      if (!currentSchemeCapabilities.supportsMaybe(currentFilter) &&
+          !currentSchemeCapabilities.supports(currentFilter)) {
+        m_StartBut.setEnabled(false);
+      }
+    }
   }",Buggy,"Fixed a bug where the enabled/disabled state of the start button was not being updated when a new data set was set on this panel.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@5384 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix for a bug where the start button's enabled/disabled state wasn't updating correctly when a new dataset was set. The diff shows that the `updateCapabilitiesFilter` method now includes logic to explicitly enable the start button and then disable it if the associator's capabilities don't support the current filter. This aligns with the commit message, indicating a bug fix related to the start button's state. The answer is **Buggy**.
"
weka,21328.json,be43f6517961a36acbbc66eeb4fe2bc861f6c154,"@@ -1,8 +1,13 @@
   public Object getValue() {
     String path = getAsText();
     if (path != null && path.length() > 0) {
       return new File(path);
     }
 
-    return new File(""."");
+    JFileChooser embeddedEditor = (JFileChooser) m_fileEditor.getCustomEditor();
+    if (embeddedEditor.getFileSelectionMode() == JFileChooser.DIRECTORIES_ONLY) {
+      return new File(""."");
+    } else {
+      return new File("""");
+    }
   }",Buggy,"Fixed a bug where '.' was returned when the field was empty in the case where the editor has been configured for files only (rather than directories)

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@13132 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to how an empty file path is handled when the file editor is configured for files only. The diff shows that the code now checks the file selection mode of the JFileChooser. If it's set to directories only, it returns a File("".""), otherwise, it returns a File(""""). This change addresses the bug where ""."" was incorrectly returned when the editor was configured for files only and the field was empty. The answer is **Buggy**."
weka,6470.json,bae2e9e85c74323389279cac539860e8b88d49d2,"@@ -1,27 +1,26 @@
             public void run() {
               PythonSession session;
-              m_executeScriptBut.setEnabled(false);
               try {
                 session = PythonSession.acquireSession(PythonPanel.this);
-                m_logPanel.statusMessage(""Executing script..."");
                 List<String> outAndErr =
                   session.executeScript(m_scriptEditor.getText(),
                     m_debug.isSelected());
                 if (outAndErr.get(0).length() > 0) {
                   logMessage(outAndErr.get(0), null);
                 }
                 if (outAndErr.get(1).length() > 0) {
                   throw new WekaException(outAndErr.get(1));
                 }
                 refreshVarList(session);
                 checkDebug(session);
                 m_logPanel.statusMessage(""OK"");
               } catch (WekaException ex) {
                 ex.printStackTrace();
                 logMessage(null, ex);
                 m_logPanel.statusMessage(""An error occurred. See log."");
               } finally {
                 PythonSession.releaseSession(PythonPanel.this);
                 m_executeScriptBut.setEnabled(true);
+                revalidate();
               }
             }",Buggy,"Fixed a refresh bug that affected the variables list

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@11810 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to refreshing the variables list. The code changes involve moving `m_executeScriptBut.setEnabled(true)` into the `finally` block to ensure the button is always re-enabled after script execution, regardless of success or failure. Additionally, `revalidate()` is called within the `finally` block. This suggests that the variables list wasn't being updated correctly in some scenarios, and these changes aim to ensure proper refreshing and UI updates. The answer is **Buggy**."
weka,31459.json,3b4b438af6d33017965dabcd7943e5de02464e2c,"@@ -1,29 +1,29 @@
   void forceDeleteAttributeAt(int position) {
 
     int index = locateIndex(position);
 
     m_NumAttributes--;
     if ((index >= 0) && (m_Indices[index] == position)) {
       int[] tempIndices = new int[m_Indices.length - 1];
       double[] tempValues = new double[m_AttValues.length - 1];
       System.arraycopy(m_Indices, 0, tempIndices, 0, index);
       System.arraycopy(m_AttValues, 0, tempValues, 0, index);
       for (int i = index; i < m_Indices.length - 1; i++) {
 	tempIndices[i] = m_Indices[i + 1] - 1;
 	tempValues[i] = m_AttValues[i + 1];
       }
       m_Indices = tempIndices;
       m_AttValues = tempValues;
     } else {
       int[] tempIndices = new int[m_Indices.length];
       double[] tempValues = new double[m_AttValues.length];
       System.arraycopy(m_Indices, 0, tempIndices, 0, index + 1);
       System.arraycopy(m_AttValues, 0, tempValues, 0, index + 1);
-      for (int i = index + 1; i < m_Indices.length - 1; i++) {
+      for (int i = index + 1; i < m_Indices.length; i++) {
 	tempIndices[i] = m_Indices[i] - 1;
 	tempValues[i] = m_AttValues[i];
       }
       m_Indices = tempIndices;
       m_AttValues = tempValues;
     }
   }",Buggy,"Fixed bug in SparseInstance in forceAttributeDelete.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@1236 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix in `SparseInstance` related to `forceAttributeDelete`. The diff modifies the `forceDeleteAttributeAt` method. The core of the change involves adjusting attribute indices after deleting an attribute at a specific position. There are two code paths, one where the attribute to be deleted is found at the expected index and another where it is not. In both cases, the code aims to decrement the indices of subsequent attributes to reflect the removal. The bug fix is likely related to an incorrect index calculation or array copy operation in the original code, which would lead to incorrect attribute values or indices after the deletion. Specifically, the loop condition `i < m_Indices.length - 1` in the `else` block has been changed to `i < m_Indices.length`. This suggests that the original code was not processing the last element in the `m_Indices` and `m_AttValues` arrays correctly when the attribute to be deleted was not found at the expected index.

The answer is **Buggy**."
weka,29293.json,64d11bc2714c068b7bdba2410a82a0d50c430b55,"@@ -1,24 +1,24 @@
   public void setOptions(String[] options) throws Exception {
     String tmpStr;
 
     tmpStr = Utils.getOption('R', options);
     if (tmpStr.length() != 0) {
       setCombinationRule(new SelectedTag(tmpStr, TAGS_RULES));
     } else {
       setCombinationRule(new SelectedTag(AVERAGE_RULE, TAGS_RULES));
     }
 
     m_classifiersToLoad.clear();
     while (true) {
       String loadString = Utils.getOption('P', options);
       if (loadString.length() == 0) {
         break;
       }
 
       m_classifiersToLoad.add(loadString);
     }
 
-    setDoNotPrintModels(Utils.getFlag(""-do-not-print"", options));
+    setDoNotPrintModels(Utils.getFlag(""do-not-print"", options));
 
     super.setOptions(options);
   }",Buggy,"Fixed an option handling bug

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@13518 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to option handling. The diff shows a change in the `setOptions` method where `Utils.getFlag(""-do-not-print"", options)` is corrected to `Utils.getFlag(""do-not-print"", options)`. The removal of the hyphen suggests that the original code was not correctly identifying the ""do-not-print"" flag, which constitutes a bug in option parsing. The answer is **Buggy**."
weka,9850.json,feaeb8e65657f876489351748361cd4cbc9a7fe5,"@@ -1,104 +1,107 @@
   protected boolean makeTree(FastVector BestFirstElements, BFTree root,
       Instances train, int[][] sortedIndices, double[][] weights,
       double[][][] dists, double[] classProbs, double totalWeight,
       double[] branchProps, int minNumObj, boolean useHeuristic, boolean useGini)
   throws Exception {
 
     if (BestFirstElements.size()==0) return false;
 
     ///////////////////////////////////////////////////////////////////////
     // All information about the node to split (first BestFirst object in
     // BestFirstElements)
     FastVector firstElement = (FastVector)BestFirstElements.elementAt(0);
 
     // node to split
     BFTree nodeToSplit = (BFTree)firstElement.elementAt(0);
 
     // split attribute
     Attribute att = (Attribute)firstElement.elementAt(1);
 
     // info of split value or split string
     double splitValue = Double.NaN;
     String splitStr = null;
     if (att.isNumeric())
       splitValue = ((Double)firstElement.elementAt(2)).doubleValue();
     else {
       splitStr=((String)firstElement.elementAt(2)).toString();
     }
 
     // the best gini gain or information gain of this node
     double gain = ((Double)firstElement.elementAt(3)).doubleValue();
     ///////////////////////////////////////////////////////////////////////
 
     // If no enough data to split for this node or this node can not be split find next node to split.
     if (totalWeight < 2*minNumObj || branchProps[0]==0
 	|| branchProps[1]==0) {
       // remove the first element
       BestFirstElements.removeElementAt(0);
       nodeToSplit.makeLeaf(train);
+      if (BestFirstElements.size() == 0) {
+        return false;
+      }
       BFTree nextNode = (BFTree)
       ((FastVector)BestFirstElements.elementAt(0)).elementAt(0);
       return root.makeTree(BestFirstElements, root, train,
 	  nextNode.m_SortedIndices, nextNode.m_Weights, nextNode.m_Dists,
 	  nextNode.m_ClassProbs, nextNode.m_TotalWeight,
 	  nextNode.m_Props, minNumObj, useHeuristic, useGini);
     }
 
     // If gini gain or information is 0, make all nodes in the BestFirstElements leaf nodes
     // because these node sorted descendingly according to gini gain or information gain.
     // (namely, gini gain or information gain of all nodes in BestFirstEelements is 0).
     if (gain==0) {
       for (int i=0; i<BestFirstElements.size(); i++) {
 	FastVector element = (FastVector)BestFirstElements.elementAt(i);
 	BFTree node = (BFTree)element.elementAt(0);
 	node.makeLeaf(train);
       }
       BestFirstElements.removeAllElements();
       return false;
     }
 
     else {
       // remove the first element
       BestFirstElements.removeElementAt(0);
       nodeToSplit.m_Attribute = att;
       if (att.isNumeric()) nodeToSplit.m_SplitValue = splitValue;
       else nodeToSplit.m_SplitString = splitStr;
 
       int[][][] subsetIndices = new int[2][train.numAttributes()][0];
       double[][][] subsetWeights = new double[2][train.numAttributes()][0];
 
       splitData(subsetIndices, subsetWeights, nodeToSplit.m_Attribute,
 	  nodeToSplit.m_SplitValue, nodeToSplit.m_SplitString,
 	  nodeToSplit.m_SortedIndices, nodeToSplit.m_Weights, train);
 
       // if split will generate node(s) which has total weights less than m_minNumObj,
       // do not split
       int attIndex = att.index();
       if (subsetIndices[0][attIndex].length<minNumObj ||
 	  subsetIndices[1][attIndex].length<minNumObj) {
 
 	nodeToSplit.makeLeaf(train);
 	BFTree nextNode = (BFTree)
 	((FastVector)BestFirstElements.elementAt(0)).elementAt(0);
 	return root.makeTree(BestFirstElements, root, train,
 	    nextNode.m_SortedIndices, nextNode.m_Weights, nextNode.m_Dists,
 	    nextNode.m_ClassProbs, nextNode.m_TotalWeight,
 	    nextNode.m_Props, minNumObj, useHeuristic, useGini);
       }
 
       // split the node
       else {
 	nodeToSplit.m_isLeaf = false;
 	nodeToSplit.m_Attribute = att;
 
 	nodeToSplit.makeSuccessors(BestFirstElements,train,subsetIndices,
 	    subsetWeights,dists, nodeToSplit.m_Attribute,useHeuristic,useGini);
 
 	for (int i=0; i<2; i++){
 	  nodeToSplit.m_Successors[i].makeLeaf(train);
 	}
 
 	return true;
       }
     }
   }",Buggy,"Fixed a bug that would occur occasionally when PREPRUNING was selected

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@6949 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to PREPRUNING. The diff shows that when `BestFirstElements.size()` is 0, the method returns false. However, after removing an element from `BestFirstElements`, there's a check to see if `BestFirstElements.size()` is 0. If it is, the code now returns false. Without this check, an `IndexOutOfBoundsException` could occur when trying to access `BestFirstElements.elementAt(0)` on an empty vector. This change addresses a potential error condition, indicating a bug fix. The answer is **Buggy**."
weka,27361.json,84effa0146bd9bb228110c26aaf7a40f959c9043,"@@ -1,3 +1,3 @@
-  public double [][] coefficents() {
+  public double [][] coefficients() {
     return m_Par;
   }",Buggy,"Fixed spelling mistake in method to get coefficients :-)


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@4337 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",NotBuggy,"The commit message indicates a spelling correction in a method name. The diff confirms this by showing the change from ""coefficents"" to ""coefficients"". This is a minor correction and doesn't involve any bug fix related to functionality. The answer is **NotBuggy**.
"
weka,13928.json,83d877a74c163b3242c845bd9ecc4d2857ef2fa1,"@@ -1,33 +1,37 @@
   public double[] distributionForInstance(Instance instance) throws Exception {
 
     int [] arrIdc = new int[m_BaseClassifiers.length+1];
     arrIdc[m_BaseClassifiers.length]=m_MetaFormat.numAttributes()-1;
     double [] classProbs = new double[m_BaseFormat.numClasses()];
     Instance newInst;
     double sum=0;
 
     for (int i = 0; i<m_MetaClassifiers.length; i++) {
       for (int j = 0; j<m_BaseClassifiers.length; j++)
           arrIdc[j]=m_BaseFormat.numClasses()*j+i;
 
-      m_attrFilter.setAttributeIndicesArray(arrIdc);
+      m_makeIndicatorFilter.setAttributeIndex("""" + (m_MetaFormat.classIndex() + 1));
+      m_makeIndicatorFilter.setNumeric(true);
       m_makeIndicatorFilter.setValueIndex(i);
-
+      m_makeIndicatorFilter.setInputFormat(m_MetaFormat);
       m_makeIndicatorFilter.input(metaInstance(instance));
       m_makeIndicatorFilter.batchFinished();
       newInst = m_makeIndicatorFilter.output();
+
+      m_attrFilter.setAttributeIndicesArray(arrIdc);
+      m_attrFilter.setInvertSelection(true);
       m_attrFilter.setInputFormat(m_makeIndicatorFilter.getOutputFormat());
       m_attrFilter.input(newInst);
       m_attrFilter.batchFinished();
       newInst = m_attrFilter.output();
 
       classProbs[i]=m_MetaClassifiers[i].classifyInstance(newInst);
       if (classProbs[i]>1) { classProbs[i]=1; }
       if (classProbs[i]<0) { classProbs[i]=0; }
       sum+= classProbs[i];
     }
 
     if (sum!=0) Utils.normalize(classProbs,sum);
 
     return classProbs;
   }",Buggy,"Fixed problem resulting from changing Range.java


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@1815 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix related to changes in `Range.java`. The diff involves modifications to the `distributionForInstance` method. Specifically, it adds and configures a `m_makeIndicatorFilter` to handle the class index and its values, and sets `m_attrFilter` to invert the selection. These changes suggest that the previous implementation had issues with how it was handling class indices or attribute selection, leading to incorrect results. The addition of checks to ensure `classProbs[i]` is within the range [0, 1] further supports the idea of a bug fix. Therefore, the answer is **Buggy**."
weka,18243.json,5e2b216f64dbf04497ebd70c8db41a5f21e4ae37,"@@ -1,83 +1,86 @@
   protected void setInstancesFromDBaseQuery() {
 
     try {
       if (m_InstanceQuery == null) {
 	m_InstanceQuery = new InstanceQuery();
       }
       String dbaseURL = m_InstanceQuery.getDatabaseURL();
       String username = m_InstanceQuery.getUsername();
       String passwd = m_InstanceQuery.getPassword();
       /*dbaseURL = (String) JOptionPane.showInputDialog(this,
 					     ""Enter the database URL"",
 					     ""Query Database"",
 					     JOptionPane.PLAIN_MESSAGE,
 					     null,
 					     null,
 					     dbaseURL);*/
      
       
       
       DatabaseConnectionDialog dbd= new DatabaseConnectionDialog(null,dbaseURL,username);
       dbd.setVisible(true);
       
       //if (dbaseURL == null) {
       if (dbd.getReturnValue()==JOptionPane.CLOSED_OPTION) {
 	m_FromLab.setText(""Cancelled"");
 	return;
       }
       dbaseURL=dbd.getURL();
       username=dbd.getUsername();
       passwd=dbd.getPassword();
       m_InstanceQuery.setDatabaseURL(dbaseURL);
       m_InstanceQuery.setUsername(username);
       m_InstanceQuery.setPassword(passwd);
       m_InstanceQuery.setDebug(dbd.getDebug());
       
       m_InstanceQuery.connectToDatabase();
       if (!m_InstanceQuery.experimentIndexExists()) {
 	System.err.println(""not found"");
 	m_FromLab.setText(""No experiment index"");
+        m_InstanceQuery.disconnectFromDatabase();
 	return;
       }
       System.err.println(""found"");
       m_FromLab.setText(""Getting experiment index"");
       Instances index = m_InstanceQuery.retrieveInstances(""SELECT * FROM ""
 				       + InstanceQuery.EXP_INDEX_TABLE);
       if (index.numInstances() == 0) {
 	m_FromLab.setText(""No experiments available"");
+        m_InstanceQuery.disconnectFromDatabase();
 	return;	
       }
       m_FromLab.setText(""Got experiment index"");
 
       DefaultListModel lm = new DefaultListModel();
       for (int i = 0; i < index.numInstances(); i++) {
 	lm.addElement(index.instance(i).toString());
       }
       JList jl = new JList(lm);
       jl.setSelectedIndex(0);
       int result;
       // display dialog only if there's not just one result!
       if (jl.getModel().getSize() != 1) {
         ListSelectorDialog jd = new ListSelectorDialog(null, jl);
         result = jd.showDialog();
       }
       else {
         result = ListSelectorDialog.APPROVE_OPTION;
       }
       if (result != ListSelectorDialog.APPROVE_OPTION) {
 	m_FromLab.setText(""Cancelled"");
+        m_InstanceQuery.disconnectFromDatabase();
 	return;
       }
       Instance selInst = index.instance(jl.getSelectedIndex());
       Attribute tableAttr = index.attribute(InstanceQuery.EXP_RESULT_COL);
       String table = InstanceQuery.EXP_RESULT_PREFIX
 	+ selInst.toString(tableAttr);
       setInstancesFromDatabaseTable(table);
       
     } catch (Exception ex) {
        // 1. print complete stacktrace
        ex.printStackTrace();
        // 2. print message in panel
        m_FromLab.setText(""Problem reading database: '"" + ex.getMessage() + ""'"");
     }
   }",Buggy,"Fixed a couple of bugs where database connection was not being closed if close/cancel was pressed. Thanks to Miryam Gomez and Raquel Porras for the bug fix


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@4243 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates that the changes fix bugs related to database connection not being closed when the user presses close/cancel. The diff adds `m_InstanceQuery.disconnectFromDatabase();` in three different locations within the `setInstancesFromDBaseQuery()` method. These locations correspond to scenarios where the user cancels the database connection setup or when no experiments are available. This ensures that the database connection is properly closed in these cases, addressing the bug described in the commit message. The answer is **Buggy**."
weka,19592.json,54c6ebfe7355bada5df3972c74469299c98fc417,"@@ -1,80 +1,82 @@
   protected void updateChart(double [] dataPoint) {
-    int iwidth = m_plotPanel.getWidth();
-    int iheight = m_plotPanel.getHeight();
+    //    int iwidth = m_plotPanel.getWidth();
+    //    int iheight = m_plotPanel.getHeight();
+    int iwidth = m_osi.getWidth(this);
+    int iheight = m_osi.getHeight(this);
 
     //    System.err.println(dataPoint[0]);
     if (dataPoint.length-1 != m_previousY.length) {
       m_previousY = new double [dataPoint.length-1];
       //      m_plotCount = 0;
       for (int i = 0; i < dataPoint.length-1; i++) {
 	m_previousY[i] = convertToPanelY(0);
       }
     }
 
     Graphics osg = m_osi.getGraphics();
     
     Graphics g = m_plotPanel.getGraphics();
 
     // paint the old scale onto the plot if a scale update has occured
     if (m_yScaleUpdate) {
       String maxVal = numToString(m_oldMax);
       String minVal = numToString(m_oldMin);
       String midVal = numToString((m_oldMax - m_oldMin) / 2.0);
       if (m_labelMetrics == null) {
 	m_labelMetrics = g.getFontMetrics(m_labelFont);
       }
       osg.setFont(m_labelFont);
       int wmx = m_labelMetrics.stringWidth(maxVal);
       int wmn = m_labelMetrics.stringWidth(minVal);
       int wmd = m_labelMetrics.stringWidth(midVal);
 
       int hf = m_labelMetrics.getAscent();
       osg.setColor(m_colorList[m_colorList.length-1]);
       osg.drawString(maxVal, iwidth-wmx, hf-2);
       osg.drawString(midVal, iwidth-wmd, (iheight / 2)+(hf / 2));
       osg.drawString(minVal, iwidth-wmn, iheight-1);
       m_yScaleUpdate = false;
       System.err.println(""Here"");
     }
 
     osg.copyArea(m_refreshWidth,0,iwidth-m_refreshWidth,
 		 iheight,-m_refreshWidth,0);
     osg.setColor(Color.black);
     osg.fillRect(iwidth-m_refreshWidth,0, iwidth, iheight);
 
     double pos;
     for (int i = 0; i < dataPoint.length-1; i++) {
       osg.setColor(m_colorList[(i % m_colorList.length)]);
       pos = convertToPanelY(dataPoint[i]);
       osg.drawLine(iwidth-m_refreshWidth, (int)m_previousY[i], 
 		   iwidth-1, (int)pos);
       m_previousY[i] = pos;
       if (dataPoint[dataPoint.length-1] % m_xValFreq == 0) {
 	// draw the actual y value onto the plot for this curve
 	String val = numToString(dataPoint[i]);
 	if (m_labelMetrics == null) {
 	  m_labelMetrics = g.getFontMetrics(m_labelFont);
 	}
 	int hf = m_labelMetrics.getAscent();
 	if (pos - hf < 0) {
 	  pos += hf;
 	}
 	int w = m_labelMetrics.stringWidth(val);
 	osg.setFont(m_labelFont);
 	osg.drawString(val, iwidth-w, (int)pos);
       }
     }
     
     // last element in the data point array contains the data point number
     if (dataPoint[dataPoint.length-1] % m_xValFreq == 0) {
 
       String xVal = """"+(int)dataPoint[dataPoint.length-1];
       osg.setColor(m_colorList[m_colorList.length-1]);
       int w = m_labelMetrics.stringWidth(xVal);
       osg.setFont(m_labelFont);
       osg.drawString(xVal, iwidth-w, iheight - 1);
     }
     g.drawImage(m_osi,0,0,m_plotPanel);
     //    System.err.println(""Finished"");
     //    m_plotCount++;
   }",Buggy,"Fixed scrolling problem under java 1.4 on Linux


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@1484 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix for a scrolling problem under Java 1.4 on Linux. The diff includes changes to how the width and height of the plot panel are obtained. Specifically, the code switches from using `m_plotPanel.getWidth()` and `m_plotPanel.getHeight()` directly to using `m_osi.getWidth(this)` and `m_osi.getHeight(this)`. This suggests that the original method of obtaining the width and height was causing issues, likely related to how the panel was being rendered or updated, leading to the scrolling problem. This change likely addresses a bug related to component sizing or rendering, especially considering the context of Java 1.4 on Linux, which might have had specific rendering quirks. The answer is **Buggy**."
weka,30975.json,32ee55c07bd6bb0d5afd27817fc05d405bd35c4f,"@@ -1,50 +1,67 @@
   public void writeIncremental(Instance inst) throws IOException {
     int writeMode = getWriteMode();
     Instances structure = getInstances();
 
     if (getRetrieval() == BATCH || getRetrieval() == NONE) {
       throw new IOException(""Batch and incremental saving cannot be mixed."");
     }
 
     if (writeMode == WAIT) {
       if (structure == null) {
         setWriteMode(CANCEL);
         if (inst != null) {
-          throw new IOException(
-            ""Structure (header Information) has to be set "" + ""in advance"");
+          throw new IOException(""Structure (header Information) has to be set ""
+            + ""in advance"");
         }
       } else {
-        m_dictionaryBuilder.reset();
-        try {
-          m_dictionaryBuilder.setup(structure);
-        } catch (Exception ex) {
-          throw new IOException(ex);
-        }
-        setWriteMode(WRITE);
+        setWriteMode(STRUCTURE_READY);
       }
       writeMode = getWriteMode();
     }
+    if (writeMode == CANCEL) {
+      cancel();
+    }
+
+    if (writeMode == STRUCTURE_READY) {
+      m_dictionaryBuilder.reset();
+      try {
+        m_dictionaryBuilder.setup(structure);
+      } catch (Exception ex) {
+        throw new IOException(ex);
+      }
+      setWriteMode(WRITE);
+      writeMode = getWriteMode();
+    }
 
     if (writeMode == WRITE) {
       if (structure == null) {
         throw new IOException(""No instances information available."");
       }
 
       if (inst != null) {
         m_dictionaryBuilder.processInstance(inst);
       } else {
+        try {
+          m_dictionaryBuilder.finalizeDictionary();
+        } catch (Exception e) {
+          throw new IOException(e);
+        }
         if (retrieveFile() == null && getWriter() == null) {
           if (getSaveBinaryDictionary()) {
             throw new IOException(
               ""Can't output binary dictionary to standard out!"");
           }
           m_dictionaryBuilder.saveDictionary(System.out);
         } else {
-          m_dictionaryBuilder.saveDictionary(getWriter());
+          if (getSaveBinaryDictionary()) {
+            m_dictionaryBuilder.saveDictionary(m_binaryStream);
+          } else {
+            m_dictionaryBuilder.saveDictionary(getWriter());
+          }
         }
 
         resetStructure();
         resetWriter();
       }
     }
   }",Buggy,"Fixed a bug in incremental mode

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@12690 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix in incremental mode. The diff introduces a new state `STRUCTURE_READY` and adjusts the logic for setting up the dictionary builder. The original code directly set the write mode to `WRITE` after setting up the dictionary, potentially causing issues if the setup failed or if the writer wasn't ready. The new state ensures that the dictionary is set up only once and that the writer is ready before proceeding. Additionally, the diff includes a fix for saving the binary dictionary to the correct stream. The original code always saved the dictionary to the writer, regardless of whether it was a binary dictionary. The updated code now correctly saves the binary dictionary to the binary stream. The answer is **Buggy**."
weka,20298.json,2de75b28460973901afd12db84cb5288c20a8918,"@@ -1,24 +1,23 @@
   private void setupRendererOptsTipText(JLabel optsLab) {
     String renderer = m_rendererCombo.getSelectedItem().toString();
     if (renderer.equalsIgnoreCase(""weka chart renderer"")) {
       // built-in renderer
       WekaOffscreenChartRenderer rcr = new WekaOffscreenChartRenderer();
       String tipText = rcr.optionsTipTextHTML();
       tipText = tipText.replace(""<html>"", ""<html>Comma separated list of options:<br>"");
       optsLab.setToolTipText(tipText);
     } else {
       try {
-        Object rendererO = PluginManager.getPluginInstance(""weka.gui.beans.OffscreenChartRender"",
+        Object rendererO = PluginManager.getPluginInstance(""weka.gui.beans.OffscreenChartRenderer"",
             renderer);
 
         if (rendererO != null) {
           String tipText = ((OffscreenChartRenderer)rendererO).optionsTipTextHTML();
           if (tipText != null && tipText.length() > 0) {
             optsLab.setToolTipText(tipText);
           }
         }
       } catch (Exception ex) {
-
       }
     }
   }",Buggy,"Fixed a bug in the routine that sets the tool tip for additional options in plugin renderers.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@7690 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to setting tooltips for additional options in plugin renderers. The code changes involve retrieving a plugin instance of `OffscreenChartRenderer` using `PluginManager.getPluginInstance`. The original code had a typo `""weka.gui.beans.OffscreenChartRender""` which would prevent the correct class from being loaded, thus causing the tooltip to not be set correctly. The corrected code `""weka.gui.beans.OffscreenChartRenderer""` fixes this issue. Therefore, the changes indicate a bug fix. The answer is **Buggy**."
weka,17871.json,e62dd28a920a99053a89d45c44b4520741764624,"@@ -1,91 +1,92 @@
   public static void main (String [] args) {
     try {
       if (args.length < 8) {
 	System.err.println(""Usage : BoundaryPanel <dataset> ""
 			   +""<class col> <xAtt> <yAtt> ""
 			   +""<base> <# loc/pixel> <kernel bandwidth> ""
 			   +""<display width> ""
 			   +""<display height> <classifier ""
 			   +""[classifier options]>"");
 	System.exit(1);
       }
       final javax.swing.JFrame jf = 
 	new javax.swing.JFrame(""Weka classification boundary visualizer"");
       jf.getContentPane().setLayout(new BorderLayout());
 
       System.err.println(""Loading instances from : ""+args[0]);
       java.io.Reader r = new java.io.BufferedReader(
 			 new java.io.FileReader(args[0]));
       final Instances i = new Instances(r);
       i.setClassIndex(Integer.parseInt(args[1]));
 
       //      bv.setClassifier(new Logistic());
       final int xatt = Integer.parseInt(args[2]);
       final int yatt = Integer.parseInt(args[3]);
       int base = Integer.parseInt(args[4]);
       int loc = Integer.parseInt(args[5]);
 
       int bandWidth = Integer.parseInt(args[6]);
       int panelWidth = Integer.parseInt(args[7]);
       int panelHeight = Integer.parseInt(args[8]);
 
       final String classifierName = args[9];
       final BoundaryPanel bv = new BoundaryPanel(panelWidth,panelHeight);
       bv.addActionListener(new ActionListener() {
 	  public void actionPerformed(ActionEvent e) {
 	    String classifierNameNew = 
 	      classifierName.substring(classifierName.lastIndexOf('.')+1, 
 				       classifierName.length());
 	    bv.saveImage(classifierNameNew+""_""+i.relationName()
 			 +""_X""+xatt+""_Y""+yatt+"".jpg"");
 	  }
 	});
 
       jf.getContentPane().add(bv, BorderLayout.CENTER);
       jf.setSize(bv.getMinimumSize());
       //      jf.setSize(200,200);
       jf.addWindowListener(new java.awt.event.WindowAdapter() {
 	  public void windowClosing(java.awt.event.WindowEvent e) {
 	    jf.dispose();
 	    System.exit(0);
 	  }
 	});
 
       jf.pack();
       jf.setVisible(true);
       //      bv.initialize();
       bv.repaint();
       
 
       String [] argsR = null;
       if (args.length > 10) {
+	System.err.println(""""+(args.length-10));
 	argsR = new String [args.length-10];
-	for (int j = 9; j < args.length; j++) {
+	for (int j = 10; j < args.length; j++) {
 	  argsR[j-10] = args[j];
 	}
       }
       Classifier c = Classifier.forName(args[9], argsR);
       KDDataGenerator dataGen = new KDDataGenerator();
       dataGen.setKernelBandwidth(bandWidth);
       bv.setDataGenerator(dataGen);
       bv.setNumSamplesPerRegion(loc);
       bv.setGeneratorSamplesBase(base);
       bv.setClassifier((DistributionClassifier)c);
       bv.setTrainingData(i);
       bv.setXAttribute(xatt);
       bv.setYAttribute(yatt);
 
       try {
 	// try and load a color map if one exists
 	FileInputStream fis = new FileInputStream(""colors.ser"");
 	ObjectInputStream ois = new ObjectInputStream(fis);
 	FastVector colors = (FastVector)ois.readObject();
 	bv.setColors(colors);	
       } catch (Exception ex) {
 	System.err.println(""No color map file"");
       }
       bv.start();
     } catch (Exception ex) {
       ex.printStackTrace();
     }
   }",Buggy,"Fixed bug in main method


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@1588 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix in the main method. The diff shows a change in the loop that populates the `argsR` array. The original loop started at index 9, which is incorrect because `args[9]` is already used for the classifier name. The corrected loop starts at index 10, which is the correct index for the classifier options. Also, a print statement was added to print the length of the `argsR` array. This change fixes an off-by-one error in the loop, which would have caused an ArrayIndexOutOfBoundsException or incorrect parsing of classifier options. The answer is **Buggy**."
weka,30018.json,044bb44ae261d7619c1f9d9616f4e996330d7043,"@@ -1,77 +1,75 @@
   protected List<String> checkForNativeLibs(Package toLoad, File packageDir) {
     List<String> jarsForClassloaderToIgnore = new ArrayList<>();
 
     if (toLoad.getPackageMetaDataElement(""NativeLibs"") != null) {
       String nativeLibs =
         toLoad.getPackageMetaDataElement(""NativeLibs"").toString();
       if (nativeLibs.length() > 0) {
         String[] jarsWithLibs = nativeLibs.split("";"");
         for (String entry : jarsWithLibs) {
           String[] jarAndEntries = entry.split("":"");
           if (jarAndEntries.length != 2) {
             System.err
               .println(""Was expecting two entries for native lib spec - ""
                 + ""jar:comma-separated lib paths"");
             continue;
           }
           String jarPath = jarAndEntries[0].trim();
           String[] libPathsInJar = jarAndEntries[1].split("","");
           List<String> libsToInstall = new ArrayList<>();
-          List<String> libsToAddToPath = new ArrayList<>();
           // look at named libs and check if they are already in
           // $WEKA_HOME/native-libs - don't extract a second time, but DO
           // add entries to java.library.path
           for (String lib : libPathsInJar) {
             String libName = lib.trim().replace(""\\"", ""/"");
             if (!nativeLibInstalled(libName.substring(
               libName.lastIndexOf(""/"") + 1, libName.length()))) {
-              libsToInstall.add(libName);
+              libsToInstall.add(libName.substring(
+                libName.lastIndexOf(""/"") + 1, libName.length()));
             }
-            libsToAddToPath.add(libName.substring(libName.lastIndexOf(""/"") + 1,
-              libName.length()));
           }
 
           if (libsToInstall.size() > 0) {
             try {
               installNativeLibs(packageDir, jarPath, libsToInstall);
             } catch (IOException e) {
               e.printStackTrace();
             }
           }
           /*
            * if (libsToAddToPath.size() > 0) {
            * addNativeLibsToLibsProp(libsToAddToPath); }
            */
         }
       }
     }
 
     // now check to see if there is a native loader to inject into the
     // root class loader
     if (toLoad.getPackageMetaDataElement(""InjectLoader"") != null) {
       String injectDetails =
         toLoad.getPackageMetaDataElement(""InjectLoader"").toString();
       String[] entries = injectDetails.split("";"");
 
       for (String entry : entries) {
         String jarPath = entry.trim();
         boolean rootClassLoader = false;
         if (jarPath.startsWith(""root|"")) {
           jarPath = jarPath.replace(""root|"", """");
           rootClassLoader = true;
         }
         String ignoreJar = jarPath.replace(""\\"", ""/"");
         ignoreJar = ignoreJar.substring(ignoreJar.lastIndexOf(""/"") + 1);
 
         jarsForClassloaderToIgnore.add(ignoreJar);
         try {
           WekaPackageClassLoaderManager.injectAllClassesInJar(new File(
             packageDir.toString() + File.separator + jarPath.trim()));
         } catch (Exception e) {
           e.printStackTrace();
         }
       }
     }
 
     return jarsForClassloaderToIgnore;
   }",Buggy,"Fixed a bug in the native lib installation routine

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@13561 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix in the native library installation routine. The code changes involve modifications to how native libraries are handled during package loading. Specifically, the code now extracts the library name directly when adding it to the list of libraries to install, instead of extracting the full path. This suggests a potential issue where the full path was being incorrectly used, leading to installation failures or other problems. The removal of `libsToAddToPath` also suggests a simplification or correction in how the library path is managed. Therefore, the changes indicate a bug fix. The answer is **Buggy**."
weka,3629.json,34ee50c28e6d5f6dc2ea9bff29e6d09545fb601e,"@@ -1,38 +1,38 @@
   public void start() throws WekaException {
     if (getStepManager().numIncomingConnections() == 0) {
       getStepManager().processing();
       ClassLoader orig = Thread.currentThread().getContextClassLoader();
       try {
         String jobName = ""WekaKF:"" + m_runningJob.getJobName();
         Thread.currentThread()
           .setContextClassLoader(this.getClass().getClassLoader());
 
         List<StepManager> outConns =
           getStepManager().getOutgoingConnections().get(""success"");
         if (outConns != null) {
           for (StepManager manager : outConns) {
             jobName += ""+"" + ((AbstractSparkJob) ((StepManagerImpl) manager)
               .getManagedStep()).getUnderlyingJob().getJobName();
           }
         }
         m_runningJob.setJobName(jobName);
         getStepManager()
           .logBasic(""Starting Spark job as start point: "" + jobName);
 
         // we are a start point. Assumption is that we're the *only* start point
         // as things will break down if there are more than one.
         try {
           m_sparkLogAppender = m_runningJob.initJob(null);
         } catch (Exception ex) {
-          ex.printStackTrace();
+          m_runningJob = null;
           throw new WekaException(ex);
         }
         m_currentContext = m_runningJob.getSparkContext();
         m_cachingStrategy = m_runningJob.getCachingStrategy();
         m_outputDirectory = m_runningJob.getSparkJobConfig().getOutputDir();
       } finally {
         Thread.currentThread().setContextClassLoader(orig);
       }
       runJob();
     }
   }",Buggy,"Fixed a bug that prevented spark step options from being altered after a job had been run and suffered a failure

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@12960 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to altering spark step options after a job failure. The diff shows a change in the `start()` method where, in the `catch` block for `m_runningJob.initJob(null)`, `m_runningJob` is set to `null` before throwing a `WekaException`. This likely addresses a scenario where a failed job initialization left `m_runningJob` in an inconsistent state, preventing subsequent modifications to step options. By setting `m_runningJob` to `null`, the code ensures a clean state, allowing for alterations. The answer is **Buggy**.
"
weka,29409.json,c06f15670aa5dc47487a677160a087b8ab2d869a,"@@ -1,3 +1,3 @@
   public String getRevision() {
-    return RevisionUtils.extract(""$Revision: 1.1 $"");
+    return RevisionUtils.extract(""$Revision$"");
   }",Buggy,"Fixed a bug introduced by the improvements in derived fields where field definitions for inputs were not being set correctly.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@5028 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to field definitions for inputs. The provided diff modifies the `getRevision()` method to extract the revision information. While seemingly unrelated, the revision information might be used internally for debugging or version control purposes related to the bug fix. However, without more context, it's hard to directly correlate this change with the bug fix described in the commit message. It's possible that this change is part of a larger set of changes that collectively address the bug. Given the limited information, it's difficult to definitively say if this specific diff is directly related to fixing the bug. However, the commit message explicitly states a bug fix.

The answer is **Buggy**."
weka,25760.json,b63980e747df9f6abe802dfdf83f00c171dec0ea,"@@ -1,17 +1,25 @@
     protected double[][] getCoefficients(){
 	double[][] coefficients = new double[m_numClasses][m_numericDataHeader.numAttributes() + 1];
 	for (int j = 0; j < m_numClasses; j++) {
 	    //go through simple regression functions and add their coefficient to the coefficient of
 	    //the attribute they are built on.
 	    for (int i = 0; i < m_numRegressions; i++) {
 		
 		double slope = m_regressions[j][i].getSlope();
 		double intercept = m_regressions[j][i].getIntercept();
 		int attribute = m_regressions[j][i].getAttributeIndex();
 		
 		coefficients[j][0] += intercept;
 		coefficients[j][attribute + 1] += slope;
 	    }
 	}
+        
+        // Need to multiply all coefficients by (J-1) / J
+        for (int j = 0; j < coefficients.length; j++) {
+          for (int i = 0; i < coefficients[0].length; i++) {
+            coefficients[j][i] *= (double)(m_numClasses - 1) / (double)m_numClasses;
+          }
+        }
+
 	return coefficients;
     }",Buggy,"Fixed bug in output of coefficients and intercept: they needed to be multiplied by (J-1)/J, where J is the number of classes. This affected LMT and SimpleLogistic.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@3930 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message clearly states that a bug was fixed in the output of coefficients and intercept. The diff shows that the code now multiplies all coefficients by `(J-1)/J`, where `J` is the number of classes. This aligns perfectly with the commit message, indicating that the previous output was incorrect due to the missing multiplication factor. The added code corrects this error. The answer is **Buggy**.
"
weka,10869.json,d7b954bf6ad31fcb8c1e3fd0ed630c511674efa0,"@@ -1,103 +1,105 @@
     protected Instances determineOutputFormat(Instances inputFormat) throws Exception {
 
         // Sample subset of instances
         Filter filter = Filter.makeCopy(getFilter());
         filter.setInputFormat(inputFormat);
         m_Sample = Filter.useFilter(inputFormat, filter);
 
         // Compute kernel-based matrices for subset
         m_Kernel = Kernel.makeCopy(m_Kernel);
         m_Kernel.buildKernel(m_Sample);
         int m = m_Sample.numInstances();
         int n = inputFormat.numInstances();
         Matrix khatM = new UpperSymmDenseMatrix(m);
         for (int i = 0; i < m; i++) {
             for (int j = i; j < m; j++) {
                 khatM.set(i, j, m_Kernel.eval(i, j, m_Sample.instance(i)));
             }
         }
         m_Kernel.clean();
 
         if (m_Debug) {
             Matrix kbM = new DenseMatrix(n, m);
             for (int i = 0; i < n; i++) {
                 for (int j = 0; j < m; j++) {
                     kbM.set(i, j, m_Kernel.eval(-1, j, inputFormat.instance(i)));
                 }
             }
 
             // Calculate SVD of kernel matrix
             SVD svd = SVD.factorize(khatM);
 
             double[] singularValues = svd.getS();
             Matrix sigmaI = new UpperSymmDenseMatrix(m);
             for (int i = 0; i < singularValues.length; i++) {
                 if (singularValues[i] > SMALL) {
                     sigmaI.set(i, i, 1.0 / singularValues[i]);
                 }
             }
 
             System.err.println(""U :\n"" + svd.getU());
             System.err.println(""Vt :\n"" + svd.getVt());
             System.err.println(""Reciprocal of singular values :\n"" + sigmaI);
 
             Matrix pseudoInverse = svd.getU().mult(sigmaI, new DenseMatrix(m,m)).mult(svd.getVt(), new DenseMatrix(m,m));
 
             // Compute reduced-rank version
             Matrix khatr = kbM.mult(pseudoInverse, new DenseMatrix(n, m)).mult(kbM.transpose(new DenseMatrix(m, n)), new DenseMatrix(n,n));
 
             System.err.println(""Reduced rank matrix: \n"" + khatr);
         }
 
         // Compute weighting matrix
         if (getUseSVD()) {
             SVD svd = SVD.factorize(khatM);
             double[] e = svd.getS();
             Matrix dhatr = new UpperSymmDenseMatrix(e.length);
             for (int i = 0; i < e.length; i++) {
                 if (Math.sqrt(e[i]) > SMALL) {
                     dhatr.set(i, i, 1.0 / Math.sqrt(e[i]));
                 }
             }
             if (m_Debug) {
                 System.err.println(""U matrix :\n"" + svd.getU());
                 System.err.println(""Vt matrix :\n"" + svd.getVt());
                 System.err.println(""Singluar values \n"" + Utils.arrayToString(svd.getS()));
                 System.err.println(""Reciprocal of square root of singular values :\n"" + dhatr);
             }
             m_WeightingMatrix = dhatr.mult(svd.getVt(), new DenseMatrix(m,m));
         } else {
 
             SymmDenseEVD evd = SymmDenseEVD.factorize(khatM);
             double[] e = evd.getEigenvalues();
             Matrix dhatr = new UpperSymmDenseMatrix(e.length);
             for (int i = 0; i < e.length; i++) {
                 if (Math.sqrt(e[i]) > SMALL) {
                     dhatr.set(i, i, 1.0 / Math.sqrt(e[i]));
                 }
             }
             if (m_Debug) {
                 System.err.println(""Eigenvector matrix :\n"" + evd.getEigenvectors());
                 System.err.println(""Eigenvalues \n"" + Utils.arrayToString(evd.getEigenvalues()));
                 System.err.println(""Reciprocal of square root of eigenvalues :\n"" + dhatr);
             }
             m_WeightingMatrix = dhatr.mult(evd.getEigenvectors().transpose(), new DenseMatrix(m,m));
         }
 
         if (m_Debug) {
             System.err.println(""Weighting matrix: \n"" + m_WeightingMatrix);
         }
 
         // Construct header for output format
         boolean hasClass = (inputFormat.classIndex() >= 0);
         ArrayList<Attribute> atts = new ArrayList<Attribute>(m + ((hasClass) ? 1 : 0));
         for (int i = 0; i < m; i++) {
             atts.add(new Attribute(""z"" + (i + 1)));
         }
         if (hasClass) {
             atts.add((Attribute) inputFormat.classAttribute().copy());
         }
         Instances d = new Instances(inputFormat.relationName(), atts, 0);
-        d.setClassIndex(d.numAttributes() - 1);
+        if (hasClass) {
+          d.setClassIndex(d.numAttributes() - 1);
+        }
         return d;
     }",Buggy,"Bug fix: class index in output was set to last attribute even if input data did not have class.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@14365 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to the class index in the output format. The diff modifies the `determineOutputFormat` method. Specifically, the code that sets the class index is now conditionally executed based on whether the input data has a class attribute (`hasClass`). The original code unconditionally set the class index, which would lead to an incorrect class index if the input data did not have a class. The corrected code only sets the class index if `hasClass` is true, resolving the bug. The answer is **Buggy**."
weka,10869.json,ff769b98dbd0a017a91ca677f90dc3750b239511,"@@ -1,67 +1,87 @@
     protected Instances determineOutputFormat(Instances inputFormat) throws Exception {
 
         // Sample subset of instances
         Filter filter = Filter.makeCopy(getFilter());
         filter.setInputFormat(inputFormat);
         m_Sample = Filter.useFilter(inputFormat, filter);
 
         // Compute kernel-based matrices for subset
         m_Kernel = Kernel.makeCopy(m_Kernel);
         m_Kernel.buildKernel(m_Sample);
         int m = m_Sample.numInstances();
         int n = inputFormat.numInstances();
         double[][] khat = new double[m][m];
         for (int i = 0; i < m; i++) {
             for (int j = i; j < m; j++) {
                 khat[i][j] = m_Kernel.eval(i, j, m_Sample.instance(i));
                 khat[j][i] = khat[i][j];
             }
         }
         Matrix khatM = new Matrix(khat);
-        /*double[][] kb = new double[m][n];
-        for (int i = 0; i < m; i++) {
-            for (int j = i; j < n; j++) {
-                kb[i][j] = m_Kernel.eval(-1, i, inputFormat.instance(i));
+
+        if (m_Debug) {
+            double[][] kb = new double[n][m];
+            for (int i = 0; i < n; i++) {
+                for (int j = 0; j < m; j++) {
+                    kb[i][j] = m_Kernel.eval(-1, j, inputFormat.instance(i));
+                }
             }
-        }
-        Matrix kbM = new Matrix(kb).transpose();*/
+            Matrix kbM = new Matrix(kb);
 
-        // Calculate SVD of kernel matrix
-        SingularValueDecomposition svd = new SingularValueDecomposition(new Matrix(khat));
+            // Calculate SVD of kernel matrix
+            SingularValueDecomposition svd = new SingularValueDecomposition(new Matrix(khat));
 
-        double[] singularValues = svd.getSingularValues();
-        Matrix sigmaI = new Matrix(m,m);
-        for (int i = 0; i < singularValues.length; i++) {
-            sigmaI.set(i, i, 1.0 / singularValues[i]);
+            double[] singularValues = svd.getSingularValues();
+            Matrix sigmaI = new Matrix(m, m);
+            for (int i = 0; i < singularValues.length; i++) {
+                if (singularValues[i] > 1e-6) {
+                    sigmaI.set(i, i, 1.0 / singularValues[i]);
+                }
+            }
+
+            System.out.println(""U :\n"" + svd.getU());
+            System.out.println(""V :\n"" + svd.getV());
+            System.out.println(""Reciprocal of singular values :\n"" + sigmaI);
+
+            Matrix pseudoInverse = svd.getV().times(sigmaI).times(svd.getU().transpose());
+
+            // Compute reduced-rank version
+            Matrix khatr = kbM.times(pseudoInverse).times(kbM.transpose());
+
+            System.out.println(""Reduced rank matrix: \n"" + khatr);
         }
 
-        m_WeightingMatrix = sigmaI.times(svd.getV().transpose());
-
-
-        /* Matrix pseudoInverse = svd.getV().transpose().times(sigmaI).times(svd.getU().transpose());
-
-        // Compute reduced-rank version
-        Matrix khatr = kbM.times(pseudoInverse).times(kbM.transpose());
-
-        // Get eigenvalues and eigenvectors of reduced-rank matrix
-        EigenvalueDecomposition evd = new EigenvalueDecomposition(khatr);
+        // Get eigenvalues and eigenvectors
+        EigenvalueDecomposition evd = new EigenvalueDecomposition(khatM);
         double[] e = evd.getRealEigenvalues();
         Matrix dhatr = new Matrix(e.length, e.length);
         for (int i  = 0; i < e.length; i++) {
-            dhatr.set(i, i, 1./Math.sqrt(e[i]));
+            dhatr.set(i, i, 1.0/Math.sqrt(e[i]));
         }
-        m_WeightingMatrix = dhatr.times(evd.getV()); */
+        if (m_Debug) {
+            System.out.println(""Eigenvector matrix :\n"" + evd.getV());
+            System.out.println(""Eigenvalue matrix \n"" + evd.getD());
+            System.out.println(""Reciprocal of square root of eigenvalues :\n"" + dhatr);
+        }
+
+        //System.out.println(""Reconstructed matrix: \n"" + evd.getV().times(evd.getD()).times(evd.getV().inverse())
+
+        m_WeightingMatrix = dhatr.times(evd.getV().transpose());
+
+        if (m_Debug) {
+            System.out.println(""Weighting matrix: \n"" + m_WeightingMatrix);
+        }
 
         // Construct header for output format
         boolean hasClass = (inputFormat.classIndex() >= 0);
         ArrayList<Attribute> atts = new ArrayList<Attribute>(m + ((hasClass) ? 1 : 0));
         for (int i = 0; i < m; i++) {
             atts.add(new Attribute(""z"" + (i + 1)));
         }
         if (hasClass) {
             atts.add((Attribute) inputFormat.classAttribute().copy());
         }
         Instances d = new Instances("""", atts, 0);
         d.setClassIndex(d.numAttributes() - 1);
         return d;
     }",Buggy,"Several bug fixes.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@12516 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message ""Several bug fixes"" indicates that the changes in the diff are likely related to fixing bugs.

The diff contains several changes, including:

1.  Commenting out a block of code related to kernel-based matrices.
2.  Adding a debug mode (`m_Debug`) that prints intermediate matrices and values, which is helpful for debugging.
3.  Adding a check for singular values to avoid division by zero.
4.  Modifying the calculation of the weighting matrix.

The addition of debug print statements, the check for singular values, and the modification to the weighting matrix calculation all suggest that the original code had issues that were being addressed. The check for singular values explicitly fixes a potential division by zero error, which is a bug.

Therefore, the answer is **Buggy**."
weka,18657.json,c9a6740cbebd93b5978b7e993e314995eb4db94f,"@@ -1,17 +1,27 @@
   public void insertInstance(int index, boolean notify) {
     if (!m_IgnoreChanges) {
       addUndoPoint();
     }
     double[] vals = new double[m_Data.numAttributes()];
+
+    // set any string or relational attribute values to missing
+    // in the new instance, just in case this is the very first
+    // instance in the dataset.
+    for (int i = 0; i < m_Data.numAttributes(); i++) {
+      if (m_Data.attribute(i).isString()
+        || m_Data.attribute(i).isRelationValued()) {
+        vals[i] = Utils.missingValue();
+      }
+    }
     Instance toAdd = new DenseInstance(1.0, vals);
     if (index < 0) {
       m_Data.add(toAdd);
     } else {
       m_Data.add(index, toAdd);
     }
     if (notify) {
       notifyListener(new TableModelEvent(this, m_Data.numInstances() - 1,
         m_Data.numInstances() - 1, TableModelEvent.ALL_COLUMNS,
         TableModelEvent.INSERT));
     }
   }",Buggy,"Fixed a bug in the insertion of new instances. Now sets the value of relational and string attributes in the new instance to missing, just in case the new instance is the very first one in the dataset

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@12708 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to the insertion of new instances, specifically when the new instance is the first one in the dataset. The diff shows that the code now initializes string and relational attributes of the new instance to a missing value. This prevents potential issues that could arise if these attributes are not properly initialized, especially when the instance is the first one added to the dataset. Therefore, the changes clearly indicate a bug fix.
The answer is **Buggy**."
weka,33458.json,b7e34d731ed42ab40ad353a159d93fb4247a9bc9,"@@ -1,24 +1,26 @@
   public String substitute(String source) throws Exception {
     // Grab each variable out of the string
     int index = source.indexOf(""${"");
 
     while (index >= 0) {
       index += 2;
       int endIndex = source.indexOf('}');
       if (endIndex >= 0 && endIndex > index +1) {
         String key = source.substring(index, endIndex);
 
         // look this sucker up
         String replace = m_envVars.get(key);
         if (replace != null) {
           String toReplace = ""${"" + key + ""}"";
           source = source.replace(toReplace, replace);
         } else {
           throw new Exception(""[Environment] Variable "" 
                               + key + "" doesn't seem to be set."");
         }
+      } else {
+        break;
       }
       index = source.indexOf(""${"");
     }
     return source;
   }",Buggy,"Fixed a bug that caused an infinite loop in substitute(). Now uses a TreeMap instead of a HashMap so that keys are kept in sorted order.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@5368 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to an infinite loop in the `substitute()` method. The change adds a `break` statement within the `while` loop when `endIndex` is not valid, preventing the loop from continuing indefinitely. This addresses the infinite loop issue described in the commit message. The answer is **Buggy**.
"
weka,24845.json,ceb6ad5f19eef0a321d4c4ad851a2bba76e1ebb0,"@@ -1,5 +1,4 @@
   public String globalInfo() {
     return ""Hierarchical clustering class.\n""
-      + ""Implements a number of classic agglomorative (i.e. bottom up) hierarchical clustering methods""
-      + ""based on ."";
+      + ""Implements a number of classic agglomerative (i.e., bottom up) hierarchical clustering methods."";
   }",Buggy,"Fixed bug in globalInfo() method.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@13174 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix in the `globalInfo()` method. The diff shows a change in the string returned by this method, specifically correcting ""agglomorative"" to ""agglomerative"" and removing ""based on ."". This correction addresses a typo and improves the clarity of the description, which can be considered a bug fix. The answer is **Buggy**."
weka,15454.json,be743132c9f71754ed911dce8ed0a797b9314166,"@@ -1,18 +1,18 @@
   public Enumeration<Option> listOptions() {
 
     Vector<Option> newVector = new Vector<Option>(1);
 
     newVector
       .addElement(new Option(
-        ""\tNew field specification (name@type@value).\n""
+        ""\tNew field specification (name@type:value).\n""
           + ""\t Environment variables may be used for any/all parts of the\n""
           + ""\tspecification. Type can be one of (numeric, nominal, string or date).\n""
           + ""\tThe value for date be a specific date string or the special string\n""
           + ""\t\""now\"" to indicate the current date-time. A specific date format\n""
           + ""\tstring for parsing specific date values can be specified by suffixing\n""
           + ""\tthe type specification - e.g. \""myTime@date:MM-dd-yyyy@08-23-2009\"".""
           + ""This option may be specified multiple times"", ""A"", 1,
         ""-A <name@type@value>""));
 
     return newVector.elements();
   }",Buggy,"Fixed a bug in the help info for the -A option

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@13337 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix to the help information for the `-A` option. The diff modifies the text within the `addElement` call, specifically changing `name@type@value` to `name@type:value`. This change directly addresses the help information, correcting the example format. Therefore, the changes indicate a bug fix. The answer is **Buggy**."
weka,15454.json,611c95b4ddf959002e0360d9c109a532143b3d2c,"@@ -1,18 +1,18 @@
   public Enumeration<Option> listOptions() {
 
     Vector<Option> newVector = new Vector<Option>(1);
 
     newVector
       .addElement(new Option(
         ""\tNew field specification (name@type@value).\n""
           + ""\t Environment variables may be used for any/all parts of the\n""
           + ""\tspecification. Type can be one of (numeric, nominal, string or date).\n""
           + ""\tThe value for date be a specific date string or the special string\n""
           + ""\t\""now\"" to indicate the current date-time. A specific date format\n""
           + ""\tstring for parsing specific date values can be specified by suffixing\n""
           + ""\tthe type specification - e.g. \""myTime@date:MM-dd-yyyy@08-23-2009\"".""
           + ""This option may be specified multiple times"", ""A"", 1,
-        ""-A <name:type:value>""));
+        ""-A <name@type@value>""));
 
     return newVector.elements();
   }",Buggy,"Fixed an error in the listOptions output

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@12731 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix in the `listOptions` output. The diff shows a change in the string representation of the option, specifically changing the separator from "":"" to ""@"" in the help text. This correction in the output suggests a bug related to how the options were displayed or interpreted. The answer is **Buggy**."
weka,26007.json,6fcd46acab99a453150b3418dc7da518be7033d2,"@@ -1,43 +1,43 @@
   private String ruleToString() {
     StringBuffer text = new StringBuffer();
 
     if (m_splitAtts.length > 0) {
       text.append(""IF\n"");
 
       for (int i = m_splitAtts.length - 1; i >= 0; i--) {
 	text.append(""\t"" + m_covered.attribute(m_splitAtts[i]).name() + "" "");
 
 	if (m_relOps[i] == 0) {
 	  text.append(""<= "");
 	} else {
 	  text.append(""> "");
 	} 
 
 	text.append(Utils.doubleToString(m_splitVals[i], 1, 3) + ""\n"");
       } 
 
       text.append(""THEN\n"");
     } 
 
     if (m_ruleModel != null) {
       try {
 	text.append(m_ruleModel.printNodeLinearModel());
 	text.append("" ["" + m_numCovered/*m_covered.numInstances()*/);
 
 	if (m_globalAbsDev > 0.0) {
 	  text.append(""/""+Utils.doubleToString((100 * 
 						   m_ruleModel.
 						   rootMeanSquaredError() / 
-						   m_globalAbsDev), 1, 3) 
+						   m_globalStdDev), 1, 3) 
 		      + ""%]\n\n"");
 	} else {
 	  text.append(""]\n\n"");
 	} 
       } catch (Exception e) {
 	return ""Can't print rule"";
       } 
     } 
     
     //    System.out.println(m_instances);
     return text.toString();
   }",Buggy,"Fixed bug in rule output. Second number in the brackets at a leaf now reports the rms error as a percentage of the global standard deviation.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@3106 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message states that the second number in the brackets at a leaf now reports the RMS error as a percentage of the global standard deviation. The diff shows that `m_globalAbsDev` is changed to `m_globalStdDev` in the calculation of the percentage. This aligns with the commit message, indicating a bug fix where the wrong variable was used for the calculation. The answer is **Buggy**."
weka,14982.json,e15ee656dfd1f2636611269ded7d3ce754f975c0,"@@ -1,80 +1,90 @@
   public int[] search (ASEvaluation ASEval, Instances data)
     throws Exception {
     int i, j;
 
     if (!(ASEval instanceof AttributeEvaluator)) {
       throw  new Exception(ASEval.getClass().getName() 
 			   + "" is not a"" 
 			   + ""Attribute evaluator!"");
     }
-    
-    if (ASEval instanceof AttributeTransformer) {
-      data = ((AttributeTransformer)ASEval).transformedHeader();
-    }
 
     m_numAttribs = data.numAttributes();
 
     if (ASEval instanceof UnsupervisedAttributeEvaluator) {
       m_hasClass = false;
     }
     else {
-      m_hasClass = true;
       m_classIndex = data.classIndex();
+      if (m_classIndex >= 0) {	
+	m_hasClass = true;
+      } else {
+	m_hasClass = false;
+      }
+    }
+
+    // get the transformed data and check to see if the transformer
+    // preserves a class index
+    if (ASEval instanceof AttributeTransformer) {
+      data = ((AttributeTransformer)ASEval).transformedHeader();
+      if (m_classIndex >= 0 && data.classIndex() >= 0) {
+	m_classIndex = data.classIndex();
+	m_hasClass = true;
+      }
     }
 
 
     m_startRange.setUpper(m_numAttribs - 1);
     if (!(getStartSet().equals(""""))) {
       m_starting = m_startRange.getSelection();
     }
     
     int sl=0;
     if (m_starting != null) {
       sl = m_starting.length;
     }
     if ((m_starting != null) && (m_hasClass == true)) {
       // see if the supplied list contains the class index
       boolean ok = false;
       for (i = 0; i < sl; i++) {
 	if (m_starting[i] == m_classIndex) {
 	  ok = true;
 	  break;
 	}
       }
       
       if (ok == false) {
 	sl++;
       }
     }
     else {
       if (m_hasClass == true) {
 	sl++;
       }
     }
 
 
     m_attributeList = new int[m_numAttribs - sl];
     m_attributeMerit = new double[m_numAttribs - sl];
 
     // add in those attributes not in the starting (omit list)
     for (i = 0, j = 0; i < m_numAttribs; i++) {
       if (!inStarting(i)) {
 	m_attributeList[j++] = i;
       }
     }
 
     AttributeEvaluator ASEvaluator = (AttributeEvaluator)ASEval;
 
     for (i = 0; i < m_attributeList.length; i++) {
       m_attributeMerit[i] = ASEvaluator.evaluateAttribute(m_attributeList[i]);
     }
 
     double[][] tempRanked = rankedAttributes();
     int[] rankedAttributes = new int[m_attributeList.length];
 
     for (i = 0; i < m_attributeList.length; i++) {
       rankedAttributes[i] = (int)tempRanked[i][0];
     }
 
     return  rankedAttributes;
   }",Buggy,"Fixed a bug with handling class index when used with attribute transformers


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@2023 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix related to handling the class index, specifically when attribute transformers are involved. The code diff shows several changes that support this:

1.  **Class Index Handling**: The code now explicitly checks if `m_classIndex` is valid (greater than or equal to 0) before setting `m_hasClass` to true. This prevents potential issues if the class index is not properly set.
2.  **Attribute Transformer Integration**: The code now retrieves the transformed header from the `AttributeTransformer` and checks if the transformed data still has a valid class index. If both the original and transformed data have valid class indices, the `m_classIndex` is updated to reflect the transformed data's class index, and `m_hasClass` is set to true.
3.  **Starting List Adjustment**: The code adjusts the size of the starting list (`sl`) based on whether the class index is present and valid. This ensures that the class attribute is correctly handled during the search process.

These changes suggest that the original code had issues with correctly identifying and handling the class index when attribute transformers were used, potentially leading to incorrect attribute evaluation or search results. The updated code addresses these issues by explicitly checking and updating the class index based on the transformed data, ensuring that the class attribute is correctly handled throughout the process.

Therefore, the changes indicate a bug fix. The answer is **Buggy**."
weka,8085.json,908420a70784469c92b61a26142bb14cd748d5d5,"@@ -1,57 +1,59 @@
   public void finalizeTask() throws Exception {
     if (m_classifier == null) {
       throw new Exception(""No classifier has been set"");
     }
 
     if (m_classifier instanceof UpdateableClassifier
       && !m_batchTrainedIncremental) {
       // nothing to do except possibly down-sample predictions for
       // auc/prc
       if (m_predFrac > 0) {
         ((AggregateableEvaluationWithPriors) m_eval).prunePredictions(
           m_predFrac, m_seed);
       }
 
       return;
     }
 
     m_trainingHeader.compactify();
 
     Instances test = m_trainingHeader;
     Random r = new Random(m_seed);
     test.randomize(r);
     if (test.classAttribute().isNominal() && m_totalFolds > 1) {
       test.stratify(m_totalFolds);
     }
 
     if (m_totalFolds > 1 && m_foldNumber >= 1) {
       test = test.testCV(m_totalFolds, m_foldNumber - 1);
     }
 
     m_numTestInstances = test.numInstances();
 
-    if (m_classifier instanceof BatchPredictor) {
+    if (m_classifier instanceof BatchPredictor
+      && ((BatchPredictor) m_classifier)
+        .implementsMoreEfficientBatchPrediction()) {
 
       // this method always stores the predictions for AUC, so we need to get
-      // rid of them if we're note doing any AUC computation
+      // rid of them if we're not doing any AUC computation
       m_eval.evaluateModel(m_classifier, test);
-      if (m_predFrac < 0) {
+      if (m_predFrac <= 0) {
         ((AggregateableEvaluationWithPriors) m_eval).deleteStoredPredictions();
       }
     } else {
       for (int i = 0; i < test.numInstances(); i++) {
         if (m_predFrac > 0) {
           m_eval.evaluateModelOnceAndRecordPrediction(m_classifier,
             test.instance(i));
         } else {
           m_eval.evaluateModelOnce(m_classifier, test.instance(i));
         }
       }
     }
 
     // down-sample predictions for auc/prc
     if (m_predFrac > 0) {
       ((AggregateableEvaluationWithPriors) m_eval).prunePredictions(m_predFrac,
         m_seed);
     }
   }",Buggy,"Fixed a bug (due to changes in BatchPredictor) that caused predictions for AUC, AUPRC to always be retained regardless of the user-specified fraction

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@12342 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to prediction retention for AUC and AUPRC, which was occurring regardless of the user-specified fraction. The code diff shows that the condition `m_predFrac < 0` was changed to `m_predFrac <= 0`. This change ensures that predictions are deleted when `m_predFrac` is zero, addressing the bug where predictions were always retained. The addition of `((BatchPredictor) m_classifier).implementsMoreEfficientBatchPrediction()` also suggests an optimization or bug fix related to batch prediction. The answer is **Buggy**."
weka,26472.json,aab16fcf5619d64b56a044bb7acaa276a2f2857b,"@@ -1,12 +1,15 @@
   public double matthewsCorrelationCoefficient(int classIndex) {
     double numTP = numTruePositives(classIndex);
     double numTN = numTrueNegatives(classIndex);
     double numFP = numFalsePositives(classIndex);
     double numFN = numFalseNegatives(classIndex);
     double n = (numTP * numTN) - (numFP * numFN);
     double d = (numTP + numFP) * (numTP + numFN) * (numTN + numFP)
         * (numTN + numFN);
     d = Math.sqrt(d);
+    if (d == 0) {
+      d = 1;
+    }
 
     return n / d;
   }",Buggy,"Fixed a bug in the MCC calculation - MCC should be zero if the denominator of the calculation is zero.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@9101 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix in the Matthews Correlation Coefficient (MCC) calculation. The fix addresses a scenario where the denominator of the MCC calculation is zero, which would lead to an undefined result (division by zero). The code change introduces a check for `d == 0` and sets `d = 1` if the denominator is zero, preventing the division by zero error. This clearly indicates a bug fix. The answer is **Buggy**."
weka,32917.json,db5c23a0208b68d40384c60c377e2e204c0b098e,"@@ -1,17 +1,17 @@
   public void setOptions(String[] options) throws Exception {
-    String nnSearchClass = Utils.getOption('D', options);
+    String nnSearchClass = Utils.getOption('A', options);
     if(nnSearchClass.length() != 0) {
       String nnSearchClassSpec[] = Utils.splitOptions(nnSearchClass);
       if(nnSearchClassSpec.length == 0) { 
         throw new Exception(""Invalid DistanceFunction specification string.""); 
       }
       String className = nnSearchClassSpec[0];
       nnSearchClassSpec[0] = """";
 
       setDistanceFunction( (DistanceFunction)
-                            Utils.forName( NearestNeighbourSearch.class, 
+                            Utils.forName( DistanceFunction.class, 
                                            className, nnSearchClassSpec) );
     }
     else 
       this.setDistanceFunction(new EuclideanDistance());  
   }",Buggy,"New Reorder filter. Improvement of docs for Copy filter. Improvements to ArffViewer. Bug fix in NearestNeighbourSearch option setting. All from Peter.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@2325 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message mentions a ""Bug fix in NearestNeighbourSearch option setting"". The diff shows that the option flag used to retrieve the nearest neighbor search class was changed from 'D' to 'A'. This indicates that the original option flag was incorrect, leading to a bug in setting the nearest neighbor search option. The code now correctly retrieves the option using the correct flag. The answer is **Buggy**."
weka,15986.json,a67dbdf599b532b4d05eec72b79cc0a92a58bb0f,"@@ -1,27 +1,29 @@
   public boolean setInputFormat(Instances instanceInfo) throws Exception {
     
     super.setInputFormat(instanceInfo);
 
     int[] attsToDelete = new int[instanceInfo.numAttributes()];
     int numToDelete = 0;
     for (int i=0; i<instanceInfo.numAttributes(); i++) {
-      if (i == instanceInfo.classIndex()) continue; // skip class
+      if ((i == instanceInfo.classIndex() && !m_invert)) {
+	continue; // skip class
+      }
       if (instanceInfo.attribute(i).type() == m_attTypeToDelete)
 	attsToDelete[numToDelete++] = i;
     }
 
     int[] finalAttsToDelete = new int[numToDelete];
     System.arraycopy(attsToDelete, 0, finalAttsToDelete, 0, numToDelete);
     
     m_attributeFilter.setAttributeIndicesArray(finalAttsToDelete);
     m_attributeFilter.setInvertSelection(m_invert);
     
     boolean result = m_attributeFilter.setInputFormat(instanceInfo);
     Instances afOutputFormat = m_attributeFilter.getOutputFormat();
     
     // restore old relation name to hide attribute filter stamp
     afOutputFormat.setRelationName(instanceInfo.relationName());
 
     setOutputFormat(afOutputFormat);
     return result;
   }",Buggy,"Fixed bug.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@1805 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message ""Fixed bug"" is very generic. The code changes involve modifying the condition for skipping the class index in the loop that identifies attributes to delete. Specifically, the original code `if (i == instanceInfo.classIndex()) continue;` is changed to `if ((i == instanceInfo.classIndex() && !m_invert)) continue;`. This suggests that the previous code had a bug where the class attribute was always skipped, regardless of the `m_invert` flag. The new condition ensures that the class attribute is skipped only when `m_invert` is false. This is a bug fix. The answer is **Buggy**."
weka,10085.json,7d3176f1d203310d133e13abf01de23e6eb5784f,"@@ -1,7 +1,5 @@
-    public int[] postProcess(int[] attributeSet) {
+  public void clean() {
 
     // save memory
     m_trainInstances = new Instances(m_trainInstances, 0);
-
-    return attributeSet;
   }",Buggy,"Fixed a bug when generating a ranking via GreedyStepwise

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@11854 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to ranking generation using GreedyStepwise. The diff shows a method called `clean` that sets `m_trainInstances` to an empty set of instances. This suggests that the bug fix might involve releasing memory used by the training instances after the ranking is generated. The original code had a `postProcess` method that simply returned the input `attributeSet` without any modification, and this method has been removed. The new `clean` method suggests a memory management fix, which can be considered a bug fix. The answer is **Buggy**."
weka,30928.json,656caee9c08b0537064158bcef54389068beb1eb,"@@ -1,55 +1,57 @@
   public static void runFileLoader(AbstractFileLoader loader, String[] options) {
     // help request?
     try {
       String[] tmpOptions = options.clone();
       if (Utils.getFlag('h', tmpOptions)) {
         System.err.println(""\nHelp requested\n"" + makeOptionStr(loader));
         return;
       }
     } catch (Exception e) {
       // ignore it
     }
 
     if (options.length > 0) {
+      String fileName = options[0];
+      options[0] = """";
       if (loader instanceof OptionHandler) {
         // set options
         try {
           ((OptionHandler) loader).setOptions(options);
           // find file
           for (int i = 0; i < options.length; i++) {
             if (options[i].length() > 0) {
               options = new String[] { options[i] };
               break;
             }
           }
         } catch (Exception ex) {
           System.err.println(makeOptionStr(loader));
           System.exit(1);
         }
       }
 
       try {
-        loader.setFile(new File(options[0]));
+        loader.setFile(new File(fileName));
         // incremental
         if (loader instanceof IncrementalConverter) {
           Instances structure = loader.getStructure();
           System.out.println(structure);
           Instance temp;
           do {
             temp = loader.getNextInstance(structure);
             if (temp != null) {
               System.out.println(temp);
             }
           } while (temp != null);
         }
         // batch
         else {
           System.out.println(loader.getDataSet());
         }
       } catch (Exception ex) {
         ex.printStackTrace();
       }
     } else {
       System.err.println(makeOptionStr(loader));
     }
   }",Buggy,"Bug fix: file name wasn't removed from options array.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@12104 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix where the file name was not being removed from the options array. The diff shows that the code now extracts the file name from `options[0]` and stores it in a separate variable `fileName`. It then sets `options[0]` to an empty string. This ensures that the file name is not treated as an option when `setOptions` is called on the loader. The file is then loaded using the `fileName` variable. This change addresses the issue described in the commit message. The answer is **Buggy**.
"
weka,21177.json,e137d2cd150f941d4ad96e9bcb3d3a27f7153b2e,"@@ -1,29 +1,33 @@
   protected int getIndexOfAttribute(Instances insts, String attName) {
     
+    if (attName == null) {
+      return -1;
+    }
+    
     // special first and last strings
     if (attName.equalsIgnoreCase(""/last"")) {
       return insts.numAttributes() - 1;
     }
     if (attName.equalsIgnoreCase(""/first"")) {
       return 0;
     }
     if (attName.startsWith(""/"")) {
       // try and parse remainder as a number
       String numS = attName.replace(""/"", """");
       try {
         int index = Integer.parseInt(numS);
         index--; // from 1-based to 0-based
         if (index >= 0 && index < insts.numAttributes()) {
           return index;
         }
       } catch (NumberFormatException e) {        
       }      
     }
     
     Attribute att = insts.attribute(attName);
     if (att != null) {
       return att.index();
     }
     
     return -1; // not found
   }",Buggy,"Small bug fix in the routine that finds attribute indexes.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@7639 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix in the attribute index finding routine. The diff adds a null check for the `attName` parameter at the beginning of the `getIndexOfAttribute` method. If `attName` is null, the method now returns -1, indicating that the attribute was not found. Without this check, a `NullPointerException` could occur if `attName` is null, leading to a crash or unexpected behavior. This change directly addresses a potential bug. The answer is **Buggy**."
weka,20328.json,8e2c885e0e2766ce262502a0ba98b112379f314f,"@@ -1,24 +1,24 @@
   private void setupRendererOptsTipText(JLabel optsLab) {
     String renderer = m_rendererCombo.getSelectedItem().toString();
     if (renderer.equalsIgnoreCase(""weka chart renderer"")) {
       // built-in renderer
       WekaOffscreenChartRenderer rcr = new WekaOffscreenChartRenderer();
       String tipText = rcr.optionsTipTextHTML();
       tipText = tipText.replace(""<html>"", ""<html>Comma separated list of options:<br>"");
       optsLab.setToolTipText(tipText);
     } else {
       try {
-        Object rendererO = PluginManager.getPluginInstance(""weka.gui.beans.OffscreenChartRender"",
+        Object rendererO = PluginManager.getPluginInstance(""weka.gui.beans.OffscreenChartRenderer"",
             renderer);
 
         if (rendererO != null) {
           String tipText = ((OffscreenChartRenderer)rendererO).optionsTipTextHTML();
           if (tipText != null && tipText.length() > 0) {
             optsLab.setToolTipText(tipText);
           }
         }
       } catch (Exception ex) {
 
       }
     }
   }",Buggy,"Fixed a bug in the routine that sets the tool tip for additional options in plugin renderers.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@7689 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to setting tooltips for additional options in plugin renderers. The code diff modifies the `setupRendererOptsTipText` method. Specifically, it corrects the class name used when obtaining a plugin instance from `PluginManager`. The original code used ""weka.gui.beans.OffscreenChartRender"", while the corrected code uses ""weka.gui.beans.OffscreenChartRenderer"". This suggests that the original class name was incorrect, leading to a failure in retrieving the plugin instance and, consequently, an incorrect or missing tooltip. The try-catch block suggests that the code was attempting to handle potential exceptions during plugin instantiation, which is consistent with addressing a bug. The answer is **Buggy**."
weka,20328.json,68eef00749a6bf58cbca2344dd6f91e526407d21,"@@ -1,24 +1,24 @@
   private void setupRendererOptsTipText(JLabel optsLab) {
     String renderer = m_rendererCombo.getSelectedItem().toString();
     if (renderer.equalsIgnoreCase(""weka chart renderer"")) {
       // built-in renderer
       WekaOffscreenChartRenderer rcr = new WekaOffscreenChartRenderer();
       String tipText = rcr.optionsTipTextHTML();
-      tipText = tipText.replace(""<html>"", ""<html>Comma separate list of options:<br>"");
+      tipText = tipText.replace(""<html>"", ""<html>Comma separated list of options:<br>"");
       optsLab.setToolTipText(tipText);
     } else {
       try {
         Object rendererO = PluginManager.getPluginInstance(""weka.gui.beans.OffscreenChartRender"",
             renderer);
 
         if (rendererO != null) {
           String tipText = ((OffscreenChartRenderer)rendererO).optionsTipTextHTML();
           if (tipText != null && tipText.length() > 0) {
             optsLab.setToolTipText(tipText);
           }
         }
       } catch (Exception ex) {
 
       }
     }
   }",Buggy,"Fixed a spelling mistake in a tooltip.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@7635 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix for a spelling mistake in a tooltip. The diff replaces ""Comma separate list of options"" with ""Comma separated list of options"", correcting the spelling of ""separate"". This change directly aligns with the commit message and represents a bug fix, albeit a minor one. The answer is **Buggy**."
weka,3145.json,d00cb09c5d77936fe79ca4e6697fc4b8dde11de4,"@@ -1,50 +1,51 @@
   protected List<Instances> initializeWithRandomCenters(
     JavaRDD<Instance> dataset, Instances headerWithSummary, int numRuns,
     int numClusters) throws IOException, DistributedWekaException {
 
     Instances headerNoSummary =
       CSVToARFFHeaderReduceTask.stripSummaryAtts(headerWithSummary);
 
     // sample all runs worth of initial centers in one hit
     // take twice as many as needed in case there are duplicates
     int seed = 1;
     if (!DistributedJobConfig.isEmpty(getRandomSeed())) {
       try {
         seed = Integer.parseInt(environmentSubstitute(getRandomSeed()));
       } catch (NumberFormatException e) {
         // don't complain
       }
     }
 
     // oversample for > 1 cluster per run, so that we have some options if there
     // are duplicates in the list. numClusters == 1 will be used when seeding
     // the k-means|| initialization process
+    int oversampleFactor = numClusters > 1 ? 2 : 1;
     List<Instance> centerList =
-      dataset.takeSample(true, numClusters > 1 ? 2 : 1 * numRuns * numClusters,
-        seed);
+      dataset.takeSample(true, oversampleFactor * numRuns * numClusters,
+                         seed);
 
     // make sure that start points and header have been through any filters
     KMeansMapTask forFilteringOnly = new KMeansMapTask();
     try {
       forFilteringOnly.setOptions(Utils
         .splitOptions(environmentSubstitute(getKMeansMapTaskOpts())));
 
       // initialize sketches
       headerNoSummary = forFilteringOnly.init(headerWithSummary);
 
       for (int i = 0; i < centerList.size(); i++) {
         Instance filtered = forFilteringOnly.applyFilters(centerList.get(i));
         centerList.set(i, filtered);
       }
 
     } catch (Exception ex) {
       logMessage(ex);
       throw new DistributedWekaException(ex);
     }
 
     List<Instances> centreCandidates =
       KMeansMapTask.assignStartPointsFromList(numRuns, numClusters, centerList,
         headerNoSummary);
 
     return centreCandidates;
   }",Buggy,"Fixed a bug in the standard random initialization routine

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@11926 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix in the standard random initialization routine. The code changes involve modifying the sampling process of initial cluster centers. Specifically, the code now includes an `oversampleFactor` which is set to 2 if `numClusters` is greater than 1, otherwise it's 1. The `takeSample` method now uses this `oversampleFactor` to sample more data points. This change suggests that the original sampling process might have been insufficient, potentially leading to issues like duplicate or unsuitable initial centers, especially when dealing with multiple clusters. The oversampling addresses this by providing more options for selecting distinct and representative centers. The answer is **Buggy**."
weka,26979.json,483a2a5d5184c1590893885c0a1be41649ba25de,"@@ -1,113 +1,113 @@
   public String toString() {
 
     StringBuffer text = new StringBuffer();
     int printed = 0;
 	
     if ((m_alpha == null) && (m_sparseWeights == null)) {
       return ""SMOreg : No model built yet."";
     }
     try {
       text.append(""SMOreg\n\n"");
 	    
       text.append(""Kernel used : \n"");
       if(m_useRBF) {
 	text.append(""  RBF kernel : K(x,y) = e^-("" + m_gamma + ""* <x-y,x-y>^2)"");
       } else if (m_exponent == 1){
 	text.append(""  Linear Kernel : K(x,y) = <x,y>"");
       } else {
 	if (m_featureSpaceNormalization) {
 	  if (m_lowerOrder){
 	    text.append(""  Normalized Poly Kernel with lower order : K(x,y) = (<x,y>+1)^"" + m_exponent + ""/"" + 
 			""((<x,x>+1)^"" + m_exponent + ""*"" + ""(<y,y>+1)^"" + m_exponent + "")^(1/2)"");		    
 	  } else {
 	    text.append(""  Normalized Poly Kernel : K(x,y) = <x,y>^"" + m_exponent + ""/"" + ""(<x,x>^"" + 
 			m_exponent + ""*"" + ""<y,y>^"" + m_exponent + "")^(1/2)"");
 	  }
 	} else {
 	  if (m_lowerOrder){
 	    text.append(""  Poly Kernel with lower order : K(x,y) = (<x,y> + 1)^"" + m_exponent);
 	  } else {
 	    text.append(""  Poly Kernel : K(x,y) = <x,y>^"" + m_exponent);		
 	  }
 	}
       }
       text.append(""\n\n"");
 
       // display the linear transformation
       String trans = """";
       if (m_filterType == FILTER_STANDARDIZE) {
 	//text.append(""LINEAR TRANSFORMATION APPLIED : \n"");
 	trans = ""(standardized) "";
 	//text.append(trans + m_data.classAttribute().name() + ""  = "" + 
 	//	    m_Alin + "" * "" + m_data.classAttribute().name() + "" + "" + m_Blin + ""\n\n"");
       } else if (m_filterType == FILTER_NORMALIZE) {
 	//text.append(""LINEAR TRANSFORMATION APPLIED : \n"");
 	trans = ""(normalized) "";
 	//text.append(trans + m_data.classAttribute().name() + ""  = "" + 
 	//	    m_Alin + "" * "" + m_data.classAttribute().name() + "" + "" + m_Blin + ""\n\n"");
       }
 
       // If machine linear, print weight vector
       if (!m_useRBF && m_exponent == 1.0) {
 	text.append(""Machine Linear: showing attribute weights, "");
 	text.append(""not support vectors.\n"");
 		
 	// We can assume that the weight vector is stored in sparse
 	// format because the classifier has been built
 	text.append(trans + m_data.classAttribute().name() + "" =\n"");
 	for (int i = 0; i < m_sparseWeights.length; i++) {
-	  if (i != (int)m_classIndex) {
+	  if (m_sparseIndices[i] != (int)m_classIndex) {
 	    if (printed > 0) {
 	      text.append("" + "");
 	    } else {
 	      text.append(""   "");
 	    }
 	    text.append(Utils.doubleToString(m_sparseWeights[i], 12, 4) +
 			"" * "");
 	    if (m_filterType == FILTER_STANDARDIZE) {
 	      text.append(""(standardized) "");
 	    } else if (m_filterType == FILTER_NORMALIZE) {
 	      text.append(""(normalized) "");
 	    }
 	    if (!m_checksTurnedOff) {
 	      text.append(m_data.attribute(m_sparseIndices[i]).name()+""\n"");
 	    } else {
 	      text.append(""attribute with index "" + 
 			  m_sparseIndices[i] +""\n"");
 	    }
 	    printed++;
 	  }
 	}
       } else {
 	text.append(""Support Vector Expansion :\n"");
 	text.append(trans + m_data.classAttribute().name() + "" =\n"");
 	printed = 0;
 	for (int i = 0; i < m_alpha.length; i++) {
 	  double val = m_alpha[i] - m_alpha_[i];
 	  if (java.lang.Math.abs(val) < 1e-4)
 	    continue;
 	  if (printed > 0) {
 	    text.append("" + "");
 	  } else {
 	    text.append(""   "");		    
 	  }
 	  text.append(Utils.doubleToString(val, 12, 4) 
 		      + "" * K[X("" + i + ""), X]\n"");
 	  printed++;
 	}
       }
       if (m_b > 0) {
 	text.append("" + "" + Utils.doubleToString(m_b, 12, 4));
       } else {
 	text.append("" - "" + Utils.doubleToString(-m_b, 12, 4));
       }
       if (m_useRBF || m_exponent != 1.0) {
 	text.append(""\n\nNumber of support vectors: "" + printed);
       }
       text.append(""\n\nNumber of kernel evaluations: "" + m_kernel.numEvals()+ ""\n"");
     } catch (Exception e) {
       return ""Can't print the classifier."";
     }
 
     return text.toString();
   }",Buggy,"Fixed bug in output of sparse linear machines (class index was not dealt with correctly (thanks, Bernhard)).


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@2091 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to the class index in the output of sparse linear machines. The diff modifies the condition `if (i != (int)m_classIndex)` to `if (m_sparseIndices[i] != (int)m_classIndex)`. This change suggests that the original code was incorrectly comparing the loop index `i` with the class index, instead of comparing the value at the `i`-th index of the `m_sparseIndices` array with the class index. This would lead to incorrect attribute weights being displayed in the output. Therefore, the change fixes a bug in how the class index was handled, aligning with the commit message. The answer is **Buggy**."
weka,28766.json,78f99a6107b459a31b5ab63e2f403eb841974e27,"@@ -1,14 +1,19 @@
   public double[] distributionForInstance(Instance instance) throws Exception {
 
     double[] dist = new double[instance.numClasses()];
     switch (instance.classAttribute().type()) {
     case Attribute.NOMINAL:
-      dist[(int)classifyInstance(instance)] = 1.0;
+      double classification = classifyInstance(instance);
+      if (Instance.isMissingValue(classification)) {
+	return dist;
+      } else {
+	dist[(int)classification] = 1.0;
+      }
       return dist;
     case Attribute.NUMERIC:
       dist[0] = classifyInstance(instance);
       return dist;
     default:
       return dist;
     }
   }",Buggy,"Fixed bug that caused incorrect handling of unclassified instances in distributionForInstance()


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@2006 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to handling unclassified instances in the `distributionForInstance()` method. The diff shows that the code now checks if the classification result (`classification`) is a missing value using `Instance.isMissingValue(classification)`. If it is, the method returns the original `dist` array without assigning a class, effectively handling unclassified instances. Otherwise, it proceeds to assign a probability of 1.0 to the predicted class. This change addresses the bug described in the commit message. The answer is **Buggy**."
weka,28908.json,b842fc63d279fec85c6c091749c5a8b41f36f775,"@@ -1,25 +1,25 @@
   protected Instances metaFormat(Instances instances) throws Exception {
 
     FastVector attributes = new FastVector();
     Instances metaFormat;
     Attribute attribute;
     int i = 0;
 
     for (int k = 0; k < m_Classifiers.length; k++) {
       Classifier classifier = (Classifier) getClassifier(k);
       String name = classifier.getClass().getName();
       if (m_BaseFormat.classAttribute().isNumeric()) {
 	attributes.addElement(new Attribute(name));
       } else {
 	for (int j = 0; j < m_BaseFormat.classAttribute().numValues(); j++) {
 	  attributes.addElement(new Attribute(name + "":"" + 
 					      m_BaseFormat
 					      .classAttribute().value(j)));
 	}
       }
     }
-    attributes.addElement(m_BaseFormat.classAttribute());
+    attributes.addElement(m_BaseFormat.classAttribute().copy());
     metaFormat = new Instances(""Meta format"", attributes, 0);
     metaFormat.setClassIndex(metaFormat.numAttributes() - 1);
     return metaFormat;
   }",Buggy,"Apply bug fix from Alexander K. Seewald <alexsee@oefai.at>


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@2237 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix. The diff involves the `metaFormat` method. Specifically, the line `attributes.addElement(m_BaseFormat.classAttribute());` is changed to `attributes.addElement(m_BaseFormat.classAttribute().copy());`. This suggests that the original code was directly adding the class attribute from `m_BaseFormat` to the new `metaFormat`. This could lead to unintended side effects if the `metaFormat` is modified, as it would also affect the original `m_BaseFormat`. By creating a copy of the class attribute, the code ensures that the `metaFormat` has its own independent class attribute, preventing any unintended modifications to the original `m_BaseFormat`. This is a bug fix because the original code could lead to unexpected behavior and data corruption. The answer is **Buggy**."
weka,4826.json,6809d47f9b4ddab9de675679cc0cd2f38a66799c,"@@ -1,104 +1,103 @@
   protected void display(ArrayList<Prediction> preds, Attribute classAtt,
     int classValue) {
 
     if (preds == null) {
       JOptionPane.showMessageDialog(null, ""No data available for display!"");
       return;
     }
 
     // Remove prediction objects where either the prediction or the actual value are missing
     ArrayList<Prediction> newPreds = new ArrayList<>();
     for (Prediction p : preds) {
       if (!Utils.isMissingValue(p.actual()) && !Utils
         .isMissingValue(p.predicted())) {
         newPreds.add(p);
       }
     }
     preds = newPreds;
 
     ArrayList<Attribute> attributes = new ArrayList<>(1);
     attributes.add(new Attribute(""class_prob""));
     Instances data =
       new Instances(""class_probabilities"", attributes, preds.size());
 
     for (int i = 0; i < preds.size(); i++) {
       double[] inst =
         { ((NominalPrediction) preds.get(i)).distribution()[classValue] };
       data.add(new DenseInstance(preds.get(i).weight(), inst));
     }
 
     try {
       Discretize d = new Discretize();
       d.setUseEqualFrequency(true);
       d.setBins(
         Integer.max(1, (int) Math.round(Math.sqrt(data.sumOfWeights()))));
       d.setUseBinNumbers(true);
       d.setInputFormat(data);
       data = Filter.useFilter(data, d);
 
       int numBins = data.attribute(0).numValues();
       double[] sumClassProb = new double[numBins];
       double[] sumTrueClass = new double[numBins];
       double[] sizeOfBin = new double[numBins];
       for (int i = 0; i < data.numInstances(); i++) {
         int binIndex = (int) data.instance(i).value(0);
         sizeOfBin[binIndex] += preds.get(i).weight();
         sumTrueClass[binIndex] +=
           preds.get(i).weight() * ((((int) preds.get(i).actual())
             == classValue) ? 1.0 : 0.0);
         sumClassProb[binIndex] +=
           preds.get(i).weight() * ((NominalPrediction) preds.get(i))
             .distribution()[classValue];
       }
 
       ArrayList<Attribute> atts = new ArrayList<>(1);
       atts.add(new Attribute(""average_class_prob""));
       atts.add(new Attribute(""average_true_class_value""));
 
       // Collect data for plotting, making sure that 0,0 and 1,1 are included as invisible points
       Instances cdata =
         new Instances(""calibration_curve_data"", atts, numBins + 2);
       int[] shapeType = new int[numBins + 2];
       boolean[] connectPoint = new boolean[numBins + 2];
       for (int i = 0; i < numBins; i++) {
         double[] v = new double[2];
         v[0] = sumClassProb[i] / sizeOfBin[i];
         v[1] = sumTrueClass[i] / sizeOfBin[i];
         cdata.add(new DenseInstance(sizeOfBin[i], v));
         shapeType[i] = Plot2D.PLUS_SHAPE;
         connectPoint[i] = true;
       }
       double[] zero = new double[2];
       double[] one = new double[2];
       one[0] = 1.0;
       one[1] = 1.0;
       cdata.add(new DenseInstance(0.0, zero));
       cdata.add(new DenseInstance(0.0, one));
       shapeType[shapeType.length - 2] =
         -2; // Hack to make sure that corner points are invisible
       shapeType[shapeType.length - 1] = -2;
 
       PlotData2D plotInfo = new PlotData2D(cdata);
       plotInfo.setConnectPoints(connectPoint);
       plotInfo.setShapeType(shapeType);
-      plotInfo.setPlotName(
-        ""\""Calibration curve for class value \"" + classAtt.value(classValue)"");
+      plotInfo.setPlotName(""Calibration curve for class value "" + classAtt.value(classValue));
       VisualizePanel vp = new VisualizePanel();
       vp.setName(
         ""Calibration curve (x: estimated probability, y: observed probability) for ""
           + classAtt.value(classValue) + "" based on"" + "" "" + numBins
           + "" equal-frequency bins"");
       vp.setMasterPlot(plotInfo);
 
       JFrame frame = new JFrame(
         ""Calibration curve (x: estimated probability, y: observed probability) for ""
           + classAtt.value(classValue) + "" based on"" + "" "" + numBins
           + "" equal-frequency bins"");
       frame.setSize(1024, 800);
       frame.setContentPane(vp);
       frame.setVisible(true);
 
     } catch (Exception ex) {
       ex.printStackTrace();
     }
   }",Buggy,"Committed one small bug fix in name of Instances object. Removed one unnecessary import.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@14764 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to the name of the Instances object. The diff modifies the plot name string, removing the extra quotes around the class value. This suggests that the original plot name might have been incorrectly formatted, which can be considered a bug. The answer is **Buggy**."
weka,22844.json,0f8a283a8a350a9341d33f14127fb4d90fbe0b6c,"@@ -1,25 +1,40 @@
   protected void updateCapabilitiesFilter(Capabilities filter) {
     Instances 		tempInst;
     Capabilities 	filterClass;
 
     if (filter == null) {
       m_ClassifierEditor.setCapabilitiesFilter(new Capabilities(null));
       return;
     }
     
     if (!ExplorerDefaults.getInitGenericObjectEditorFilter())
       tempInst = new Instances(m_Instances, 0);
     else
       tempInst = new Instances(m_Instances);
     tempInst.setClassIndex(m_ClassCombo.getSelectedIndex());
 
     try {
       filterClass = Capabilities.forInstances(tempInst);
     }
     catch (Exception e) {
       filterClass = new Capabilities(null);
     }
     
     // set new filter
     m_ClassifierEditor.setCapabilitiesFilter(filterClass);
+    
+    // Check capabilities
+    m_StartBut.setEnabled(true);
+    Capabilities currentFilter = m_ClassifierEditor.getCapabilitiesFilter();
+    Classifier classifier = (Classifier) m_ClassifierEditor.getValue();
+    Capabilities currentSchemeCapabilities =  null;
+    if (classifier != null && currentFilter != null && 
+        (classifier instanceof CapabilitiesHandler)) {
+      currentSchemeCapabilities = ((CapabilitiesHandler)classifier).getCapabilities();
+      
+      if (!currentSchemeCapabilities.supportsMaybe(currentFilter) &&
+          !currentSchemeCapabilities.supports(currentFilter)) {
+        m_StartBut.setEnabled(false);
+      }
+    }
   }",Buggy,"Fixed a bug where the enabled/disabled state of the start button was not being updated when a new data set was set on this panel.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@5382 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to the start button's enabled/disabled state not being updated correctly when a new dataset is set. The diff shows that the `updateCapabilitiesFilter` method is modified to check the capabilities of the classifier against the current filter. If the classifier doesn't support the filter, the start button is disabled. This aligns with the commit message, indicating a fix for the described bug. The answer is **Buggy**."
weka,15202.json,e6a516aae06f40c5da24d36795d1ee48b4688fb5,"@@ -1,11 +1,17 @@
   public void setOptions(String[] options) throws Exception {
     
     setAttributeIndices(Utils.getOption('R', options));
     setInvertSelection(Utils.getFlag('V', options));
-    setClassName(Utils.getOption('C', options));
-    setMethodName(Utils.getOption('M', options));
+    String classString = Utils.getOption('C', options);
+    if (classString.length() != 0) {
+      setClassName(classString);
+    }
+    String methodString = Utils.getOption('M', options);
+    if (methodString.length() != 0) {
+      setMethodName(methodString);
+    }
 
     if (getInputFormat() != null) {
       setInputFormat(getInputFormat());
     }
   }",Buggy,"Fixed some option-handling bugs.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@2193 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates that option-handling bugs have been fixed. The diff shows that the code now checks if the classString and methodString obtained from the options are not empty before setting the className and methodName. This prevents the program from setting empty class or method names, which could lead to unexpected behavior or errors. This change fixes a bug where empty strings could be passed as class or method names, so the answer is **Buggy**."
weka,23203.json,af433bd414a79a55cf517aeccf73cfc90d5c8089,"@@ -1,13 +1,14 @@
   public void setInstances(Instances inst) {
     m_Instances = inst;
     m_RelationNameLab.setText(m_Instances.relationName());
+    m_RelationNameLab.setToolTipText(m_Instances.relationName());
     m_NumInstancesLab.setText("""" + 
         ((m_showZeroInstancesAsUnknown && m_Instances.numInstances() == 0) 
             ? ""?"" 
             : """" + m_Instances.numInstances()));
     m_NumAttributesLab.setText("""" + m_Instances.numAttributes());
     m_sumOfWeightsLab.setText("""" + 
         ((m_showZeroInstancesAsUnknown && m_Instances.numInstances() == 0) 
             ? ""?"" 
             : """" + Utils.doubleToString(m_Instances.sumOfWeights(), 3)));
   }",Buggy,"Fixed a bug where the relation name would overlap the attributes. Now sets the tip text for the relation name label to be equal to the relation name (so that the full relation name can be seen when the mouse hovers over the name on the panel).

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@7229 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix where the relation name would overlap the attributes. The diff adds a tooltip to the relation name label, displaying the full relation name when the mouse hovers over it. This change directly addresses the issue described in the commit message by allowing users to see the full relation name, even if it's too long to fit in the label. The answer is **Buggy**."
weka,27297.json,334f1a26a1895da32f511cae40ee3143b87295d7,"@@ -1,18 +1,18 @@
   public double SVMOutput(Instance inst) throws Exception {
     
     double result = -m_b;
     // Is the machine linear?
     if (m_weights != null) {
       // Is weight vector stored in sparse format?
-      for (int i = 0; i < m_weights.length; i++) {
+      for (int i = 0; i < inst.numValues(); i++) {
 	if (inst.index(i) != m_classIndex) {
 	  result += m_weights[inst.index(i)] * inst.valueSparse(i);
 	}
       }
     } else {
       for (int i = m_supportVectors.getNext(-1); i != -1; i = m_supportVectors.getNext(i)) {
 	result += (m_alpha[i] - m_alphaStar[i]) * m_kernel.eval(-1, i, inst);
       }
     }
     return result;
   }",Buggy,"Fixed bug where a loop a loop iteration was not correct for sparse instances.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@6621 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to loop iteration in sparse instances. The code diff shows a change in the loop condition from `i < m_weights.length` to `i < inst.numValues()`. The original loop iterated based on the length of `m_weights`, which might not be correct for sparse instances where the number of values in the instance (`inst.numValues()`) determines the relevant indices. The corrected loop iterates based on the number of values in the instance, which is the correct approach for sparse instances. Therefore, the change fixes a bug related to incorrect loop iteration. The answer is **Buggy**."
weka,9635.json,d31aa1c73607a81824ab967b78e8bf672d9c1a1b,"@@ -1,110 +1,110 @@
   protected void purgeTasks(long purgeInterval) {
     Date now = new Date();
     long nowMilli = now.getTime();
     boolean doPurge = false;
 
     List<WekaTaskEntry> taskList = m_taskMap.getTaskList();
     for (WekaTaskEntry t : taskList) {
       NamedTask task = m_taskMap.getTask(t);
       doPurge = false;
       if (!(task instanceof Scheduled)) {
         // Date lastExecuted = getExecutionTime(t);
         Date lastExecuted = t.getLastExecution();
         if (lastExecuted != null) {
           if (task.getTaskStatus().getExecutionStatus() == TaskStatusInfo.PROCESSING) {
             // don't purge executing tasks!!
             continue;
           }
           long milli = lastExecuted.getTime();
 
           // leave tasks that were sent to us from another server for twice as
           // long in
           // order to give the master a chance to tell us to purge them
           long pI =
             (t.getCameFromMaster() ? (purgeInterval * 2) : purgeInterval);
 
           if (nowMilli - milli > pI) {
             doPurge = true;
           }
         }
       } else {
         Date lastExecuted = t.getLastExecution();
         Date nextExecution =
           ((Scheduled) task).getSchedule().nextExecution(lastExecuted);
         if (nextExecution == null && lastExecuted != null) {
           long milli = lastExecuted.getTime();
 
           // leave tasks that were sent to us from another server for twice as
           // long in
           // order to give the master a chance to tell us to purge them
           long pI =
             (t.getCameFromMaster() ? (purgeInterval * 2) : purgeInterval);
 
           if (nowMilli - milli > pI) {
             doPurge = true;
           }
         }
       }
 
       if (doPurge) {
         PostMethod post = null;
         InputStream is = null;
 
         try {
           String url = ""http://"" + getHostname() + "":"" + getPort();
           url = url.replace("" "", ""%20"");
           url += PurgeTaskServlet.CONTEXT_PATH;
-          url += ""/?name="" + t.toString();
+	  url += ""/?name="" + URLEncoder.encode(t.toString(), ""UTF-8"");
           url += ""&client=Y"";
 
           post = new PostMethod(url);
           post.setDoAuthentication(true);
           post.addRequestHeader(new Header(""Content-Type"", ""text/plain""));
 
           // Get HTTP client
           HttpClient client =
             ConnectionManager.getSingleton().createHttpClient();
           ConnectionManager.addCredentials(client, m_username, m_password);
 
           // Execute request
           int result = client.executeMethod(post);
           // System.out.println(""[WekaServer] Response from master server : "" +
           // result);
           if (result == 401) {
             System.err.println(""[WekaServer] Unable to purge task""
               + "" - authentication required.\n"");
           } else {
             // the response
             is = post.getResponseBodyAsStream();
             ObjectInputStream ois = new ObjectInputStream(is);
             Object response = ois.readObject();
             if (response.toString().startsWith(WekaServlet.RESPONSE_ERROR)) {
               System.err.println(""[WekaServer] A problem occurred while ""
                 + ""trying to purge task ("" + t.toString() + ""): \n"" + ""\t""
                 + response.toString());
             } else {
               System.out.println(""[WekaServer] purged task: "" + t.toString());
             }
           }
         } catch (Exception ex) {
           System.err
             .println(""[WekaServer] A problem occurred while ""
               + ""trying to purge task ("" + t.toString() + ""): ""
               + ex.getMessage());
           ex.printStackTrace();
         } finally {
           if (is != null) {
             try {
               is.close();
             } catch (IOException e) {
               e.printStackTrace();
             }
           }
 
           if (post != null) {
             post.releaseConnection();
           }
         }
       }
     }
   }",Buggy,"Fixed a bug in the purge task thread

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@12578 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix in the purge task thread. The diff shows a change in how the task name is encoded in the URL for the purge request. Specifically, it uses `URLEncoder.encode(t.toString(), ""UTF-8"")` to ensure that the task name is properly encoded for transmission in the URL. Without this encoding, special characters in the task name could cause issues with the request. This change addresses a potential bug where task names with special characters were not being handled correctly during the purge process. The answer is **Buggy**."
weka,33400.json,a69dfab4b09e9d52542306b240900fdbfb951df3,"@@ -1,145 +1,146 @@
   private Instance vectorizeInstance(Instance input, int[] offsetHolder,
     boolean retainStringAttValuesInMemory) throws Exception {
 
     if (!m_inputContainsStringAttributes) {
       return input;
     }
 
     if (m_inputFormat == null) {
       throw new Exception(""No input format available. Call setup() and ""
         + ""make sure a dictionary has been built first."");
     }
 
     if (m_consolidatedDict == null) {
       throw new Exception(""Dictionary hasn't been built or consolidated yet!"");
     }
 
     int indexOffset = 0;
     int classIndex = m_outputFormat.classIndex();
     Map<Integer, double[]> contained = new TreeMap<Integer, double[]>();
     for (int i = 0; i < m_inputFormat.numAttributes(); i++) {
       if (!m_selectedRange.isInRange(i)) {
         if (!m_inputFormat.attribute(i).isString()
           && !m_inputFormat.attribute(i).isRelationValued()) {
 
           // add nominal and numeric directly
           if (input.value(i) != 0.0) {
             contained.put(indexOffset, new double[] { input.value(i) });
           }
         } else {
           if (input.isMissing(i)) {
             contained.put(indexOffset, new double[] { Utils.missingValue() });
           } else if (m_inputFormat.attribute(i).isString()) {
             String strVal = input.stringValue(i);
             if (retainStringAttValuesInMemory) {
               double strIndex =
                 m_outputFormat.attribute(indexOffset).addStringValue(strVal);
               contained.put(indexOffset, new double[] { strIndex });
             } else {
               m_outputFormat.attribute(indexOffset).setStringValue(strVal);
               contained.put(indexOffset, new double[] { 0 });
             }
           } else {
             // relational
             if (m_outputFormat.attribute(indexOffset).numValues() == 0) {
               Instances relationalHeader =
                 m_outputFormat.attribute(indexOffset).relation();
 
               // hack to defeat sparse instances bug
               m_outputFormat.attribute(indexOffset).addRelation(
                 relationalHeader);
             }
             int newIndex =
               m_outputFormat.attribute(indexOffset).addRelation(
                 input.relationalValue(i));
             contained.put(indexOffset, new double[] { newIndex });
           }
         }
         indexOffset++;
       }
     }
 
     offsetHolder[0] = indexOffset;
 
     // dictionary entries
     for (int i = 0; i < m_inputFormat.numAttributes(); i++) {
       if (m_selectedRange.isInRange(i) && !input.isMissing(i)) {
         m_tokenizer.tokenize(input.stringValue(i));
 
         while (m_tokenizer.hasMoreElements()) {
           String word = m_tokenizer.nextElement();
           if (m_lowerCaseTokens) {
             word = word.toLowerCase();
           }
           word = m_stemmer.stem(word);
 
           int[] idxAndDocCount = m_consolidatedDict.get(word);
           if (idxAndDocCount != null) {
             if (m_outputCounts) {
               double[] inputCount =
                 contained.get(idxAndDocCount[0] + indexOffset);
               if (inputCount != null) {
                 inputCount[0]++;
               } else {
                 contained.put(idxAndDocCount[0] + indexOffset,
                   new double[] { 1 });
               }
             } else {
               contained
                 .put(idxAndDocCount[0] + indexOffset, new double[] { 1 });
             }
           }
         }
       }
     }
 
     // TF transform
     if (m_TFTransform) {
       for (Map.Entry<Integer, double[]> e : contained.entrySet()) {
         int index = e.getKey();
         if (index >= indexOffset) {
           double[] val = e.getValue();
           val[0] = Math.log(val[0] + 1);
         }
       }
     }
 
     // IDF transform
     if (m_IDFTransform) {
       for (Map.Entry<Integer, double[]> e : contained.entrySet()) {
         int index = e.getKey();
         if (index >= indexOffset) {
           double[] val = e.getValue();
           String word = m_outputFormat.attribute(index).name();
+          word = word.substring(m_Prefix.length());
           int[] idxAndDocCount = m_consolidatedDict.get(word);
           if (idxAndDocCount == null) {
             throw new Exception(""This should never occur"");
           }
           if (idxAndDocCount.length != 2) {
             throw new Exception(""Can't compute IDF transform as document ""
               + ""counts are not available"");
           }
           val[0] = val[0] * Math.log(m_count / (double) idxAndDocCount[1]);
         }
       }
     }
 
     double[] values = new double[contained.size()];
     int[] indices = new int[contained.size()];
     int i = 0;
     for (Map.Entry<Integer, double[]> e : contained.entrySet()) {
       values[i] = e.getValue()[0];
       indices[i++] = e.getKey().intValue();
     }
 
     Instance inst =
       new SparseInstance(input.weight(), values, indices,
         m_outputFormat.numAttributes());
     inst.setDataset(m_outputFormat);
 
     if (m_normalize) {
       normalizeInstance(inst, indexOffset);
     }
 
     return inst;
   }",Buggy,"Fixed a bug that affected the -P option when used in conjuction with the IDF transform

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@12931 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to the `-P` option when used with the IDF transform. The diff modifies the `vectorizeInstance` method, specifically within the IDF transform block. A line is added to remove the prefix from the word before retrieving it from the consolidated dictionary: `word = word.substring(m_Prefix.length());`. This suggests that the prefix was causing an issue when looking up the word in the dictionary, leading to incorrect IDF calculations. This correction directly addresses the bug described in the commit message. The answer is **Buggy**."
weka,3374.json,b97c9f7d4a47bf18c449215421f95140d8c6c5ea,"@@ -1,222 +1,222 @@
   public boolean runJobWithContext(JavaSparkContext sparkContext)
     throws IOException, DistributedWekaException {
 
     m_currentContext = sparkContext;
     setJobStatus(JobStatus.RUNNING);
     boolean success = false;
 
     if (m_env == null) {
       m_env = Environment.getSystemWide();
     }
 
     // Make sure that we save out to a subdirectory of the output
     // directory
     String outputPath = environmentSubstitute(m_sjConfig.getOutputDir());
     outputPath = addSubdirToPath(outputPath, OUTPUT_SUBDIR);
 
     JavaRDD<Instance> inputData = null;
     Instances headerWithSummary = null;
     if (getDataset(TRAINING_DATA) != null) {
       inputData = getDataset(TRAINING_DATA).getDataset();
       headerWithSummary = getDataset(TRAINING_DATA).getHeaderWithSummary();
       logMessage(""RDD<Instance> dataset provided: ""
         + inputData.partitions().size() + "" partitions."");
     }
 
     if (inputData == null && headerWithSummary == null) {
       logMessage(""[Randomly shuffle data] Invoking ARFF Job..."");
       m_arffHeaderJob.setEnvironment(m_env);
       m_arffHeaderJob.setLog(getLog());
       m_arffHeaderJob.setStatusMessagePrefix(m_statusMessagePrefix);
       m_arffHeaderJob.setCachingStrategy(getCachingStrategy());
 
       // header job necessary?
       success = m_arffHeaderJob.runJobWithContext(sparkContext);
 
       if (!success) {
         setJobStatus(JobStatus.FAILED);
         statusMessage(""Unable to continue - creating the ARFF header failed!"");
         logMessage(""[Randomly shuffle data] Unable to continue - creating the ARFF header failed!"");
         return false;
       }
 
       Dataset d = m_arffHeaderJob.getDataset(TRAINING_DATA);
 
       headerWithSummary = d.getHeaderWithSummary();
       inputData = d.getDataset();
       logMessage(""Fetching RDD<Instance> dataset from ARFF job: ""
         + inputData.partitions().size() + "" partitions."");
     }
 
     /*
      * int minSlices = 1; if
      * (!DistributedJobConfig.isEmpty(m_sjConfig.getMinInputSlices())) { try {
      * minSlices = Integer
      * .parseInt(environmentSubstitute(m_sjConfig.getMinInputSlices())); } catch
      * (NumberFormatException e) { } }
      */
 
     /*
      * if (!m_cleanOutputDir) { // check for existing chunk files... String
      * pathPlusChunk = outputPath + ""/part-00000"";
      * logMessage(""[Randomly shuffle data] Checking output directory: "" +
      * outputPath); if (SparkJob.checkFileExists(pathPlusChunk)) {
      * logMessage(""[Randomly shuffle data] Output directory is populated "" +
      * ""with randomly shuffled chunk files already - "" + ""no need to execute."");
      * 
      * loadShuffledDataFiles(outputPath, sparkContext,
      * CSVToARFFHeaderReduceTask.stripSummaryAtts(headerWithSummary),
      * minSlices); return true; } }
      */
 
     /*
      * // TODO revisit at some stage... Current assumption: if you // have
      * output from this job as serialized instances then you // are happy with
      * the shuffling and will not want to re-shuffle if
      * (m_sjConfig.getSerializedInput()) { throw new DistributedWekaException(
      * ""Randomly shuffling serialized Instance "" +
      * ""input is not supported yet.""); }
      */
 
     // clean the output directory
     SparkJob.deleteDirectory(outputPath);
 
     String inputFile = environmentSubstitute(m_sjConfig.getInputFile());
 
     int seed = 1;
     if (!DistributedJobConfig.isEmpty(getRandomSeed())) {
       seed = Integer.parseInt(environmentSubstitute(getRandomSeed()));
     }
     final Instances headerNoSummary =
       CSVToARFFHeaderReduceTask.stripSummaryAtts(headerWithSummary);
 
     try {
       WekaClassifierSparkJob.setClassIndex(
         environmentSubstitute(m_classAttribute), headerNoSummary,
         !m_dontDefaultToLastAttIfClassNotSpecified);
 
     } catch (Exception e) {
       logMessage(e);
       throw new DistributedWekaException(e);
     }
 
     // find summary attribute for class (if set, otherwise just use the first
     // numeric on nominal attribute). We're using this simply to find out
     // the total number of instances in the dataset
     String className = null;
     if (headerNoSummary.classIndex() >= 0) {
       className = headerNoSummary.classAttribute().name();
     } else {
       for (int i = 0; i < headerNoSummary.numAttributes(); i++) {
         if (headerNoSummary.attribute(i).isNumeric()
           || headerNoSummary.attribute(i).isNominal()) {
           className = headerNoSummary.attribute(i).name();
           break;
         }
       }
     }
     Attribute summaryClassAtt =
       headerWithSummary
         .attribute(CSVToARFFHeaderMapTask.ARFF_SUMMARY_ATTRIBUTE_PREFIX
           + className);
     if (summaryClassAtt == null) {
       throw new DistributedWekaException(
         ""Was unable to find the summary attribute for "" + ""the class: ""
           + className);
     }
 
     int totalNumInstances = 0;
     int numFoldSlices = 10;
     // summary attribute for getting the total number of instances
     Attribute summaryAttOrig = null;
     for (int i = 0; i < headerNoSummary.numAttributes(); i++) {
       if (headerNoSummary.attribute(i).isNumeric()
         || headerNoSummary.attribute(i).isNominal()) {
         summaryAttOrig = headerNoSummary.attribute(i);
         break;
       }
     }
     String summaryName = summaryAttOrig.name();
     Attribute summaryAtt =
       headerWithSummary
         .attribute(
           CSVToARFFHeaderMapTask.ARFF_SUMMARY_ATTRIBUTE_PREFIX + summaryName);
     if (summaryAtt == null) {
       logMessage(""[RandomizedDataSparkJob] Was unable to find the summary ""
         + ""attribute for attribute: "" + summaryName);
       throw new DistributedWekaException(""Was unable to find the summary ""
         + ""attribute for attribute: "" + summaryName);
     }
 
-    if (summaryAtt.isNominal()) {
+    if (summaryAttOrig.isNominal()) {
       NominalStats stats = NominalStats.attributeToStats(summaryAtt);
       for (String label : stats.getLabels()) {
         totalNumInstances += stats.getCount(label);
       }
     } else {
       NumericStats stats = NumericStats.attributeToStats(summaryAtt);
       totalNumInstances =
         (int) stats.getStats()[ArffSummaryNumericMetric.COUNT.ordinal()];
     }
 
     if (DistributedJobConfig.isEmpty(getNumRandomlyShuffledSplits())
       && DistributedJobConfig.isEmpty(getNumInstancesPerShuffledSplit())) {
       logMessage(""[RandomizedDataSparkJob] Must specify either the number of ""
         + ""splits or the number of instances per split"");
       throw new DistributedWekaException(""Must specify either the number of ""
         + ""splits or the number of instances per split"");
     }
 
     if (!DistributedJobConfig.isEmpty(getNumRandomlyShuffledSplits())) {
       numFoldSlices =
         Integer.parseInt(environmentSubstitute(getNumRandomlyShuffledSplits()));
     } else {
       int numInsts = 0;
       try {
         numInsts =
           Integer
             .parseInt(environmentSubstitute(getNumInstancesPerShuffledSplit()));
       } catch (NumberFormatException ex) {
         throw new DistributedWekaException(ex);
       }
 
       if (numInsts <= 0) {
         throw new DistributedWekaException(
           ""Number of instances per split must "" + ""be > 0"");
       }
 
       if (numInsts > totalNumInstances) {
         throw new DistributedWekaException(""Can't have more instances per split ""
           + ""than there are instances in the dataset!"");
       }
       double nc = (double) totalNumInstances / numInsts;
       nc = Math.ceil(nc);
       numFoldSlices = (int) nc;
     }
     logMessage(""[Randomly shuffle] creating "" + numFoldSlices + "" splits."");
 
     if (headerNoSummary.attribute(className).isNominal()) {
       NominalStats stats = NominalStats.attributeToStats(summaryClassAtt);
       for (String label : stats.getLabels()) {
         totalNumInstances += stats.getCount(label);
       }
     } else {
       NumericStats stats = NumericStats.attributeToStats(summaryClassAtt);
       totalNumInstances =
         (int) stats.getStats()[ArffSummaryNumericMetric.COUNT.ordinal()];
     }
 
     logMessage(""[Randomly shuffle data] Num slices = "" + numFoldSlices);
 
     final Random random = new Random(seed);
     for (int i = 0; i < 20; i++) {
       random.nextInt();
     }
 
     performRandomShuffle(inputData, outputPath, numFoldSlices, random,
       headerWithSummary, headerNoSummary.classIndex());
 
     setJobStatus(JobStatus.FINISHED);
 
     return true;
   }",Buggy,"Fixed a bug in the calculation of the total number of instances in the dataset

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@11588 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix in the calculation of the total number of instances in the dataset. The diff shows changes in how the total number of instances is calculated based on whether the attribute is nominal or numeric. Specifically, the code now uses `summaryAttOrig.isNominal()` instead of `summaryAtt.isNominal()` to determine if the original attribute is nominal. Also, the code calculating the total number of instances is duplicated. The first time, the code calculates the total number of instances and the second time it does the same calculation again. This duplication suggests a potential bug fix related to how the total number of instances is determined. The answer is **Buggy**."
weka,4830.json,9202bc66506c46f20388951597366af165de4928,"@@ -1,76 +1,77 @@
   public void processIncoming(Data data) throws WekaException {
 
     if (isStopRequested()) {
       getStepManager().interrupted();
       return;
     }
 
     getStepManager().processing();
 
     Instances predictedInsts = data.getPrimaryPayload();
     int maxSetNum =
       (Integer) data.getPayloadElement(StepManager.CON_AUX_DATA_MAX_SET_NUM);
     int setNum =
       (Integer) data.getPayloadElement(StepManager.CON_AUX_DATA_SET_NUM);
 
     if (maxSetNum > 1 && getPoolSets()) {
       if (m_isReset) {
         m_pooledData = new Instances(predictedInsts);
       } else {
         m_pooledData.addAll(predictedInsts);
       }
     }
 
     m_isReset = false;
 
     if (getPoolSets() && setNum < maxSetNum) {
+      getStepManager().finished();
       return;
     }
 
     if (getPoolSets() && maxSetNum > 1) {
       predictedInsts = m_pooledData;
     }
 
     if (predictedInsts.classIndex() < 0) {
       throw new WekaException(""No class set in the predicted data!"");
     }
 
     int numAttributes = predictedInsts.numAttributes();
     int numClasses = predictedInsts.classAttribute().numValues();
 
     // we always produce a curve for the first label. The user can
     // always choose a label by using a ClassValuePicker step
     Attribute classAtt = predictedInsts.classAttribute();
     Attribute predictedLabelProbAtt =
       predictedInsts.attribute(numAttributes - numClasses);
 
     ArrayList<Prediction> preds = new ArrayList<>();
     for (int i = 0; i < predictedInsts.numInstances(); i++) {
       Instance current = predictedInsts.instance(i);
       double[] dist = new double[numClasses];
       dist[0] = current.value(predictedLabelProbAtt.index());
       double actual = current.classValue();
 
       preds.add(new NominalPrediction(actual, dist, current.weight()));
     }
 
     try {
       Instances curveInsts =
         CalibrationCurveUtils
           .getCalibrationCurveAsInstances(preds, classAtt, 0);
       curveInsts.setRelationName(""__"" + curveInsts.relationName());
 
       Instance zero = curveInsts.remove(curveInsts.numInstances() - 2);
       zero.setWeight(-1);
       curveInsts.add(0, zero);
       curveInsts.lastInstance().setWeight(-1);
       Data output = new Data(StepManager.CON_DATASET, curveInsts);
       output.setPayloadElement(StepManager.CON_AUX_DATA_SET_NUM, 1);
       output.setPayloadElement(StepManager.CON_AUX_DATA_MAX_SET_NUM, 1);
       getStepManager().outputData(output);
     } catch (Exception ex) {
       throw new WekaException(ex);
     }
 
     getStepManager().finished();
   }",Buggy,"Small bug fix in reporting finished state for step

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@14772 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to reporting the finished state of a step. The diff shows that `getStepManager().finished()` is called inside the `if (getPoolSets() && setNum < maxSetNum)` block. This means that the step is incorrectly marked as finished when `getPoolSets()` is true and `setNum` is less than `maxSetNum`. This is a bug because the step is not actually finished processing in this case; it's just waiting for more data. Therefore, the change fixes a bug where the finished state was being reported prematurely. The answer is **Buggy**."
weka,36414.json,81a1e225e8da398eb2d17f911d2de0b246ac22aa,"@@ -1,3 +1,3 @@
     void setUpdateIncrementalClassifier(boolean update) {
-    m_updateIncrementalClassifier = true;
+    m_updateIncrementalClassifier = update;
   }",Buggy,"Fixed a bug in the set method for updating an incremental classifier

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@13045 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix in the `setUpdateIncrementalClassifier` method. The diff shows that the original code always set `m_updateIncrementalClassifier` to `true`, regardless of the input parameter. The corrected code now assigns the input parameter `update` to the member variable, fixing the bug. The answer is **Buggy**."
weka,15017.json,a9246f58e2844ebadfa2c2b12f3f4d15755f5006,"@@ -1,58 +1,59 @@
   protected Instances process(Instances instances) throws Exception {
     // initializing necessary?
     if (!m_Initialized) {
       // do we have a file to initialize with?
       if ((getInitFile() != null) && getInitFile().isFile()) {
 	DataSource source = new DataSource(getInitFile().getAbsolutePath());
 	Instances data = source.getDataSet();
 	m_InitFileClassIndex.setUpper(data.numAttributes() - 1);
 	data.setClassIndex(m_InitFileClassIndex.getIndex());
 	initFilter(data);
       }
       else {
 	initFilter(instances);
       }
     }
 
     // apply filters
     if (m_Missing != null)
       instances = Filter.useFilter(instances, m_Missing); 
     if (m_NominalToBinary != null)
       instances = Filter.useFilter(instances, m_NominalToBinary); 
     if (m_ActualFilter != null)
       instances = Filter.useFilter(instances, m_ActualFilter);
 
     // backup class attribute and remove it
     double[] classes = instances.attributeToDoubleArray(instances.classIndex());
     int classIndex = instances.classIndex();
+    Attribute classAttribute = (Attribute)instances.classAttribute().copy();
     instances.setClassIndex(-1);
     instances.deleteAttributeAt(classIndex);
 
     // generate new header
     FastVector atts = new FastVector();
     for (int j = 0; j < m_NumTrainInstances; j++)
       atts.addElement(new Attribute(""Kernel "" + j));
-    atts.addElement(new Attribute(""Class""));
+    atts.addElement(classAttribute);
     Instances result = new Instances(""Kernel"", atts, 0);
     result.setClassIndex(result.numAttributes() - 1);
 
     // compute matrix
     for (int i = 0; i < instances.numInstances(); i++) {
       double[] k = new double[m_NumTrainInstances + 1];
       
       for (int j = 0; j < m_NumTrainInstances; j++) {
 	double v = m_ActualKernel.eval(-1, j, instances.instance(i));
 	k[j] = v;
       }
       k[k.length - 1] = classes[i];
 
       // create new instance
       Instance in = new DenseInstance(1.0, k);
       result.add(in);    
     }
 
     if (!isFirstBatchDone())
       setOutputFormat(result);
     
     return result;
   }",Buggy,"Fixed bug in KernelFilter: now works with nominal class attributes as well.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@9561 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to the KernelFilter working with nominal class attributes. The diff shows that the code now copies the class attribute before deleting it and re-adds the copied attribute to the new Instances object. This ensures that the class attribute information is preserved when creating the new dataset, which is crucial for nominal class attributes. Without this, the class attribute information would be lost, leading to incorrect classification results. The answer is **Buggy**."
weka,36274.json,8072e35dd2cf69509b7d8d77a0a509d3ff3f5202,"@@ -1,82 +1,86 @@
   protected static void addStepJSONtoFlowArray(JSONNode stepArray,
     StepManagerImpl stepManager) throws WekaException {
 
     JSONNode step = stepArray.addObjectArrayElement();
     step.addPrimitive(""class"", stepManager.getManagedStep().getClass()
       .getCanonicalName());
     // step.addPrimitive(STEP_NAME, stepManager.getManagedStep().getName());
     JSONNode properties = step.addObject(PROPERTIES);
     try {
       Step theStep = stepManager.getManagedStep();
       BeanInfo bi = Introspector.getBeanInfo(theStep.getClass());
       PropertyDescriptor[] stepProps = bi.getPropertyDescriptors();
 
       for (PropertyDescriptor p : stepProps) {
         if (p.isHidden() || p.isExpert()) {
           continue;
         }
 
         String name = p.getDisplayName();
         Method getter = p.getReadMethod();
         Method setter = p.getWriteMethod();
         if (getter == null || setter == null) {
           continue;
         }
         boolean skip = false;
         for (Annotation a : getter.getAnnotations()) {
           if (a instanceof NotPersistable) {
             skip = true;
             break;
           }
         }
         if (skip) {
           continue;
         }
 
         Object[] args = {};
         Object propValue = getter.invoke(theStep, args);
         if (propValue == null) {
           properties.addNull(name);
         } else if (propValue instanceof Boolean) {
           properties.addPrimitive(name, (Boolean) propValue);
         } else if (propValue instanceof Integer || propValue instanceof Long) {
           properties.addPrimitive(name,
             new Integer(((Number) propValue).intValue()));
         } else if (propValue instanceof Double) {
           properties.addPrimitive(name, (Double) propValue);
         } else if (propValue instanceof Number) {
           properties.addPrimitive(name,
             new Double(((Number) propValue).doubleValue()));
         } else if (propValue instanceof weka.core.converters.Loader) {
           addLoader(name, (weka.core.converters.Loader) propValue, properties);
         } else if (propValue instanceof weka.core.converters.Saver) {
           addSaver(name, (weka.core.converters.Saver) propValue, properties);
         } else if (propValue instanceof OptionHandler) {
           addOptionHandler(name, (OptionHandler) propValue, properties);
         } else if (propValue instanceof Enum) {
           addEnum(name, (Enum) propValue, properties);
+        } else if (propValue instanceof File) {
+          String fString = propValue.toString();
+          fString = fString.replace('\\', '/');
+          properties.addPrimitive(name, fString);
         } else {
           properties.addPrimitive(name, propValue.toString());
         }
       }
     } catch (Exception ex) {
       throw new WekaException(ex);
     }
 
     JSONNode connections = step.addObject(CONNECTIONS);
     for (Map.Entry<String, List<StepManager>> e : stepManager.m_connectedByTypeOutgoing
       .entrySet()) {
       String connName = e.getKey();
       JSONNode connTypeArray = connections.addArray(connName);
       for (StepManager c : e.getValue()) {
         connTypeArray.addArrayElement(c.getName());
       }
     }
 
     if (stepManager.getStepVisual() != null) {
       String coords =
         """" + stepManager.getStepVisual().getX() + "",""
           + stepManager.getStepVisual().getY();
       step.addPrimitive(COORDINATES, coords);
     }
   }",Buggy,"Fixed a bug that affected the parsing of step properties involving files containing Windows separator charactors

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@12964 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to parsing step properties that involve files containing Windows separator characters. The diff includes a change that replaces backslashes with forward slashes in file paths before adding them as properties. This replacement addresses the issue of incorrect parsing when Windows-style file paths are encountered. The answer is **Buggy**.
"
weka,26684.json,f36aad315fb901cb71922f7ae08459999978c9d9,"@@ -1,65 +1,83 @@
   public Instances getCurve(FastVector predictions, int classIndex) {
 
     if ((predictions.size() == 0) ||
         (((NominalPrediction)predictions.elementAt(0))
          .distribution().length <= classIndex)) {
       return null;
     }
 
     double totPos = 0, totNeg = 0;
     double [] probs = getProbabilities(predictions, classIndex);
 
     // Get distribution of positive/negatives
     for (int i = 0; i < probs.length; i++) {
       NominalPrediction pred = (NominalPrediction)predictions.elementAt(i);
       if (pred.actual() == Prediction.MISSING_VALUE) {
         System.err.println(getClass().getName() 
                            + "" Skipping prediction with missing class value"");
         continue;
       }
       if (pred.weight() < 0) {
         System.err.println(getClass().getName() 
                            + "" Skipping prediction with negative weight"");
         continue;
       }
       if (pred.actual() == classIndex) {
         totPos += pred.weight();
       } else {
         totNeg += pred.weight();
       }
     }
 
     Instances insts = makeHeader();
     int [] sorted = Utils.sort(probs);
     TwoClassStats tc = new TwoClassStats(totPos, totNeg, 0, 0);
+    double threshold = 0;
+    double cumulativePos = 0;
+    double cumulativeNeg = 0;
     for (int i = 0; i < sorted.length; i++) {
+
+      if ((i == 0) || (probs[sorted[i]] > threshold)) {
+	tc.setTruePositive(tc.getTruePositive() - cumulativePos);
+	tc.setFalseNegative(tc.getFalseNegative() + cumulativePos);
+	tc.setFalsePositive(tc.getFalsePositive() - cumulativeNeg);
+	tc.setTrueNegative(tc.getTrueNegative() + cumulativeNeg);
+	threshold = probs[sorted[i]];
+	insts.add(makeInstance(tc, threshold));
+	cumulativePos = 0;
+	cumulativeNeg = 0;
+	if (i == sorted.length - 1) {
+	  break;
+	}
+      }
+
       NominalPrediction pred = (NominalPrediction)predictions.elementAt(sorted[i]);
+
       if (pred.actual() == Prediction.MISSING_VALUE) {
-        System.err.println(getClass().getName()
-                           + "" Skipping prediction with missing class value"");
-        continue;
+	System.err.println(getClass().getName()
+			   + "" Skipping prediction with missing class value"");
+	continue;
       }
       if (pred.weight() < 0) {
-        System.err.println(getClass().getName() 
-                           + "" Skipping prediction with negative weight"");
-        continue;
+	System.err.println(getClass().getName() 
+			   + "" Skipping prediction with negative weight"");
+	continue;
       }
       if (pred.actual() == classIndex) {
-        tc.setTruePositive(tc.getTruePositive() - pred.weight());
-        tc.setFalseNegative(tc.getFalseNegative() + pred.weight());
+	cumulativePos += pred.weight();
       } else {
-        tc.setFalsePositive(tc.getFalsePositive() - pred.weight());
-        tc.setTrueNegative(tc.getTrueNegative() + pred.weight());
+	cumulativeNeg += pred.weight();
       }
+
       /*
       System.out.println(tc + "" "" + probs[sorted[i]] 
                          + "" "" + (pred.actual() == classIndex));
       */
-      if ((i != (sorted.length - 1)) &&
+      /*if ((i != (sorted.length - 1)) &&
           ((i == 0) ||  
           (probs[sorted[i]] != probs[sorted[i - 1]]))) {
         insts.add(makeInstance(tc, probs[sorted[i]]));
-      }
+	}*/
     }
     return insts;
   }",Buggy,"Fixed bug in ThresholdCurve, which resulted in one instance being on the wrong side of the threshold.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@2134 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix in the ThresholdCurve class, where an instance was incorrectly positioned relative to the threshold. The diff introduces a significant change in the logic for calculating and adding instances to the curve. The code now accumulates positive and negative weights (`cumulativePos`, `cumulativeNeg`) and updates `TwoClassStats` (`tc`) only when the probability changes or at the beginning of the loop. This suggests a correction to the original algorithm, which likely led to the reported bug. The addition of threshold handling and the cumulative calculations strongly indicate a bug fix. The answer is **Buggy**."
weka,27269.json,8e4d3f359b3b7e7182d587aaab8f838daae5c8fa,"@@ -1,11 +1,14 @@
   public double eval(int id1, int id2, Instance inst1) 
     throws Exception {
 	
-    double div = Math.sqrt(super.eval(id1, id1, inst1) * 
-			   super.eval(id2, id2, m_data.instance(id2)));
-    if(div != 0){
+
+    double div = Math.sqrt(super.eval(id1, id1, inst1) * ((m_keys != null)
+                           ? super.eval(id2, id2, m_data.instance(id2))
+                           : super.eval(-1, -1, m_data.instance(id2))));
+
+    if(div != 0){      
       return super.eval(id1, id2, inst1) / div;
     } else {
       return 0;
     }
-  }

+  }",Buggy,"Fixed a bug in NormalizedPolyKernel that resulted in an attempted access to the cache for test instances


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@2490 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix in `NormalizedPolyKernel` related to accessing the cache for test instances. The diff modifies the `eval` method, specifically how the divisor `div` is calculated. The change introduces a conditional check `(m_keys != null)` to determine whether to use `id2` or `-1` as the indices when calling `super.eval`. This suggests that the original code was incorrectly accessing the cache when `m_keys` was null, potentially leading to an out-of-bounds access or incorrect results. The corrected code now uses `-1` as the indices in this case, likely to avoid the problematic cache access. Therefore, the changes indicate a bug fix. The answer is **Buggy**."
weka,30125.json,4f854d24790f7ba7429f74355b69542147801667,"@@ -1,105 +1,104 @@
   public String toSummaryString() {
 
     StringBuffer result = new StringBuffer();
     result.append(""Relation Name:  "").append(relationName()).append('\n');
     result.append(""Num Instances:  "").append(numInstances()).append('\n');
     result.append(""Num Attributes: "").append(numAttributes()).append('\n');
     result.append('\n');
 
     result.append(Utils.padLeft("""", 5)).append(Utils.padRight(""Name"", 25));
     result.append(Utils.padLeft(""Type"", 5)).append(Utils.padLeft(""Nom"", 5));
     result.append(Utils.padLeft(""Int"", 5)).append(Utils.padLeft(""Real"", 5));
     result.append(Utils.padLeft(""Missing"", 12));
     result.append(Utils.padLeft(""Unique"", 12));
     result.append(Utils.padLeft(""Dist"", 6)).append('\n');
     Instances temp = new Instances(this);
     int total = temp.numInstances();
     for (int i = 0; i < numAttributes(); i++) {
       Attribute a = attribute(i);
       temp.sort(i);
       int intCount = 0, realCount = 0, missingCount = 0;
       int distinctCount = 0, uniqueCount = 0, currentCount = 0;
       double prev = Instance.missingValue();
       for (int j = 0; j < temp.numInstances(); j++) {
 	Instance current = temp.instance(j);
 	if (current.isMissing(i)) {
 	  missingCount = temp.numInstances() - j;
 	  break;
 	}
 	if (Utils.eq(current.value(i), prev)) {
 	  currentCount++;
 	} else {
 	  distinctCount++;
 	  if (currentCount == 1) {
 	    uniqueCount++;
 	  }
 	  if (currentCount > 0) {
 	    if (Utils.eq(prev, (double)((int)prev))) {
 	      intCount += currentCount;
 	    } else {
 	      realCount += currentCount;
 	    }
 	  }
 	  currentCount = 1;
 	  prev = current.value(i);
 	}
       }
       if (currentCount == 1) {
 	uniqueCount++;
       }
       if (currentCount > 0) {
 	if (Utils.eq(prev, (double)((int)prev))) {
 	  intCount += currentCount;
 	} else {
 	  realCount += currentCount;
 	}
       }
-      
       result.append(Utils.padLeft("""" + (i + 1), 4)).append(' ');
       result.append(Utils.padRight(a.name(), 25)).append(' ');
-      int percent;
+      long percent;
       switch (a.type()) {
       case Attribute.NOMINAL:
 	result.append(Utils.padLeft(""Nom"", 4)).append(' ');
-	percent = 100 * intCount / total;
+	percent = Math.round(100.0 * intCount / total);
 	result.append(Utils.padLeft("""" + percent, 3)).append(""% "");
 	result.append(Utils.padLeft("""" + 0, 3)).append(""% "");
-	percent = 100 * realCount / total;
+	percent = Math.round(100.0 * realCount / total);
 	result.append(Utils.padLeft("""" + percent, 3)).append(""% "");
 	break;
       case Attribute.NUMERIC:
 	result.append(Utils.padLeft(""Num"", 4)).append(' ');
 	result.append(Utils.padLeft("""" + 0, 3)).append(""% "");
-	percent = 100 * intCount / total;
+	percent = Math.round(100.0 * intCount / total);
 	result.append(Utils.padLeft("""" + percent, 3)).append(""% "");
-	percent = 100 * realCount / total;
+	percent = Math.round(100.0 * realCount / total);
 	result.append(Utils.padLeft("""" + percent, 3)).append(""% "");
 	break;
       case Attribute.STRING:
 	result.append(Utils.padLeft(""Str"", 4)).append(' ');
-	percent = 100 * intCount / total;
+	percent = Math.round(100.0 * intCount / total);
 	result.append(Utils.padLeft("""" + percent, 3)).append(""% "");
 	result.append(Utils.padLeft("""" + 0, 3)).append(""% "");
-	percent = 100 * realCount / total;
+	percent = Math.round(100.0 * realCount / total);
 	result.append(Utils.padLeft("""" + percent, 3)).append(""% "");
 	break;
       default:
 	result.append(Utils.padLeft(""???"", 4)).append(' ');
 	result.append(Utils.padLeft("""" + 0, 3)).append(""% "");
-	percent = 100 * intCount / total;
+	percent = Math.round(100.0 * intCount / total);
 	result.append(Utils.padLeft("""" + percent, 3)).append(""% "");
-	percent = 100 * realCount / total;
+	percent = Math.round(100.0 * realCount / total);
 	result.append(Utils.padLeft("""" + percent, 3)).append(""% "");
 	break;
       }
       result.append(Utils.padLeft("""" + missingCount, 5)).append("" /"");
-      percent = 100 * missingCount / total;
+      percent = Math.round(100.0 * missingCount / total);
       result.append(Utils.padLeft("""" + percent, 3)).append(""% "");
       result.append(Utils.padLeft("""" + uniqueCount, 5)).append("" /"");
-      percent = 100 * uniqueCount / total;
+      percent = Math.round(100.0 * uniqueCount / total);
       result.append(Utils.padLeft("""" + percent, 3)).append(""% "");
       result.append(Utils.padLeft("""" + distinctCount, 5)).append(' ');
       result.append('\n');
     }
     return result.toString();
   }",Buggy,"- Fixed small rounding problem in summary method.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@105 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix for a rounding problem in the summary method. The diff changes the type of the `percent` variable from `int` to `long` and uses `Math.round` to calculate the percentage. This suggests that the original calculation using integer division was causing rounding errors, which is now fixed by using floating-point division and rounding to the nearest integer. The answer is **Buggy**."
weka,24450.json,d1fccde7b3f541c040561dbcfd7c68890d26bfad,"@@ -1,37 +1,37 @@
   public double logDensity(double[] valuePassed) {
     double[] value = valuePassed.clone();
     double logProb = 0;
     // calculate mean subtractions
     double[] subtractedMean = new double[value.length];
     for (int i = 0; i < value.length; i++) {
       subtractedMean[i] = value[i] - mean[i];
     }
-
+    value = subtractedMean.clone();
     double[][] L = this.chol.getL().getArray();
     int n = this.chol.getL().getRowDimension();
     // Solve L*Y = B;
     for (int k = 0; k < this.chol.getL().getRowDimension(); k++) {
       for (int i = 0; i < k; i++) {
         value[k] -= value[i] * L[k][i];
       }
 
       value[k] /= L[k][k];
     }
 
     // Solve L'*X = Y;
     for (int k = n - 1; k >= 0; k--) {
       for (int i = k + 1; i < n; i++) {
         value[k] -= value[i] * L[i][k];
       }
       value[k] /= L[k][k];
     }
 
     // compute dot product
     double innerProduct = 0;
     // do a fast dot product
     for (int i = 0; i < value.length; i++) {
       innerProduct += value[i] * subtractedMean[i];
     }
     logProb = lnconstant - innerProduct * 0.5;
     return logProb;
   }",Buggy,"Bug fixes from Uday

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@10459 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates bug fixes. The code modifies the `logDensity` method. The original code calculates `subtractedMean` and then proceeds to solve linear equations using Cholesky decomposition, storing intermediate results back into the `value` array. However, the dot product calculation at the end uses `value[i] * subtractedMean[i]`. The `value` array has been overwritten during the linear equation solving, so the dot product is calculated using transformed values instead of the original `subtractedMean`. The fix `value = subtractedMean.clone();` copies `subtractedMean` to `value` before the Cholesky decomposition, preserving `subtractedMean` for the final dot product calculation. This correction aligns with the commit message's indication of bug fixes. The answer is **Buggy**."
weka,15350.json,21d3ace55220b46f80de8bfd5ac1a94760fe142b,"@@ -1,39 +1,38 @@
   protected Instances process(Instances instances) throws Exception {
 
     Instances result = getOutputFormat();
 
     for (Instance inst : instances) {
       if (instances.numAttributes() < result.numAttributes()) { // Do we actually need to add an attribute?
         double[] newVals = new double[result.numAttributes()];
         for (int i = 0; i < inst.numValues(); i++) {
           newVals[inst.index(i)] = inst.valueSparse(i);
         }
         String value = """";
         for (int i = 0; i < inst.numAttributes(); i++) {
           if (instances.attribute(i).isNominal() && m_Attributes.isInRange(i) && i != instances.classIndex()) {
             if (Utils.isMissingValue(newVals[i])) {
               value = null;
               break;
             } else {
               value += (value.length() > 0) ? ""_x_"" + instances.attribute(i).value((int) newVals[i]) :
                       instances.attribute(i).value((int) newVals[i]);
             }
           }
         }
         if (value == null) {
           newVals[newVals.length - 1] = Double.NaN;
         } else {
           newVals[newVals.length - 1] = result.attribute(result.numAttributes() - 1).indexOfValue(value);;
         }
-        if (inst instanceof DenseInstance) {
-          result.add(new DenseInstance(inst.weight(), newVals));
-        } else {
-          result.add(new SparseInstance(inst.weight(), newVals));
-        }
+        Instance newInst = inst.copy(newVals);
+        copyValues(newInst, false, inst.dataset(), result);
+        result.add(newInst);
       } else {
+        copyValues(inst, false, inst.dataset(), result);
         result.add(inst);
       }
     }
 
     return result;
   }",Buggy,"Bug fix: CartesianProduct should now work correctly with string and relational attributes.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@15075 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to the CartesianProduct functionality, specifically mentioning its interaction with string and relational attributes. The code diff modifies the `process` method, which seems to be the core logic for handling instances. The changes include:

1.  **Attribute Handling:** The code iterates through the attributes of an instance and constructs a string `value` based on nominal attributes within a specified range. If a missing value is encountered, `value` is set to `null`.
2.  **Missing Value Handling:** If `value` is `null` (due to a missing value), the last attribute in `newVals` is set to `Double.NaN`. Otherwise, it's set to the index of the constructed `value` in the result's attribute.
3.  **Instance Creation:** The code creates a new instance (`newInst`) by copying the original instance (`inst`) and its values (`newVals`). The `copyValues` method is called to ensure proper copying of values between datasets.
4.  **Conditional Instance Addition:** The code checks if the number of attributes in the input instances is less than the number of attributes in the result. If it is, it creates a new instance with the extended attributes. Otherwise, it adds the original instance to the result after copying the values.

The key change here is the introduction of `copyValues` and the more robust handling of attribute values, especially in cases where the number of attributes differs between the input and output datasets. The original code had different logic for dense and sparse instances, which has been replaced with a more unified approach using `inst.copy(newVals)` and `copyValues`. The commit message and the code changes are aligned, and the changes appear to address a bug related to handling different attribute types and missing values in the CartesianProduct operation.

Therefore, the answer is **Buggy**."
weka,32640.json,f8fa07eb4f3b6e38a4952f907df8574bab292919,"@@ -1,3 +1,3 @@
   public static synchronized void loadPackages(boolean verbose) {
-    loadPackages(verbose, true, true);
+    loadPackages(verbose, false, true);
   }",Buggy,"Standard loadPackages() call was not invoking full class discovery due to a bug introduced with the latest changes - fixed.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@10392 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix in the `loadPackages()` method related to class discovery. The diff shows a change in the arguments passed to the `loadPackages()` method, specifically changing the second argument from `true` to `false`. This suggests that the previous value of `true` was causing an issue with full class discovery, and changing it to `false` resolves the problem. The answer is **Buggy**."
weka,32072.json,e68cbb019cd730299b6a056ba88d78a04e771a90,"@@ -1,18 +1,17 @@
   protected void initialize() {
-    String part;
+    String part = """";
     URLClassLoader sysLoader;
     URL[] urls;
 
     m_Cache = new Hashtable<String, HashSet<String>>();
 
     sysLoader = (URLClassLoader) getClass().getClassLoader();
     urls = sysLoader.getURLs();
     for (URL url : urls) {
+      part = url.toString();
       if (VERBOSE) {
         System.out.println(""Classpath-part: "" + part);
       }
-
-      part = url.toString();
       initFromClasspathPart(part);
     }
   }",Buggy,"Fixed a compilation problem that occurs when VERBOSE is set to true.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@11202 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix for a compilation problem when VERBOSE is true. The diff moves the initialization of the `part` variable to the beginning of the `initialize` method and assigns an empty string to it. Then, inside the loop, `part` is assigned the URL string. The original code attempts to print `part` before it is initialized within the loop, which could lead to a compilation error if VERBOSE is true and the compiler requires definite assignment. The corrected code initializes `part` to an empty string, ensuring that it is always assigned before being used. This resolves the compilation issue. The answer is **Buggy**."
weka,21163.json,65f17814a1d6158b5dc0b85d70c32f30efca4908,"@@ -1,18 +1,26 @@
   public EventSetDescriptor [] getEventSetDescriptors() {
     try {
       EventSetDescriptor [] esds =  
       { new EventSetDescriptor(PredictionAppender.class, 
-			       ""dataSet"",
-			       DataSourceListener.class,
-			       ""acceptDataSet""),
-	new EventSetDescriptor(DataSource.class, 
-			       ""instance"",
-			       InstanceListener.class,
-			       ""acceptInstance"")
-         };
+	  ""dataSet"",
+	  DataSourceListener.class,
+      ""acceptDataSet""),
+      new EventSetDescriptor(PredictionAppender.class, 
+	  ""instance"",
+	  InstanceListener.class,
+      ""acceptInstance""),
+      new EventSetDescriptor(PredictionAppender.class, 
+	  ""trainingSet"",
+	  TrainingSetListener.class,
+      ""acceptTrainingSet""),
+      new EventSetDescriptor(PredictionAppender.class, 
+	  ""testSet"",
+	  TestSetListener.class,
+      ""acceptTestSet"")
+      };
       return esds;
     } catch (Exception ex) {
       ex.printStackTrace();
     }
     return null;
   }",Buggy,"Fixed a small bug and added event sets for training at test set events.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@3814 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix and the addition of event sets. The diff shows the addition of two new EventSetDescriptor objects related to ""trainingSet"" and ""testSet"". The try-catch block suggests that an exception might occur during the creation of EventSetDescriptor objects, and the catch block prints the stack trace. The original code only had two EventSetDescriptor objects, and the new code adds two more. The commit message mentions a bug fix, but the diff focuses on adding new event sets. Without more context, it is difficult to determine the nature of the bug fix. However, the commit message explicitly states a bug was fixed, and the code adds functionality. The answer is **Buggy**."
weka,32626.json,1d67da514cfac522915affd97cf003f12258f2f5,"@@ -1,46 +1,47 @@
   protected static void establishMirror() {
     if (m_offline) {
       return;
     }
 
     try {
       String mirrorListURL =
-        ""http://www.cs.waikato.ac.nz/ml/weka/packageMetaDataMirror.txt"";
+        ""https://www.cs.waikato.ac.nz/ml/weka/packageMetaDataMirror.txt"";
 
       URLConnection conn = null;
       URL connURL = new URL(mirrorListURL);
 
       if (PACKAGE_MANAGER.setProxyAuthentication(connURL)) {
         conn = connURL.openConnection(PACKAGE_MANAGER.getProxy());
       } else {
         conn = connURL.openConnection();
       }
 
       conn.setConnectTimeout(10000); // timeout after 10 seconds
       conn.setReadTimeout(10000);
 
       BufferedReader bi =
         new BufferedReader(new InputStreamReader(conn.getInputStream()));
 
       REP_MIRROR = bi.readLine();
 
       bi.close();
       if (REP_MIRROR != null && REP_MIRROR.length() > 0) {
         // use the mirror if it is different from the primary repo
         // and the user hasn't specified an explicit repo via the
         // property
         if (!REP_MIRROR.equals(PRIMARY_REPOSITORY) && !USER_SET_REPO) {
 
           log(weka.core.logging.Logger.Level.INFO,
             ""[WekaPackageManager] Package manager using repository mirror: ""
               + REP_MIRROR);
 
           REP_URL = new URL(REP_MIRROR);
         }
       }
     } catch (Exception ex) {
+      ex.printStackTrace();
       log(weka.core.logging.Logger.Level.WARNING,
         ""[WekaPackageManager] The repository meta data mirror file seems ""
           + ""to be unavailable ("" + ex.getMessage() + "")"");
     }
   }",Buggy,"Package mirror settings now accessed via https instead of http. Fixes an error that gets printed to the log

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@14973 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix for an error that gets printed to the log, and the diff changes the URL scheme from HTTP to HTTPS for accessing package mirror settings. The change to HTTPS addresses a potential security issue and resolves an error related to accessing the package metadata mirror. The addition of `ex.printStackTrace();` suggests debugging or error reporting related to the exception. The answer is **Buggy**."
weka,22555.json,6b82052500cf75da83431ce041e21658720d1816,"@@ -1,21 +1,21 @@
   public void layoutEditor() {
     m_stepToBlockBox.setEditable(true);
 
     StepManager sm = getStepToEdit().getStepManager();
     List<StepManagerImpl> flowSteps =
       getMainPerspective().getCurrentLayout().getFlow().getSteps();
     for (StepManagerImpl smi : flowSteps) {
       m_stepToBlockBox.addItem(smi.getName());
     }
 
     JPanel p = new JPanel(new BorderLayout());
-    p.setBorder(BorderFactory.createTitledBorder(""Choose class attribute""));
+    p.setBorder(BorderFactory.createTitledBorder(""Choose step to wait for""));
     p.add(m_stepToBlockBox, BorderLayout.NORTH);
 
     add(p, BorderLayout.CENTER);
 
     String userSelected = ((Block) getStepToEdit()).getStepToWaitFor();
     if (userSelected != null) {
       m_stepToBlockBox.setSelectedItem(userSelected);
     }
   }",Buggy,"Fixed a bug in a label

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@12606 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to a label. The diff changes the title of a titled border from ""Choose class attribute"" to ""Choose step to wait for"". This change directly corresponds to the commit message's claim of fixing a bug in a label. The answer is **Buggy**."
weka,10201.json,2b9d972bd34de6f7911f39d55f81ab6194452ebe,"@@ -1,14 +1,16 @@
   public Instance getNextInstance(Instances structure) throws IOException {
     if (getRetrieval() == BATCH) {
       throw new IOException(""Cannot mix getting instances in both incremental and batch modes"");
     }
     m_structure = structure;
     setRetrieval(INCREMENTAL);
 
     //Have we read all the data?
     if ((m_currentTimeSlot == 0 && m_dataSet.TDIM == 0) || (m_currentTimeSlot < m_dataSet.TDIM)) {
-      return new SparseInstance(1.0, make1Darray(m_currentTimeSlot++));
+      Instance inst = new SparseInstance(1.0, make1Darray(m_currentTimeSlot++));
+      inst.setDataset(m_structure);
+      return inst;
      } else {
       return null;
     }
   }",Buggy,"Fixed bug in incremental loading mode: instance was not assigned to a dataset.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@12106 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to incremental loading mode where an instance was not being assigned to a dataset. The diff shows that a new `SparseInstance` is created and assigned to the variable `inst`. Then, `inst.setDataset(m_structure)` is called to associate the instance with the dataset structure. This confirms that the bug fix described in the commit message is implemented in the code. The answer is **Buggy**."
weka,23962.json,f48889f6b6237d7493f1e78fd7c3c417b636f27f,"@@ -1,20 +1,24 @@
   protected String doubleToString(double d, int prec) {
     String result;
     int currentPrec;
     int i;
 
     result = Utils.doubleToString(d, prec);
 
+    if (prec <= 0 || Double.isInfinite(d) || Double.isNaN(d)) {
+      return result;
+    }
+
     // decimal point?
     if (result.indexOf(""."") == -1) {
       result += ""."";
     }
 
     // precision so far?
     currentPrec = result.length() - result.indexOf(""."") - 1;
     for (i = currentPrec; i < prec; i++) {
       result += ""0"";
     }
 
     return result;
   }",Buggy,"Fixed bug in output of experiment results occurring when number was NaN (or infinite). In that case, no decimal point and 0s should be added. Also, decimal point is no longer printed if user requests 0 decimal places (e.g., if the user requests precision 0).

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@13955 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to the output of experiment results when the number is NaN or infinite. The diff modifies the `doubleToString` method to handle these cases correctly. Specifically, it adds a condition to return the original result if the precision is less than or equal to 0, or if the number is infinite or NaN. This prevents the addition of a decimal point and zeros in these cases, as described in the commit message. The code also ensures that a decimal point is not printed if the user requests 0 decimal places. Therefore, the changes align with the commit message and indicate a bug fix. The answer is **Buggy**."
weka,17296.json,9e4a9dc3de5bc88ccad6654a7df72b7c8807bb97,"@@ -1,76 +1,86 @@
   private void determineBounds() {
      double value,min,max;
     
     if (m_plotInstances != null && 
 	m_plotInstances.numAttributes() > 0 &&
 	m_plotInstances.numInstances() > 0) {
       // x bounds
       min=Double.POSITIVE_INFINITY;
       max=Double.NEGATIVE_INFINITY;
       if (m_plotInstances.attribute(m_xIndex).isNominal()) {
 	m_minX = 0;
 	m_maxX = m_plotInstances.attribute(m_xIndex).numValues()-1;
       } else {
 	for (int i=0;i<m_plotInstances.numInstances();i++) {
 	  if (!m_plotInstances.instance(i).isMissing(m_xIndex)) {
 	    value = m_plotInstances.instance(i).value(m_xIndex);
 	    if (value < min) {
 	      min = value;
 	    }
 	    if (value > max) {
 	      max = value;
 	    }
 	  }
 	}
 	
+	// handle case where all values are missing
+	if (min == Double.POSITIVE_INFINITY) min = max = 0.0;
+	
 	m_minX = min; m_maxX = max;
 	if (min == max) {
 	  m_maxX += 0.05;
 	  m_minX -= 0.05;
 	}
       }
 
       // y bounds
       min=Double.POSITIVE_INFINITY;
       max=Double.NEGATIVE_INFINITY;
       if (m_plotInstances.attribute(m_yIndex).isNominal()) {
 	m_minY = 0;
 	m_maxY = m_plotInstances.attribute(m_yIndex).numValues()-1;
       } else {
 	for (int i=0;i<m_plotInstances.numInstances();i++) {
 	  if (!m_plotInstances.instance(i).isMissing(m_yIndex)) {
 	    value = m_plotInstances.instance(i).value(m_yIndex);
 	    if (value < min) {
 	      min = value;
 	    }
 	    if (value > max) {
 	      max = value;
 	    }
 	  }
 	}
 	
+	// handle case where all values are missing
+	if (min == Double.POSITIVE_INFINITY) min = max = 0.0;
+
 	m_minY = min; m_maxY = max;
 	if (min == max) {
 	  m_maxY += 0.05;
 	  m_minY -= 0.05;
 	}
       }
       
       // colour bounds
       min=Double.POSITIVE_INFINITY;
       max=Double.NEGATIVE_INFINITY;
 
       for (int i=0;i<m_plotInstances.numInstances();i++) {
 	if (!m_plotInstances.instance(i).isMissing(m_cIndex)) {
 	  value = m_plotInstances.instance(i).value(m_cIndex);
 	  if (value < min) {
 	    min = value;
 	  }
 	  if (value > max) {
 	    max = value;
 	  }
 	}
       }
+
+      // handle case where all values are missing
+      if (min == Double.POSITIVE_INFINITY) min = max = 0.0;
+
       m_minC = min; m_maxC = max;
     }
   }",Buggy,"Fixed bug caused when all numeric values are missing.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@1220 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix for a bug that occurs when all numeric values are missing. The diff introduces checks to handle cases where `min` remains `Double.POSITIVE_INFINITY` after iterating through the instances, indicating that all values for the x, y, and color attributes are missing. In such cases, `min` and `max` are set to 0.0 to avoid issues with subsequent calculations. This aligns with the commit message, indicating a bug fix. The answer is **Buggy**."
weka,20037.json,af57a617ece4a265b0e23698d4b5ab07ebd17dc5,"@@ -1,12 +1,13 @@
   public boolean eventGeneratable(String eventName) {
     if (m_listenee == null) {
       return false;
     }
 
     if (m_listenee instanceof EventConstraints) {
-      if (!((EventConstraints)m_listenee).eventGeneratable(""classifier"")) {
+      if (!((EventConstraints)m_listenee).
+	  eventGeneratable(""batchClassifier"")) {
 	return false;
       }
     }
     return true;
   }",Buggy,"Fixed eventGeneratable bug


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@1631 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to `eventGeneratable`. The code checks if a listener exists and if it's an `EventConstraints` instance. If it is, the original code checks if the event ""classifier"" is generatable. The corrected code checks if ""batchClassifier"" is generatable instead. This suggests that the original code was incorrectly checking for the ""classifier"" event when it should have been checking for the ""batchClassifier"" event, which is a bug. The answer is **Buggy**."
weka,17545.json,9d199c8ab7badc5fe3d5f2680f4caf570d1806f2,"@@ -1,132 +1,133 @@
   private void paintData(Graphics gx) {
 
     for (int j=0;j<m_plots.size();j++) {
       PlotData2D temp_plot = (PlotData2D)(m_plots.elementAt(j));
 
       for (int i=0;i<temp_plot.m_plotInstances.numInstances();i++) {
 	if (temp_plot.m_plotInstances.instance(i).isMissing(m_xIndex) ||
 	    temp_plot.m_plotInstances.instance(i).isMissing(m_yIndex)) {
 	} else {
 	  double x = (temp_plot.m_pointLookup[i][0] + 
 		      temp_plot.m_pointLookup[i][2]);
 	  double y = (temp_plot.m_pointLookup[i][1] + 
 		      temp_plot.m_pointLookup[i][3]);
 
 	  double prevx = 0;
 	  double prevy = 0;
 	  if (i > 0) {
 	    prevx = (temp_plot.m_pointLookup[i - 1][0] + 
 			temp_plot.m_pointLookup[i - 1][2]);
 	    prevy = (temp_plot.m_pointLookup[i - 1][1] + 
 			temp_plot.m_pointLookup[i - 1][3]);
 	  }
 
 	  int x_range = (int)x - m_XaxisStart;
 	  int y_range = (int)y - m_YaxisStart;
 
 	  if (x_range >= 0 && y_range >= 0) {
 	    if (m_drawnPoints[x_range][y_range] == i 
 		|| m_drawnPoints[x_range][y_range] == 0
 		|| temp_plot.m_displayAllPoints == true) {
 	      m_drawnPoints[x_range][y_range] = i;
 	      if (temp_plot.m_plotInstances.attribute(m_cIndex).isNominal()) {
 		if (temp_plot.m_plotInstances.attribute(m_cIndex).numValues() >
 		    m_colorList.size() && 
 		    !temp_plot.m_useCustomColour) {
 		  extendColourMap(temp_plot.m_plotInstances.
 				  attribute(m_cIndex).numValues());
 		}
 
 		Color ci;
 		if (temp_plot.m_plotInstances.instance(i).
 		    isMissing(m_cIndex)) {
 		  ci = Color.gray;
 		} else {
 		  int ind = (int)temp_plot.m_plotInstances.instance(i).
 		    value(m_cIndex);
 		  ci = (Color)m_colorList.elementAt(ind);
 		}
 
 		if (!temp_plot.m_useCustomColour) {
 		  gx.setColor(ci);	    
 		} else {
 		  gx.setColor(temp_plot.m_customColour);
 		}
 
 		if (temp_plot.m_plotInstances.instance(i).
 		    isMissing(m_cIndex)) {
 		  if (temp_plot.m_connectPoints[i] == true) {
 		    drawDataPoint(x,y,prevx,prevy,temp_plot.m_shapeSize[i],
 				  MISSING_SHAPE,gx);
 		  } else {
 		    drawDataPoint(x,y,temp_plot.m_shapeSize[i],
 				  MISSING_SHAPE,gx);
 		  }
 		} else {
 		  if (temp_plot.m_shapeType[i] == CONST_AUTOMATIC_SHAPE) {
 		    if (temp_plot.m_connectPoints[i] == true) {
 		      drawDataPoint(x,y,prevx,prevy,
 				    temp_plot.m_shapeSize[i],j,gx);
 		    } else {
 		      drawDataPoint(x,y,temp_plot.m_shapeSize[i],j,gx);
 		    }
 		  } else {
 		    if (temp_plot.m_connectPoints[i] == true) {
-
+		       drawDataPoint(x,y,prevx,prevy,temp_plot.m_shapeSize[i],
+				     temp_plot.m_shapeType[i],gx);
 		    } else {
-		      drawDataPoint(x,y,prevx,prevy,temp_plot.m_shapeSize[i],
+		      drawDataPoint(x,y,temp_plot.m_shapeSize[i],
 				    temp_plot.m_shapeType[i],gx);
 		    }
 		  }
 		}
 	      } else {
 		double r;
 		Color ci = null;
 		if (!temp_plot.m_plotInstances.instance(i).
 		    isMissing(m_cIndex)) {
 		  r = (temp_plot.m_plotInstances.instance(i).
 		       value(m_cIndex) - m_minC) / (m_maxC - m_minC);
 		  r = (r * 240) + 15;
 		  ci = new Color((int)r,150,(int)(255-r));
 		} else {
 		  ci = Color.gray;
 		}
 		if (!temp_plot.m_useCustomColour) {
 		  gx.setColor(ci);
 		} else {
 		  gx.setColor(temp_plot.m_customColour);
 		}
 		if (temp_plot.m_plotInstances.instance(i).
 		    isMissing(m_cIndex)) {
 		  if (temp_plot.m_connectPoints[i] == true) {
 		    drawDataPoint(x,y,prevx,prevy,temp_plot.m_shapeSize[i],
 				  MISSING_SHAPE,gx);
 		  } else {
 		    drawDataPoint(x,y,temp_plot.m_shapeSize[i],
 				  MISSING_SHAPE,gx);
 		  }
 		} else {
 		  if (temp_plot.m_shapeType[i] == CONST_AUTOMATIC_SHAPE) {
 		    if (temp_plot.m_connectPoints[i] == true) {
 		      drawDataPoint(x,y,prevx,prevy,
 				    temp_plot.m_shapeSize[i],j,gx);
 		    } else {
 		      drawDataPoint(x,y,temp_plot.m_shapeSize[i],j,gx);
 		    }
 		  } else {
 		    if (temp_plot.m_connectPoints[i] == true) {
 		      drawDataPoint(x,y,prevx,prevy,temp_plot.m_shapeSize[i],
 				    temp_plot.m_shapeType[i],gx);
 		    } else {
 		      drawDataPoint(x,y,temp_plot.m_shapeSize[i],
 				    temp_plot.m_shapeType[i],gx);
 		    }
 		  }
 		}
 	      }
 	    }
 	  }
 	}
       }
     }
   }",Buggy,"Fixed bug that caused some lines to be drawn when visualizing
classifier errors.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@977 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to drawing lines when visualizing classifier errors. The diff modifies the `paintData` method, specifically within the nested conditional blocks that determine how data points are drawn. The change involves how `drawDataPoint` is called when `temp_plot.m_connectPoints[i]` is true and `temp_plot.m_shapeType[i]` is not `CONST_AUTOMATIC_SHAPE`. The original code was calling `drawDataPoint` with `prevx` and `prevy` even when not connecting points, which is incorrect. The corrected code now calls `drawDataPoint` correctly based on whether points are to be connected or not. This indicates a bug fix in how lines were being drawn. The answer is **Buggy**."
weka,30087.json,4eeaa9f5f013cff1e7007f66a4529ac662b22751,"@@ -1,46 +1,46 @@
   public void replaceAttributeAt(/* @non_null@ */Attribute att, int position) {
 
     if ((position < 0) || (position > m_Attributes.size())) {
       throw new IllegalArgumentException(""Index out of range"");
     }
 
     // Does the new attribute have a different name?
     if (!att.name().equals(m_Attributes.get(position).name())) {
 
       // Need to check if attribute name already exists
       Attribute candidate = attribute(att.name());
       if ((candidate != null) && (position != candidate.index())) {
         throw new IllegalArgumentException(""Attribute name '"" + att.name()
           + ""' already in use at position #"" + 
           attribute(att.name()).index());
       }
     }
     att = (Attribute) att.copy();
     att.setIndex(position);
 
     ArrayList<Attribute> newList = new ArrayList<Attribute>(m_Attributes.size());
     HashMap<String, Integer> newMap = new HashMap<String, Integer>((int) ((m_Attributes.size() + 1) / 0.75));
     for (int i = 0 ; i < position; i++) {
       Attribute oldAtt = m_Attributes.get(i);
       newList.add(oldAtt);
       newMap.put(oldAtt.name(), i);
     }
     newList.add(att);
     newMap.put(att.name(), position);
     for (int i = position + 1; i < m_Attributes.size(); i++) {
       Attribute newAtt = (Attribute) m_Attributes.get(i);
       newList.add(newAtt);
       newMap.put(newAtt.name(), i);
     }
     m_Attributes = newList;
     m_NamesToAttributeIndices = newMap;
 
     for (int i = 0; i < numInstances(); i++) {
       instance(i).setDataset(null);
       instance(i).setMissing(position);
       instance(i).setDataset(this);
     }
-    if (m_ClassIndex >= position) {
+    if (m_ClassIndex > position) {
       m_ClassIndex++;
     }
   }",Buggy,"Fixed bug in replaceAttributeAt(): class index was incremented when class attribute was replaced.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@12100 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix in the `replaceAttributeAt()` method where the class index was incorrectly incremented when a class attribute was replaced. The code diff shows a change in the condition `m_ClassIndex >= position` to `m_ClassIndex > position`. This suggests that the original code was incrementing the class index even when the replaced attribute was the class attribute itself (i.e., when `m_ClassIndex == position`), which is incorrect. The corrected code only increments the class index if it's greater than the position of the replaced attribute, which aligns with the bug fix described in the commit message. The answer is **Buggy**."
weka,30087.json,e4ae148238732024ecdbbd5e7b3ec87d4457ebb0,"@@ -1,46 +1,46 @@
   public void replaceAttributeAt(/* @non_null@ */Attribute att, int position) {
 
     if ((position < 0) || (position > m_Attributes.size())) {
       throw new IllegalArgumentException(""Index out of range"");
     }
 
     // Does the new attribute have a different name?
     if (!att.name().equals(m_Attributes.get(position).name())) {
 
       // Need to check if attribute name already exists
       Attribute candidate = attribute(att.name());
       if ((candidate != null) && (position != candidate.index())) {
         throw new IllegalArgumentException(""Attribute name '"" + att.name()
           + ""' already in use at position #"" + 
           attribute(att.name()).index());
       }
     }
     att = (Attribute) att.copy();
     att.setIndex(position);
 
     ArrayList<Attribute> newList = new ArrayList<Attribute>(m_Attributes.size());
     HashMap<String, Integer> newMap = new HashMap<String, Integer>((int) ((m_Attributes.size() + 1) / 0.75));
     for (int i = 0 ; i < position; i++) {
       Attribute oldAtt = m_Attributes.get(i);
       newList.add(oldAtt);
       newMap.put(oldAtt.name(), i);
     }
     newList.add(att);
     newMap.put(att.name(), position);
-    for (int i = position; i < m_Attributes.size(); i++) {
+    for (int i = position + 1; i < m_Attributes.size(); i++) {
       Attribute newAtt = (Attribute) m_Attributes.get(i);
       newList.add(newAtt);
       newMap.put(newAtt.name(), i);
     }
     m_Attributes = newList;
     m_NamesToAttributeIndices = newMap;
 
     for (int i = 0; i < numInstances(); i++) {
       instance(i).setDataset(null);
       instance(i).setMissing(position);
       instance(i).setDataset(this);
     }
     if (m_ClassIndex >= position) {
       m_ClassIndex++;
     }
   }",Buggy,"Fixed bug in replaceAttributeAt() in Instances that I introduced in the modifications I made today.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@11339 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix in the `replaceAttributeAt()` method, specifically mentioning that the bug was introduced in recent modifications. The code changes involve replacing an attribute at a given position in a dataset. The loop `for (int i = position + 1; i < m_Attributes.size(); i++)` seems to be the correction, as the original loop `for (int i = position; i < m_Attributes.size(); i++)` would have included the attribute that was just replaced, potentially leading to issues. The surrounding code handles checks for attribute name conflicts, updates the attribute list and map, and updates the instances in the dataset. The change in the loop condition suggests a correction to how the remaining attributes are handled after the replacement, which aligns with the bug fix mentioned in the commit message. The answer is **Buggy**."
weka,29873.json,c5e5a38b7cb456c56a091a2581ec85b5e9c97afa,"@@ -1,16 +1,16 @@
   public double convictionForRule(AprioriItemSet premise,
 				   AprioriItemSet consequence,
 				   int premiseCount,
 				   int consequenceCount) {
     double num = 
-      (double)premiseCount * (double)(m_totalTransactions - consequenceCount) *
+      (double)premiseCount * (double)(m_totalTransactions - consequenceCount) /
        (double)m_totalTransactions;
     double denom = 
       ((premiseCount - consequence.m_counter)+1);
     
     if (num < 0 || denom < 0) {
       System.err.println(""*** ""+num+"" ""+denom);
       System.err.println(""premis count: ""+premiseCount+"" consequence count ""+consequenceCount+"" total trans ""+m_totalTransactions);
     }
     return num / denom;
   }",Buggy,"Fixed bug in the computation of the conviction measure


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@2297 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix in the computation of the conviction measure. The code diff changes the calculation of `num` from multiplication to division: `(double)premiseCount * (double)(m_totalTransactions - consequenceCount) * (double)m_totalTransactions;` becomes `(double)premiseCount * (double)(m_totalTransactions - consequenceCount) / (double)m_totalTransactions;`. This correction suggests a previous error in the formula, which constitutes a bug fix. The answer is **Buggy**."
weka,25075.json,51f46d90e1a9e13dbc38b52a664024090bef89d4,"@@ -1,68 +1,98 @@
   protected void adjustCanopies(double[] densities) {
     if (m_numClustersRequested < 0) {
       assignCanopiesToCanopyCenters();
 
       m_trainingData = new Instances(m_canopies, 0);
       return;
     }
 
     // more canopies than requested?
     if (m_canopies.numInstances() > m_numClustersRequested) {
       int[] sortedIndexes = Utils.stableSort(densities);
 
       Instances finalCanopies = new Instances(m_canopies, 0);
       int count = 0;
       for (int i = sortedIndexes.length - 1; count < m_numClustersRequested; i--) {
         finalCanopies.add(m_canopies.instance(sortedIndexes[i]));
         count++;
       }
 
       m_canopies = finalCanopies;
+      List<double[][]> tempCanopyCenters = new ArrayList<double[][]>();
+      List<double[]> tempT2Dists = new ArrayList<double[]>();
+      List<double[]> tempMissings = new ArrayList<double[]>();
+
+      // make sure that the center sums, densities and missing counts are
+      // aligned with the new canopy list
+      count = 0;
+      for (int i = sortedIndexes.length - 1; count < finalCanopies
+        .numInstances(); i--) {
+        tempCanopyCenters.add(m_canopyCenters.get(sortedIndexes[i]));
+        tempT2Dists.add(m_canopyT2Density.get(sortedIndexes[i]));
+        tempMissings.add(m_canopyNumMissingForNumerics.get(sortedIndexes[i]));
+        count++;
+      }
+      m_canopyCenters = tempCanopyCenters;
+      m_canopyT2Density = tempT2Dists;
+      m_canopyNumMissingForNumerics = tempMissings;
+
     } else if (m_canopies.numInstances() < m_numClustersRequested
       && m_trainingData != null && m_trainingData.numInstances() > 0) {
 
       // make up the difference with randomly selected instances (if possible)
       Random r = new Random(getSeed());
       for (int i = 0; i < 10; i++) {
         r.nextInt();
       }
       HashMap<DecisionTableHashKey, Integer> initC = new HashMap<DecisionTableHashKey, Integer>();
       DecisionTableHashKey hk = null;
 
       // put the existing canopies in the lookup
       for (int i = 0; i < m_canopies.numInstances(); i++) {
         try {
           hk = new DecisionTableHashKey(m_canopies.instance(i),
             m_canopies.numAttributes(), true);
 
           initC.put(hk, null);
         } catch (Exception e) {
           e.printStackTrace();
         }
       }
 
       for (int j = m_trainingData.numInstances() - 1; j >= 0; j--) {
         int instIndex = r.nextInt(j + 1);
         try {
           hk = new DecisionTableHashKey(m_trainingData.instance(instIndex),
             m_trainingData.numAttributes(), true);
         } catch (Exception e) {
           e.printStackTrace();
         }
         if (!initC.containsKey(hk)) {
-          m_canopies.add(m_trainingData.instance(instIndex));
+          Instance newInstance = m_trainingData.instance(instIndex);
+          m_canopies.add(newInstance);
+
+          double[] density = new double[1];
+          density[0] = 1.0;
+          m_canopyT2Density.add(density);
+
+          double[][] center = new double[newInstance.numAttributes()][0];
+          double[] numMissingNumerics = new double[newInstance.numAttributes()];
+          updateCanopyCenter(newInstance, center, numMissingNumerics);
+          m_canopyCenters.add(center);
+          m_canopyNumMissingForNumerics.add(numMissingNumerics);
+
           initC.put(hk, null);
         }
         m_trainingData.swap(j, instIndex);
 
         if (m_canopies.numInstances() == m_numClustersRequested) {
           break;
         }
       }
     }
 
     assignCanopiesToCanopyCenters();
 
     // save memory
     m_trainingData = new Instances(m_canopies, 0);
   }",Buggy,"Fixed a bug that affected aggregation of Canopy clusterers - canopy center sums were not getting adjusted after the final number of canopies was determined.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@10597 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to the aggregation of Canopy clusterers, specifically that canopy center sums were not being adjusted after the final number of canopies was determined.

The code diff shows that when the number of canopies exceeds the requested number of clusters, the code now correctly updates `m_canopyCenters`, `m_canopyT2Density`, and `m_canopyNumMissingForNumerics` to align with the new canopy list. Additionally, when the number of canopies is less than the requested number of clusters, the code adds new canopies and updates the corresponding `m_canopyCenters`, `m_canopyT2Density`, and `m_canopyNumMissingForNumerics` lists.

These changes directly address the bug described in the commit message, ensuring that the canopy center sums are correctly adjusted after the final number of canopies is determined.

The answer is **Buggy**."
weka,37063.json,2a4806e3c2bd860a4e7b0496b5b05ff72dfdf46b,"@@ -1,84 +1,85 @@
   public void start() throws WekaException {
     if (getStepManager().numOutgoingConnections() > 0) {
       weka.datagenerators.DataGenerator generator = getDataGenerator();
       if (getStepManager()
         .numOutgoingConnectionsOfType(StepManager.CON_DATASET) > 0) {
         getStepManager().processing();
         StringWriter output = new StringWriter();
         try {
           generator.setOutput(new PrintWriter(output));
           getStepManager().statusMessage(""Generating..."");
           getStepManager().logBasic(""Generating data"");
           weka.datagenerators.DataGenerator.makeData(generator,
             generator.getOptions());
           Instances instances =
             new Instances(new StringReader(output.toString()));
 
           if (!isStopRequested()) {
             Data outputData = new Data(StepManager.CON_DATASET, instances);
             getStepManager().outputData(outputData);
           }
         } catch (Exception ex) {
           throw new WekaException(ex);
         }
         if (isStopRequested()) {
           getStepManager().interrupted();
         } else {
           getStepManager().finished();
         }
       } else {
         // streaming case
         try {
           if (!generator.getSingleModeFlag()) {
             throw new WekaException(""Generator does not support ""
               + ""incremental generation, so cannot be used with ""
               + ""outgoing 'instance' connections"");
           }
         } catch (Exception ex) {
           throw new WekaException(ex);
         }
         String stm =
           getName() + ""$"" + hashCode() + 99 + ""| overall flow throughput -|"";
         m_flowThroughput =
           new StreamThroughput(stm, ""Starting flow..."",
             ((StepManagerImpl) getStepManager()).getLog());
 
         try {
           getStepManager().logBasic(""Generating..."");
           generator.setDatasetFormat(generator.defineDataFormat());
 
           for (int i = 0; i < generator.getNumExamplesAct(); i++) {
+            m_flowThroughput.updateStart();
             getStepManager().throughputUpdateStart();
             if (isStopRequested()) {
               getStepManager().interrupted();
               return;
             }
 
             // over all examples to be produced
             Instance inst = generator.generateExample();
             m_incrementalData.setPayloadElement(StepManager.CON_INSTANCE, inst);
             getStepManager().throughputUpdateEnd();
 
             getStepManager().outputData(m_incrementalData);
             m_flowThroughput.updateEnd(((StepManagerImpl) getStepManager())
               .getLog());
           }
 
           if (isStopRequested()) {
             ((StepManagerImpl) getStepManager()).getLog().statusMessage(
               stm + ""remove"");
             getStepManager().interrupted();
             return;
           }
           m_flowThroughput.finished(((StepManagerImpl) getStepManager())
             .getLog());
 
           // signal end of input
           m_incrementalData.clearPayload();
           getStepManager().throughputFinished(m_incrementalData);
         } catch (Exception ex) {
           throw new WekaException(ex);
         }
       }
     }
   }",Buggy,"Fixed a bug in the overall throughput status

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@13209 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to the ""overall throughput status"". The diff introduces `m_flowThroughput.updateStart()` at the beginning of the `for` loop in the streaming case. This suggests that the throughput calculation was previously incorrect, likely not accounting for the time spent in each iteration of the loop. By adding `updateStart()` at the beginning, the throughput calculation now includes the time spent generating each instance, leading to a more accurate overall throughput status. The answer is **Buggy**."
weka,33600.json,4fbfba6d434318852d50cf855de7282fd21ceae3,"@@ -1,62 +1,63 @@
   public boolean isCompatibleBaseSystem() throws Exception {
 
     String baseSystemName = m_packageManager.getBaseSystemName();
     String systemVersion = m_packageManager.getBaseSystemVersion().toString();
     // System.err.println(""Base system version "" + systemVersion);
 
-    String dependencies = getPackageMetaDataElement(""Depends"").toString();
+    String dependencies = getPackageMetaDataElement(""Depends"") == null
+      ? null : getPackageMetaDataElement(""Depends"").toString();
     if (dependencies == null) {
       return true;
     }
-
+    
     boolean ok = true;
     StringTokenizer tok = new StringTokenizer(dependencies, "","");
     while (tok.hasMoreTokens()) {
       String nextT = tok.nextToken().trim();
       String[] split = splitNameVersion(nextT);
       if (split[0].startsWith(baseSystemName.toLowerCase())) {
         // check the system version
         if (split[1] != null) {
           if (split.length == 3) {
             VersionPackageConstraint.VersionComparison constraint =
               VersionPackageConstraint.getVersionComparison(split[1]);
             if (!VersionPackageConstraint.checkConstraint(systemVersion,
               constraint, split[2])) {
               ok = false;
               break;
             }
           } else {
             // construct a ""dummy"" package for the base system
             Map<String, String> baseMap = new HashMap<String, String>();
             baseMap.put(""PackageName"", ""weka"");
 
             baseMap.put(""Version"", systemVersion);
             Package basePackage =
               new DefaultPackage(null, m_packageManager, baseMap);
 
             VersionRangePackageConstraint versionRConstraint =
               new VersionRangePackageConstraint(basePackage);
             VersionPackageConstraint.VersionComparison comp1 =
               VersionPackageConstraint.getVersionComparison(split[1]);
             VersionPackageConstraint.VersionComparison comp2 =
               VersionPackageConstraint.getVersionComparison(split[3]);
 
             versionRConstraint.setRangeConstraint(split[2], comp1, split[4],
               comp2);
 
             if (!versionRConstraint.checkConstraint(basePackage)) {
               ok = false;
               break;
             }
           }
           /*
            * int comparisonResult =
            * VersionPackageConstraint.compare(systemVersion, split[2]); ok =
            * versionOK(split[1], comparisonResult); if (!ok) { break; }
            */
         }
       }
     }
 
     return ok;
   }",Buggy,"Fixed a bug in DefaultPackage that would result in npe when checking dependencies for a malformed Description.props (that did not list at least a base Weka dependency).

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@15152 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix for a NullPointerException (NPE) in `DefaultPackage` when checking dependencies for a malformed `Description.props` file that doesn't list a base Weka dependency. The diff shows that the code now checks if `getPackageMetaDataElement(""Depends"")` returns null before attempting to call `toString()` on it. If it's null, the `dependencies` variable is set to null, which prevents the NPE. The answer is **Buggy**."
weka,17605.json,d6ac3ed81063bb608413aa9d346e6e8e8b868874,"@@ -1,220 +1,221 @@
 	  public void mouseClicked(MouseEvent e) {
 	    
 	    if ((m_sIndex == 2 || m_sIndex == 3) && 
 		(m_createShape || 
 		 (e.getModifiers() & e.BUTTON1_MASK) == e.BUTTON1_MASK)) {
 	      if (m_createShape) {
 		//then it has been started already.
 
 		Graphics g = m_plot2D.getGraphics();
 		g.setColor(Color.black);
 		g.setXORMode(Color.white);
-		if ((e.getModifiers() & e.BUTTON1_MASK) == e.BUTTON1_MASK) {
+		if ((e.getModifiers() & e.BUTTON1_MASK) == e.BUTTON1_MASK &&
+                    !e.isAltDown()) {
 		  m_shapePoints.addElement(new 
 		    Double(m_plot2D.convertToAttribX(e.getX())));
 		  
 		  m_shapePoints.addElement(new 
 		    Double(m_plot2D.convertToAttribY(e.getY())));
 		  
 		  m_newMousePos.width = e.getX();
 		  m_newMousePos.height = e.getY();
 		  g.drawLine((int)Math.ceil
 			     (m_plot2D.convertToPanelX
 			      (((Double)m_shapePoints.
 				elementAt(m_shapePoints.size() - 2)).
 			       doubleValue())),
 			     
 			     (int)Math.ceil
 			     (m_plot2D.convertToPanelY
 			      (((Double)m_shapePoints.
 				elementAt(m_shapePoints.size() - 1)).
 			       doubleValue())),
 			     m_newMousePos.width, m_newMousePos.height);
 		  
 		}
 		else if (m_sIndex == 3) {
 		  //then extend the lines to infinity 
 		  //(100000 or so should be enough).
 		  //the area is selected by where the user right clicks 
 		  //the mouse button
 		  
 		  m_createShape = false;
 		  if (m_shapePoints.size() >= 5) {
 		    double cx = Math.ceil
 		      (m_plot2D.convertToPanelX
 		       (((Double)m_shapePoints.elementAt
 			 (m_shapePoints.size() - 4)).doubleValue()));
 		    
 		    double cx2 = Math.ceil
 		      (m_plot2D.convertToPanelX
 		       (((Double)m_shapePoints.elementAt
 			 (m_shapePoints.size() - 2)).doubleValue())) - 
 		      cx;
 		    
 		    cx2 *= 50000;
 		    
 		    double cy = Math.ceil
 		      (m_plot2D.
 		       convertToPanelY(((Double)m_shapePoints.
 					elementAt(m_shapePoints.size() - 3)).
 				       doubleValue()));
 		    double cy2 = Math.ceil
 		      (m_plot2D.convertToPanelY(((Double)m_shapePoints.
 					  elementAt(m_shapePoints.size() - 1)).
 					  doubleValue())) - cy;
 		    cy2 *= 50000;
 			    
 		    
 		    double cxa = Math.ceil(m_plot2D.convertToPanelX
 					   (((Double)m_shapePoints.
 					     elementAt(3)).
 					    doubleValue()));
 		    double cxa2 = Math.ceil(m_plot2D.convertToPanelX
 					    (((Double)m_shapePoints.
 					      elementAt(1)).
 					     doubleValue())) - cxa;
 		    cxa2 *= 50000;
 		    
 		    
 		    double cya = Math.ceil
 		      (m_plot2D.convertToPanelY
 		       (((Double)m_shapePoints.elementAt(4)).
 			doubleValue()));
 		    double cya2 = Math.ceil
 		      (m_plot2D.convertToPanelY
 		       (((Double)m_shapePoints.elementAt(2)).
 			doubleValue())) - cya;
 		    
 		    cya2 *= 50000;
 		    
 		    m_shapePoints.setElementAt
 		      (new Double(m_plot2D.convertToAttribX(cxa2 + cxa)), 1);
 		    
 		    m_shapePoints.setElementAt
 		      (new Double(m_plot2D.convertToAttribY(cy2 + cy)), 
 		       m_shapePoints.size() - 1);
 		    
 		    m_shapePoints.setElementAt
 		      (new Double(m_plot2D.convertToAttribX(cx2 + cx)), 
 		       m_shapePoints.size() - 2);
 		    
 		    m_shapePoints.setElementAt
 		      (new Double(m_plot2D.convertToAttribY(cya2 + cya)), 2);
 		    
 		    
 		    //determine how infinity line should be built
 		    
 		    cy = Double.POSITIVE_INFINITY;
 		    cy2 = Double.NEGATIVE_INFINITY;
 		    if (((Double)m_shapePoints.elementAt(1)).
 			doubleValue() > 
 			((Double)m_shapePoints.elementAt(3)).
 			doubleValue()) {
 		      if (((Double)m_shapePoints.elementAt(2)).
 			  doubleValue() == 
 			  ((Double)m_shapePoints.elementAt(4)).
 			  doubleValue()) {
 			cy = ((Double)m_shapePoints.elementAt(2)).
 			  doubleValue();
 		      }
 		    }
 		    if (((Double)m_shapePoints.elementAt
 			 (m_shapePoints.size() - 2)).doubleValue() > 
 			((Double)m_shapePoints.elementAt
 			 (m_shapePoints.size() - 4)).doubleValue()) {
 		      if (((Double)m_shapePoints.elementAt
 			   (m_shapePoints.size() - 3)).
 			  doubleValue() == 
 			  ((Double)m_shapePoints.elementAt
 			   (m_shapePoints.size() - 1)).doubleValue()) {
 			cy2 = ((Double)m_shapePoints.lastElement()).
 			  doubleValue();
 		      }
 		    }
 		    m_shapePoints.addElement(new Double(cy));
 		    m_shapePoints.addElement(new Double(cy2));
 		    
 		    if (!inPolyline(m_shapePoints, m_plot2D.convertToAttribX
 				    (e.getX()), 
 				    m_plot2D.convertToAttribY(e.getY()))) {
 		      Double tmp = (Double)m_shapePoints.
 			elementAt(m_shapePoints.size() - 2);
 		      m_shapePoints.setElementAt
 			(m_shapePoints.lastElement(), 
 			 m_shapePoints.size() - 2);
 		      m_shapePoints.setElementAt
 			(tmp, m_shapePoints.size() - 1);
 		    }
 		    
 		    if (m_shapes == null) {
 		      m_shapes = new FastVector(4);
 		    }
 		    m_shapes.addElement(m_shapePoints);
 
 		    m_submit.setText(""Submit"");
 		    m_submit.setActionCommand(""Submit"");
 		    
 		    m_submit.setEnabled(true);
 		  }
 		  
 		  m_shapePoints = null;
 		  PlotPanel.this.repaint();
 		  
 		}
 		else {
 		  //then close the shape
 		  m_createShape = false;
 		  if (m_shapePoints.size() >= 7) {
 		    m_shapePoints.addElement(m_shapePoints.elementAt(1));
 		    m_shapePoints.addElement(m_shapePoints.elementAt(2));
 		    if (m_shapes == null) {
 		      m_shapes = new FastVector(4);
 		    }
 		    m_shapes.addElement(m_shapePoints);
 			   
 		    m_submit.setText(""Submit"");
 		    m_submit.setActionCommand(""Submit"");
 		    
 		    m_submit.setEnabled(true);
 		  }
 		  m_shapePoints = null;
 		  PlotPanel.this.repaint();
 		}
 		g.dispose();
 		//repaint();
 	      }
 	      else if ((e.getModifiers() & e.BUTTON1_MASK) == e.BUTTON1_MASK) {
 		//then this is the first point
 		m_createShape = true;
 		m_shapePoints = new FastVector(17);
 		m_shapePoints.addElement(new Double(m_sIndex));
 		m_shapePoints.addElement(new 
 		  Double(m_plot2D.convertToAttribX(e.getX()))); //the new point
 		m_shapePoints.addElement(new 
 		  Double(m_plot2D.convertToAttribY(e.getY())));
 		m_newMousePos.width = e.getX();      //the temp mouse point
 		m_newMousePos.height = e.getY();
 
 		Graphics g = m_plot2D.getGraphics();
 		g.setColor(Color.black);
 		g.setXORMode(Color.white);
 		g.drawLine((int)Math.ceil
 			   (m_plot2D.convertToPanelX(((Double)m_shapePoints.
 					     elementAt(1)).doubleValue())),
 			   (int)Math.ceil
 			   (m_plot2D.convertToPanelY(((Double)m_shapePoints.
 					     elementAt(2)).doubleValue())),
 			   m_newMousePos.width, m_newMousePos.height);
 		g.dispose();
 	      }
 	    }
 	    else {
 	      if ((e.getModifiers() & InputEvent.BUTTON1_MASK) == 
 		  InputEvent.BUTTON1_MASK) {
 		
 		m_plot2D.searchPoints(e.getX(),e.getY(), false);
 	      } else {
 		m_plot2D.searchPoints(e.getX(), e.getY(), true);
 	      }
 	    }
 	  }",Buggy,"Fixed bug that prevented polyline and polygon selection options from working with single button mice. The Alt key can now be held down to simulate button 2


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@2621 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to polyline and polygon selection with single-button mice. The diff introduces `!e.isAltDown()` to the condition `(e.getModifiers() & e.BUTTON1_MASK) == e.BUTTON1_MASK`. This change allows the Alt key to simulate a second mouse button, addressing the reported bug. The answer is **Buggy**."
weka,20397.json,f2a4c20a55fa0e3b48af5bb5a2cb98a33f245eab,"@@ -1,24 +1,24 @@
   private void setupRendererOptsTipText(JLabel optsLab) {
     String renderer = m_rendererCombo.getSelectedItem().toString();
     if (renderer.equalsIgnoreCase(""weka chart renderer"")) {
       // built-in renderer
       WekaOffscreenChartRenderer rcr = new WekaOffscreenChartRenderer();
       String tipText = rcr.optionsTipTextHTML();
       tipText = tipText.replace(""<html>"", ""<html>Comma separated list of options:<br>"");
       optsLab.setToolTipText(tipText);
     } else {
       try {
-        Object rendererO = PluginManager.getPluginInstance(""weka.gui.beans.OffscreenChartRender"",
+        Object rendererO = PluginManager.getPluginInstance(""weka.gui.beans.OffscreenChartRenderer"",
             renderer);
 
         if (rendererO != null) {
           String tipText = ((OffscreenChartRenderer)rendererO).optionsTipTextHTML();
           if (tipText != null && tipText.length() > 0) {
             optsLab.setToolTipText(tipText);
           }
         }
       } catch (Exception ex) {
 
       }
     }
   }",Buggy,"Fixed a bug in the routine that sets the tool tip for additional options in plugin renderers.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@7691 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to setting tooltips for additional options in plugin renderers. The code change involves retrieving a plugin instance using `PluginManager.getPluginInstance`. The original code had a typo, searching for ""weka.gui.beans.OffscreenChartRender"" instead of ""weka.gui.beans.OffscreenChartRenderer"". This typo would prevent the correct plugin from being loaded, leading to an incorrect or missing tooltip. The corrected code fixes this typo, ensuring the correct plugin is loaded and the tooltip is set properly. The answer is **Buggy**."
weka,23551.json,0f79454f25618e64781b12a24a482ef8a0441bca,"@@ -1,44 +1,44 @@
   public Instance generateExample() throws Exception {
     Instance result;
     Random rand;
     double x;
     double y;
     double[] atts;
     Instance inst;
 
-    result = null;
     rand = getRandom();
 
     if (m_DatasetFormat == null) {
       throw new Exception(""Dataset format not defined."");
     }
 
     // random x
     x = rand.nextDouble();
     // fit into range
     x = x * (getMaxRange() - getMinRange()) + getMinRange();
 
     // generate y
     atts = new double[1];
     atts[0] = x;
     inst = new DenseInstance(1.0, atts);
+    inst.setDataset(m_RawData);
     m_Filter.input(inst);
     m_Filter.batchFinished();
     inst = m_Filter.output();
 
     // noise
     y = inst.value(1) + getAmplitude() * m_NoiseRandom.nextGaussian()
       * getNoiseRate() * getNoiseVariance();
 
     // generate attributes
     atts = new double[m_DatasetFormat.numAttributes()];
 
     atts[0] = x;
     atts[1] = y;
     result = new DenseInstance(1.0, atts);
 
     // dataset reference
     result.setDataset(m_DatasetFormat);
 
     return result;
   }",Buggy,"Fixed problem that caused assertion to fail when executing unit test.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@11504 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix for a failing unit test assertion. The diff shows that the line `inst.setDataset(m_RawData);` was added. Without this line, the filter might not work correctly, leading to incorrect output and a failing assertion in the unit test. This indicates a bug fix. The answer is **Buggy**.
"
weka,21037.json,05cc4c3b0752474605f5ea11f2c85e4a9c3d6043,"@@ -1,16 +1,37 @@
   public boolean eventGeneratable(String eventName) {
 
-    if (!m_listeneeTypes.contains(eventName)) {
-      return false;
+    if (eventName.equals(""instance"")) {
+
+      if (!m_listeneeTypes.contains(eventName)) {
+        return false;
+      }
+
+      for (Object listenee : m_listenees.values()) {
+        if (listenee instanceof EventConstraints
+          && !((EventConstraints) listenee).eventGeneratable(eventName)) {
+          return false;
+        }
+      }
     }
 
-    for (Object listenee : m_listenees.values()) {
-      if (listenee instanceof EventConstraints) {
-        if (!((EventConstraints) listenee).eventGeneratable(eventName)) {
-          return false;
+    if (eventName.equals(""dataSet"") || eventName.equals(""trainingSet"")
+      || eventName.equals(""testSet"")) {
+
+      if (!m_listeneeTypes.contains(""dataSet"")
+        && !m_listeneeTypes.contains(""trainingSet"")
+        && !m_listeneeTypes.contains(""testSet"")) {
+        return false;
+      }
+      for (Object listenee : m_listenees.values()) {
+        if (listenee instanceof EventConstraints) {
+          if (!((EventConstraints) listenee).eventGeneratable(""dataSet"")
+            && !((EventConstraints) listenee).eventGeneratable(""trainingSet"")
+            && !((EventConstraints) listenee).eventGeneratable(""testSet"")) {
+            return false;
+          }
         }
       }
     }
 
     return true;
   }",Buggy,"Fixed a bug in the specification of event types produced by Appender and in the routine that determines whether a particular event type can be generated at a given point in time.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@10149 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix for a bug related to event types produced by Appender and the routine determining event generation. The code changes in `eventGeneratable` method introduce specific checks for ""instance"", ""dataSet"", ""trainingSet"", and ""testSet"" events. It appears the original logic was too general, and the fix adds specific handling for these event types, including checks against `EventConstraints` for each listener. This suggests that the original implementation had a bug where these specific event types were not being handled correctly, or were being incorrectly filtered. The answer is **Buggy**.
"
weka,23139.json,50e14ddcea9246c4b6c83142b493163b1e9b862c,"@@ -1,27 +1,42 @@
   protected void updateCapabilitiesFilter(Capabilities filter) {
     Instances 		tempInst;
     Capabilities 	filterClass;
 
     if (filter == null) {
       m_AttributeEvaluatorEditor.setCapabilitiesFilter(new Capabilities(null));
       m_AttributeSearchEditor.setCapabilitiesFilter(new Capabilities(null));
       return;
     }
     
     if (!ExplorerDefaults.getInitGenericObjectEditorFilter())
       tempInst = new Instances(m_Instances, 0);
     else
       tempInst = new Instances(m_Instances);
     tempInst.setClassIndex(m_ClassCombo.getSelectedIndex());
 
     try {
       filterClass = Capabilities.forInstances(tempInst);
     }
     catch (Exception e) {
       filterClass = new Capabilities(null);
     }
     
     // set new filter
     m_AttributeEvaluatorEditor.setCapabilitiesFilter(filterClass);
     m_AttributeSearchEditor.setCapabilitiesFilter(filterClass);
+    
+    m_StartBut.setEnabled(true);
+    // check capabilities...
+    Capabilities currentFilter = m_AttributeEvaluatorEditor.getCapabilitiesFilter();
+    ASEvaluation evaluator = (ASEvaluation) m_AttributeEvaluatorEditor.getValue();
+    Capabilities currentSchemeCapabilities =  null;
+    if (evaluator != null && currentFilter != null && 
+        (evaluator instanceof CapabilitiesHandler)) {
+      currentSchemeCapabilities = ((CapabilitiesHandler)evaluator).getCapabilities();
+      
+      if (!currentSchemeCapabilities.supportsMaybe(currentFilter) &&
+          !currentSchemeCapabilities.supports(currentFilter)) {
+        m_StartBut.setEnabled(false);
+      }
+    }
   }",Buggy,"Fixed a bug where the enabled/disabled state of the start button was not being updated when a new data set was set on this panel.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@5385 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix for the start button's enabled/disabled state not updating correctly when a new dataset is set. The diff shows that the `updateCapabilitiesFilter` method is modified to update the start button's state based on the capabilities of the selected attribute evaluator and the current filter. It checks if the evaluator supports the current filter and disables the start button if it doesn't. This aligns with the commit message, indicating a bug fix. The answer is **Buggy**.
"
xerces2-j,5527.json,2c3c54c0011dbb81897f1f1484b2a1da2d9bce5a,"@@ -1,3 +1,3 @@
     protected boolean versionSupported(String version) {
-        return version.equals(""1.1"");
+        return (version.equals(""1.1"") || version.equals(""1.0""));
     } // versionSupported(String):  boolean",Buggy,"fix for bug 18789; thanks to Neil Delima for the patch


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@319325 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for bug 18789, and the diff modifies the `versionSupported` method to now also return true if the version is ""1.0"". This suggests that the original code only supported version ""1.1"", which was a bug. The updated code now correctly supports both ""1.1"" and ""1.0"". The answer is **Buggy**."
xerces2-j,1404.json,4e11453e19d58d4481d2367028b0cb86c0deafeb,"@@ -1,62 +1,66 @@
 	protected void checkUnboundNamespacePrefixedNode (Node node) throws IOException{
 
 		if (fNamespaces) {
 
    			if (DEBUG) {
 			    System.out.println(""==>serializeNode(""+node.getNodeName()+"") [Entity Reference - Namespaces on]"");
 				System.out.println(""==>Declared Prefix Count: "" + fNSBinder.getDeclaredPrefixCount());
 				System.out.println(""==>Node Name: "" + node.getNodeName());
 				System.out.println(""==>First Child Node Name: "" + node.getFirstChild().getNodeName());
 				System.out.println(""==>First Child Node Prefix: "" + node.getFirstChild().getPrefix());
 				System.out.println(""==>First Child Node NamespaceURI: "" + node.getFirstChild().getNamespaceURI());			
 			}
 
 		
 			Node child, next;
 	        for (child = node.getFirstChild(); child != null; child = next) {
 	            next = child.getNextSibling();
 			    if (DEBUG) {
 			        System.out.println(""==>serializeNode(""+child.getNodeName()+"") [Child Node]"");
 			        System.out.println(""==>serializeNode(""+child.getPrefix()+"") [Child Node Prefix]"");
 	            }    
 	
 		 	    //If a NamespaceURI is not declared for the current
 		 	    //node's prefix, raise a fatal error.
 		 	    String prefix = child.getPrefix();
+                prefix = (prefix == null || 
+                        prefix.length() == 0) ? XMLSymbols.EMPTY_STRING : fSymbolTable.addSymbol(prefix);
 		 	    if (fNSBinder.getURI(prefix) == null && prefix != null) {
 					fatalError(""The replacement text of the entity node '"" 
 								+ node.getNodeName()  
 								+ ""' contains an element node '"" 
 								+ child.getNodeName() 
 								+ ""' with an undeclared prefix '"" 
 								+ prefix + ""'."");
 		 	    }	
 
 				if (child.getNodeType() == Node.ELEMENT_NODE) {
 					
 					NamedNodeMap attrs = child.getAttributes();
 					
 					for (int i = 0; i< attrs.getLength(); i++ ) {
 						
 				 	    String attrPrefix = attrs.item(i).getPrefix();
+                        attrPrefix = (attrPrefix == null || 
+                                attrPrefix.length() == 0) ? XMLSymbols.EMPTY_STRING : fSymbolTable.addSymbol(attrPrefix);
 				 	    if (fNSBinder.getURI(attrPrefix) == null && attrPrefix != null) {
 							fatalError(""The replacement text of the entity node '"" 
 										+ node.getNodeName()  
 										+ ""' contains an element node '"" 
 										+ child.getNodeName() 
 										+ ""' with an attribute '"" 
 										+ attrs.item(i).getNodeName() 										
 										+ ""' an undeclared prefix '"" 
 										+ attrPrefix + ""'."");
 				 	    }	
 						
 					}	
 
 				}
 					
 				if (child.hasChildNodes()) {
 					checkUnboundNamespacePrefixedNode(child);
 				}	
 	        }
 		}    
 	}",Buggy,"Fixing JIRA Bug #1043:
http://issues.apache.org/jira/browse/XERCESJ-1043

Looking up a namespace URI from a NamespaceContext requires that we pass in the interned
String from the SymbolTable as the prefix. We were failing to do that when checking for unbound
prefixes. Should be fixed now.


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@320220 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for JIRA bug #1043, which involves ensuring that the prefix used to look up a namespace URI from a NamespaceContext is an interned String from the SymbolTable. The code diff modifies the `checkUnboundNamespacePrefixedNode` method to intern the `prefix` and `attrPrefix` using `fSymbolTable.addSymbol(prefix)` before checking if the URI is bound. This ensures that the prefix used for lookup is indeed the interned string, addressing the bug described in the commit message. The answer is **Buggy**."
xerces2-j,8183.json,a01d7a8db8b4fb1ff2a2f8c62736b56bb2d84d63,"@@ -1,12 +1,14 @@
-    public boolean equals(XMLGrammarDescription desc) {
+    public boolean equals(Object descObj) {
+        if(!(descObj instanceof XMLGrammarDescription)) return false;
+        XMLGrammarDescription desc = (XMLGrammarDescription)descObj;
         if (!getGrammarType().equals(desc.getGrammarType())) {
             return false;
         }
         if (fTargetNamespace != null && fTargetNamespace.equals(((XSDDescription)desc).getTargetNamespace())) {
             return true;
         }
         else if (fTargetNamespace == null && ((XSDDescription)desc).getTargetNamespace() == null) {
             return true;
         }
         return false;
     }",Buggy,"fix a couple of bugs with schema preparsing:  make sure the error reporter knows about schemas; and make sure XSDDescription .equals method actually works in contexts such as Hashtables (the parameter should be an Object, not an XMLGrammarDescription).


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@318470 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates fixes for two bugs related to schema preparsing. One bug involves ensuring the error reporter knows about schemas, and the other concerns the `XSDDescription.equals` method.

The provided diff focuses on the `equals` method. The original method signature took an `XMLGrammarDescription` as a parameter, which is not the standard `equals` method signature (which should take an `Object`). This would prevent the method from working correctly in contexts like `Hashtable` or `HashMap`, where the `equals` method is used to compare objects.

The corrected code changes the parameter type to `Object`, performs a type check using `instanceof`, and casts the object to `XMLGrammarDescription` if it's of the correct type. This ensures that the `equals` method can be used correctly in various contexts, resolving the bug described in the commit message.

Therefore, the changes in the diff align with the commit message and indicate a bug fix. The answer is **Buggy**.
"
xerces2-j,8285.json,f33e487490b168dd0b29e7df099a1832fb79ee79,"@@ -1,84 +1,85 @@
     private int scanMixed(QName element) throws Exception {
 
         int valueIndex = -1;  // -1 is special value for #PCDATA
         int prevNodeIndex = -1;
         boolean starRequired = false;
         int[] valueSeen = new int[32];
         int valueCount = 0;
         boolean dupAttrType = false;
         int nodeIndex = -1;
 
         while (true) {
             if (fValidationEnabled) {
                 for (int i=0; i<valueCount;i++) {
                     if ( valueSeen[i] == valueIndex) {
                         dupAttrType = true;
                         break;
                     }
                 }
             }
             if (dupAttrType && fValidationEnabled) {
                 reportRecoverableXMLError(XMLMessages.MSG_DUPLICATE_TYPE_IN_MIXED_CONTENT,
                                           XMLMessages.VC_NO_DUPLICATE_TYPES,
                                           valueIndex);
                 dupAttrType = false;
 
             }
             else {
                 try {
                     valueSeen[valueCount] = valueIndex;
                 }
                 catch (ArrayIndexOutOfBoundsException ae) {
                     int[] newArray = new int[valueSeen.length*2];
                     System.arraycopy(valueSeen,0,newArray,0,valueSeen.length);
+                    valueSeen = newArray;
                     valueSeen[valueCount] = valueIndex;
                 }
                 valueCount++;
 
                 nodeIndex = fDTDGrammar.addUniqueLeafNode(valueIndex);
             }
 
             checkForPEReference(false);
             if (!fEntityReader.lookingAtChar('|', true)) {
                 if (!fEntityReader.lookingAtChar(')', true)) {
                     reportFatalXMLError(XMLMessages.MSG_CLOSE_PAREN_REQUIRED_IN_MIXED,
                                         XMLMessages.P51_CLOSE_PAREN_REQUIRED,
                                         element.rawname);
                     return -1;
                 }
                 decreaseParenDepth();
                 if (nodeIndex == -1) {
                     nodeIndex = prevNodeIndex;
                 } else if (prevNodeIndex != -1) {
                     nodeIndex = fDTDGrammar.addContentSpecNode(XMLContentSpec.CONTENTSPECNODE_CHOICE, prevNodeIndex, nodeIndex);
                 }
                 if (fEntityReader.lookingAtChar('*', true)) {
                     nodeIndex = fDTDGrammar.addContentSpecNode(XMLContentSpec.CONTENTSPECNODE_ZERO_OR_MORE, nodeIndex);
                 } else if (starRequired) {
                     reportFatalXMLError(XMLMessages.MSG_MIXED_CONTENT_UNTERMINATED,
                                         XMLMessages.P51_UNTERMINATED,
                                         fStringPool.toString(element.rawname),
                                         fDTDGrammar.getContentSpecNodeAsString(nodeIndex));
                     return -1;
                 }
                 return nodeIndex;
             }
             if (nodeIndex != -1) {
                 if (prevNodeIndex != -1) {
                     nodeIndex = fDTDGrammar.addContentSpecNode(XMLContentSpec.CONTENTSPECNODE_CHOICE, prevNodeIndex, nodeIndex);
                 }
                 prevNodeIndex = nodeIndex;
             }
             starRequired = true;
             checkForPEReference(false);
             checkForElementTypeWithPEReference(fEntityReader, ')', fElementRefQName);
             valueIndex = fElementRefQName.rawname;
             if (valueIndex == -1) {
                 reportFatalXMLError(XMLMessages.MSG_ELEMENT_TYPE_REQUIRED_IN_MIXED_CONTENT,
                                     XMLMessages.P51_ELEMENT_TYPE_REQUIRED,
                                     element.rawname);
                 return -1;
             }
         }
 
     } // scanMixed(QName):int",Buggy,"Bug - fix is to assign the array reference


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@315690 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message ""Bug - fix is to assign the array reference"" indicates a bug fix related to an array. The diff shows an `ArrayIndexOutOfBoundsException` being caught. Inside the `catch` block, a new array `newArray` is created with double the size of the original `valueSeen` array. The contents of `valueSeen` are copied to `newArray`. The critical part is the line `valueSeen = newArray;`, which assigns the reference of the new array to `valueSeen`. Without this assignment, the `valueSeen` array would remain the same size, and the loop would continue to throw `ArrayIndexOutOfBoundsException` when `valueCount` exceeds the original size. The subsequent line `valueSeen[valueCount] = valueIndex;` then correctly uses the newly allocated array. This confirms that the change fixes a bug where the array was not being resized properly, leading to an `ArrayIndexOutOfBoundsException`. The answer is **Buggy**."
xerces2-j,5598.json,c08116e59d73beba99136a5c0d0b920e74a6c891,"@@ -1,33 +1,33 @@
     synchronized RangeToken getCaseInsensitiveToken() {
         if (this.icaseCache != null)
             return this.icaseCache;
             
         RangeToken uppers = this.type == Token.RANGE ? Token.createRange() : Token.createNRange();
         for (int i = 0;  i < this.ranges.length;  i += 2) {
             for (int ch = this.ranges[i];  ch <= this.ranges[i+1];  ch ++) {
                 if (ch > 0xffff)
                     uppers.addRange(ch, ch);
                 else {
                     char uch = Character.toUpperCase((char)ch);
                     uppers.addRange(uch, uch);
                 }
             }
         }
         RangeToken lowers = this.type == Token.RANGE ? Token.createRange() : Token.createNRange();
         for (int i = 0;  i < uppers.ranges.length;  i += 2) {
             for (int ch = uppers.ranges[i];  ch <= uppers.ranges[i+1];  ch ++) {
                 if (ch > 0xffff)
                     lowers.addRange(ch, ch);
                 else {
-                    char uch = Character.toUpperCase((char)ch);
+                    char uch = Character.toLowerCase((char)ch);
                     lowers.addRange(uch, uch);
                 }
             }
         }
         lowers.mergeRanges(uppers);
         lowers.mergeRanges(this);
         lowers.compactRanges();
 
         this.icaseCache = lowers;
         return lowers;
     }",Buggy,"Fix a problem with range tokens where lower case characters where not added (in a case insensitive mode)

git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@827769 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,The commit message indicates a fix for range tokens where lowercase characters were not being added in case-insensitive mode. The diff shows that the code was modified to add lowercase characters to the `lowers` RangeToken. The original code was incorrectly converting the characters to uppercase instead of lowercase. This change directly addresses the bug described in the commit message. The answer is **Buggy**.
xerces2-j,2798.json,c12fd694626841e15a23993537a61d1ef75260c4,"@@ -1,3 +1,6 @@
     public TypeInfo getSchemaTypeInfo(){
-      return this;
+        if(needsSyncData()) {
+            synchronizeData();
+        }
+        return this;
     }",Buggy,"A TypeInfo.getTypeName(...) bug fix.


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@320508 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix in `TypeInfo.getTypeName(...)`. The diff shows that the `getSchemaTypeInfo()` method now calls `synchronizeData()` if `needsSyncData()` returns true. This suggests that there was a bug where the type information was not properly synchronized before being accessed, potentially leading to incorrect or outdated information being returned. The added synchronization step aims to resolve this issue. The answer is **Buggy**."
xerces2-j,4988.json,860cb3b9c2ad281211d6d52beb871b2439720fa5,"@@ -1,3 +1,6 @@
     public static Element getParent(Element elem) {
-        return (Element)elem.getParentNode();
+        Node parent = elem.getParentNode();
+        if (parent instanceof Element)
+            return (Element)parent;
+        return null;
     } // getParent(Element):Element",Buggy,"Fixed some small bugs. Now we can have a sequence of two elements.


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@317613 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates that some small bugs have been fixed, and the code change modifies the `getParent` method to handle cases where the parent node is not an `Element`. The original code directly casts the parent node to an `Element` without checking its type, which could lead to a `ClassCastException` if the parent node is not an `Element`. The modified code checks if the parent node is an instance of `Element` before casting, and returns `null` otherwise. This change addresses a potential bug where the code would throw an exception when the parent node was not an `Element`. The answer is **Buggy**."
xerces2-j,4757.json,c7c7140fbc0baf115a90871a337515ebc33c7265,"@@ -1,45 +1,45 @@
     public XMLInputSource resolveEntity(XMLResourceIdentifier resourceIdentifier)
             throws XNIException, IOException {
         
         if (fEntityResolver != null) {
             
             String pubId = resourceIdentifier.getPublicId();
-            String sysId = resourceIdentifier.getExpandedSystemId();
+            String sysId = resourceIdentifier.getLiteralSystemId();
             String baseURI = resourceIdentifier.getBaseSystemId();
             String name = null;
             if (resourceIdentifier instanceof XMLDTDDescription) {
                 name = ""[dtd]"";
             }
             else if (resourceIdentifier instanceof XMLEntityDescription) {
                 name = ((XMLEntityDescription) resourceIdentifier).getEntityName();
             }
             
             // When both pubId and sysId are null, the user's entity resolver
             // can do nothing about it. We'd better not bother calling it.
             // This happens when the resourceIdentifier is a GrammarDescription,
             // which describes a schema grammar of some namespace, but without
             // any schema location hint. -Sg
             if (pubId == null && sysId == null) {
                 return null;
             }
             
             // Resolve using EntityResolver2
             try {
                 InputSource inputSource = 
                     fEntityResolver.resolveEntity(name, pubId, baseURI, sysId);
                 return (inputSource != null) ? createXMLInputSource(inputSource, baseURI) : null;
             }
             // error resolving entity
             catch (SAXException e) {
                 Exception ex = e.getException();
                 if (ex == null) {
                     ex = e;
                 }
                 throw new XNIException(ex);
             }   
         }
         
         // unable to resolve entity
         return null;
         
     } // resolveEntity(XMLResourceIdentifier):XMLInputSource",Buggy,"Fixing a bug. The systemId passed to EntityResolver2.resolveEntity may be an absolute or relative URI.
That is it should be the literal system identifier, not the expanded one which resolved from the base URI.


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@320213 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix related to the systemId passed to EntityResolver2.resolveEntity. The bug is that the expanded system identifier (resolved from the base URI) was being passed instead of the literal system identifier. The diff replaces `getExpandedSystemId()` with `getLiteralSystemId()`, which aligns with the commit message and fixes the bug. The answer is **Buggy**."
xerces2-j,1574.json,474a683ae11a84aaa7da8861708189c9d0c5d551,"@@ -1,11 +1,14 @@
     public boolean isNonEscapingElement( String tagName )
     {
         int i;
 
-        if ( _nonEscapingElements == null )
-            return false;
+        if ( _nonEscapingElements == null ) {
+            // non escaping was not set
+            // by default output value for elements as unescaped
+            return true;
+        }
         for ( i = 0 ; i < _nonEscapingElements.length ; ++i )
             if ( _nonEscapingElements[ i ].equals( tagName ) )
                 return true;
         return false;
     }",Buggy,"Bug fix: http://nagoya.apache.org/bugzilla/show_bug.cgi?id=2389


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@318097 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix. The code change modifies the `isNonEscapingElement` method. Specifically, when `_nonEscapingElements` is null, the method now returns `true` instead of `false`. This suggests that the previous behavior was incorrect, and the fix ensures that elements are treated as non-escaping by default when the non-escaping elements list is not explicitly set. This correction addresses a bug related to the handling of non-escaping elements. The answer is **Buggy**."
xerces2-j,5596.json,1543bbc70bee5fdf7d5abd54532f9b21e7617de5,"@@ -1,86 +1,86 @@
     protected void intersectRanges(Token token) {
         RangeToken tok = (RangeToken)token;
         if (tok.ranges == null || this.ranges == null)
             return;
         this.icaseCache = null;
         this.sortRanges();
         this.compactRanges();
         tok.sortRanges();
         tok.compactRanges();
 
         int[] result = new int[this.ranges.length+tok.ranges.length];
         int wp = 0, src1 = 0, src2 = 0;
         while (src1 < this.ranges.length && src2 < tok.ranges.length) {
             int src1begin = this.ranges[src1];
             int src1end = this.ranges[src1+1];
             int src2begin = tok.ranges[src2];
             int src2end = tok.ranges[src2+1];
             if (src1end < src2begin) {          // Not overlapped
                                                 // src1: o-----o
                                                 // src2:         o-----o
                                                 // res:  empty
                                                 // Reuse src2
                 src1 += 2;
             } else if (src1end >= src2begin
                        && src1begin <= src2end) { // Overlapped
                                                 // src1:    o--------o
                                                 // src2:  o----o
                                                 // src2:      o----o
                                                 // src2:          o----o
                                                 // src2:  o------------o
-                if (src2begin <= src2begin && src1end <= src2end) {
+                if (src2begin <= src1begin && src1end <= src2end) {
                                                 // src1:    o--------o
                                                 // src2:  o------------o
                                                 // res:     o--------o
                                                 // Reuse src2
                     result[wp++] = src1begin;
                     result[wp++] = src1end;
                     src1 += 2;
                 } else if (src2begin <= src1begin) {
                                                 // src1:    o--------o
                                                 // src2:  o----o
                                                 // res:     o--o
                                                 // Reuse the rest of src1
                     result[wp++] = src1begin;
                     result[wp++] = src2end;
                     this.ranges[src1] = src2end+1;
                     src2 += 2;
                 } else if (src1end <= src2end) {
                                                 // src1:    o--------o
                                                 // src2:          o----o
                                                 // res:           o--o
                                                 // Reuse src2
                     result[wp++] = src2begin;
                     result[wp++] = src1end;
                     src1 += 2;
                 } else {
                                                 // src1:    o--------o
                                                 // src2:      o----o
                                                 // res:       o----o
                                                 // Reuse the rest of src1
                     result[wp++] = src2begin;
                     result[wp++] = src2end;
                     this.ranges[src1] = src2end+1;
                 }
             } else if (src2end < src1begin) {
                                                 // Not overlapped
                                                 // src1:          o-----o
                                                 // src2: o----o
                 src2 += 2;
             } else {
                 throw new RuntimeException(""Token#intersectRanges(): Internal Error: [""
                                            +this.ranges[src1]
                                            +"",""+this.ranges[src1+1]
                                            +""] & [""+tok.ranges[src2]
                                            +"",""+tok.ranges[src2+1]
                                            +""]"");
             }
         }
         while (src1 < this.ranges.length) {
             result[wp++] = this.ranges[src1++];
             result[wp++] = this.ranges[src1++];
         }
         this.ranges = new int[wp];
         System.arraycopy(result, 0, this.ranges, 0, wp);
                                                 // this.ranges is sorted and compacted.
     }",Buggy,"Fixing JIRA Bug #1224:
http://issues.apache.org/jira/browse/XERCESJ-1224

Correcting a typo in intersectRanges(). Thanks to Dave Brosius 
for catching this error and suggesting how to correct it.


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@504431 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix in the `intersectRanges()` method, specifically a typo. The diff shows a change from `src2begin <= src2begin` to `src2begin <= src1begin`. This correction directly addresses a logical error in the range intersection logic. The original condition `src2begin <= src2begin` is always true and doesn't contribute to the intended logic of comparing the start of the second range with the start of the first range. The corrected condition `src2begin <= src1begin` properly compares the two ranges, leading to a correct intersection calculation. The answer is **Buggy**."
xerces2-j,3365.json,163dc12c725a4e9bc243970c82016cb18998c25c,"@@ -1,34 +1,37 @@
     public TimeZone getTimeZone(int defaultZoneoffset) {
         TimeZone result = null;
         int zoneoffset = getTimezone();
 
         if (zoneoffset == DatatypeConstants.FIELD_UNDEFINED) {
             zoneoffset = defaultZoneoffset;
         }
         if (zoneoffset == DatatypeConstants.FIELD_UNDEFINED) {
             result = TimeZone.getDefault();
         } else {
             // zoneoffset is in minutes. Convert to custom timezone id format.
             char sign = zoneoffset < 0 ? '-' : '+';
             if (sign == '-') {
                 zoneoffset = -zoneoffset;
             }
             int hour = zoneoffset / 60;
             int minutes = zoneoffset - (hour * 60);
 
             // Javadoc for java.util.TimeZone documents max length
             // for customTimezoneId is 8 when optional ':' is not used.
             // Format is 
             // ""GMT"" ('-'|''+') (digit digit?) (digit digit)?
             //                   hour          minutes
             StringBuffer customTimezoneId = new StringBuffer(8);
             customTimezoneId.append(""GMT"");
             customTimezoneId.append(sign);
             customTimezoneId.append(hour);
             if (minutes != 0) {
+                if (minutes < 10) {
+                    customTimezoneId.append('0');
+                }
                 customTimezoneId.append(minutes);
             }
             result = TimeZone.getTimeZone(customTimezoneId.toString());
         }
         return result;
     }",Buggy,"Fixing JIRA Bug #1243:
http://issues.apache.org/jira/browse/XERCESJ-1243

When we're building the TimeZone string if minutes < 10 we need to prepend a zero
to conform to the format expected by TimeZone.getTimeZone().


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@524223 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for JIRA bug XERCESJ-1243, which involves prepending a zero to the minutes part of the TimeZone string when it's less than 10. This is necessary to conform to the format expected by TimeZone.getTimeZone(). The diff shows the addition of a conditional statement that appends a '0' to the customTimezoneId when minutes < 10. This directly addresses the bug described in the commit message. The answer is **Buggy**.
"
xerces2-j,4419.json,0414900df065f25d451157d2c0f4d9ed5079d3a8,"@@ -1,13 +1,18 @@
     public void startParameterEntity (String name,
     XMLResourceIdentifier identifier,
     String encoding,
     Augmentations augs) throws XNIException {
         if (DEBUG_EVENTS) {
             System.out.println (""==>startParameterEntity: ""+name);
             if (DEBUG_BASEURI) {
                 System.out.println (""   expandedSystemId: ""+identifier.getExpandedSystemId ());
                 System.out.println (""   baseURI:""+ identifier.getBaseSystemId ());
             }
         }
+        if (augs != null && fInternalSubset != null && 
+            !fInDTDExternalSubset && 
+            Boolean.TRUE.equals(augs.getItem(Constants.ENTITY_SKIPPED))) {
+            fInternalSubset.append(name).append("";\n"");
+        }
         fBaseURIStack.push (identifier.getExpandedSystemId ());
     }",Buggy,"Fixing JIRA Bug #1114:
http://issues.apache.org/jira/browse/XERCESJ-1114

If a parameter entity is skipped the parameter entity reference 
should be written into the internal subset string.

git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@441668 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,The commit message indicates a fix for a specific bug (XERCESJ-1114) where skipped parameter entities were not being correctly handled in the internal subset string. The diff introduces a conditional block that appends the entity name to the internal subset string if the entity is skipped (indicated by the `ENTITY_SKIPPED` augmentation). This aligns directly with the bug description in the commit message. The answer is **Buggy**.
xerces2-j,2331.json,c8ed9f7617b7aef095f3e03f97aefda48972f356,"@@ -1,178 +1,197 @@
     public short compareTreePosition(Node other) {
         // Questions of clarification for this method - to be answered by the
         // DOM WG.   Current assumptions listed - LM
         // 
         // 1. How do ENTITY nodes compare?  
         //    Current assumption: TREE_POSITION_DISCONNECTED, as ENTITY nodes 
         //    aren't really 'in the tree'
         //
         // 2. How do NOTATION nodes compare?
         //    Current assumption: TREE_POSITION_DISCONNECTED, as NOTATION nodes
         //    aren't really 'in the tree'
         //
         // 3. Are TREE_POSITION_ANCESTOR and TREE_POSITION_DESCENDANT     
         //    only relevant for nodes that are ""part of the document tree""?   
         //     <outer>
         //         <inner  myattr=""true""/>
         //     </outer>
         //    Is the element node ""outer"" considered an ancestor of ""myattr""?
         //    Current assumption: No.                                     
         //
         // 4. How do children of ATTRIBUTE nodes compare (with eachother, or  
         //    with children of other attribute nodes with the same element)    
         //    Current assumption: Children of ATTRIBUTE nodes are treated as if 
         //    they they are the attribute node itself, unless the 2 nodes 
         //    are both children of the same attribute. 
         //
         // 5. How does an ENTITY_REFERENCE node compare with it's children? 
         //    Given the DOM, it should precede its children as an ancestor. 
         //    Given ""document order"",  does it represent the same position?     
         //    Current assumption: An ENTITY_REFERENCE node is an ancestor of its
         //    children.
         //
         // 6. How do children of a DocumentFragment compare?   
         //    Current assumption: If both nodes are part of the same document 
         //    fragment, there are compared as if they were part of a document. 
 
         
         // If the nodes are the same...
         if (this==other) 
           return (TREE_POSITION_SAME_NODE | TREE_POSITION_EQUIVALENT);
         
         // If either node is of type ENTITY or NOTATION, compare as disconnected
         short thisType = this.getNodeType();
         short otherType = other.getNodeType();
 
         // If either node is of type ENTITY or NOTATION, compare as disconnected
         if (thisType == Node.ENTITY_NODE || 
             thisType == Node.NOTATION_NODE ||
             otherType == Node.ENTITY_NODE ||
             otherType == Node.NOTATION_NODE ) {
           return TREE_POSITION_DISCONNECTED; 
         }
 
         // Find the ancestor of each node, and the distance each node is from 
         // its ancestor.
         // During this traversal, look for ancestor/descendent relationships 
         // between the 2 nodes in question. 
         // We do this now, so that we get this info correct for attribute nodes 
         // and their children. 
 
         Node node; 
         Node thisAncestor = this;
         Node otherAncestor = other;
         int thisDepth=0;
         int otherDepth=0;
         for (node=this; node != null; node = node.getParentNode()) {
             thisDepth +=1;
             if (node == other) 
               // The other node is an ancestor of this one.
               return (TREE_POSITION_ANCESTOR | TREE_POSITION_PRECEDING);
             thisAncestor = node;
         }
 
         for (node=other; node!=null; node=node.getParentNode()) {
             otherDepth +=1;
             if (node == this) 
               // The other node is a descendent of the reference node.
               return (TREE_POSITION_DESCENDANT | TREE_POSITION_FOLLOWING);
             otherAncestor = node;
         }
         
        
         Node thisNode = this;
         Node otherNode = other;
 
         int thisAncestorType = thisAncestor.getNodeType();
         int otherAncestorType = otherAncestor.getNodeType();
 
         // if the ancestor is an attribute, get owning element. 
         // we are now interested in the owner to determine position.
 
         if (thisAncestorType == Node.ATTRIBUTE_NODE)  {
            thisNode = ((AttrImpl)thisAncestor).getOwnerElement();
         }
         if (otherAncestorType == Node.ATTRIBUTE_NODE) {
            otherNode = ((AttrImpl)otherAncestor).getOwnerElement();
         }
 
         // Before proceeding, we should check if both ancestor nodes turned
         // out to be attributes for the same element
         if (thisAncestorType == Node.ATTRIBUTE_NODE &&  
             otherAncestorType == Node.ATTRIBUTE_NODE &&  
             thisNode==otherNode)              
             return TREE_POSITION_EQUIVALENT;
 
         // Now, find the ancestor of the owning element, if the original
         // ancestor was an attribute
  
         // Note:  the following 2 loops are quite close to the ones above.
         // May want to common them up.  LM.
         if (thisAncestorType == Node.ATTRIBUTE_NODE) {
             thisDepth=0;
             for (node=thisNode; node != null; node=node.getParentNode()) {
                 thisDepth +=1;
                 if (node == otherNode) 
                   // The other node is an ancestor of the owning element
+                  {
                   return TREE_POSITION_PRECEDING;
+                  }
                 thisAncestor = node;
             }
         }
 
         // Now, find the ancestor of the owning element, if the original
         // ancestor was an attribute
         if (otherAncestorType == Node.ATTRIBUTE_NODE) {
             otherDepth=0;
             for (node=otherNode; node != null; node=node.getParentNode()) {
                 otherDepth +=1;
                 if (node == thisNode) 
                   // The other node is a descendent of the reference 
                   // node's element
                   return TREE_POSITION_FOLLOWING;
                 otherAncestor = node;
             }
         }
 
         // thisAncestor and otherAncestor must be the same at this point,  
         // otherwise, we are not in the same tree or document fragment
         if (thisAncestor != otherAncestor) 
           return TREE_POSITION_DISCONNECTED; 
 
-        // Determine which node is of the greatest depth.  
+      
+        // Go up the parent chain of the deeper node, until we find a node 
+        // with the same depth as the shallower node
+
         if (thisDepth > otherDepth) {
           for (int i=0; i<thisDepth - otherDepth; i++)
             thisNode = thisNode.getParentNode();
+          // Check if the node we have reached is in fact ""otherNode"". This can
+          // happen in the case of attributes.  In this case, otherNode 
+          // ""precedes"" this.
+          if (thisNode == otherNode) 
+            return TREE_POSITION_PRECEDING;
         }
+ 
         else {
           for (int i=0; i<otherDepth - thisDepth; i++)
             otherNode = otherNode.getParentNode();
+          // Check if the node we have reached is in fact ""thisNode"".  This can
+          // happen in the case of attributes.  In this case, otherNode 
+          // ""follows"" this.
+          if (otherNode == thisNode) 
+            return TREE_POSITION_FOLLOWING;
         }
-          
+             
         // We now have nodes at the same depth in the tree.  Find a common 
         // ancestor.                                   
         Node thisNodeP, otherNodeP;
         for (thisNodeP=thisNode.getParentNode(),
                   otherNodeP=otherNode.getParentNode();
              thisNodeP!=otherNodeP;) {
              thisNode = thisNodeP;
              otherNode = otherNodeP;
              thisNodeP = thisNodeP.getParentNode();
              otherNodeP = otherNodeP.getParentNode();
         }
 
+        // At this point, thisNode and otherNode are direct children of 
+        // the common ancestor.  
         // See whether thisNode or otherNode is the leftmost
+
         for (Node current=thisNodeP.getFirstChild(); 
                   current!=null;
                   current=current.getNextSibling()) {
                if (current==otherNode) {
                  return TREE_POSITION_PRECEDING;
                }
                else if (current==thisNode) {
                  return TREE_POSITION_FOLLOWING;
                }
         }
         // REVISIT:  shouldn't get here.   Should probably throw an 
         // exception
         return 0;
 
     }",Buggy,"Fix for bug 13054


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@318777 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix. The code changes involve modifications to the `compareTreePosition` method, which compares the positions of nodes in a DOM tree. The changes include additional checks and adjustments to handle cases where one node is an attribute of an element and the other node is the element itself or an ancestor of the element. Specifically, the code now checks if the nodes at the same depth are in fact the same node (which can happen with attributes) and returns the appropriate preceding/following relationship. The addition of `{}` blocks around single-line returns suggests that the original code might have had issues with control flow or scope, which is indicative of a bug fix. The answer is **Buggy**."
xerces2-j,4598.json,1ab22779af45496ed09d0d35aa50271c6e881b33,"@@ -1,3 +1,4 @@
     public void setLocale(Locale locale) throws XNIException {
+    	super.setLocale(locale);
         fErrorReporter.setLocale(locale);
     } // setLocale(Locale)",Buggy,"fix for bug id 5927. Thanks to 'Taki' for the solution.


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@318115 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix related to bug ID 5927, and credits 'Taki' for the solution. The code change involves adding `super.setLocale(locale);` to the `setLocale` method. This suggests that the original implementation was missing a call to the superclass's `setLocale` method, potentially leading to incorrect locale handling or initialization. This omission constitutes a bug. The answer is **Buggy**."
xerces2-j,1891.json,61370c8cd1f67817077ae8b356b0f69747471746,"@@ -1,72 +1,74 @@
     protected boolean matchChildSequence(QName element, int event)
             throws XNIException {
     	
         // need to resize fCurrentChildSequence
         if (fCurrentChildDepth >= fCurrentChildSequence.length) {
             int tmpCurrentChildSequence[] = new int[fCurrentChildSequence.length];
             System.arraycopy(fCurrentChildSequence, 0, tmpCurrentChildSequence,
                     0, fCurrentChildSequence.length);
 
             // Increase the size by a factor of 2 (?)
             fCurrentChildSequence = new int[fCurrentChildDepth * 2];
             System.arraycopy(tmpCurrentChildSequence, 0, fCurrentChildSequence,
                     0, tmpCurrentChildSequence.length);
         }
 
         //     
         if (fIsResolveElement) {
             // start
             if (event == XPointerPart.EVENT_ELEMENT_START) {
                 fCurrentChildSequence[fCurrentChildDepth] = fCurrentChildPosition;
                 fCurrentChildDepth++;
 
                 // reset the current child position 
                 fCurrentChildPosition = 1;
 
                 //if (!fSchemeNameFound) {
                 if ((fCurrentChildDepth <= fFoundDepth) || (fFoundDepth == 0)) {
                     if (checkMatch()) {
                         fIsElementFound = true;
                         fFoundDepth = fCurrentChildDepth;
                     } else {
                         fIsElementFound = false;
                         fFoundDepth = 0;
                     }
                 }
 
             } else if (event == XPointerPart.EVENT_ELEMENT_END) {
                 if (fCurrentChildDepth == fFoundDepth) {
                     fIsElementFound = true;
                 } else if (((fCurrentChildDepth < fFoundDepth) && (fFoundDepth != 0))
                         || ((fCurrentChildDepth > fFoundDepth) // or empty element found
                         && (fFoundDepth == 0))) {
                     fIsElementFound = false;
                 }
 
                 // reset array position of last child
                 fCurrentChildSequence[fCurrentChildDepth] = 0;
 
                 fCurrentChildDepth--;
                 fCurrentChildPosition = fCurrentChildSequence[fCurrentChildDepth] + 1;
                 
             } else if (event == XPointerPart.EVENT_ELEMENT_EMPTY) {
 
                 fCurrentChildSequence[fCurrentChildDepth] = fCurrentChildPosition;
                 fCurrentChildPosition++;
 
                 // Donot check for empty elements if the empty element is 
                 // a child of a found parent element 
-                //if (!fIsElementFound) {
-                    if (checkMatch()) {
-                        fIsElementFound = true;
+                if (checkMatch()) {
+                    if (!fIsElementFound) {
                         fWasOnlyEmptyElementFound = true;
                     } else {
-                        fIsElementFound = false;
+                        fWasOnlyEmptyElementFound = false;
                     }
-                //} 
-                
+                    fIsElementFound = true;
+                } else {
+                    fIsElementFound = false;
+                    fWasOnlyEmptyElementFound = false;
+                }  
             }
         }
 
         return fIsElementFound;
     }",Buggy,"Fixing JIRA Bug #1134: http://issues.apache.org/jira/browse/XERCESJ-1134

git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@415823 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix related to JIRA Bug #1134. The code changes involve modifications to the `matchChildSequence` method, specifically addressing the handling of empty elements and the `fIsElementFound` flag. The logic for determining if an element is found and setting the `fWasOnlyEmptyElementFound` flag has been adjusted. The original code had a commented-out condition `//if (!fIsElementFound) {` which has been removed, and the logic within the `if (checkMatch())` block has been modified to correctly handle cases where an empty element is found. These changes suggest a correction to the algorithm's behavior, indicating a bug fix. The answer is **Buggy**."
xerces2-j,7694.json,60697c3db7e640a090be94027b7b5e35be8de0e4,"@@ -1,19 +1,21 @@
     public synchronized XSObjectList getAnnotations() {
         if(fAnnotations != null) 
             return fAnnotations;
 
         // do this in two passes to avoid inaccurate array size
         int totalAnnotations = 0;
         for (int i = 0; i < fGrammarCount; i++) {
             totalAnnotations += fGrammarList[i].fNumAnnotations;
         }
         XSAnnotationImpl [] annotations = new XSAnnotationImpl [totalAnnotations];
         int currPos = 0;
         for (int i = 0; i < fGrammarCount; i++) {
             SchemaGrammar currGrammar = fGrammarList[i];
-            System.arraycopy(currGrammar.fAnnotations, 0, annotations, currPos, currGrammar.fNumAnnotations);
-            currPos += currGrammar.fNumAnnotations;
+            if (currGrammar.fNumAnnotations > 0) {
+                System.arraycopy(currGrammar.fAnnotations, 0, annotations, currPos, currGrammar.fNumAnnotations);
+                currPos += currGrammar.fNumAnnotations;
+            }
         }
         fAnnotations = new XSObjectListImpl(annotations, annotations.length);
         return fAnnotations;
     }",Buggy,"Fixing Jira Bug #968:
http://nagoya.apache.org/jira/browse/XERCESJ-968

If no annotations exist for a SchemaGrammar the array
of annotations is never created. For such schema docs
we were passing null to System.arrayCopy which caused
a NullPointerException to be thrown. This is now fixed.


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@319923 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,The commit message indicates a fix for a NullPointerException that occurs when System.arraycopy is called with a null array (currGrammar.fAnnotations). The code diff adds a check to ensure that currGrammar.fNumAnnotations is greater than 0 before calling System.arraycopy. This prevents the NullPointerException when there are no annotations for a SchemaGrammar. The answer is **Buggy**.
xerces2-j,2219.json,5a14bab0762a851069a18eebd7f278b943f88c00,"@@ -1,20 +1,20 @@
     public Object getTypeInfo(int nodeIndex) {
         if (nodeIndex == -1) {
             return null;
         }
 
         int chunk = nodeIndex >> CHUNK_SHIFT;
         int index = nodeIndex & CHUNK_MASK;
         
         
-        Object value = fNodeValue[chunk][index];
+        Object value = fNodeValue[chunk] != null ? fNodeValue[chunk][index] : null;
         if (value != null) {
             fNodeValue[chunk][index] = null;
             RefCount c = (RefCount) fNodeValue[chunk][CHUNK_SIZE];
             c.fCount--;
             if (c.fCount == 0) {
                 fNodeValue[chunk] = null;
             }
         }
         return value;
     }",Buggy,"Fix a bug introduced in the new getTypeInfo method (DOM L3)


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@318962 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,The commit message indicates a bug fix in the `getTypeInfo` method. The diff shows a change that adds a null check `fNodeValue[chunk] != null` before accessing `fNodeValue[chunk][index]`. This prevents a potential `NullPointerException` if `fNodeValue[chunk]` is null. This is a bug fix. The answer is **Buggy**.
xerces2-j,4324.json,53ec54f2822c25f6815aa81b3b2eb8f46dd3caf5,"@@ -1,91 +1,91 @@
     protected void configurePipeline() {
         if (fCurrentDVFactory != fDatatypeValidatorFactory) {
             fCurrentDVFactory = fDatatypeValidatorFactory;
             // use XML 1.0 datatype library
             setProperty(DATATYPE_VALIDATOR_FACTORY, fCurrentDVFactory);
         }
 
         // setup DTD pipeline
         if (fCurrentDTDScanner != fDTDScanner) {
-			fCurrentDTDScanner = fDTDScanner;
+            fCurrentDTDScanner = fDTDScanner;
             setProperty(DTD_SCANNER, fCurrentDTDScanner);
             setProperty(DTD_PROCESSOR, fDTDProcessor);
-            fDTDScanner.setDTDHandler(fDTDProcessor);
-            fDTDProcessor.setDTDSource(fDTDScanner);
-            fDTDProcessor.setDTDHandler(fDTDHandler);
-            if (fDTDHandler != null) {
-                 fDTDHandler.setDTDSource(fDTDProcessor);
-            }
+        }
+        fDTDScanner.setDTDHandler(fDTDProcessor);
+        fDTDProcessor.setDTDSource(fDTDScanner);
+        fDTDProcessor.setDTDHandler(fDTDHandler);
+        if (fDTDHandler != null) {
+            fDTDHandler.setDTDSource(fDTDProcessor);
+        }
 
-            fDTDScanner.setDTDContentModelHandler(fDTDProcessor);
-            fDTDProcessor.setDTDContentModelSource(fDTDScanner);
-            fDTDProcessor.setDTDContentModelHandler(fDTDContentModelHandler);
-            if (fDTDContentModelHandler != null) {
-                fDTDContentModelHandler.setDTDContentModelSource(fDTDProcessor);
-            }            
+        fDTDScanner.setDTDContentModelHandler(fDTDProcessor);
+        fDTDProcessor.setDTDContentModelSource(fDTDScanner);
+        fDTDProcessor.setDTDContentModelHandler(fDTDContentModelHandler);
+        if (fDTDContentModelHandler != null) {
+            fDTDContentModelHandler.setDTDContentModelSource(fDTDProcessor);
         }
 
         // setup document pipeline
         if (fFeatures.get(NAMESPACES) == Boolean.TRUE) {
             if (fCurrentScanner != fNamespaceScanner) {
                 fCurrentScanner = fNamespaceScanner;
                 setProperty(DOCUMENT_SCANNER, fNamespaceScanner);
                 setProperty(DTD_VALIDATOR, fDTDValidator);
             }
             fNamespaceScanner.setDTDValidator(fDTDValidator);
             fNamespaceScanner.setDocumentHandler(fDTDValidator);
             fDTDValidator.setDocumentSource(fNamespaceScanner);
             fDTDValidator.setDocumentHandler(fDocumentHandler);
             if (fDocumentHandler != null) {
                 fDocumentHandler.setDocumentSource(fDTDValidator);
             }
             fLastComponent = fDTDValidator;
         } else {
             // create components
             if (fNonNSScanner == null) {
                 fNonNSScanner = new XMLDocumentScannerImpl();
                 fNonNSDTDValidator = new XMLDTDValidator();
                 // add components
                 addComponent((XMLComponent) fNonNSScanner);
                 addComponent((XMLComponent) fNonNSDTDValidator);
             }
             if (fCurrentScanner != fNonNSScanner) {
                 fCurrentScanner = fNonNSScanner;
                 setProperty(DOCUMENT_SCANNER, fNonNSScanner);
                 setProperty(DTD_VALIDATOR, fNonNSDTDValidator);
             }
 
             fNonNSScanner.setDocumentHandler(fNonNSDTDValidator);
             fNonNSDTDValidator.setDocumentSource(fNonNSScanner);
             fNonNSDTDValidator.setDocumentHandler(fDocumentHandler);
             if (fDocumentHandler != null) {
                 fDocumentHandler.setDocumentSource(fNonNSDTDValidator);
             }
             fLastComponent = fNonNSDTDValidator;
         }
 
         // add XML Schema validator if needed
         if (fFeatures.get(XMLSCHEMA_VALIDATION) == Boolean.TRUE) {
             // If schema validator was not in the pipeline insert it.
             if (fSchemaValidator == null) {
                 fSchemaValidator = new XMLSchemaValidator();
                 // add schema component
                 setProperty(SCHEMA_VALIDATOR, fSchemaValidator);
                 addCommonComponent(fSchemaValidator);
-				fSchemaValidator.reset(this);
+                fSchemaValidator.reset(this);
                 // add schema message formatter
                 if (fErrorReporter.getMessageFormatter(XSMessageFormatter.SCHEMA_DOMAIN) == null) {
                     XSMessageFormatter xmft = new XSMessageFormatter();
                     fErrorReporter.putMessageFormatter(XSMessageFormatter.SCHEMA_DOMAIN, xmft);
                 }
 
             }
             fLastComponent.setDocumentHandler(fSchemaValidator);
             fSchemaValidator.setDocumentSource(fLastComponent);
             fSchemaValidator.setDocumentHandler(fDocumentHandler);
             if (fDocumentHandler != null) {
                 fDocumentHandler.setDocumentSource(fSchemaValidator);
             }
             fLastComponent = fSchemaValidator;
         }
     } // configurePipeline()",Buggy,"Fixing a bug in XML10 configurePipeline: we should always setup DTD pipeline (was only
setup if scanner has not been changed)


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@319532 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix in `XML10.configurePipeline` related to setting up the DTD pipeline. The original code only set up the DTD pipeline if the scanner had not been changed. The corrected code sets up the DTD pipeline unconditionally. This ensures that the DTD pipeline is always configured correctly, addressing the bug described in the commit message. The answer is **Buggy**."
xerces2-j,4808.json,70b3c80f83b7fcd277b5905ccde6e49f5e6274bb,"@@ -1,4 +1,4 @@
     public String getType(String qname) {
         int index = getIndex(qname);
-        return index != -1 ? fAttributes[index].type : null;
+        return index != -1 ? getType(index): null;
     } // getType(String):String",Buggy,"fixing bug 15584.  Thanks to Michael Glavassevich for pointing out this bug in a SAX context; it was equally an XNI bug, however.


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@319115 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix (bug 15584) reported by Michael Glavassevich. The code change in `getType(String)` replaces direct access to the `fAttributes` array with a call to `getType(index)`. This suggests that the original code might have had an issue with how it retrieved the type from the attributes array, potentially leading to incorrect or null values being returned under certain conditions. The change likely addresses this issue by using a more robust or correct method (`getType(index)`) to retrieve the attribute type. The answer is **Buggy**."
xerces2-j,2281.json,f905e13bf111fee815eabfc522f4dfc0949fb422,"@@ -1,3 +1,3 @@
     public void setType(XSTypeDefinition type) {
-        type = type;
-    }

+        this.type = type;
+    }",Buggy,"Bug fix: http://nagoya.apache.org/bugzilla/show_bug.cgi?id=17064
Patch submitter: Michael Glavassevich


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@319093 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message clearly indicates a bug fix, referencing a specific bug report. The diff shows a change in the `setType` method where the assignment `type = type;` is corrected to `this.type = type;`. The original code had no effect, as it was assigning the parameter to itself, thus not updating the object's type. This correction directly addresses the bug. The answer is **Buggy**."
xerces2-j,3095.json,955ef6b2ca713901bd702935c131a09390af1389,"@@ -1,9 +1,8 @@
     int indexOf(Node child, Node parent) {
-        Node node;
-        int i = 0;
         if (child.getParentNode() != parent) return -1;
-        for(node = child; node!= null; node=node.getPreviousSibling()) {
+        int i = 0;
+        for(Node node = parent.getFirstChild(); node!= child; node=node.getNextSibling()) {
             i++;
         }
         return i;
     }",Buggy,"applied patch from Lynn Monson:
fixed a bug in indexOf which was off by 1


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@316616 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message clearly states that a bug in `indexOf` was fixed, specifically mentioning that the index was ""off by 1"". The diff shows a change in the implementation of the `indexOf` method. The original code iterated backwards from the child node to find its index, while the corrected code iterates forward from the first child of the parent node until it reaches the target child node. The original implementation was flawed and would not return the correct index in all cases. The new implementation fixes this issue. The answer is **Buggy**.
"
xerces2-j,3693.json,b23b90432763872d07b931251896abce3e1e90e6,"@@ -1,39 +1,39 @@
         public InputSource resolveEntity(String name, String publicId, 
                 String baseURI, String systemId) throws SAXException, IOException {
             if (fEntityResolver != null) {
                 LSInput lsInput = fEntityResolver.resolveResource(XML_TYPE, null, publicId, systemId, baseURI);
                 if (lsInput != null) {
                     final String pubId = lsInput.getPublicId();
                     final String sysId = lsInput.getSystemId();
                     final String baseSystemId = lsInput.getBaseURI();
                     final Reader charStream = lsInput.getCharacterStream();
                     final InputStream byteStream = lsInput.getByteStream();
                     final String data = lsInput.getStringData();
                     final String encoding = lsInput.getEncoding();
 
                     /**
                      * An LSParser looks at inputs specified in LSInput in
                      * the following order: characterStream, byteStream,
                      * stringData, systemId, publicId. For consistency
                      * with the DOM Level 3 Load and Save Recommendation
                      * use the same lookup order here.
                      */
                     InputSource inputSource = new InputSource();
                     inputSource.setPublicId(pubId);
-                    inputSource.setSystemId((baseSystemId != null) ? resolveSystemId(systemId, baseSystemId) : systemId);
+                    inputSource.setSystemId((baseSystemId != null) ? resolveSystemId(sysId, baseSystemId) : sysId);
                     
                     if (charStream != null) {
                         inputSource.setCharacterStream(charStream);
                     }
                     else if (byteStream != null) {
                         inputSource.setByteStream(byteStream);
                     }
                     else if (data != null && data.length() != 0) {
                         inputSource.setCharacterStream(new StringReader(data));
                     }
                     inputSource.setEncoding(encoding);
                     return inputSource;
                 }
             }
             return null;
         }",Buggy,"Fixing a minor bug. The InputSource returned from the resolution forwarder should 
contain the system identifier returned from the application's LSResourceResolver 
not the one which was passed to the resolveEntity() method.

git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@374971 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix related to the `InputSource` returned by the resolution forwarder. The bug is that the `InputSource` was using the system identifier passed to the `resolveEntity()` method instead of the one returned from the application's `LSResourceResolver`.

The diff shows that the `setSystemId` method of the `InputSource` is now being called with `sysId` (the system ID from `lsInput`) instead of `systemId` (the system ID passed to the `resolveEntity` method). This aligns with the commit message's description of the bug fix.

Therefore, the answer is **Buggy**."
xerces2-j,1803.json,6c7a16c3202d0e50e4cd05449e9a5a9963f80e1b,"@@ -1,6 +1,7 @@
     public int hashCode() {
         if (uri != null) {
-            return uri.hashCode() + localpart.hashCode();
+            return uri.hashCode() + 
+                ((localpart != null) ? localpart.hashCode() : 0);
         }
-        return rawname.hashCode();
+        return (rawname != null) ? rawname.hashCode() : 0;
     } // hashCode():int",Buggy,"Fixing Jira Bug #997:
http://nagoya.apache.org/jira/browse/XERCESJ-997

Fixing an NPE which may occur in the hashCode method
when either the localpart or rawname fields are null. This
seems to only be a problem for external users of this class.


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@320004 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for a NullPointerException (NPE) in the `hashCode` method, specifically when `localpart` or `rawname` fields are null. The diff shows that the code has been modified to handle null values for `localpart` and `rawname` by adding a null check and returning 0 if they are null. This prevents the NPE from occurring when these fields are null. Therefore, the changes indicate a bug fix. The answer is **Buggy**."
xerces2-j,3399.json,0df32768beedf6778651639c24d234d5b70f3b6e,"@@ -1,30 +1,33 @@
     private static void organizeParts(
         String whole,
         String[] parts,
         int[] partsIndex,
         int len,
         String tokens)
         throws IllegalArgumentException {
 
         int idx = tokens.length();
         for (int i = len - 1; i >= 0; i--) {
+            if (parts[i] == null) {
+                throw new IllegalArgumentException(whole);
+            }
             int nidx =
                 tokens.lastIndexOf(
                     parts[i].charAt(parts[i].length() - 1),
                     idx - 1);
             if (nidx == -1) {
                 throw new IllegalArgumentException(whole);
                 // ,partsIndex[i]+parts[i].length()-1);
             }
 
             for (int j = nidx + 1; j < idx; j++) {
                 parts[j] = null;
             }
             idx = nidx;
             parts[idx] = parts[i];
             partsIndex[idx] = partsIndex[i];
         }
         for (idx--; idx >= 0; idx--) {
             parts[idx] = null;
         }
     }",Buggy,"Fixing JIRA Bug #1416: http://issues.apache.org/jira/browse/XERCESJ-1416. An NPE could occur when the value of the duration is invalid. Check for this condition and throw the correct exception.

git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@906803 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for a NullPointerException (NPE) that could occur when the duration value is invalid. The diff adds a check `if (parts[i] == null)` within the `organizeParts` method. If a part is null, an IllegalArgumentException is thrown with the original input string `whole`. This prevents the potential NPE that could occur if the code proceeded to use a null `parts[i]` value. This aligns with the commit message's description of fixing an NPE caused by invalid duration values. The answer is **Buggy**.
"
xerces2-j,3918.json,3260ba200fec37c5d074e5bb96ebd856e494ebe9,"@@ -1,14 +1,22 @@
     public void setFeature(String name, boolean value)
         throws ParserConfigurationException, SAXNotRecognizedException, 
 		SAXNotSupportedException
     {
         // XXX This is ugly.  We have to collect the features and then
         // later create an XMLReader to verify the features.
         if (features == null) {
             features = new Hashtable();
         }
         features.put(name, new Boolean(value));
 
         // Test the feature by possibly throwing SAX exceptions
-        newSAXParserImpl();
+        try {
+            newSAXParserImpl();
+        } catch (SAXNotSupportedException e) {
+            features.remove(name);
+            throw e;
+        } catch (SAXNotRecognizedException e) {
+            features.remove(name);
+            throw e;
+        }
     }",Buggy,"Fix setFeature() bug provided by Kohsuke Kawaguchi


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@317014 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix in the `setFeature()` method, attributed to Kohsuke Kawaguchi. The code diff shows that the `newSAXParserImpl()` method call, which is responsible for testing the feature, is now enclosed in a try-catch block. This block specifically catches `SAXNotSupportedException` and `SAXNotRecognizedException`. If either of these exceptions is caught, the feature is removed from the `features` Hashtable, and the exception is re-thrown. This suggests that the original code was not properly handling these exceptions when setting features, leading to a bug. The fix ensures that if a feature is not supported or recognized, it's removed and the appropriate exception is propagated. The answer is **Buggy**."
xerces2-j,5468.json,7b91d52436909c5274ee9d683e37e15b4d73c03b,"@@ -1,111 +1,116 @@
     public void print(Node node) {
 
         // is there anything to do?
         if ( node == null ) {
             return;
         }
 
         int type = node.getNodeType();
         switch ( type ) {
         // print document
         case Node.DOCUMENT_NODE: {
                 if ( !canonical ) {
                     String  Encoding = this.getWriterEncoding();
                     if ( Encoding.equalsIgnoreCase( ""DEFAULT"" ) )
                         Encoding = ""UTF-8"";
                     else if ( Encoding.equalsIgnoreCase( ""Unicode"" ) )
                         Encoding = ""UTF-16"";
                     else
                         Encoding = MIME2Java.reverse( Encoding );
 
                     out.println(""<?xml version=\""1.0\"" encoding=\""""+
                                 Encoding + ""\""?>"");
                 }
-                print(((Document)node).getDocumentElement());
+                //print(((Document)node).getDocumentElement());
+                
+                NodeList children = node.getChildNodes(); 
+                for ( int iChild = 0; iChild < children.getLength(); iChild++ ) { 
+                    print(children.item(iChild)); 
+                } 
                 out.flush();
                 break;
             }
 
             // print element with attributes
         case Node.ELEMENT_NODE: {
                 out.print('<');
                 out.print(node.getNodeName());
                 Attr attrs[] = sortAttributes(node.getAttributes());
                 for ( int i = 0; i < attrs.length; i++ ) {
                     Attr attr = attrs[i];
                     out.print(' ');
                     out.print(attr.getNodeName());
                     out.print(""=\"""");
                     out.print(normalize(attr.getNodeValue()));
                     out.print('""');
                 }
                 out.print('>');
                 NodeList children = node.getChildNodes();
                 if ( children != null ) {
                     int len = children.getLength();
                     for ( int i = 0; i < len; i++ ) {
                         print(children.item(i));
                     }
                 }
                 break;
             }
 
             // handle entity reference nodes
         case Node.ENTITY_REFERENCE_NODE: {
                 if ( canonical ) {
                     NodeList children = node.getChildNodes();
                     if ( children != null ) {
                         int len = children.getLength();
                         for ( int i = 0; i < len; i++ ) {
                             print(children.item(i));
                         }
                     }
                 } else {
                     out.print('&');
                     out.print(node.getNodeName());
                     out.print(';');
                 }
                 break;
             }
 
             // print cdata sections
         case Node.CDATA_SECTION_NODE: {
                 if ( canonical ) {
                     out.print(normalize(node.getNodeValue()));
                 } else {
                     out.print(""<![CDATA["");
                     out.print(node.getNodeValue());
                     out.print(""]]>"");
                 }
                 break;
             }
 
             // print text
         case Node.TEXT_NODE: {
                 out.print(normalize(node.getNodeValue()));
                 break;
             }
 
             // print processing instruction
         case Node.PROCESSING_INSTRUCTION_NODE: {
                 out.print(""<?"");
                 out.print(node.getNodeName());
                 String data = node.getNodeValue();
                 if ( data != null && data.length() > 0 ) {
                     out.print(' ');
                     out.print(data);
                 }
-                out.print(""?>"");
+                out.println(""?>"");
                 break;
             }
         }
 
         if ( type == Node.ELEMENT_NODE ) {
             out.print(""</"");
             out.print(node.getNodeName());
             out.print('>');
         }
 
         out.flush();
 
     } // print(Node)",Buggy,"Fix submitted by Ed Stub. Fixes problem reported by Stephane.RAULT@r2isa... where we
are no reporting PI's


PR:
Obtained from:
Submitted by:
Reviewed by:


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@316112 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix related to reporting Processing Instructions (PIs). The diff includes a change in the `print` method, specifically within the `PROCESSING_INSTRUCTION_NODE` case. The original code prints ""<?...?>"" on a single line, while the corrected code prints ""<?...?>"" followed by a newline character. This change addresses the issue of not reporting PIs correctly, suggesting a bug fix. The answer is **Buggy**."
xerces2-j,684.json,d1fd0668a071ddcf44c6bdf1b50030bab12d18ee,"@@ -1,25 +1,26 @@
     public synchronized void deleteRow( int index )
     {
         Node    child;
         
         child = getFirstChild();
         while ( child != null )
         {
             if ( child instanceof HTMLTableRowElement )
             {
                 if ( index == 0 )
                 {
                     removeChild ( child );
                     return;
                 }
+                --index;
             }
             else
             if ( child instanceof HTMLTableSectionElementImpl )
             {
                 index = ( (HTMLTableSectionElementImpl) child ).deleteRowX( index );
                 if ( index < 0 )
                     return;
             }
             child = child.getNextSibling();
         }
     }",Buggy,"Fixing bug in HTML DOM (http://nagoya.apache.org/bugzilla/show_bug.cgi?id=18744). Deletion of rows other than row 0 were failing siliently.

Thanks to Brett Sutton for the fix.


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@319291 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix related to deleting rows in an HTML DOM table. The diff shows a change in the `deleteRow` method. Specifically, it introduces a decrement `--index;` within the `if ( child instanceof HTMLTableRowElement )` block. This suggests that the original code was not correctly tracking the row index when iterating through the table rows, leading to the silent failure of deleting rows other than the first one. The added decrement fixes this logic error, aligning with the bug fix described in the commit message. The answer is **Buggy**.
"
xerces2-j,5580.json,af847434ae5a2c3408cae3c4b738622800c93e4f,"@@ -1,43 +1,43 @@
     private static void buildCaseInsensitiveMap() {
         caseInsensitiveMap = new int[INITIAL_CHUNK_COUNT][][];
         for (int i=0; i<INITIAL_CHUNK_COUNT; i++) {
             caseInsensitiveMap[i] = new int[CHUNK_SIZE][];
         }
         
         int lc, uc;
         for (int i=0; i<0x10000; i++) {
-            lc = Character.toLowerCase(i);
-            uc = Character.toUpperCase(i);
+            lc = Character.toLowerCase((char) i);
+            uc = Character.toUpperCase((char) i);
 
             // lower/upper case value is not the same as code point
             if (lc != uc || lc != i) {
                 int[] map = new int[2];
                 int index = 0;
 
                 if (lc != i) {
                     map[index++] = lc;
                     map[index++] = LOWER_CASE_MATCH;
                     int[] lcMap = getMapping(lc);
                     if (lcMap != null) {
                         map = updateMap(i, map, lc, lcMap, LOWER_CASE_MATCH);
                     }
                 }
                 
                 if (uc != i) {
                     if (index == map.length) {
                         map = expandMap(map, 2);
                     }
                     map[index++] = uc;
                     map[index++] = UPPER_CASE_MATCH;
                     int[] ucMap = getMapping(uc);
                     if (ucMap != null) {
                         map = updateMap(i, map, uc, ucMap, UPPER_CASE_MATCH);
                     }
                 }
                 
                 set(i, map);
             }
         }
 
         mapBuilt = Boolean.TRUE;
     }",Buggy,"Fixing compilation errors on Java SE 1.4 and below. Character.toLowerCase(int) and Character.toUpperCase(int) didn't exist until Java SE 5.

git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@834593 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for compilation errors on older Java versions (1.4 and below). The error stems from using `Character.toLowerCase(int)` and `Character.toUpperCase(int)`, which were introduced in Java SE 5. The diff replaces these methods with `Character.toLowerCase((char) i)` and `Character.toUpperCase((char) i)`, casting the integer `i` to a character before passing it to the `toLowerCase` and `toUpperCase` methods. This resolves the compilation issue on older Java versions by using the `char` accepting methods instead of the `int` accepting methods. The answer is **Buggy**."
xerces2-j,7513.json,13892ec89f2b5203650caf1d7b5355df433dacfd,"@@ -1,74 +1,74 @@
         private void mergeSchemaGrammars(SchemaGrammar cachedGrammar, SchemaGrammar newGrammar) {
 
             /** Add new top-level element declarations. **/
             XSNamedMap map = newGrammar.getComponents(XSConstants.ELEMENT_DECLARATION);
             int length = map.getLength();
             for (int i = 0; i < length; ++i) {
                 XSElementDecl decl = (XSElementDecl) map.item(i);
-                if (cachedGrammar.getElementDeclaration(decl.getName()) == null) {
+                if (cachedGrammar.getGlobalElementDecl(decl.getName()) == null) {
                     cachedGrammar.addGlobalElementDecl(decl);
                 }
             }
             
             /** Add new top-level attribute declarations. **/
             map = newGrammar.getComponents(XSConstants.ATTRIBUTE_DECLARATION);
             length = map.getLength();
             for (int i = 0; i < length; ++i) {
                 XSAttributeDecl decl = (XSAttributeDecl) map.item(i);
-                if (cachedGrammar.getAttributeDeclaration(decl.getName()) == null) {
+                if (cachedGrammar.getGlobalAttributeDecl(decl.getName()) == null) {
                     cachedGrammar.addGlobalAttributeDecl(decl);
                 }
             }
             
             /** Add new top-level type definitions. **/
             map = newGrammar.getComponents(XSConstants.TYPE_DEFINITION);
             length = map.getLength();
             for (int i = 0; i < length; ++i) {
                 XSTypeDefinition decl = (XSTypeDefinition) map.item(i);
                 if (cachedGrammar.getGlobalTypeDecl(decl.getName()) == null) {
                     cachedGrammar.addGlobalTypeDecl(decl);
                 }
             }
             
             /** Add new top-level attribute group definitions. **/
             map = newGrammar.getComponents(XSConstants.ATTRIBUTE_GROUP);
             length = map.getLength();
             for (int i = 0; i < length; ++i) {
                 XSAttributeGroupDecl decl = (XSAttributeGroupDecl) map.item(i);
-                if (cachedGrammar.getAttributeDeclaration(decl.getName()) == null) {
+                if (cachedGrammar.getGlobalAttributeGroupDecl(decl.getName()) == null) {
                     cachedGrammar.addGlobalAttributeGroupDecl(decl);
                 }
             }
             
             /** Add new top-level model group definitions. **/
             map = newGrammar.getComponents(XSConstants.MODEL_GROUP);
             length = map.getLength();
             for (int i = 0; i < length; ++i) {
                 XSGroupDecl decl = (XSGroupDecl) map.item(i);
                 if (cachedGrammar.getGlobalGroupDecl(decl.getName()) == null) {
                     cachedGrammar.addGlobalGroupDecl(decl);
                 }
             }
             
             /** Add new top-level notation declarations. **/
             map = newGrammar.getComponents(XSConstants.NOTATION_DECLARATION);
             length = map.getLength();
             for (int i = 0; i < length; ++i) {
                 XSNotationDecl decl = (XSNotationDecl) map.item(i);
                 if (cachedGrammar.getGlobalNotationDecl(decl.getName()) == null) {
                     cachedGrammar.addGlobalNotationDecl(decl);
                 }
             }
             
             /** 
              * Add all annotations. Since these components are not named it's
              * possible we'll add duplicate components. There isn't much we can
              * do. It's no worse than XMLSchemaLoader when used as an XSLoader.
              */
             XSObjectList annotations = newGrammar.getAnnotations();
             length = annotations.getLength();
             for (int i = 0; i < length; ++i) {
                 cachedGrammar.addAnnotation((XSAnnotationImpl) annotations.item(i));
             }
             
         }",Buggy,"Fixing a few minor bugs.


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@320162 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates that the commit fixes minor bugs. The code changes involve replacing calls to `getElementDeclaration`, `getAttributeDeclaration` with `getGlobalElementDecl`, `getGlobalAttributeDecl`, and so on, within the `mergeSchemaGrammars` method. These changes suggest that the original code was incorrectly retrieving schema components, potentially leading to issues when merging schema grammars. The change ensures that the correct global declarations are retrieved and added, which is a bug fix. The answer is **Buggy**."
xerces2-j,2176.json,b16dd097ce2ebce1533edaab4e6743b08da63bfe,"@@ -1,26 +1,26 @@
     public int createDeferredDocumentType(String rootElementName,
                                           String publicId, String systemId) {
 
         // create node
         int nodeIndex = createNode(Node.DOCUMENT_TYPE_NODE);
         int chunk     = nodeIndex >> CHUNK_SHIFT;
         int index     = nodeIndex & CHUNK_MASK;
 
         // added for DOM2: createDoctype factory method includes
         // name, publicID, systemID
 
         // create extra data node
         int extraDataIndex = createNode((short)0); // node type unimportant
         int echunk = extraDataIndex >> CHUNK_SHIFT;
         int eindex = extraDataIndex & CHUNK_MASK;
 
         // save name, public id, system id
         setChunkValue(fNodeName, rootElementName, chunk, index);
-        setChunkValue(fNodeValue, publicId, chunk, eindex);
-        setChunkValue(fNodeURI, systemId, chunk, eindex);
+        setChunkValue(fNodeValue, publicId, chunk, index);
+        setChunkValue(fNodeURI, systemId, chunk, index);
         setChunkIndex(fNodeExtra, extraDataIndex, chunk, index);
 
         // return node index
         return nodeIndex;
 
     } // createDeferredDocumentType(String,String,String):int",Buggy,"Fixed bug that caused namespace information to be lost
in the deferred DOM.


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@317632 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix related to namespace information loss in the deferred DOM. The diff shows changes in the `createDeferredDocumentType` method. Specifically, the `setChunkValue` calls for `publicId` and `systemId` were originally using `eindex` (index of the extra data node) but are now using `index` (index of the document type node). This suggests that the public ID and system ID were being incorrectly associated with the extra data node instead of the document type node itself, leading to the loss of this information. This correction aligns with the commit message's claim of fixing a bug that caused namespace information to be lost. The answer is **Buggy**."
xerces2-j,5971.json,b3530fffbb47d1dc2dab5e538c013f388b4e142f,"@@ -1,26 +1,31 @@
         protected void resolveExternalSubsetAndRead()
             throws IOException, XNIException {
             
             fDTDDescription.setValues(null, null, fEntityManager.getCurrentResourceIdentifier().getExpandedSystemId(), null);
             fDTDDescription.setRootName(fElementQName.rawname);
             XMLInputSource src = fExternalSubsetResolver.getExternalSubset(fDTDDescription);
             
             if (src != null) {
                 fDoctypeName = fElementQName.rawname;
                 fDoctypePublicId = src.getPublicId();
                 fDoctypeSystemId = src.getSystemId();
                 // call document handler
                 if (fDocumentHandler != null) {
                     // This inserts a doctypeDecl event into the stream though no 
                     // DOCTYPE existed in the instance document.
                     fDocumentHandler.doctypeDecl(fDoctypeName, fDoctypePublicId, fDoctypeSystemId, null);
                 }
                 try {
-                    fDTDScanner.setInputSource(src);
-                    while (fDTDScanner.scanDTDExternalSubset(true));
+                    if (fValidationManager == null || !fValidationManager.isCachedDTD()) {
+                        fDTDScanner.setInputSource(src);
+                        while (fDTDScanner.scanDTDExternalSubset(true));
+                    }
+                    else {
+                        fDTDScanner.setInputSource(null);
+                    }
                 }
                 finally {
                     fEntityManager.setEntityHandler(XMLDocumentScannerImpl.this);
                 }
             }
         } // resolveExternalSubsetAndRead()",Buggy,"Fixing a bug reported by Ritu Raj Tiwari on xerces-j-dev. When resolving
an external subset if there's already a DTD in the grammar pool do not
read the input source.


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@320234 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix related to external subset resolution in Xerces-J. The bug occurs when resolving an external subset if a DTD already exists in the grammar pool. The fix avoids reading the input source in this scenario.

The code diff shows a modification within the `resolveExternalSubsetAndRead()` method. Specifically, the code now checks if `fValidationManager` is null or if `fValidationManager.isCachedDTD()` returns false. If either of these conditions is true, the code proceeds to set the input source for the `fDTDScanner` and scan the external subset. Otherwise, if a cached DTD exists, the input source is set to null, effectively preventing the redundant reading of the input source.

This change directly addresses the bug described in the commit message. By preventing the input source from being read when a cached DTD is available, the fix avoids potential errors or inconsistencies that could arise from processing the same DTD information multiple times.

Therefore, the changes indicate a bug fix.
**Buggy**
"
xerces2-j,2755.json,2b2593eef2e2d2c5352fbc9b4e356053a93e34ab,"@@ -1,6 +1,12 @@
     Node matchNodeOrParent(Node node) {
+        // Additions and removals in the underlying data structure may occur
+        // before any iterations, and in this case the reference_node is null.
+        if (fCurrentNode == null) return null;
+        
+        // check if the removed node is an _ancestor_ of the 
+        // reference node
         for (Node n = fCurrentNode; n != fRoot; n = n.getParentNode()) {
             if (node == n) return n;
         }
         return null;
     }",Buggy,"Bug fix: http://nagoya.apache.org/bugzilla/show_bug.cgi?id=13062


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@318813 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix related to a bugzilla issue. The code change adds a null check for `fCurrentNode` at the beginning of the `matchNodeOrParent` method. This check handles cases where the underlying data structure has been modified before any iterations, potentially leading to a null pointer exception if `fCurrentNode` is null. The added check prevents this exception, indicating a bug fix. The answer is **Buggy**."
xerces2-j,2755.json,7f856ddb5537fc5e98c71ff23700a48bac7f652d,"@@ -1,6 +1,6 @@
     Node matchNodeOrParent(Node node) {
-        for (Node n = node; n != fRoot; n = n.getParentNode()) {
+        for (Node n = fCurrentNode; n != fRoot; n = n.getParentNode()) {
             if (node == n) return n;
         }
         return null;
     }",Buggy,"Applying patch suggested by Joe Kesselman. This fixes the following bug:
http://nagoya.apache.org/bugzilla/show_bug.cgi?id=6888


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@318267 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message explicitly states that the patch fixes a bug reported in bugzilla. The code change modifies the starting point of the loop from the input `node` to `fCurrentNode`. This suggests that the original code had an issue where it was not correctly traversing the parent nodes from the intended starting point, leading to incorrect matching. The change ensures the loop starts from the correct node (`fCurrentNode`), which aligns with the bug fix described in the commit message. The answer is **Buggy**.
"
xerces2-j,4545.json,3893a210786fea6e8236c54214287515c5333708,"@@ -1,8 +1,9 @@
     public void setFeature(String featureId, boolean state)
         throws XMLConfigurationException {
         if (featureId.equals(XINCLUDE_FEATURE)) {
             fXIncludeEnabled = state;
+            fConfigUpdated = true;
             return;
         }
         super.setFeature(featureId,state);
     }",Buggy,"Fixing a bug. It was possible that the XIncludeHandler could be used without being properly reset.


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@320336 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix related to the XIncludeHandler not being properly reset. The diff shows that when the `XINCLUDE_FEATURE` is set, a flag `fConfigUpdated` is set to true. This flag likely indicates that the configuration has been updated and that the XIncludeHandler needs to be reset or reinitialized. This change directly addresses the bug described in the commit message. The answer is **Buggy**."
xerces2-j,376.json,9768ff6cd6cd57357deb3c4be70cb29bb476e3d2,"@@ -1,15 +1,16 @@
     public String getJavaEncoding() {
         String javaEncoding = null;
         String mimeEncoding = getMimeEncoding();
 
         if (mimeEncoding != null) {
             if (mimeEncoding.equals( ""DEFAULT"" ))
                 javaEncoding =  ""UTF8"";
             else if (mimeEncoding.equalsIgnoreCase( ""UTF-16"" ))
                 javaEncoding = ""Unicode"";
             else
                 javaEncoding = EncodingMap.getIANA2JavaMapping( mimeEncoding );    
-        } else   // Should never return null
+        } 
+        if(javaEncoding == null)   // Should never return null
             javaEncoding = ""UTF8"";
         return(javaEncoding);
     }",Buggy,"fix for bug #6008


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@318133 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for bug #6008. The diff modifies the `getJavaEncoding` method. The original code could potentially return null if `mimeEncoding` was null and no mapping was found. The modified code adds a check for `javaEncoding == null` and defaults to ""UTF8"" if it is null. This ensures that the method always returns a non-null value, which likely addresses the bug. The answer is **Buggy**."
xerces2-j,5806.json,f6fbb6219a5d2f0cfd71f23812b4571049b5613b,"@@ -1,93 +1,97 @@
     Token parseFactor() throws ParseException {        
         int ch = this.read();
         Token tok;
         switch (ch) {
           case T_CARET:         return this.processCaret();
           case T_DOLLAR:        return this.processDollar();
           case T_LOOKAHEAD:     return this.processLookahead();
           case T_NEGATIVELOOKAHEAD: return this.processNegativelookahead();
           case T_LOOKBEHIND:    return this.processLookbehind();
           case T_NEGATIVELOOKBEHIND: return this.processNegativelookbehind();
 
           case T_COMMENT:
             this.next();
             return Token.createEmpty();
 
           case T_BACKSOLIDUS:
             switch (this.chardata) {
               case 'A': return this.processBacksolidus_A();
               case 'Z': return this.processBacksolidus_Z();
               case 'z': return this.processBacksolidus_z();
               case 'b': return this.processBacksolidus_b();
               case 'B': return this.processBacksolidus_B();
               case '<': return this.processBacksolidus_lt();
               case '>': return this.processBacksolidus_gt();
             }
                                                 // through down
         }
         tok = this.parseAtom();
         ch = this.read();
         switch (ch) {
           case T_STAR:  return this.processStar(tok);
           case T_PLUS:  return this.processPlus(tok);
           case T_QUESTION: return this.processQuestion(tok);
           case T_CHAR:
             if (this.chardata == '{' && this.offset < this.regexlen) {
 
                 int off = this.offset;          // this.offset -> next of '{'
                 int min = 0, max = -1;
 
                 if ((ch = this.regex.charAt(off++)) >= '0' && ch <= '9') {
 
                     min = ch -'0';
                     while (off < this.regexlen
                            && (ch = this.regex.charAt(off++)) >= '0' && ch <= '9') {
                         min = min*10 +ch-'0';
+                        if (min < 0)
+                            throw ex(""parser.quantifier.5"", this.offset);
                     }
                 }
                 else {
                     throw ex(""parser.quantifier.1"", this.offset);
                 }
 
                 max = min;
                 if (ch == ',') {
 
                    if (off >= this.regexlen) {
                        throw ex(""parser.quantifier.3"", this.offset);
                    }
                    else if ((ch = this.regex.charAt(off++)) >= '0' && ch <= '9') {                       
 
                         max = ch -'0';       // {min,max}
                         while (off < this.regexlen
                                && (ch = this.regex.charAt(off++)) >= '0'
                                && ch <= '9') {
                             max = max*10 +ch-'0';
+                            if (max < 0)
+                                throw ex(""parser.quantifier.5"", this.offset);
                         }
 
                         if (min > max)
                             throw ex(""parser.quantifier.4"", this.offset);
                    }
                    else { // assume {min,}
                         max = -1;           
                     }
                 }
 
                if (ch != '}')
                    throw ex(""parser.quantifier.2"", this.offset);
 
                if (this.checkQuestion(off)) {  // off -> next of '}'
                     tok = Token.createNGClosure(tok);
                     this.offset = off+1;
                 } else {
                     tok = Token.createClosure(tok);
                     this.offset = off;
                 }
 
                 tok.setMin(min);
                 tok.setMax(max);
                 //System.err.println(""CLOSURE: ""+min+"", ""+max);
                 this.next();
             }
         }
         return tok;
     }",Buggy,"Fixing bugs 17415: Regexes with large min/max not handled correctly.
Many thanks to Khaled Noaman for the patch.


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@319162 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for a bug related to regular expressions with large min/max values in quantifiers. The diff introduces checks for integer overflow (`min < 0` and `max < 0`) when parsing the min and max values within a quantifier like `{min,max}`. If an overflow occurs, a `ParseException` is thrown. This addition directly addresses the bug described in the commit message, preventing incorrect handling of large quantifier values. The answer is **Buggy**."
xerces2-j,5806.json,218c54d8d80c22fd4d9f45f8013e14eb23838699,"@@ -1,89 +1,93 @@
     Token parseFactor() throws ParseException {        
         int ch = this.read();
         Token tok;
         switch (ch) {
           case T_CARET:         return this.processCaret();
           case T_DOLLAR:        return this.processDollar();
           case T_LOOKAHEAD:     return this.processLookahead();
           case T_NEGATIVELOOKAHEAD: return this.processNegativelookahead();
           case T_LOOKBEHIND:    return this.processLookbehind();
           case T_NEGATIVELOOKBEHIND: return this.processNegativelookbehind();
 
           case T_COMMENT:
             this.next();
             return Token.createEmpty();
 
           case T_BACKSOLIDUS:
             switch (this.chardata) {
               case 'A': return this.processBacksolidus_A();
               case 'Z': return this.processBacksolidus_Z();
               case 'z': return this.processBacksolidus_z();
               case 'b': return this.processBacksolidus_b();
               case 'B': return this.processBacksolidus_B();
               case '<': return this.processBacksolidus_lt();
               case '>': return this.processBacksolidus_gt();
             }
                                                 // through down
         }
         tok = this.parseAtom();
         ch = this.read();
         switch (ch) {
           case T_STAR:  return this.processStar(tok);
           case T_PLUS:  return this.processPlus(tok);
           case T_QUESTION: return this.processQuestion(tok);
           case T_CHAR:
-            if (this.chardata == '{') {
-                                                // this.offset -> next of '{'
-                int off = this.offset;
+            if (this.chardata == '{' && this.offset < this.regexlen) {
+
+                int off = this.offset;          // this.offset -> next of '{'
                 int min = 0, max = -1;
-                if (off >= this.regexlen)  break;
-                ch = this.regex.charAt(off++);
-                if (ch != ',' && (ch < '0' || ch > '9'))  break;
-                if (ch != ',') {                // 0-9
-                    min = ch-'0';
+
+                if ((ch = this.regex.charAt(off++)) >= '0' && ch <= '9') {
+
+                    min = ch -'0';
                     while (off < this.regexlen
                            && (ch = this.regex.charAt(off++)) >= '0' && ch <= '9') {
                         min = min*10 +ch-'0';
-                        ch = -1;
                     }
-                    if (ch < 0)  break;
                 }
-                //if (off >= this.regexlen)  break;
+                else {
+                    throw ex(""parser.quantifier.1"", this.offset);
+                }
+
                 max = min;
                 if (ch == ',') {
-                    if (off >= this.regexlen
-                        || ((ch = this.regex.charAt(off++)) < '0' || ch > '9')
-                        && ch != '}')
-                        break;
-                    if (ch == '}') {
-                        max = -1;           // {min,}
-                    } else {
-                        max = ch-'0';       // {min,max}
+
+                   if (off >= this.regexlen) {
+                       throw ex(""parser.quantifier.3"", this.offset);
+                   }
+                   else if ((ch = this.regex.charAt(off++)) >= '0' && ch <= '9') {                       
+
+                        max = ch -'0';       // {min,max}
                         while (off < this.regexlen
                                && (ch = this.regex.charAt(off++)) >= '0'
                                && ch <= '9') {
                             max = max*10 +ch-'0';
-                            ch = -1;
                         }
-                        if (ch < 0)  break;
-                        //if (min > max)
-                        //    throw new ParseException(""parseFactor(): min > max: ""+min+"", ""+max);
+
+                        if (min > max)
+                            throw ex(""parser.quantifier.4"", this.offset);
+                   }
+                   else { // assume {min,}
+                        max = -1;           
                     }
                 }
-                if (ch != '}')  break;
-                                                // off -> next of '}'
-                if (this.checkQuestion(off)) {
+
+               if (ch != '}')
+                   throw ex(""parser.quantifier.2"", this.offset);
+
+               if (this.checkQuestion(off)) {  // off -> next of '}'
                     tok = Token.createNGClosure(tok);
                     this.offset = off+1;
                 } else {
                     tok = Token.createClosure(tok);
                     this.offset = off;
                 }
+
                 tok.setMin(min);
                 tok.setMax(max);
                 //System.err.println(""CLOSURE: ""+min+"", ""+max);
                 this.next();
             }
         }
         return tok;
     }",Buggy,"Fixing bugs 17417: Regex {min,max} with min > max not rejected.
Many thanks to Khaled Noaman for the patch.


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@319159 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for a bug where the regex `{min,max}` was not rejected when `min > max`. The diff modifies the `parseFactor` method to correctly handle quantifiers of the form `{min,max}`. Specifically, it adds checks to ensure that `min` and `max` are valid numbers, and it throws an exception if `min > max`. The code also includes more thorough error handling for various parsing scenarios related to quantifiers. The added exception throwing for invalid quantifier syntax and the check for `min > max` clearly indicate a bug fix. The answer is **Buggy**."
xerces2-j,7034.json,ba7898b39060234e9c88e9e06c0dd5fbed52a9fb,"@@ -1,12 +1,13 @@
     public void startDocument() throws SAXException {
         fNeedPushNSContext = true;
+        fNamespaceContext.reset();
         try {
             fSchemaDOMParser.startDocument(fSAXLocatorWrapper, null, fNamespaceContext, null);
         }
         catch (XMLParseException e) {
             convertToSAXParseException(e);
         }
         catch (XNIException e) {
             convertToSAXException(e);
         }
     }",Buggy,"Fixing a bug. We weren't reseting the NamespaceContext which can cause all sorts
of problems including the loss of some namespace decls on XSAnnotation components.

git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@644489 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix related to the `NamespaceContext` not being reset, potentially leading to loss of namespace declarations. The diff shows that `fNamespaceContext.reset()` is added to the `startDocument()` method. This aligns with the commit message, as resetting the namespace context at the start of a document would prevent the described issues. The answer is **Buggy**."
xerces2-j,3375.json,883dc18e1cd70be3f4dc6fcaf48ddcdfe4c2a99d,"@@ -1,19 +1,19 @@
         private int parseInt(int minDigits, int maxDigits)
             throws IllegalArgumentException {
             int vstart = vidx;
-            while (isDigit(peek()) && (vidx - vstart) <= maxDigits) {
+            while (isDigit(peek()) && (vidx - vstart) < maxDigits) {
                 vidx++;
             }
             if ((vidx - vstart) < minDigits) {
                 // we are expecting more digits
                 throw new IllegalArgumentException(value); //,vidx);
             }
 
             // NumberFormatException is IllegalArgumentException            
             //           try {
             return Integer.parseInt(value.substring(vstart, vidx));
             //            } catch( NumberFormatException e ) {
             //                // if the value is too long for int, NumberFormatException is thrown
             //                throw new IllegalArgumentException(value,vstart);
             //            }
         }",Buggy,"Fixing a bug. parseInt() was allowing maxDigits + 1 which caused 
XMLGregorianCalendar to accept bogus dates like ""2007-008-003"".

git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@565088 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix related to the `parseInt()` method accepting invalid dates due to allowing one extra digit. The diff modifies the `while` loop condition from `(vidx - vstart) <= maxDigits` to `(vidx - vstart) < maxDigits`. This change ensures that the method now correctly enforces the maximum number of digits, preventing the acceptance of bogus dates. The answer is **Buggy**."
xerces2-j,6542.json,3574857d4a4ddaf8aabec99216c178a5a6089a19,"@@ -1,46 +1,46 @@
     private void setOrdered(){
 
-        // When {variety} is atomic, {value} is inherited from {value} of {base type definition}. For all primitive types {value} is as specified in the table in Fundamental Facets (C.1).
+        // When {variety} is atomic, {value} is inherited from {value} of {base type definition}. For all ""primitive"" types {value} is as specified in the table in Fundamental Facets (C.1).
         if(fVariety == VARIETY_ATOMIC){
             this.fOrdered = fBase.fOrdered;
         }
 
         // When {variety} is list, {value} is false.
         else if(fVariety == VARIETY_LIST){
             this.fOrdered = ORDERED_FALSE;
         }
 
         // When {variety} is union, the {value} is partial unless one of the following:
         // 1. If every member of {member type definitions} is derived from a common ancestor other than the simple ur-type, then {value} is the same as that ancestor's ordered facet.
         // 2. If every member of {member type definitions} has a {value} of false for the ordered facet, then {value} is false.
         else if(fVariety == VARIETY_UNION){
             int length = fMemberTypes.length;
             // REVISIT: is the length possible to be 0?
             if (length == 0) {
                 this.fOrdered = ORDERED_PARTIAL;
                 return;
             }
             // we need to process the first member type before entering the loop
             short ancestorId = getPrimitiveDV(fMemberTypes[0].fValidationDV);
             boolean commonAnc = ancestorId != DV_ANYSIMPLETYPE;
             boolean allFalse = fMemberTypes[0].fOrdered == ORDERED_FALSE;
             // for the other member types, check whether the value is false
             // and whether they have the same ancestor as the first one
             for (int i = 1; i < fMemberTypes.length && (commonAnc || allFalse); i++) {
                 if (commonAnc)
                     commonAnc = ancestorId == getPrimitiveDV(fMemberTypes[i].fValidationDV);
                 if (allFalse)
                     allFalse = fMemberTypes[i].fOrdered == ORDERED_FALSE;
             }
             if (commonAnc) {
                 // REVISIT: all member types should have the same ordered value
                 //          just use the first one. Can we assume this?
                 this.fOrdered = fMemberTypes[0].fOrdered;
             } else if (allFalse) {
                 this.fOrdered = ORDERED_FALSE;
             } else {
                 this.fOrdered = ORDERED_PARTIAL;
             }
         }
 
     }//setOrdered",Buggy,"fixing bug 16714


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@319088 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix (""fixing bug 16714""). The code modifies the `setOrdered` method, specifically addressing the logic for determining the `ordered` facet of a simple type definition, especially for union types. The comments clarify the conditions under which the `ordered` facet should be set to `partial`, `false`, or the same as a common ancestor. The code ensures that the logic correctly handles cases where all member types are derived from a common ancestor or have a `false` value for the ordered facet. This suggests that the original logic might have been flawed, leading to incorrect `ordered` facet values for union types. The changes appear to correct this flaw. The answer is **Buggy**.
"
xerces2-j,7850.json,d6e418eec78c8b253b3f353be05259d353faa58a,"@@ -1,12 +1,12 @@
     private CMNode multiNodes(CMNode node, int num, boolean copyFirst) {
         if (num == 0) {
             return null;
         }
         if (num == 1) {
             return copyFirst ? copyNode(node) : node;
         }
         int num1 = num/2;
         return fNodeFactory.getCMBinOpNode(XSModelGroupImpl.MODELGROUP_SEQUENCE,
                                            multiNodes(node, num1, copyFirst),
-                                           multiNodes(node, num-num1, false));
+                                           multiNodes(node, num-num1, true));
     }",Buggy,"Fixing a bug introduced by a previous commit when trying to balance the
syntax tree.


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@319270 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix related to balancing the syntax tree, which was introduced by a previous commit. The diff modifies the `multiNodes` method, specifically changing the `copyFirst` parameter in the recursive call from `false` to `true`. This suggests a logical correction in how nodes are copied and combined during the tree balancing process. The original `false` value likely led to incorrect tree structures, which the change to `true` aims to rectify. The answer is **Buggy**.
"
xerces2-j,8612.json,841a543458d7b19286dbcc32207d8978ccf195f5,"@@ -1,19 +1,19 @@
     protected void init() {
         if(fValidation || fDynamicValidation) {
             super.init();
             // now overwrite some entries in parent:
 
             try {
                 fValID       = fDatatypeValidatorFactory.getBuiltInDV(""XML11ID"");
-                fValIDRef    = fDatatypeValidatorFactory.getBuiltInDV(""XML11IDREFS"");
+                fValIDRef    = fDatatypeValidatorFactory.getBuiltInDV(""XML11IDREF"");
                 fValIDRefs   = fDatatypeValidatorFactory.getBuiltInDV(""XML11IDREFS"");
-                fValNMTOKEN  = fDatatypeValidatorFactory.getBuiltInDV(""XML11NMTOKENSymbol"");
+                fValNMTOKEN  = fDatatypeValidatorFactory.getBuiltInDV(""XML11NMTOKEN"");
                 fValNMTOKENS = fDatatypeValidatorFactory.getBuiltInDV(""XML11NMTOKENS"");
 
             }
             catch (Exception e) {
                 // should never happen
                 e.printStackTrace(System.err);
             }
         }
     } // init()",Buggy,"Fixing 2 bugs:
1) Bugzilla# 18429, NPE thrown validating NMTOKEN. Thanks to Neil Delima for the patch.
2) S production is unchanged in XML 1.1, so shouldn't override isSpace()


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@319269 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates the fixing of two bugs, one of which is a NullPointerException (NPE) when validating NMTOKEN. The diff shows changes in the `init()` method where datatype validators are being retrieved. Specifically, `fValNMTOKEN` is being assigned a validator with the id ""XML11NMTOKEN"" instead of ""XML11NMTOKENSymbol"". This change suggests a correction in how the NMTOKEN validator is initialized, which could directly address the reported NPE. The other changes, such as the change from XML11IDREFS to XML11IDREF, also indicate bug fixes. Therefore, the changes are related to fixing bugs. The answer is **Buggy**.
"
xerces2-j,27.json,874363b725dcbddf23ed1f1d07d9a6fcd6e3ca4c,"@@ -1,46 +1,47 @@
     public void attributeDecl(String elementName, String attributeName,
                               String type, String[] enumeration,
                               String defaultType, XMLString defaultValue, 
+                              XMLString nonNormalizedDefaultValue,
                               Augmentations augs) throws XNIException {
 
         printIndent();
         fOut.print(""attributeDecl("");
         fOut.print(""elementName="");
         printQuotedString(elementName);
         fOut.print(',');
         fOut.print(""attributeName="");
         printQuotedString(attributeName);
         fOut.print(',');
         fOut.print(""type="");
         printQuotedString(type);
         fOut.print(',');
         fOut.print(""enumeration="");
         if (enumeration == null) {
             fOut.print(""null"");
         }
         else {
             fOut.print('{');
             for (int i = 0; i < enumeration.length; i++) {
                 printQuotedString(enumeration[i]);
                 if (i < enumeration.length - 1) {
                     fOut.print(',');
                 }
             }
             fOut.print('}');
         }
         fOut.print(',');
         fOut.print(""defaultType="");
         printQuotedString(defaultType);
         fOut.print(',');
         fOut.print(""defaultValue="");
         if (defaultValue == null) {
             fOut.print(""null"");
         }
         else {
             printQuotedString(defaultValue.ch, defaultValue.offset,
                               defaultValue.length);
         }
         fOut.println(')');
         fOut.flush();
 
     } // attributeDecl(String,String,String,String[],String,XMLString)",Buggy,"corrected attributeDecl signature in samples.
Thanks to John Spitzer for pointing that out (bug#6447).


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@318206 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a correction to the `attributeDecl` signature, attributed to a reported bug (bug#6447). The diff shows the addition of the `XMLString nonNormalizedDefaultValue` parameter to the `attributeDecl` method. This change directly addresses the incorrect signature mentioned in the commit message, implying a bug fix related to the method's definition. The answer is **Buggy**.
"
xerces2-j,7660.json,c034fce51f8f78ab929b74f4a902deb10034459a,"@@ -1,5 +1,5 @@
     static final Field[] resize(Field[] oldArray, int newSize) {
         Field[] newArray = new Field[newSize];
-        System.arraycopy(oldArray, 0, newArray, 0, newSize);
+        System.arraycopy(oldArray, 0, newArray, 0, oldArray.length);
         return newArray;
     }",Buggy,"Fixing bug 9022: arrayCopy should take oldArray.length, instead of newSize.


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@318392 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix related to `arrayCopy` where the wrong size was being used. The diff shows that the `arrayCopy` method's length parameter was changed from `newSize` to `oldArray.length`. This correction aligns with the commit message, indicating that the original code was using the new array's size instead of the old array's size, which would lead to incorrect data being copied if the new size was smaller than the old size. The answer is **Buggy**."
xerces2-j,938.json,0dde63af37b11224e5bc46928596405440cd0107,"@@ -1,15 +1,8 @@
     public Node cloneNode( boolean deep )
     {
         HTMLDocumentImpl    clone;
-        NodeImpl            node;
 
         clone = new HTMLDocumentImpl();
-        if ( deep ) {
-            node = (NodeImpl) getFirstChild();
-            while ( node != null ) {
-                clone.appendChild( clone.importNode( node, true ) );
-                node = (NodeImpl) node.getNextSibling();
-            }
-        }
+        cloneNode(clone, deep);
         return clone;
     }",Buggy,"Fixing JIRA Bug #1021:
http://issues.apache.org/jira/browse/XERCESJ-1021

DocumentType nodes cannot be imported. Instead of attempting to loop
over the children of the Document node we should delegate to the
cloneNode(CoreDocumentImpl,boolean) method on DocumentImpl which
knows how to do the right thing. This should also fix ID assignment
and possibly other things.

git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@418366 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message explicitly states that it's fixing a bug (JIRA Bug #1021) where DocumentType nodes couldn't be imported correctly. The code diff shows that the original implementation attempted to manually clone the children of the Document node. This approach was flawed and led to issues with ID assignment and potentially other problems. The fix replaces this manual cloning with a call to `cloneNode(clone, deep)`, which is a method within `DocumentImpl` that correctly handles the cloning process, including DocumentType nodes and ID assignment. This change addresses the bug described in the commit message. The answer is **Buggy**.
"
xerces2-j,8262.json,ef4f12df6ceef58d0976e59c42d72c11836172c8,"@@ -1,4 +1,10 @@
     public void setInputSource(XMLInputSource inputSource) throws IOException {
+        if (inputSource == null) {
+            // no system id was available
+            fDTDHandler.startDTD(null, null);
+            fDTDHandler.endDTD(null);
+            return;
+        }
         fEntityManager.setEntityHandler(this);
         fEntityManager.startDTDEntity(inputSource);
     } // setInputSource(XMLInputSource)",Buggy,"Fixing bug http://nagoya.apache.org/bugzilla/show_bug.cgi?id=11176


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@318583 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix. The diff shows a null check for `inputSource` in the `setInputSource` method. If `inputSource` is null, the code now calls `startDTD` and `endDTD` on `fDTDHandler` with null arguments and returns. This prevents a potential NullPointerException or other errors that might occur if `fEntityManager.setEntityHandler(this)` or `fEntityManager.startDTDEntity(inputSource)` were called with a null `inputSource`. This is a bug fix. The answer is **Buggy**.
"
xerces2-j,1373.json,51be532c652cf9a20ca7e950b34b2f00f7dafdd3,"@@ -1,4 +1,4 @@
     public void printText( StringBuffer text )
     {
-        _text.append( text );
+        _text.append( text.toString() );
     }",Buggy,"fixing bug 15768.  According to the javadoc for StringBuffer and String, this is what the original code would have compiled to; this fix simply makes sure that, if compiled under JDK 1.4, Xerces will work with previous JDKs


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@318938 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix related to compatibility with older JDKs, specifically addressing issue 15768. The code change involves modifying the `printText` method to explicitly convert the `StringBuffer` argument to a `String` before appending it to the internal `_text` buffer. This suggests that the original code might have behaved differently or caused issues when compiled under JDK 1.4 and used with previous JDKs, potentially due to differences in how `StringBuffer` was handled. The explicit conversion to `String` ensures consistent behavior across different JDK versions, resolving the compatibility bug. The answer is **Buggy**."
xerces2-j,5333.json,5ba592aa9007de9e5392ebab7095bfd0cfa63824,"@@ -1,13 +1,13 @@
     protected boolean sameBaseURIAsIncludeParent() {
         String parentBaseURI = getIncludeParentBaseURI();
         String baseURI = fCurrentBaseURI.getExpandedSystemId();
         // REVISIT: should we use File#sameFile() ?
         //          I think the benefit of using it is that it resolves host names
         //          instead of just doing a string comparison.
         // TODO: [base URI] is still an open issue with the working group.
         //       They're deciding if xml:base should be added if the [base URI] is different in terms
         //       of resolving relative references, or if it should be added if they are different at all.
         //       Revisit this after a final decision has been made.
         //       The decision also affects whether we output the file name of the URI, or just the path.
-        return parentBaseURI.equals(baseURI);
+        return parentBaseURI != null && parentBaseURI.equals(baseURI);
     }",Buggy,"Partial fix for Bug #24992. Fix NPE which would occur if an 
include parent has no base URI.


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@319611 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message explicitly states a fix for a NullPointerException (NPE) that occurs when an include parent has no base URI. The code diff adds a null check (`parentBaseURI != null`) before comparing `parentBaseURI` with `baseURI`. This prevents the `equals()` method from being called on a null `parentBaseURI`, thus resolving the reported NPE. The answer is **Buggy**."
hibernate-orm,14257.json,8463057b85db928fe8d3aa0b4379bc5b0f255c71,"@@ -1,110 +1,123 @@
 	private void addSetter(ClassFile classfile, final Method[] setters) throws CannotCompileException {
 		ConstPool cp = classfile.getConstPool();
 		int target_type_index = cp.addClassInfo( this.targetBean.getName() );
 		String desc = GET_SETTER_DESC;
 		MethodInfo mi = new MethodInfo( cp, GENERATED_SETTER_NAME, desc );
 
 		Bytecode code = new Bytecode( cp, 4, 6 );
+		StackMapTable stackmap = null;
 		/* | this | bean | args | i | raw bean | exception | */
 		if ( setters.length > 0 ) {
 			int start, end; // required to exception table
 			// iconst_0 // i
 			code.addIconst( 0 );
 			// istore_3 // store i
 			code.addIstore( 3 );
 			// aload_1 // load the bean
 			code.addAload( 1 );
 			// checkcast // cast the bean into a raw bean
 			code.addCheckcast( this.targetBean.getName() );
 			// astore 4 // store the raw bean
 			code.addAstore( 4 );
 			/* current stack len = 0 */
 			// start region to handling exception (BulkAccessorException)
 			start = code.currentPc();
 			int lastIndex = 0;
 			for ( int i = 0; i < setters.length; ++i ) {
 				if ( setters[i] != null ) {
 					int diff = i - lastIndex;
 					if ( diff > 0 ) {
 						// iinc 3, 1
 						code.addOpcode( Opcode.IINC );
 						code.add( 3 );
 						code.add( diff );
 						lastIndex = i;
 					}
 				}
 				/* current stack len = 0 */
 				// aload 4 // load the raw bean
 				code.addAload( 4 );
 				// aload_2 // load the args
 				code.addAload( 2 );
 				// iconst_i
 				code.addIconst( i );
 				// aaload
 				code.addOpcode( Opcode.AALOAD );
 				// checkcast
 				Class[] setterParamTypes = setters[i].getParameterTypes();
 				Class setterParamType = setterParamTypes[0];
 				if ( setterParamType.isPrimitive() ) {
 					// checkcast (case of primitive type)
 					// invokevirtual (case of primitive type)
 					this.addUnwrapper( classfile, code, setterParamType );
 				}
 				else {
 					// checkcast (case of reference type)
 					code.addCheckcast( setterParamType.getName() );
 				}
 				/* current stack len = 2 */
 				String rawSetterMethod_desc = RuntimeSupport.makeDescriptor( setters[i] );
 				if ( !this.targetBean.isInterface() ) {
 					// invokevirtual
 					code.addInvokevirtual( target_type_index, setters[i].getName(), rawSetterMethod_desc );
 				}
 				else {
 					// invokeinterface
 					Class[] params = setters[i].getParameterTypes();
 					int size;
 					if ( params[0].equals( Double.TYPE ) || params[0].equals( Long.TYPE ) ) {
 						size = 3;
 					}
 					else {
 						size = 2;
 					}
 
 					code.addInvokeinterface( target_type_index, setters[i].getName(), rawSetterMethod_desc, size );
 				}
 			}
 
 			// end region to handling exception (BulkAccessorException)
 			end = code.currentPc();
 			// return
 			code.addOpcode( Opcode.RETURN );
 			/* current stack len = 0 */
 			// register in exception table
 			int throwableType_index = cp.addClassInfo( THROWABLE_CLASS_NAME );
-			code.addExceptionHandler( start, end, code.currentPc(), throwableType_index );
+			int handler_pc = code.currentPc();
+			code.addExceptionHandler( start, end, handler_pc, throwableType_index );
 			// astore 5 // store exception
 			code.addAstore( 5 );
 			// new // BulkAccessorException
 			code.addNew( BULKEXCEPTION_CLASS_NAME );
 			// dup
 			code.addOpcode( Opcode.DUP );
 			// aload 5 // load exception
 			code.addAload( 5 );
 			// iload_3 // i
 			code.addIload( 3 );
 			// invokespecial // BulkAccessorException.<init>
 			String cons_desc = ""(Ljava/lang/Throwable;I)V"";
 			code.addInvokespecial( BULKEXCEPTION_CLASS_NAME, MethodInfo.nameInit, cons_desc );
 			// athrow
 			code.addOpcode( Opcode.ATHROW );
+			StackMapTable.Writer writer = new StackMapTable.Writer(32);
+			int[] localTags = { StackMapTable.OBJECT, StackMapTable.OBJECT, StackMapTable.OBJECT, StackMapTable.INTEGER };
+			int[] localData = { cp.getThisClassInfo(), cp.addClassInfo(""java/lang/Object""),
+                        	cp.addClassInfo(""[Ljava/lang/Object;""), 0};
+			int[] stackTags = { StackMapTable.OBJECT };
+			int[] stackData = { throwableType_index };
+			writer.fullFrame(handler_pc, localTags, localData, stackTags, stackData);
+			stackmap = writer.toStackMapTable(cp);
 		}
 		else {
 			// return
 			code.addOpcode( Opcode.RETURN );
 		}
-
-		mi.setCodeAttribute( code.toCodeAttribute() );
+		CodeAttribute ca = code.toCodeAttribute();
+		if (stackmap != null) {
+			ca.setAttribute(stackmap);
+		}
+		mi.setCodeAttribute( ca );
 		mi.setAccessFlags( AccessFlag.PUBLIC );
 		classfile.addMethod( mi );
 	}",Buggy,"HHH-7850 BulkAccessorFactory Java 7 verify error resolved by JASSIST-163 BulkAccessorFactory.java.diff2 patch from Shigeru Chiba.
",Buggy,"The commit message ""HHH-7850 BulkAccessorFactory Java 7 verify error resolved by JASSIST-163 BulkAccessorFactory.java.diff2 patch from Shigeru Chiba"" indicates a bug fix. The diff shows changes to the `addSetter` method, specifically adding a `StackMapTable` to the generated bytecode. This is essential for Java 7 and later to verify the bytecode and prevent `VerifyError` exceptions. The addition of the stack map table resolves the verification issue, which is a bug. The answer is **Buggy**."
hibernate-orm,15873.json,8fb35947ff6525a8286295f2959cd5a71502b2f9,"@@ -1,3 +1,3 @@
 	public String getQuerySequencesString() {
-		return ""select table_name from information_schema.TABLES where table_type='SEQUENCE'"";
+		return ""select table_name from information_schema.TABLES where table_schema = database() and table_type = 'SEQUENCE'"";
 	}",Buggy,"HHH-13373 fix the bug that 'different sequence names in differnt dbs' breaks SequenceInformationExtractorMariaDBDatabaseImpl
",Buggy,"The commit message indicates a bug fix related to sequence name handling in different databases, specifically for MariaDB. The code change modifies the SQL query used to retrieve sequence names. The original query only checked the `table_name` and `table_type`, but the corrected query adds a condition `table_schema = database()` to filter sequences based on the current database. This ensures that the query only returns sequences relevant to the current database, resolving the issue of conflicts arising from sequences with the same name in different databases. The answer is **Buggy**.
"
hibernate-orm,15816.json,fc6312a73b99c8edf2af32122165cc8b31b79aa7,"@@ -1,3 +1,8 @@
 	public String getQuerySequencesString() {
-		return null ;
+		if ( supportsSequences() ) {
+			return ""select SEQUENCENAME from SYS.SYSSEQUENCES"";
+		}
+		else {
+			throw new MappingException( ""Derby does not support sequence prior to release 10.6.1.0"" );
+		}
 	}",Buggy,"HHH-10110 - Fix DerbyDialect#getQuerySequencesString() causing error during schema update
",Buggy,"The commit message indicates a fix for an error in `DerbyDialect#getQuerySequencesString()` that occurs during schema updates. The diff shows that the method now returns a SQL query string to retrieve sequence names from the `SYS.SYSSEQUENCES` table if sequences are supported. If sequences are not supported, it throws a `MappingException`. This change addresses a bug where the method previously returned `null`, likely causing the schema update to fail. The answer is **Buggy**."
hibernate-orm,22027.json,b33c6d435780a96d6f911fbf8a9d796f10c7af0d,"@@ -1,15 +1,17 @@
-	public void onLoad(final LoadEvent event,
-					   final LoadEventListener.LoadType loadType) throws HibernateException {
+	public void onLoad(
+			final LoadEvent event,
+			final LoadEventListener.LoadType loadType) throws HibernateException {
 
 		final EntityPersister persister = getPersister( event );
 
 		if ( persister == null ) {
 			throw new HibernateException( ""Unable to locate persister: "" + event.getEntityClassName() );
 		}
 
 		final Class idClass = persister.getIdentifierType().getReturnedClass();
-		if ( idClass != null && !idClass.isInstance( event.getEntityId() ) )
+		if ( idClass != null && !idClass.isInstance( event.getEntityId() ) ) {
 			checkIdClass( persister, event, loadType, idClass );
+		}
 
 		doOnLoad( persister, event, loadType );
 	}",Buggy,"HHH-10366 - Fix checkstyle error
",NotBuggy,"The commit message indicates a fix for a checkstyle error. The diff shows changes in code formatting, specifically related to line breaks and spacing within the `onLoad` method. These changes do not alter the functionality of the code but improve its readability and adherence to coding style guidelines. Therefore, this is not a bug fix. The answer is **NotBuggy**."
hibernate-orm,15530.json,220365600cd7692ede1977d366966a5828dc749c,"@@ -1,7 +1,14 @@
 	public String getProcessedSql() {
-		boolean useLimitOffset = supportsLimit() && supportsLimitOffset()
-				&& LimitHelper.hasFirstRow( selection ) && LimitHelper.hasMaxRows( selection );
-		return dialect.getLimitString(
-				sql, useLimitOffset ? LimitHelper.getFirstRow( selection ) : 0, getMaxOrLimit()
-		);
+		if (LimitHelper.useLimit(this, selection)) {
+			// useLimitOffset: whether ""offset"" is set or not;
+			// if set, use ""LIMIT offset, row_count"" syntax;
+			// if not, use ""LIMIT row_count""
+			boolean useLimitOffset = LimitHelper.hasFirstRow(selection);
+
+			return new StringBuilder(sql.length() + 20).append(sql)
+							.append(useLimitOffset ? "" limit ?, ?"" : "" limit ?"").toString();
+		}
+		else {
+			return sql; // or return unaltered SQL
+		}
 	}",Buggy,"HHH-7716 Fixed a bug in CUBRIDLimitHandler. Now correctly processes
LIMIT
clause
in SQL.
",Buggy,"The commit message indicates a bug fix in CUBRIDLimitHandler related to processing the LIMIT clause in SQL queries. The diff shows that the code now correctly handles the LIMIT clause by checking if the limit should be applied using `LimitHelper.useLimit(this, selection)`. If the limit is applicable, it constructs the SQL query with the appropriate LIMIT syntax, either with or without an offset, based on whether `LimitHelper.hasFirstRow(selection)` returns true. If the limit is not applicable, it returns the original SQL query. This change fixes a bug where the LIMIT clause was not being processed correctly. The answer is **Buggy**."
hibernate-orm,26062.json,6a14aeeba79d3536a893f63154c0b697db3f3711,"@@ -1,13 +1,15 @@
 	private void processExpression(ReadWriteExpression annotation) {
 		String nonNullLogicalColumnName = logicalColumnName != null ? logicalColumnName : """"; //use the default for annotations 
-		if ( annotation != null && annotation.forColumn().equals( nonNullLogicalColumnName ) ) {
+		if ( annotation != null &&
+				( StringHelper.isEmpty( annotation.forColumn() )
+						|| annotation.forColumn().equals( nonNullLogicalColumnName ) ) ) {
 			readExpression = annotation.read();
 			if ( StringHelper.isEmpty( readExpression ) ) {
 				readExpression = null;
 			}
 			writeExpression = annotation.write();
 			if ( StringHelper.isEmpty( writeExpression ) ) {
 				writeExpression = null;
 			}
 		}
 	}",Buggy,"HHH-4510 Fix bug where explicit @Column would not match @ReadWriteExpresion with empty forColumn

git-svn-id: https://svn.jboss.org/repos/hibernate/core/trunk@20747 1b8cb986-b30d-0410-93ca-fae66ebed9b2
",Buggy,"The commit message indicates a bug fix related to `@Column` not matching `@ReadWriteExpresion` with an empty `forColumn` attribute. The code change modifies the condition that checks if the `ReadWriteExpression` annotation should be processed. It now includes a check for an empty `annotation.forColumn()` using `StringHelper.isEmpty()`. If `forColumn` is empty, the expression is processed. This fixes the bug where an explicit `@Column` would not match a `@ReadWriteExpresion` with an empty `forColumn`. The answer is **Buggy**."
hibernate-orm,4331.json,f5e10c29ebf103df4de7ea7b528702f0d2d9fc2a,"@@ -1,9 +1,18 @@
 	public void destroy() {
 		if ( !active ) {
 			return;
 		}
 		active = false;
 		destroy( classLoaderServiceBinding );
 		destroy( strategySelectorBinding );
 		destroy( integratorServiceBinding );
+
+		if ( childRegistries != null ) {
+			for(ServiceRegistry serviceRegistry : childRegistries) {
+				if(serviceRegistry instanceof ServiceRegistryImplementor) {
+					ServiceRegistryImplementor serviceRegistryImplementor = (ServiceRegistryImplementor) serviceRegistry;
+					serviceRegistryImplementor.destroy();
+				}
+			}
+		}
 	}",Buggy,"HHH-10907 - Fix connection leak problem in hibernate-core tests
",Buggy,"The commit message indicates a fix for a connection leak in hibernate-core tests. The diff adds a loop that iterates through child registries and destroys them if they are instances of `ServiceRegistryImplementor`. This suggests that the previous code was not properly closing or releasing resources associated with these child registries, leading to a connection leak. The added code ensures that these registries are destroyed, thus fixing the leak. The answer is **Buggy**.
"
hibernate-orm,3407.json,8b9f171a034f7604853d4c4bc1ffa78a8e2991fe,"@@ -1,54 +1,55 @@
 	private void cleanseUniqueKeyMap() {
 		// We need to account for a few conditions here...
 		// 	1) If there are multiple unique keys contained in the uniqueKeys Map, we need to deduplicate
 		// 		any sharing the same columns as other defined unique keys; this is needed for the annotation
 		// 		processor since it creates unique constraints automagically for the user
 		//	2) Remove any unique keys that share the same columns as the primary key; again, this is
 		//		needed for the annotation processor to handle @Id @OneToOne cases.  In such cases the
 		//		unique key is unnecessary because a primary key is already unique by definition.  We handle
 		//		this case specifically because some databases fail if you try to apply a unique key to
 		//		the primary key columns which causes schema export to fail in these cases.
 		if ( uniqueKeys.isEmpty() ) {
 			// nothing to do
 			return;
 		}
 		else if ( uniqueKeys.size() == 1 ) {
 			// we have to worry about condition 2 above, but not condition 1
 			final Map.Entry<String,UniqueKey> uniqueKeyEntry = uniqueKeys.entrySet().iterator().next();
 			if ( isSameAsPrimaryKeyColumns( uniqueKeyEntry.getValue() ) ) {
 				uniqueKeys.remove( uniqueKeyEntry.getKey() );
 			}
 		}
 		else {
 			// we have to check both conditions 1 and 2
 			final Iterator<Map.Entry<String,UniqueKey>> uniqueKeyEntries = uniqueKeys.entrySet().iterator();
 			while ( uniqueKeyEntries.hasNext() ) {
 				final Map.Entry<String,UniqueKey> uniqueKeyEntry = uniqueKeyEntries.next();
 				final UniqueKey uniqueKey = uniqueKeyEntry.getValue();
 				boolean removeIt = false;
 
 				// condition 1 : check against other unique keys
 				for ( UniqueKey otherUniqueKey : uniqueKeys.values() ) {
 					// make sure its not the same unique key
 					if ( uniqueKeyEntry.getValue() == otherUniqueKey ) {
 						continue;
 					}
 					if ( otherUniqueKey.getColumns().containsAll( uniqueKey.getColumns() )
 							&& uniqueKey.getColumns().containsAll( otherUniqueKey.getColumns() ) ) {
 						removeIt = true;
 						break;
 					}
 				}
 
 				// condition 2 : check against pk
 				if ( isSameAsPrimaryKeyColumns( uniqueKeyEntry.getValue() ) ) {
 					removeIt = true;
 				}
 
 				if ( removeIt ) {
-					uniqueKeys.remove( uniqueKeyEntry.getKey() );
+					//uniqueKeys.remove( uniqueKeyEntry.getKey() );
+					uniqueKeyEntries.remove();
 				}
 			}
 
 		}
 	}",Buggy,"HHH-7446 bug-fix
",Buggy,"The commit message indicates a bug fix (HHH-7446). The code modifies the `cleanseUniqueKeyMap` method, which deduplicates and removes unnecessary unique keys. The change replaces `uniqueKeys.remove(uniqueKeyEntry.getKey())` with `uniqueKeyEntries.remove()`. This is crucial because removing elements from a map while iterating over its entry set using `uniqueKeys.entrySet().iterator()` can lead to a `ConcurrentModificationException`. Using the iterator's `remove()` method ensures safe removal during iteration. This directly addresses a potential bug related to concurrent modification. The answer is **Buggy**."
hibernate-orm,29945.json,8e6fcce523698018a1e9952a8cf3a78485458ac7,"@@ -1,25 +1,25 @@
 	private void internalSetValue(T value) {
 		if ( procedureParameter.getMode() != ParameterMode.IN && procedureParameter.getMode() != ParameterMode.INOUT ) {
 			throw new IllegalStateException( ""Can only bind values for IN/INOUT parameters : "" + procedureParameter );
 		}
 
 		if ( procedureParameter.getParameterType() != null ) {
 			if ( value == null ) {
 				if ( !procedureParameter.isPassNullsEnabled() ) {
-					throw new IllegalArgumentException( ""The parameter with the ["" +
+					throw new IllegalArgumentException( ""The parameter "" +
 							( procedureParameter.getName() != null
-									? procedureParameter.getName() + ""] name""
-									: procedureParameter.getPosition() + ""] position"" )
+									? ""named ["" + procedureParameter.getName() + ""]""
+									: ""at position ["" + procedureParameter.getPosition() + ""]"" )
 							+ "" was null. You need to call ParameterRegistration#enablePassingNulls(true) in order to pass null parameters."" );
 				}
 			}
 			else if ( !procedureParameter.getParameterType().isInstance( value ) &&
 					!procedureParameter.getHibernateType().getReturnedClass().isInstance( value ) ) {
 				throw new IllegalArgumentException( ""Bind value ["" + value + ""] was not of specified type ["" + procedureParameter
 						.getParameterType() );
 			}
 		}
 
 		this.value = value;
 		this.isBound = true;
 	}",Buggy,"HHH-12905 Improve the error message and update the tests accordingly

Also fix a loose end in the MySQL test: at least with MariaDB, using
a bit(1) as datatype for boolean does not work: it always return true
even if you set it to 0. Using either boolean or tinyint(1) solves
the issue.

As I'm not sure older versions of MySQL supports a real boolean type I
used a tinyint(1).
",Buggy,"The commit message indicates an improvement to the error message and a fix for a MySQL test case. The diff shows a change in the error message thrown when a null value is passed to a parameter that doesn't allow nulls. The original message was less descriptive, while the new message includes the parameter name or position, making it easier to identify the problematic parameter. Additionally, the commit message mentions a fix for a MySQL test related to boolean data types. The provided diff doesn't show the test fix, but the error message improvement aligns with the commit message's description of a bug fix. The answer is **Buggy**.
"
hibernate-orm,9074.json,50b788266339ac7f22f2f43dc011378e7b0b0aad,"@@ -1,33 +1,33 @@
 	private EnumValueMapper interpretParameters(Properties parameters) {
 		if ( parameters.containsKey( NAMED ) ) {
 			final boolean useNamed = ConfigurationHelper.getBoolean( NAMED, parameters );
 			if ( useNamed ) {
 				return new NamedEnumValueMapper();
 			}
 			else {
 				return new OrdinalEnumValueMapper();
 			}
 		}
 
 		if ( parameters.containsKey( TYPE ) ) {
 			final int type = Integer.decode( (String) parameters.get( TYPE ) );
 			if ( isNumericType( type ) ) {
 				return new OrdinalEnumValueMapper();
 			}
 			else if ( isCharacterType( type ) ) {
-				return new OrdinalEnumValueMapper();
+				return new NamedEnumValueMapper();
 			}
 			else {
 				throw new HibernateException(
 						String.format(
 								Locale.ENGLISH,
 								""Passed JDBC type code [%s] not recognized as numeric nor character"",
 								type
 						)
 				);
 			}
 		}
 
 		// the fallback
 		return new OrdinalEnumValueMapper();
 	}",Buggy,"HHH-10766 Resolve mapping 'type' parameter error
",Buggy,"The commit message indicates a fix for a mapping 'type' parameter error. The code diff modifies the `interpretParameters` method, specifically how it handles the `TYPE` parameter. Originally, if the `TYPE` parameter was present and represented a character type, it would return an `OrdinalEnumValueMapper`. This has been changed to return a `NamedEnumValueMapper` instead. This change suggests that the original mapping was incorrect for character types, and the fix corrects this behavior. The answer is **Buggy**."
presto,26733.json,d9c1e2084c70835d5feecf645ed8836064809fc2,"@@ -1,18 +1,18 @@
     private void startDiscovery()
     {
         discoveryService.scheduleWithFixedDelay(() -> {
             try {
                 // jitter to avoid overloading database and overloading the backup store
-                SECONDS.sleep(ThreadLocalRandom.current().nextLong(1, organizationDiscoveryIntervalMillis));
+                MILLISECONDS.sleep(ThreadLocalRandom.current().nextLong(1, organizationDiscoveryIntervalMillis));
 
                 log.info(""Running shard organizer..."");
                 submitJobs(discoverAndInitializeTablesToOrganize());
             }
             catch (InterruptedException e) {
                 Thread.currentThread().interrupt();
             }
             catch (Throwable t) {
                 log.error(t, ""Error running shard organizer"");
             }
         }, 0, organizationDiscoveryIntervalMillis, TimeUnit.MILLISECONDS);
     }",Buggy,"fix organizationDiscoveryIntervalMillis bug
",Buggy,"The commit message indicates a bug fix related to `organizationDiscoveryIntervalMillis`. The code change replaces `SECONDS.sleep` with `MILLISECONDS.sleep`. The original code was likely intended to use milliseconds for the sleep duration, but it was incorrectly using seconds, leading to a much longer delay than intended. This correction aligns with the commit message, indicating a bug fix. The answer is **Buggy**."
presto,16973.json,83ee1b402bab99302d7c271fcd5dc8a560c027dc,"@@ -1,14 +1,14 @@
     private void loadCatalog(File file)
             throws Exception
     {
         log.info(""-- Loading catalog %s --"", file);
         Map<String, String> properties = new HashMap<>(loadProperties(file));
 
         String connectorName = properties.remove(""connector.name"");
-        checkState(connectorName != null, ""Catalog configuration %s does not contain conector.name"", file.getAbsoluteFile());
+        checkState(connectorName != null, ""Catalog configuration %s does not contain connector.name"", file.getAbsoluteFile());
 
         String catalogName = Files.getNameWithoutExtension(file.getName());
 
         connectorManager.createConnection(catalogName, connectorName, ImmutableMap.copyOf(properties));
         log.info(""-- Added catalog %s using connector %s --"", catalogName, connectorName);
     }",Buggy,"Fix typo in error message about connector.name
",Buggy,"The commit message indicates a fix for a typo in an error message. The diff shows a change from ""conector.name"" to ""connector.name"" within the `checkState` method's error message. This correction directly addresses the typo described in the commit message. The answer is **Buggy**."
presto,14423.json,8c469bda0657f41c146796ea2ccaaea6dfa552e6,"@@ -1,4 +1,4 @@
     public static boolean isPrefixSubnetOf(@SqlType(StandardTypes.IPPREFIX) Slice first, @SqlType(StandardTypes.IPPREFIX) Slice second)
     {
-        return between(ipSubnetMin(second), ipSubnetMin(first), ipSubnetMax(first));
+        return between(ipSubnetMin(second), ipSubnetMin(first), ipSubnetMax(first)) && between(ipSubnetMax(second), ipSubnetMin(first), ipSubnetMax(first));
     }",Buggy,"Fix bug in IS_SUBNET_OF function
",Buggy,"The commit message indicates a bug fix in the `IS_SUBNET_OF` function. The diff modifies the `isPrefixSubnetOf` method by adding a condition to the return statement. The original code only checked if the minimum IP address of the first subnet was within the range of the second subnet. The corrected code now also verifies that the maximum IP address of the first subnet is within the range defined by the second subnet. This additional check ensures that the first subnet is entirely contained within the second subnet, thus fixing a potential bug where a subnet could be incorrectly identified as a subnet of another. The answer is **Buggy**.
"
presto,8688.json,97566626dd63db55ee08b48b3e366d6d9453badb,"@@ -1,39 +1,39 @@
     private SymbolStatsEstimate normalizeSymbolStats(Symbol symbol, SymbolStatsEstimate symbolStats, PlanNodeStatsEstimate stats, TypeProvider types)
     {
         if (symbolStats.isUnknown()) {
             return SymbolStatsEstimate.unknown();
         }
 
         double outputRowCount = stats.getOutputRowCount();
         double distinctValuesCount = symbolStats.getDistinctValuesCount();
         double nullsFraction = symbolStats.getNullsFraction();
 
         if (!isNaN(distinctValuesCount)) {
-            Type type = requireNonNull(types.get(symbol), () -> ""No stats for symbol "" + symbol);
+            Type type = requireNonNull(types.get(symbol), () -> ""type is missing for symbol "" + symbol);
             double maxDistinctValuesByLowHigh = maxDistinctValuesByLowHigh(symbolStats, type);
             if (distinctValuesCount > maxDistinctValuesByLowHigh) {
                 distinctValuesCount = maxDistinctValuesByLowHigh;
             }
 
             if (distinctValuesCount > outputRowCount) {
                 distinctValuesCount = outputRowCount;
             }
 
             double nonNullValues = outputRowCount * (1 - nullsFraction);
             if (distinctValuesCount > nonNullValues) {
                 double difference = distinctValuesCount - nonNullValues;
                 distinctValuesCount -= difference / 2;
                 nonNullValues += difference / 2;
                 nullsFraction = 1 - nonNullValues / outputRowCount;
             }
         }
 
         if (distinctValuesCount == 0.0) {
             return SymbolStatsEstimate.zero();
         }
 
         return SymbolStatsEstimate.buildFrom(symbolStats)
                 .setDistinctValuesCount(distinctValuesCount)
                 .setNullsFraction(nullsFraction)
                 .build();
     }",Buggy,"Fix error message in StatsNormalizer
",Buggy,"The commit message indicates a fix to an error message in `StatsNormalizer`. The diff shows a change in the error message within the `requireNonNull` method call. Specifically, the message changed from ""No stats for symbol "" + symbol to ""type is missing for symbol "" + symbol. This change aims to provide a more accurate and descriptive error message when the type for a symbol is missing. This is a bug fix because the original error message was misleading or not as informative as it could be. The answer is **Buggy**.
"
presto,26619.json,3325ab0064b5fd93af18c1d5e87170f5d66bae28,"@@ -1,10 +1,7 @@
     public void deleteRows(Block rowIds)
     {
-        if (rowsToDelete == null) {
-            rowsToDelete = new BitSet(Ints.checkedCast(recordReader.getFileRowCount()));
-        }
         for (int i = 0; i < rowIds.getPositionCount(); i++) {
             long rowId = BIGINT.getLong(rowIds, i);
             rowsToDelete.set(Ints.checkedCast(rowId));
         }
     }",Buggy,"Fix bug in delete for Raptor

When no rows match for delete, rowsToDelete is not set causing a NPE.
Set it in the constructor because we already know the maximum size.
We might over allocate for cases where no rows are selected for
deletion, but this is a short lived bit vector so it should be low
impact.
",Buggy,"The commit message indicates a fix for a NullPointerException (NPE) in the `deleteRows` method when no rows match the delete condition. The original code initializes `rowsToDelete` only when rows are actually being deleted. If no rows match, `rowsToDelete` remains null, leading to an NPE when `rowsToDelete.set()` is called. The fix (which is not shown in the diff) likely involves initializing `rowsToDelete` earlier, as the commit message suggests. The provided diff shows the usage of `rowsToDelete` but not the initialization fix. Therefore, the changes indicate a bug fix. The answer is **Buggy**."
presto,14862.json,9adc687000043241a0c0ecb1b56ef6b810c2a430,"@@ -1,23 +1,26 @@
     public static Block bigintDistinct(@SqlType(""array(bigint)"") Block array)
     {
         if (array.getPositionCount() == 0) {
             return array;
         }
 
         boolean containsNull = false;
         LongSet set = new LongOpenHashSet(array.getPositionCount());
         BlockBuilder distinctElementBlockBuilder = BIGINT.createBlockBuilder(new BlockBuilderStatus(), array.getPositionCount());
         for (int i = 0; i < array.getPositionCount(); i++) {
-            if (!containsNull && array.isNull(i)) {
-                containsNull = true;
-                distinctElementBlockBuilder.appendNull();
+            if (array.isNull(i)) {
+                if (!containsNull) {
+                    containsNull = true;
+                    distinctElementBlockBuilder.appendNull();
+                }
                 continue;
             }
+
             long value = BIGINT.getLong(array, i);
             if (set.add(value)) {
                 BIGINT.writeLong(distinctElementBlockBuilder, value);
             }
         }
 
         return distinctElementBlockBuilder.build();
     }",Buggy,"Fix null-handling bug in ArrayDistinctFunction
",Buggy,"The commit message indicates a fix for a null-handling bug in the ArrayDistinctFunction. The code diff shows a modification in the bigintDistinct function. Specifically, the code now checks if `containsNull` is false *before* appending a null value to the `distinctElementBlockBuilder`. This ensures that only one null value is added to the distinct array, even if the input array contains multiple nulls. This correction addresses a potential bug where multiple null values could be incorrectly included in the distinct array. The answer is **Buggy**.
"
presto,20594.json,2d9e768a03b5c9d804825c9a489383465f373801,"@@ -1,8 +1,10 @@
         public PlanWithProperties visitFilter(FilterNode node, PreferredProperties preferredProperties)
         {
-            if (node.getSource() instanceof TableScanNode) {
+            if (node.getSource() instanceof TableScanNode && metadata.isLegacyGetLayoutSupported(session, ((TableScanNode) node.getSource()).getTable())) {
+                // If isLegacyGetLayoutSupported, then we can continue with legacy predicate pushdown logic.
+                // Otherwise, we leave the filter as is in the plan as it will be pushed into the TableScan by filter pushdown logic in the connector.
                 return planTableScan((TableScanNode) node.getSource(), node.getPredicate());
             }
 
             return rebaseAndDeriveProperties(node, planChild(node, preferredProperties));
         }",Buggy,"Fix bug AddExchanges dropping filter when pushdown is enabled
",Buggy,"The commit message indicates a bug fix in `AddExchanges` related to dropping a filter when pushdown is enabled. The code change adds a condition to the `if` statement within the `visitFilter` method. The original code only checked if the source node was a `TableScanNode`. The modified code adds `metadata.isLegacyGetLayoutSupported(session, ((TableScanNode) node.getSource()).getTable())` to the condition. This suggests that the filter was being dropped incorrectly when the legacy layout was not supported, which is now fixed by only proceeding with the legacy predicate pushdown logic when it is supported. The answer is **Buggy**."
presto,30019.json,9e755c76e9d8e3a5e0a486002b3898be9d6d5d25,"@@ -1,14 +1,13 @@
     public void close()
     {
         closed = true;
-
         // use try with resources to close everything properly
-        try (ResultSet resultSet = this.resultSet;
+        try (Connection connection = this.connection;
                 Statement statement = this.statement;
-                Connection connection = this.connection) {
+                ResultSet resultSet = this.resultSet) {
             // do nothing
         }
         catch (SQLException e) {
             throw Throwables.propagate(e);
         }
     }",Buggy,"Fix ""Connection already closed"" error in JdbcRecordCursor

The resultset, statement and connection were being closed in
the wrong order.
",Buggy,"The commit message indicates a fix for a ""Connection already closed"" error, which suggests a bug related to resource management. The diff shows that the order of closing resources (ResultSet, Statement, and Connection) within the `try-with-resources` block has been changed. The original order was ResultSet, Statement, Connection, which could lead to the Connection being closed before the ResultSet or Statement, causing issues if they were still in use. The corrected order is Connection, Statement, ResultSet, ensuring that the Connection is closed last. This change directly addresses the bug described in the commit message. The answer is **Buggy**."
presto,32288.json,1eb20ea3faeb66b18c4c9300b4a6c8c3190ca6ce,"@@ -1,8 +1,9 @@
     public List<SchemaTableName> listTables(ConnectorSession session, String schemaNameOrNull)
     {
-        checkArgument(schemaNameOrNull == null || schemaNameOrNull.equals(SCHEMA_NAME),
-                ""Only '%s' schema is supported"", SCHEMA_NAME);
+        if (schemaNameOrNull != null && !schemaNameOrNull.equals(SCHEMA_NAME)) {
+            return ImmutableList.of();
+        }
         return tables.values().stream()
                 .map(BlackHoleTableHandle::toSchemaTableName)
                 .collect(toList());
     }",Buggy,"Fix listing tables in blackhole connector

Listing tables in a non-existent schema should not be an error.
",Buggy,"The commit message indicates a fix for listing tables in the blackhole connector, specifically addressing the scenario where listing tables in a non-existent schema should not result in an error. The code diff modifies the `listTables` method to return an empty list if the provided `schemaNameOrNull` is not null and does not match the expected `SCHEMA_NAME`. This change prevents an error from being thrown when attempting to list tables in a non-existent schema, aligning with the commit message's intent. The original code used `checkArgument`, which would throw an exception if the schema name was not valid. The modified code gracefully handles the case of a non-existent schema by returning an empty list. The answer is **Buggy**."
presto,32344.json,5bc9087c740ce60796c3b92131fe7c2a09bc04f7,"@@ -1,4 +1,10 @@
     public List<File> getFiles(SchemaTableName table)
     {
-        return cachedFiles.getUnchecked(table);
+        try {
+            return cachedFiles.getUnchecked(table);
+        }
+        catch (UncheckedExecutionException e) {
+            throwIfInstanceOf(e.getCause(), PrestoException.class);
+            throw e;
+        }
     }",Buggy,"Fix error categorization in local file connector

Exceptions are wrapped by LoadingCache and must be unwrapped.
",Buggy,"The commit message indicates a fix related to error categorization within the local file connector. The code diff introduces a try-catch block around the `cachedFiles.getUnchecked(table)` call. Inside the catch block, it unwraps the `UncheckedExecutionException` to check if the cause is a `PrestoException`. If it is, it rethrows the `PrestoException`. Otherwise, it rethrows the original `UncheckedExecutionException`. This change suggests that exceptions were previously being incorrectly categorized or masked by the `LoadingCache`. The fix ensures that `PrestoException`s are properly propagated, indicating a bug fix in error handling. The answer is **Buggy**."
presto,28248.json,dce842fd5d590740f0f7ca26ec5c7eb192e64640,"@@ -1,11 +1,16 @@
         protected Node visitNegativeExpression(NegativeExpression node, Context<C> context)
         {
             if (!context.isDefaultRewrite()) {
                 Node result = nodeRewriter.rewriteNegativeExpression(node, context.get(), TreeRewriter.this);
                 if (result != null) {
                     return result;
                 }
             }
 
+            Expression child = rewrite(node.getValue(), context.get());
+            if (child != node.getValue()) {
+                return new NegativeExpression(child);
+            }
+
             return node;
         }",Buggy,"Fix bug when rewriting NegativeExpression

It was discarding the rewritten subexpression of an arithmetic negation
",Buggy,"The commit message indicates a bug fix related to rewriting `NegativeExpression` nodes, specifically that the rewritten subexpression was being discarded. The diff shows that the code now correctly rewrites the child expression of the `NegativeExpression` and creates a new `NegativeExpression` with the rewritten child if the child has been modified. This aligns with the commit message, indicating a bug fix. The answer is **Buggy**."
presto,8271.json,d560b6e644e49cab94fa9076ec62406cd62afa68,"@@ -1,12 +1,16 @@
         public InputStream handle(Request request, com.facebook.airlift.http.client.Response response)
         {
             try {
+                if (response.getStatusCode() != HTTP_OK) {
+                    String result = new BufferedReader(new InputStreamReader(response.getInputStream())).lines().collect(Collectors.joining(""\n""));
+                    throw new PrestoException(DRUID_BROKER_RESULT_ERROR, result);
+                }
                 if (APPLICATION_JSON.equals(response.getHeader(CONTENT_TYPE))) {
                     return response.getInputStream();
                 }
                 throw new PrestoException(DRUID_BROKER_RESULT_ERROR, ""Response received was not of type "" + APPLICATION_JSON);
             }
             catch (IOException e) {
                 throw new PrestoException(DRUID_BROKER_RESULT_ERROR, ""Unable to read response from worker"", e);
             }
         }",Buggy,"Fix unhandled HTTP response error for druid client
",Buggy,"The commit message indicates a fix for an unhandled HTTP response error in the Druid client. The diff adds a check for the HTTP status code. If the status code is not HTTP_OK (200), it reads the response body, joins the lines, and throws a PrestoException with the response as the message. This ensures that non-OK HTTP responses are properly handled and reported as exceptions, preventing the client from proceeding with potentially invalid or erroneous data. The addition of error handling directly addresses the bug described in the commit message. The answer is **Buggy**."
presto,22102.json,37377fdfa6f208809b77185abca3b1d0bdcb2f92,"@@ -1,72 +1,73 @@
     public static Map<List<RowExpression>, Boolean> getExpressionsPartitionedByCSE(Collection<? extends RowExpression> expressions)
     {
         if (expressions.isEmpty()) {
             return ImmutableMap.of();
         }
 
         CommonSubExpressionCollector expressionCollector = new CommonSubExpressionCollector();
         expressions.forEach(expression -> expression.accept(expressionCollector, null));
         Set<RowExpression> cse = expressionCollector.cseByLevel.values().stream().flatMap(Set::stream).collect(toImmutableSet());
 
         if (cse.isEmpty()) {
             return expressions.stream().collect(toImmutableMap(ImmutableList::of, m -> false));
         }
 
         ImmutableMap.Builder<List<RowExpression>, Boolean> expressionsPartitionedByCse = ImmutableMap.builder();
         SubExpressionChecker subExpressionChecker = new SubExpressionChecker(cse);
         Map<Boolean, List<RowExpression>> expressionsWithCseFlag = expressions.stream().collect(Collectors.partitioningBy(expression -> expression.accept(subExpressionChecker, null)));
         expressionsWithCseFlag.get(false).forEach(expression -> expressionsPartitionedByCse.put(ImmutableList.of(expression), false));
 
         List<RowExpression> expressionsWithCse = expressionsWithCseFlag.get(true);
         if (expressionsWithCse.size() == 1) {
             RowExpression expression = expressionsWithCse.get(0);
             expressionsPartitionedByCse.put(ImmutableList.of(expression), true);
             return expressionsPartitionedByCse.build();
         }
 
         List<Set<RowExpression>> cseDependency = expressionsWithCse.stream()
                 .map(expression -> subExpressions(expression).stream()
                         .filter(cse::contains)
                         .collect(toImmutableSet()))
                 .collect(toImmutableList());
 
         boolean[] merged = new boolean[expressionsWithCse.size()];
 
         int i = 0;
         while (i < merged.length) {
             while (i < merged.length && merged[i]) {
                 i++;
             }
             if (i >= merged.length) {
                 break;
             }
             merged[i] = true;
             ImmutableList.Builder<RowExpression> newList = ImmutableList.builder();
             newList.add(expressionsWithCse.get(i));
             Set<RowExpression> dependencies = new HashSet<>();
             Set<RowExpression> first = cseDependency.get(i);
             dependencies.addAll(first);
             int j = i + 1;
             while (j < merged.length) {
                 while (j < merged.length && merged[j]) {
                     j++;
                 }
                 if (j >= merged.length) {
                     break;
                 }
                 Set<RowExpression> second = cseDependency.get(j);
                 if (!Sets.intersection(dependencies, second).isEmpty()) {
                     RowExpression expression = expressionsWithCse.get(j);
                     newList.add(expression);
                     dependencies.addAll(second);
                     merged[j] = true;
+                    j = i + 1;
                 }
                 else {
                     j++;
                 }
             }
             expressionsPartitionedByCse.put(newList.build(), true);
         }
 
         return expressionsPartitionedByCse.build();
     }",Buggy,"Fixing a bug in common sub expression partitioning

The previous implementation could group expressions differently with different order or exprsesions.
",Buggy,"The commit message indicates a fix for inconsistent grouping of expressions in common subexpression partitioning due to different orderings. The code diff focuses on the `getExpressionsPartitionedByCSE` method. The core of the fix appears to be within the nested `while` loops. The outer loop iterates through expressions, and the inner loop checks for dependencies (common subexpressions) between the current expression and subsequent expressions. If a common subexpression is found, the expressions are grouped together. The key change is the addition of `j = i + 1;` inside the `if` block where expressions are merged. This resets the inner loop's index `j` to `i + 1` after a merge. Without this reset, the algorithm might miss potential merges because after merging expression `j` with expression `i`, the algorithm needs to re-evaluate all expressions from `i+1` onwards, as the dependencies have changed. This correction addresses the bug described in the commit message. The answer is **Buggy**."
presto,10913.json,e6f650722f4041eade54638d355de2b9f7a9e280,"@@ -1,4 +1,4 @@
     public boolean isFinalQueryInfo()
     {
-        return state.isDone() && getAllStages(outputStage).stream().allMatch(StageInfo::isFinalStageInfo);
+        return state.equals(QueryState.FAILED) || state.isDone() && getAllStages(outputStage).stream().allMatch(StageInfo::isFinalStageInfo);
     }",Buggy,"fix the client hang problem after query failed
",Buggy,"The commit message indicates a fix for a client hang issue that occurs after a query fails. The code change modifies the `isFinalQueryInfo` method to include a check for `state.equals(QueryState.FAILED)`. This suggests that the client was not correctly identifying a failed query as final, leading to a hang. By explicitly checking for the `FAILED` state, the code ensures that the client recognizes the query as complete even if it ended in failure, thus resolving the hang. The answer is **Buggy**.
"
presto,7159.json,130527265fd8aebee395ed2412f4058d0e269f51,"@@ -1,84 +1,83 @@
         public PlanNode visitProject(ProjectNode project, Void context)
         {
             if (!(project.getSource() instanceof TableScanNode)) {
                 return visitPlan(project, context);
             }
 
             TableScanNode tableScan = (TableScanNode) project.getSource();
             if (!isParquetDereferenceEnabled(session, tableScan.getTable())) {
                 return visitPlan(project, context);
             }
 
             Map<RowExpression, Subfield> dereferenceToNestedColumnMap = extractDereferences(
                     session,
                     rowExpressionService.getExpressionOptimizer(),
                     new HashSet<>(project.getAssignments().getExpressions()));
             if (dereferenceToNestedColumnMap.isEmpty()) {
                 return visitPlan(project, context);
             }
 
-            Map<String, HiveColumnHandle> regularHiveColumnHandles = tableScan.getAssignments().values().stream()
-                    .map(columnHandle -> (HiveColumnHandle) columnHandle)
-                    .collect(toMap(HiveColumnHandle::getName, identity()));
+            Map<String, HiveColumnHandle> regularHiveColumnHandles = tableScan.getAssignments().entrySet().stream()
+                    .collect(toMap(e -> e.getKey().getName(), e -> (HiveColumnHandle) e.getValue()));
 
             List<VariableReferenceExpression> newOutputVariables = new ArrayList<>(tableScan.getOutputVariables());
             Map<VariableReferenceExpression, ColumnHandle> newAssignments = new HashMap<>(tableScan.getAssignments());
 
             Map<RowExpression, VariableReferenceExpression> dereferenceToVariableMap = new HashMap<>();
 
             for (Map.Entry<RowExpression, Subfield> dereference : dereferenceToNestedColumnMap.entrySet()) {
                 Subfield nestedColumn = dereference.getValue();
                 RowExpression dereferenceExpression = dereference.getKey();
 
                 // Find the nested column Hive Type
                 HiveColumnHandle regularColumnHandle = regularHiveColumnHandles.get(nestedColumn.getRootName());
                 if (regularColumnHandle == null) {
                     throw new IllegalArgumentException(""nested column ["" + nestedColumn + ""]'s base column "" + nestedColumn.getRootName() + "" is not present in table scan output"");
                 }
 
                 Optional<HiveType> nestedColumnHiveType = regularHiveColumnHandles.get(nestedColumn.getRootName())
                         .getHiveType()
                         .findChildType(
                                 nestedColumn.getPath().stream()
                                         .map(p -> ((Subfield.NestedField) p).getName())
                                         .collect(Collectors.toList()));
 
                 if (!nestedColumnHiveType.isPresent()) {
                     throw new IllegalArgumentException(""nested column ["" + nestedColumn + ""] type is not present in Hive column type"");
                 }
 
                 String pushdownColumnName = pushdownColumnNameForSubfield(nestedColumn);
                 // Create column handle for nested column
                 HiveColumnHandle nestedColumnHandle = new HiveColumnHandle(
                         pushdownColumnName,
                         nestedColumnHiveType.get(),
                         dereferenceExpression.getType().getTypeSignature(),
                         -1,
                         SYNTHESIZED,
                         Optional.of(""nested column pushdown""),
                         ImmutableList.of(nestedColumn),
                         Optional.empty());
 
                 VariableReferenceExpression newOutputVariable = variableAllocator.newVariable(pushdownColumnName, dereferenceExpression.getType());
                 newOutputVariables.add(newOutputVariable);
                 newAssignments.put(newOutputVariable, nestedColumnHandle);
 
                 dereferenceToVariableMap.put(dereferenceExpression, newOutputVariable);
             }
 
             TableScanNode newTableScan = new TableScanNode(
                     idAllocator.getNextId(),
                     tableScan.getTable(),
                     newOutputVariables,
                     newAssignments,
                     tableScan.getCurrentConstraint(),
                     tableScan.getEnforcedConstraint());
 
             Assignments.Builder newProjectAssignmentBuilder = Assignments.builder();
             for (Map.Entry<VariableReferenceExpression, RowExpression> entry : project.getAssignments().entrySet()) {
                 RowExpression newExpression = rewriteWith(new DereferenceExpressionRewriter(dereferenceToVariableMap), entry.getValue());
                 newProjectAssignmentBuilder.put(entry.getKey(), newExpression);
             }
 
             return new ProjectNode(idAllocator.getNextId(), newTableScan, newProjectAssignmentBuilder.build(), project.getLocality());
         }",Buggy,"Fix base column name not present error when dereference pushdown enabled
",Buggy,"The commit message indicates a fix for a ""base column name not present error"" when dereference pushdown is enabled. The code diff modifies how `regularHiveColumnHandles` are populated. The original code used `HiveColumnHandle::getName` which extracts the column name, while the corrected code uses `e -> e.getKey().getName()` which extracts the column name from the entry key. This suggests that the original code was incorrectly retrieving the column name, leading to the reported error. The change ensures that the correct column name is used, resolving the bug. The answer is **Buggy**."
presto,22204.json,b88a9a91b24500d3c46df8cd613783effdc9d70c,"@@ -1,34 +1,34 @@
     private static Map<LambdaDefinitionExpression, CompiledLambda> generateMethodsForLambda(
             ClassDefinition containerClassDefinition,
             CallSiteBinder callSiteBinder,
             CachedInstanceBinder cachedInstanceBinder,
             List<RowExpression> expressions,
             Metadata metadata,
             SqlFunctionProperties sqlFunctionProperties,
             String methodNamePrefix,
             Set<LambdaDefinitionExpression> existingCompiledLambdas)
     {
         Set<LambdaDefinitionExpression> lambdaExpressions = expressions.stream()
                 .map(LambdaExpressionExtractor::extractLambdaExpressions)
                 .flatMap(List::stream)
                 .filter(lambda -> !existingCompiledLambdas.contains(lambda))
                 .collect(toImmutableSet());
         ImmutableMap.Builder<LambdaDefinitionExpression, CompiledLambda> compiledLambdaMap = ImmutableMap.builder();
 
-        int counter = 0;
+        int counter = existingCompiledLambdas.size();
         for (LambdaDefinitionExpression lambdaExpression : lambdaExpressions) {
             CompiledLambda compiledLambda = LambdaBytecodeGenerator.preGenerateLambdaExpression(
                     lambdaExpression,
                     methodNamePrefix + ""lambda_"" + counter,
                     containerClassDefinition,
                     compiledLambdaMap.build(),
                     callSiteBinder,
                     cachedInstanceBinder,
                     metadata,
                     sqlFunctionProperties);
             compiledLambdaMap.put(lambdaExpression, compiledLambda);
             counter++;
         }
 
         return compiledLambdaMap.build();
     }",Buggy,"Fix compiler error in LambdaBytecodeGenerator

When there are lambda expressions from different SQL functions, and they are
compiled into the same class due to CSE, we need to make sure the generated
function names are always unique.
",Buggy,"The commit message indicates a fix for a compiler error related to lambda expressions in SQL functions. The issue arises when lambda expressions from different SQL functions are compiled into the same class due to Common Subexpression Elimination (CSE). To resolve this, the generated function names need to be unique.

The diff modifies the `generateMethodsForLambda` method. Specifically, the initialization of the `counter` variable is changed from `0` to `existingCompiledLambdas.size()`. This ensures that the generated lambda method names are unique across different SQL functions, even when they are compiled into the same class. Without this change, if lambdas from different SQL functions were compiled into the same class, they might have the same generated method names, leading to a compiler error.

The change directly addresses the compiler error described in the commit message by ensuring unique method names for lambda expressions. The answer is **Buggy**.
"
presto,1269.json,4cb4d192f1cc8e0da404521946d7566738d45c00,"@@ -1,6 +1,6 @@
         private void prepareDropPartition(SchemaTableName schemaTableName, List<String> partitionValues)
         {
             metastoreDeleteOperations.add(new IrreversibleMetastoreOperation(
-                    format(""drop partition %s.%s %s"", schemaTableName, schemaTableName.getTableName(), partitionValues),
+                    format(""drop partition %s.%s %s"", schemaTableName.getSchemaName(), schemaTableName.getTableName(), partitionValues),
                     () -> delegate.dropPartition(schemaTableName.getSchemaName(), schemaTableName.getTableName(), partitionValues, true)));
         }",Buggy,"Fix SemiTransactionalHiveMetastore error message
",Buggy,"The commit message indicates a fix to an error message in SemiTransactionalHiveMetastore. The diff modifies the format string used to generate the error message for dropping a partition. Specifically, it changes `schemaTableName` to `schemaTableName.getSchemaName()` in the `format` method call. This suggests that the original error message was displaying the entire `SchemaTableName` object instead of just the schema name, which would be less informative and potentially confusing. Therefore, this change corrects a bug in the error reporting logic. The answer is **Buggy**."
presto,17974.json,0498c0df5aff14c9f7f88db1fe2e43ed3470aa5b,"@@ -1,12 +1,16 @@
     private static Optional<TaskInfo> findFailedTask(StageInfo stageInfo)
     {
+        if (stageInfo == null) {
+            return Optional.empty();
+        }
+
         for (StageInfo subStage : stageInfo.getSubStages()) {
             Optional<TaskInfo> task = findFailedTask(subStage);
             if (task.isPresent()) {
                 return task;
             }
         }
         return stageInfo.getTasks().stream()
                 .filter(taskInfo -> taskInfo.getState() == TaskState.FAILED)
                 .findFirst();
     }",Buggy,"Fix NPE when query fails during parsing/planning

Commit 135626b793b42841c77f6b8c8b7fe80fac8ffc46 introduced a bug
where queries that fail during parsing/analysis/planning
throw an NPE when the completion event is processed.

This is due to an incorrect assumption that QueryInfos always
have an output stage, which is not true for queries that fail
before execution starts.
",Buggy,"The commit message indicates a fix for a NullPointerException (NPE) that occurs when queries fail during parsing or planning. The diff adds a null check for `stageInfo` in the `findFailedTask` method. This check prevents the code from attempting to process a null `stageInfo`, which would lead to an NPE. The commit message states that the NPE was introduced by a previous commit and is due to an incorrect assumption that QueryInfos always have an output stage. The added null check directly addresses this issue by handling cases where `stageInfo` is null, thus preventing the NPE. The answer is **Buggy**."
presto,22343.json,913c3f81cdc1a8dbc3201382b38d5e429f43fa7f,"@@ -1,36 +1,37 @@
     public BytecodeNode generateExpression(Signature signature, BytecodeGeneratorContext context, Type rowType, List<RowExpression> arguments)
     {
         BytecodeBlock block = new BytecodeBlock().setDescription(""Constructor for "" + rowType.toString());
         CallSiteBinder binder = context.getCallSiteBinder();
         Scope scope = context.getScope();
         List<Type> types = rowType.getTypeParameters();
 
         block.comment(""BlockBuilder blockBuilder = new InterleavedBlockBuilder(types, new BlockBuilderStatus(), 1);"");
         Variable blockBuilder = scope.createTempVariable(BlockBuilder.class);
         Binding typesBinding = binder.bind(types, List.class);
         block.append(blockBuilder.set(
                 newInstance(InterleavedBlockBuilder.class, loadConstant(typesBinding), newInstance(BlockBuilderStatus.class), constantInt(1))));
 
         for (int i = 0; i < arguments.size(); ++i) {
             Type fieldType = types.get(i);
             Class<?> javaType = fieldType.getJavaType();
             if (javaType == void.class) {
                 block.comment(i + ""-th field type of row is undefined"");
                 block.append(blockBuilder.invoke(""appendNull"", BlockBuilder.class).pop());
             }
             else {
                 Variable field = scope.createTempVariable(javaType);
-                block.comment(""Generate + "" + i + ""-th field of row"");
+                block.comment(""Clean wasNull and Generate + "" + i + ""-th field of row"");
+                block.append(context.wasNull().set(constantFalse()));
                 block.append(context.generate(arguments.get(i)));
                 block.putVariable(field);
                 block.append(new IfStatement()
                         .condition(context.wasNull())
                         .ifTrue(blockBuilder.invoke(""appendNull"", BlockBuilder.class).pop())
                         .ifFalse(constantType(binder, fieldType).writeValue(blockBuilder, field).pop()));
             }
         }
         block.comment(""put (Block) blockBuilder.build(); wasNull = false;"");
         block.append(blockBuilder.invoke(""build"", Block.class));
         block.append(context.wasNull().set(constantFalse()));
         return block;
     }",Buggy,"Fix bug when constructing ROW with null values

Not all generators set wasNull, causing following values
(generated from one generator that also doesn't set wasNull) to be null.
",Buggy,"The commit message indicates a bug where `wasNull` was not being correctly set during the construction of a ROW with null values, leading to subsequent values being incorrectly treated as null. The diff introduces `context.wasNull().set(constantFalse())` before generating each field of the row. This ensures that the `wasNull` flag is cleared before processing each field, preventing the issue described in the commit message. The answer is **Buggy**.
"
presto,1944.json,d439d475c1eb6674ff8a9fb75ee08565217bc39c,"@@ -1,8 +1,8 @@
     public boolean isSingleValue()
     {
         return !low.isLowerUnbounded() &&
                 !high.isUpperUnbounded() &&
                 low.getBound() == Marker.Bound.EXACTLY &&
                 high.getBound() == Marker.Bound.EXACTLY &&
-                low.getValue() == high.getValue();
+                low.getValue().equals(high.getValue());
     }",Buggy,"Fix Range bug in SPI
",Buggy,"The commit message indicates a bug fix related to the `Range` class in SPI. The diff shows a change in the `isSingleValue()` method. Specifically, the `==` operator is replaced with the `.equals()` method when comparing the values of `low` and `high`. This suggests that the original code was incorrectly comparing the objects by reference instead of by value, which could lead to incorrect results when determining if a range represents a single value. This is a bug fix. The answer is **Buggy**."
presto,17855.json,18088bed1462b2cf8f46ab44293b3791136588d3,"@@ -1,8 +1,7 @@
     public String toString()
     {
         return Objects.toStringHelper(this)
                 .add(""tableHandle"", tableHandle)
-                .add(""filters"", filters)
                 .add(""addresses"", addresses)
                 .toString();
     }",Buggy,"System connector predicate pushdown bug fix
",NotBuggy,"The commit message indicates a bug fix related to system connector predicate pushdown. The provided diff removes `filters` from the `toString` method. While this change might be related to debugging or logging, it doesn't directly indicate a bug fix in the predicate pushdown logic itself. It's possible the `filters` information was causing issues in logging or debugging, which indirectly relates to the bug fix, but the code change itself doesn't reveal the nature of the bug. Therefore, based solely on this diff, it's hard to confirm a bug fix.

**NotBuggy**
"
presto,3309.json,c798add760c213bafe85a1f7934e54b5fe0b8728,"@@ -1,4 +1,4 @@
     public int getPositionCount()
     {
-        return blocks[0].getPositionCount();
+        return positionCount;
     }",Buggy,"Fix bug in count aggregation from inline view

This query now works:

SELECT COUNT(*) FROM (SELECT ... ) x
",Buggy,"The commit message indicates a fix for a bug related to count aggregation with inline views. The diff shows a change in `getPositionCount()` method, where it now returns `positionCount` instead of `blocks[0].getPositionCount()`. This suggests that the original implementation was incorrectly retrieving the position count from the first block only, leading to incorrect aggregation results when dealing with multiple blocks or inline views. The change ensures that the correct total position count is returned, thus fixing the bug. The answer is **Buggy**.
"
presto,12870.json,f20d677b8203f10728649c8f4b038f68a08dd909,"@@ -1,4 +1,4 @@
     private static List<ParameterMetadata> createInputParameterMetadata(Type value, Type key)
     {
-        return ImmutableList.of(new ParameterMetadata(STATE), new ParameterMetadata(NULLABLE_BLOCK_INPUT_CHANNEL, value), new ParameterMetadata(NULLABLE_BLOCK_INPUT_CHANNEL, key), new ParameterMetadata(BLOCK_INDEX));
+        return ImmutableList.of(new ParameterMetadata(STATE), new ParameterMetadata(NULLABLE_BLOCK_INPUT_CHANNEL, value), new ParameterMetadata(BLOCK_INPUT_CHANNEL, key), new ParameterMetadata(BLOCK_INDEX));
     }",Buggy,"Fix null-handling bug in min_by/max_by

Null keys were previously incorrectly handled. They were effectively treated
as default value of the type for comparison in the input function.
",Buggy,"The commit message indicates a fix for a null-handling bug in `min_by` and `max_by` functions, specifically related to how null keys were treated. The diff shows a change in the `createInputParameterMetadata` method, where `NULLABLE_BLOCK_INPUT_CHANNEL` is replaced with `BLOCK_INPUT_CHANNEL` for the key parameter. This suggests that the previous implementation was incorrectly treating null keys as a default value, and the fix ensures that null keys are handled correctly, likely by not allowing them. This aligns with the commit message's description of the bug fix. The answer is **Buggy**."
presto,16287.json,c5b89795521472323d392daa860240d64d3876f0,"@@ -1,35 +1,59 @@
     static DecodedBlockNode decodeBlock(BlockFlattener flattener, Closer blockLeaseCloser, Block block)
     {
         BlockLease lease = flattener.flatten(block);
         blockLeaseCloser.register(lease::close);
         Block decodedBlock = lease.get();
 
+        long estimatedSizeInBytes = decodedBlock.getLogicalSizeInBytes();
+
         if (decodedBlock instanceof ArrayBlock) {
             ColumnarArray columnarArray = ColumnarArray.toColumnarArray(decodedBlock);
-            return new DecodedBlockNode(columnarArray, ImmutableList.of(decodeBlock(flattener, blockLeaseCloser, columnarArray.getElementsBlock())));
+            Block childBlock = columnarArray.getElementsBlock();
+            return new DecodedBlockNode(
+                    columnarArray,
+                    ImmutableList.of(decodeBlock(flattener, blockLeaseCloser, childBlock)),
+                    columnarArray.getRetainedSizeInBytes(),
+                    estimatedSizeInBytes);
         }
 
         if (decodedBlock instanceof MapBlock) {
             ColumnarMap columnarMap = ColumnarMap.toColumnarMap(decodedBlock);
-            return new DecodedBlockNode(columnarMap, ImmutableList.of(decodeBlock(flattener, blockLeaseCloser, columnarMap.getKeysBlock()), decodeBlock(flattener, blockLeaseCloser, columnarMap.getValuesBlock())));
+            Block keyBlock = columnarMap.getKeysBlock();
+            Block valueBlock = columnarMap.getValuesBlock();
+            return new DecodedBlockNode(
+                    columnarMap,
+                    ImmutableList.of(decodeBlock(flattener, blockLeaseCloser, keyBlock), decodeBlock(flattener, blockLeaseCloser, valueBlock)),
+                    columnarMap.getRetainedSizeInBytes(),
+                    estimatedSizeInBytes);
         }
 
         if (decodedBlock instanceof RowBlock) {
             ColumnarRow columnarRow = ColumnarRow.toColumnarRow(decodedBlock);
             ImmutableList.Builder<DecodedBlockNode> children = ImmutableList.builder();
             for (int i = 0; i < columnarRow.getFieldCount(); i++) {
-                children.add(decodeBlock(flattener, blockLeaseCloser, columnarRow.getField(i)));
+                Block childBlock = columnarRow.getField(i);
+                children.add(decodeBlock(flattener, blockLeaseCloser, childBlock));
             }
-            return new DecodedBlockNode(columnarRow, children.build());
+            return new DecodedBlockNode(columnarRow, children.build(), columnarRow.getRetainedSizeInBytes(), estimatedSizeInBytes);
         }
 
         if (decodedBlock instanceof DictionaryBlock) {
-            return new DecodedBlockNode(decodedBlock, ImmutableList.of(decodeBlock(flattener, blockLeaseCloser, ((DictionaryBlock) decodedBlock).getDictionary())));
+            Block dictionary = ((DictionaryBlock) decodedBlock).getDictionary();
+            return new DecodedBlockNode(
+                    decodedBlock,
+                    ImmutableList.of(decodeBlock(flattener, blockLeaseCloser, dictionary)),
+                    decodedBlock.getRetainedSizeInBytes(),
+                    estimatedSizeInBytes);
         }
 
         if (decodedBlock instanceof RunLengthEncodedBlock) {
-            return new DecodedBlockNode(decodedBlock, ImmutableList.of(decodeBlock(flattener, blockLeaseCloser, ((RunLengthEncodedBlock) decodedBlock).getValue())));
+            Block childBlock = ((RunLengthEncodedBlock) decodedBlock).getValue();
+            return new DecodedBlockNode(
+                    decodedBlock,
+                    ImmutableList.of(decodeBlock(flattener, blockLeaseCloser, childBlock)),
+                    decodedBlock.getRetainedSizeInBytes(),
+                    estimatedSizeInBytes);
         }
 
-        return new DecodedBlockNode(decodedBlock, ImmutableList.of());
+        return new DecodedBlockNode(decodedBlock, ImmutableList.of(), block.getRetainedSizeInBytes(), estimatedSizeInBytes);
     }",Buggy,"Fix estimatedSerializedSizeInBytes for RLE and Dictionary Blocks

When a block passed to OptimizedPartitionedOutputOperator is a RLE or
Dictionary block, we used to estimated the serialized size using
getLogicalSize() which returns the size of the block after inflation.
However the child block of the RLE or Dictionary Block was using plain
sizeInBytes without considering it is going to be expanded. This
commit fixes this problem by adding a scale factor to estimate how many
times the child blocks are going to be expanded.
",Buggy,"The commit message indicates a fix for the estimated serialized size of RLE and Dictionary Blocks. The diff modifies the `decodeBlock` method to correctly estimate the size of child blocks within ArrayBlock, MapBlock, RowBlock, DictionaryBlock, and RunLengthEncodedBlock. The original code was using `getLogicalSize()` which returns the size of the block after inflation, while the child block was using `sizeInBytes` without considering the expansion. This commit fixes this issue by adding a scale factor to estimate how many times the child blocks are going to be expanded. The changes involve passing `estimatedSizeInBytes` and `retainedSizeInBytes` to the `DecodedBlockNode` constructor. This indicates a bug fix related to incorrect size estimation. The answer is **Buggy**."
presto,21104.json,58e78de1c0cde073cc472bb7b2d4cf52155e15e3,"@@ -1,4 +1,8 @@
         public StreamProperties visitTopN(TopNNode node, List<StreamProperties> inputProperties)
         {
+            // Partial TopN doesn't guarantee that stream is ordered
+            if (node.getStep().equals(TopNNode.Step.PARTIAL)) {
+                return Iterables.getOnlyElement(inputProperties);
+            }
             return StreamProperties.ordered();
         }",Buggy,"Fix stream properties for partial TopN

This is a bug fix, that seems to could never occur.
",Buggy,"The commit message indicates a bug fix related to stream properties for partial TopN operations. The code change introduces a condition that, when the TopN node step is ""PARTIAL"", the stream properties are taken directly from the input properties, effectively removing the ""ordered"" property. This suggests that the previous logic incorrectly assumed that partial TopN operations always resulted in an ordered stream, which could lead to incorrect downstream processing. The added condition corrects this assumption, indicating a bug fix. The answer is **Buggy**."
presto,18169.json,86913bdc9e40dd112bda300f21ae0f95e1330515,"@@ -1,4 +1,4 @@
         public Type getType(int field)
         {
-            return delegate.getType(field);
+            return delegate.getType(toDelegateField(field));
         }",Buggy,"Fix MappedRecordCursor bug
",Buggy,"The commit message indicates a bug fix in `MappedRecordCursor`. The diff shows a change in the `getType` method, where the `field` argument is now passed through a `toDelegateField` method before being passed to the `delegate.getType` method. This suggests that the original code was passing an incorrect field index to the delegate, leading to incorrect type retrieval. The `toDelegateField` method likely maps the field index to the correct index for the delegate. This correction directly addresses a bug where the wrong type was being returned. The answer is **Buggy**.
"
presto,26959.json,90e7efd07b37da0ff012bb99798ed37b4173661b,"@@ -1,85 +1,83 @@
     private String toPredicate(int columnIndex, Domain domain)
     {
         String columnName = columnsNames.get(columnIndex);
         Type type = types.get(columnIndex);
 
         if (domain.getRanges().isNone() && domain.isNullAllowed()) {
             return columnName + "" IS NULL"";
         }
 
         if (domain.getRanges().isAll() && !domain.isNullAllowed()) {
             return columnName + "" IS NOT NULL"";
         }
 
         // Add disjuncts for ranges
         List<String> disjuncts = new ArrayList<>();
         List<Comparable<?>> singleValues = new ArrayList<>();
         for (Range range : domain.getRanges()) {
             checkState(!range.isAll()); // Already checked
-            Comparable<?> lowValue = range.getLow().getValue();
             if (range.isSingleValue()) {
-                singleValues.add(lowValue);
+                singleValues.add(range.getLow().getValue());
             }
             else {
                 List<String> rangeConjuncts = new ArrayList<>();
                 if (!range.getLow().isLowerUnbounded()) {
-                    Object bindValue = getBindValue(columnIndex, lowValue);
+                    Object bindValue = getBindValue(columnIndex, range.getLow().getValue());
                     switch (range.getLow().getBound()) {
                         case ABOVE:
                             rangeConjuncts.add(toBindPredicate(columnName, "">""));
                             bindValues.add(ValueBuffer.create(columnIndex, type, bindValue));
                             break;
                         case EXACTLY:
                             rangeConjuncts.add(toBindPredicate(columnName, "">=""));
                             bindValues.add(ValueBuffer.create(columnIndex, type, bindValue));
                             break;
                         case BELOW:
                             throw new IllegalStateException(""Low Marker should never use BELOW bound: "" + range);
                         default:
                             throw new AssertionError(""Unhandled bound: "" + range.getLow().getBound());
                     }
                 }
                 if (!range.getHigh().isUpperUnbounded()) {
-                    Comparable<?> highValue = range.getHigh().getValue();
-                    Object bindValue = getBindValue(columnIndex, highValue);
+                    Object bindValue = getBindValue(columnIndex, range.getHigh().getValue());
                     switch (range.getHigh().getBound()) {
                         case ABOVE:
                             throw new IllegalStateException(""High Marker should never use ABOVE bound: "" + range);
                         case EXACTLY:
                             rangeConjuncts.add(toBindPredicate(columnName, ""<=""));
                             bindValues.add(ValueBuffer.create(columnIndex, type, bindValue));
                             break;
                         case BELOW:
                             rangeConjuncts.add(toBindPredicate(columnName, ""<""));
                             bindValues.add(ValueBuffer.create(columnIndex, type, bindValue));
                             break;
                         default:
                             throw new AssertionError(""Unhandled bound: "" + range.getHigh().getBound());
                     }
                 }
                 // If rangeConjuncts is null, then the range was ALL, which should already have been checked for
                 checkState(!rangeConjuncts.isEmpty());
                 disjuncts.add(""("" + Joiner.on("" AND "").join(rangeConjuncts) + "")"");
             }
         }
 
         // Add back all of the possible single values either as an equality or an IN predicate
         if (singleValues.size() == 1) {
             disjuncts.add(toBindPredicate(columnName, ""=""));
             bindValues.add(ValueBuffer.create(columnIndex, type, getBindValue(columnIndex, getOnlyElement(singleValues))));
         }
         else if (singleValues.size() > 1) {
             disjuncts.add(columnName + "" IN ("" + Joiner.on("","").join(limit(cycle(""?""), singleValues.size())) + "")"");
             for (Comparable<?> singleValue : singleValues) {
                 bindValues.add(ValueBuffer.create(columnIndex, type, getBindValue(columnIndex, singleValue)));
             }
         }
 
         // Add nullability disjuncts
         checkState(!disjuncts.isEmpty());
         if (domain.isNullAllowed()) {
             disjuncts.add(columnName + "" IS NULL"");
         }
 
         return ""("" + Joiner.on("" OR "").join(disjuncts) + "")"";
     }",Buggy,"Fix bug for domain with lower unbounded range
",Buggy,"The commit message indicates a bug fix related to domains with a lower unbounded range. The code changes involve constructing predicates for filtering data based on domain ranges. The original code directly accessed `range.getLow().getValue()` and `range.getHigh().getValue()` without considering the possibility of unbounded ranges. The corrected code now retrieves the low and high values from the `range` object and uses them to construct the appropriate predicates. This change addresses the bug by correctly handling domains with lower unbounded ranges, ensuring that the generated predicates accurately reflect the intended filtering logic. The answer is **Buggy**."
presto,1063.json,097536e42a4f29b25497ec5c782ef60cabdf71ef,"@@ -1,26 +1,31 @@
     public <V> Callable<V> wrap(Callable<V> callable)
     {
         return () -> {
             try (TimeStat.BlockTimer ignored = time.time()) {
                 return callable.call();
             }
             catch (Exception e) {
                 if (e instanceof MetaException) {
                     metastoreExceptions.update(1);
                     // Need to throw here instead of falling through due to JDK-8059299
                     totalFailures.update(1);
                     throw e;
                 }
 
                 if (e instanceof TException) {
+                    if (e instanceof TBase) {
+                        // This exception is an API response and not a server error
+                        throw e;
+                    }
+
                     thriftExceptions.update(1);
                     // Need to throw here instead of falling through due to JDK-8059299
                     totalFailures.update(1);
                     throw e;
                 }
 
                 totalFailures.update(1);
                 throw e;
             }
         };
     }",Buggy,"Fix Hive metastore client API stats

The stats were counting API response exceptions as server errors.
",Buggy,"The commit message indicates a fix for incorrect stats reporting in the Hive metastore client API. The code diff modifies the exception handling within the `wrap` method. Specifically, it checks if a `TException` is also a `TBase`. If it is, the exception is re-thrown without incrementing the `thriftExceptions` counter, implying it's an API response exception and not a server error. This aligns with the commit message's claim that API response exceptions were incorrectly counted as server errors. The change corrects this misclassification. The answer is **Buggy**."
presto,2919.json,54dd54cb681af311e9b6fb6910ae004dfa71988b,"@@ -1,8 +1,8 @@
     public SingleArrayBlockWriter beginBlockEntry()
     {
         if (currentEntryOpened) {
-            throw new IllegalStateException(""Expected current entry to be closed but was closed"");
+            throw new IllegalStateException(""Expected current entry to be closed but was opened"");
         }
         currentEntryOpened = true;
         return new SingleArrayBlockWriter(values, values.getPositionCount());
     }",Buggy,"Minor exception message fix for ArrayBlockBuilder

When attempting to begin an entry on an already opened block writer,
the error message does not accurately reflect the current state.
",Buggy,"The commit message indicates a fix to an exception message in `ArrayBlockBuilder`. The diff shows a change in the `IllegalStateException` message from ""Expected current entry to be closed but was closed"" to ""Expected current entry to be closed but was opened"". This correction aligns with the commit message, resolving a misleading error message. The answer is **Buggy**."
presto,37880.json,954e4714ac1aac177bbf00d5641088cc7eb15289,"@@ -1,8 +1,8 @@
     public int getPartition(Object key)
     {
         int partition = requireNonNull((Integer) key, ""key is null"");
         if (!(partition >= 0 && partition < numPartitions)) {
-            throw new IllegalArgumentException(""invalid partition: %s"" + partition);
+            throw new IllegalArgumentException(format(""Unexpected partition: %s. Total number of partitions: %s."", partition, numPartitions));
         }
         return partition;
     }",Buggy,"Fix error message in IntegerIdentityPartitioner
",Buggy,"The commit message indicates a fix to an error message in the `IntegerIdentityPartitioner`. The diff shows a change in the `IllegalArgumentException` message. The original message was `""invalid partition: %s"" + partition`, which would result in a message like ""invalid partition: %s123"" instead of ""invalid partition: 123"". The corrected message uses `format` to properly insert the partition value and also includes the total number of partitions, providing more context. This change directly addresses a bug in the error message formatting and content. The answer is **Buggy**.
"
presto,5480.json,3d776ab3a9b34d155caa2097ed5e0a82c3f9ca39,"@@ -1,6 +1,7 @@
     private static Class<? extends InputFormat<?, ?>> getInputFormatClass(JobConf conf, String inputFormatName)
             throws ClassNotFoundException
     {
         Class<?> clazz = conf.getClassByName(inputFormatName);
-        return (Class<? extends InputFormat<?, ?>>) clazz.asSubclass(InputFormat.class);
+        // TODO: remove redundant cast to Object after IDEA-118533 is fixed
+        return (Class<? extends InputFormat<?, ?>>) (Object) clazz.asSubclass(InputFormat.class);
     }",Buggy,"Fix bogus error message in IDEA 13
",Buggy,"The commit message indicates a fix for a bogus error message in IDEA 13. The diff adds a cast to `Object` before casting to `Class<? extends InputFormat<?, ?>>`. This change is likely a workaround for a bug in IDEA 13's type inference or compiler, which was causing an incorrect error message related to type casting. The added cast doesn't change the functionality of the code but suppresses the erroneous error message. Therefore, this change addresses a bug, albeit in the IDE rather than the code's logic. The answer is **Buggy**."
presto,14661.json,a5a04929e51756cac167c14cf73399a0df74f4c5,"@@ -1,4 +1,5 @@
     public static long valueAtQuantileBigint(@SqlType(""qdigest(bigint)"") Slice input, @SqlType(StandardTypes.DOUBLE) double quantile)
     {
+        checkCondition(quantile >= 0 && quantile <= 1, INVALID_FUNCTION_ARGUMENT, ""Quantile should be within bounds [0, 1], was: "" + quantile);
         return new QuantileDigest(input).getQuantile(quantile);
     }",Buggy,"Fix invalid quantile to be a user error
",Buggy,"The commit message indicates a fix related to an invalid quantile value, which is now treated as a user error. The diff adds a check to ensure the quantile value is within the valid range of [0, 1]. If the quantile is outside this range, an `INVALID_FUNCTION_ARGUMENT` exception is thrown, indicating a user error. This change directly addresses the issue described in the commit message by validating user input and providing a clear error message when the input is invalid. The answer is **Buggy**."
presto,2512.json,ff82d4c25dcaeb742f1eb3470a04e89a8d2ede7b,"@@ -1,5 +1,5 @@
     public Block getRegion(int positionOffset, int length)
     {
         checkValidRegion(positionCount, positionOffset, length);
-        return new DictionaryBlock(idsOffset + positionOffset, length, dictionary, ids, false, randomDictionaryId());
+        return new DictionaryBlock(idsOffset + positionOffset, length, getDictionary(), ids, false, getDictionarySourceId());
     }",Buggy,"Fix bug in DictionaryBlock getRegion
",Buggy,"The commit message indicates a bug fix in the `getRegion` method of the `DictionaryBlock` class. The diff shows that the `dictionary` and `randomDictionaryId()` arguments in the `DictionaryBlock` constructor are replaced with `getDictionary()` and `getDictionarySourceId()` respectively. This suggests that the original code was either using an incorrect dictionary or generating a random dictionary ID when it should have been using the existing one. This change ensures that the correct dictionary and its source ID are used when creating a region of the block, which is likely a bug fix. The answer is **Buggy**.
"
presto,22974.json,eb4e582942793631a8de8cf6e6e1c06ce3053d75,"@@ -1,8 +1,8 @@
     public void setConfigurationManager(String name, Map<String, String> properties)
     {
         SessionPropertyConfigurationManagerFactory factory = factories.get(name);
-        checkState(factory != null, ""Session property configuration manager %s is not registered"");
+        checkState(factory != null, ""Session property configuration manager '%s' is not registered"", name);
 
         SessionPropertyConfigurationManager manager = factory.create(properties, configurationManagerContext);
         checkState(delegate.compareAndSet(null, manager), ""sessionPropertyConfigurationManager is already set"");
     }",Buggy,"Fix error message in SessionPropertyDefaults
",Buggy,"The commit message indicates a fix to an error message. The diff shows a change to the `checkState` method call, specifically modifying the error message format string to include the name of the unregistered session property configuration manager. This change improves the clarity and usefulness of the error message, making it easier to diagnose the problem. This is a bug fix because it corrects an issue with the error reporting. The answer is **Buggy**."
presto,22974.json,59d297913ff6641b852bd236161f237a3f9e3af7,"@@ -1,8 +1,8 @@
     public void setConfigurationManager(String name, Map<String, String> properties)
     {
         SessionPropertyConfigurationManagerFactory factory = sessionPropertyConfigurationManagerFactories.get(name);
-        checkState(factory != null, ""Session property configuration manager %s is not registered"");
+        checkState(factory != null, ""Session property configuration manager '%s' is not registered"", name);
 
         SessionPropertyConfigurationManager manager = factory.create(properties, configurationManagerContext);
         checkState(sessionPropertyConfigurationManager.compareAndSet(null, manager), ""sessionPropertyConfigurationManager is already set"");
     }",Buggy,"Fix error message in QuerySessionSupplier
",Buggy,"The commit message indicates a fix to an error message. The diff shows a change to the `checkState` method call, where the error message string is updated to include the name of the session property configuration manager that is not registered. This allows for a more informative error message, aiding in debugging. This is a bug fix because it improves the clarity and usefulness of the error message, making it easier to identify the cause of the error. The answer is **Buggy**."
presto,31633.json,318f9bf12b4402074f5f8caf0845a121c026f0df,"@@ -1,8 +1,8 @@
     public Object getObjectValue(ConnectorSession session, Block block, int position)
     {
         if (block.isNull(position)) {
             return null;
         }
 
-        return BingTile.decode(block.getLong(0, 0));
+        return BingTile.decode(block.getLong(position, 0));
     }",Buggy,"Fix getObjectValue in BingTileType

This change fixes a bug in decoding arrays of bing tiles.
",Buggy,"The commit message indicates a bug fix related to decoding arrays of Bing tiles. The code change modifies the `getObjectValue` method, specifically how the `BingTile.decode` method is called. The original code `block.getLong(0, 0)` always used the first element of the block (index 0), regardless of the position. The corrected code `block.getLong(position, 0)` uses the correct position to retrieve the Bing tile data. This clearly fixes a bug where the wrong tile was being decoded. The answer is **Buggy**.
"
netty,5669.json,b39ffed042844adecaf0a4fc4e9a2f53edaa111d,"@@ -1,15 +1,15 @@
         private void record(int actualReadBytes) {
-            if (actualReadBytes <= SIZE_TABLE[max(0, index - INDEX_DECREMENT - 1)]) {
+            if (actualReadBytes <= SIZE_TABLE[max(0, index - INDEX_DECREMENT)]) {
                 if (decreaseNow) {
                     index = max(index - INDEX_DECREMENT, minIndex);
                     nextReceiveBufferSize = SIZE_TABLE[index];
                     decreaseNow = false;
                 } else {
                     decreaseNow = true;
                 }
             } else if (actualReadBytes >= nextReceiveBufferSize) {
                 index = min(index + INDEX_INCREMENT, maxIndex);
                 nextReceiveBufferSize = SIZE_TABLE[index];
                 decreaseNow = false;
             }
         }",Buggy,"Fix incorrect calculation of next buffer size in AdaptiveRecvByteBufAllocator (#9555)

Motivation:

Due a bug we did not always correctly calculate the next buffer size in AdaptiveRecvByteBufAllocator.

Modification:

Fix calculation and add unit test

Result:

Correct calculation is always used. 
",Buggy,"The commit message clearly states a bug fix in the calculation of the next buffer size within `AdaptiveRecvByteBufAllocator`. The diff shows a modification to the condition that determines whether to decrease the index, which directly affects the calculation of `nextReceiveBufferSize`. Specifically, `INDEX_DECREMENT - 1` was changed to `INDEX_DECREMENT`. This indicates a correction to the logic that determines when to reduce the buffer size, aligning with the commit message's claim of fixing an incorrect calculation. The answer is **Buggy**.
"
netty,10635.json,799350c369e68462b61c6aef97db2a33ea937434,"@@ -1,63 +1,67 @@
     public void updateDependencyTree(int childStreamId, int parentStreamId, short weight, boolean exclusive) {
         if (weight < MIN_WEIGHT || weight > MAX_WEIGHT) {
             throw new IllegalArgumentException(String.format(
                     ""Invalid weight: %d. Must be between %d and %d (inclusive)."", weight, MIN_WEIGHT, MAX_WEIGHT));
         }
         if (childStreamId == parentStreamId) {
             throw new IllegalArgumentException(""A stream cannot depend on itself"");
         }
 
         State state = state(childStreamId);
         if (state == null) {
             // If there is no State object that means there is no Http2Stream object and we would have to keep the
             // State object in the stateOnlyMap and stateOnlyRemovalQueue. However if maxStateOnlySize is 0 this means
             // stateOnlyMap and stateOnlyRemovalQueue are empty collections and cannot be modified so we drop the State.
             if (maxStateOnlySize == 0) {
                 return;
             }
             state = new State(childStreamId);
             stateOnlyRemovalQueue.add(state);
             stateOnlyMap.put(childStreamId, state);
         }
 
         State newParent = state(parentStreamId);
         if (newParent == null) {
             // If there is no State object that means there is no Http2Stream object and we would have to keep the
             // State object in the stateOnlyMap and stateOnlyRemovalQueue. However if maxStateOnlySize is 0 this means
             // stateOnlyMap and stateOnlyRemovalQueue are empty collections and cannot be modified so we drop the State.
             if (maxStateOnlySize == 0) {
                 return;
             }
             newParent = new State(parentStreamId);
             stateOnlyRemovalQueue.add(newParent);
             stateOnlyMap.put(parentStreamId, newParent);
+            // Only the stream which was just added will change parents. So we only need an array of size 1.
+            List<ParentChangedEvent> events = new ArrayList<ParentChangedEvent>(1);
+            connectionState.takeChild(newParent, false, events);
+            notifyParentChanged(events);
         }
 
         // if activeCountForTree == 0 then it will not be in its parent's pseudoTimeQueue and thus should not be counted
         // toward parent.totalQueuedWeights.
         if (state.activeCountForTree != 0 && state.parent != null) {
             state.parent.totalQueuedWeights += weight - state.weight;
         }
         state.weight = weight;
 
         if (newParent != state.parent || (exclusive && newParent.children.size() != 1)) {
             final List<ParentChangedEvent> events;
             if (newParent.isDescendantOf(state)) {
                 events = new ArrayList<ParentChangedEvent>(2 + (exclusive ? newParent.children.size() : 0));
                 state.parent.takeChild(newParent, false, events);
             } else {
                 events = new ArrayList<ParentChangedEvent>(1 + (exclusive ? newParent.children.size() : 0));
             }
             newParent.takeChild(state, exclusive, events);
             notifyParentChanged(events);
         }
 
         // The location in the dependency tree impacts the priority in the stateOnlyRemovalQueue map. If we created new
         // State objects we must check if we exceeded the limit after we insert into the dependency tree to ensure the
         // stateOnlyRemovalQueue has been updated.
         while (stateOnlyRemovalQueue.size() > maxStateOnlySize) {
             State stateToRemove = stateOnlyRemovalQueue.poll();
             stateToRemove.parent.removeChild(stateToRemove);
             stateOnlyMap.remove(stateToRemove.streamId);
         }
     }",Buggy,"Fix HTTP/2 dependency tree corruption

Motivation:

Chrome was randomly getting stuck loading the tiles examples.
Investigation showed that the Netty flow controller thought it had
nothing to send for the connection even though some streams has queued
data and window available.

Modifications:

Fixed an accounting error where an implicitly created parent was not
being added to the dependency tree, thus it and all of its children were
orphaned from the connection's tree and would never have data written.

Result:

Fixes #6621
",Buggy,"The commit message indicates a fix for HTTP/2 dependency tree corruption, which caused Chrome to get stuck loading tiles. The root cause was an accounting error where an implicitly created parent was not being added to the dependency tree, leading to orphaned streams.

The code diff shows that when a new parent `State` is created (because it doesn't already exist), the code now calls `connectionState.takeChild(newParent, false, events)` and `notifyParentChanged(events)`. This ensures that the newly created parent is correctly added to the dependency tree. Without this change, the new parent and its children would be orphaned, which aligns with the bug description in the commit message.

Therefore, the changes in the diff directly address the bug described in the commit message. The answer is **Buggy**."
netty,536.json,6fc7c589f023b069ac5f4df8a6766f0b501487fb,"@@ -1,10 +1,11 @@
         DatagramPacket newDatagramPacket(ByteBuf buffer, InetSocketAddress localAddress) throws UnknownHostException {
             final InetAddress address;
-            if (scopeId != 0) {
-                address = Inet6Address.getByAddress(null, addr, scopeId);
+            if (addrLen == ipv4Bytes.length) {
+                System.arraycopy(addr, 0, ipv4Bytes, 0, addrLen);
+                address = InetAddress.getByAddress(ipv4Bytes);
             } else {
-                address = InetAddress.getByAddress(addr);
+                address = Inet6Address.getByAddress(null, addr, scopeId);
             }
             return new DatagramPacket(buffer.writerIndex(count),
                     localAddress, new InetSocketAddress(address, port));
         }",Buggy,"Correctly handle ipv6 mapped ipv4 addresses when using recvmmsg (#9541)


Motivation:

394a1b3485000c211595aff7495c4f863972af29 introduced the possibility to use recvmmsg(...) but did not correctly handle ipv6 mapped ip4 addresses to make it consistent with other transports.

Modifications:

- Correctly handle ipv6 mapped ipv4 addresses by only copy over the relevant bytes
- Small improvement on how to detect ipv6 mapped ipv4 addresses by using memcmp and not byte by byte compare
- Adjust test to cover this bug

Result:

Correctly handle ipv6 mapped ipv4 addresses
",Buggy,"The commit message indicates a bug fix related to handling IPv6 mapped IPv4 addresses when using `recvmmsg`. The diff shows that the code now checks if the address length (`addrLen`) is equal to the length of IPv4 bytes. If it is, it copies the address bytes into `ipv4Bytes` and creates an `InetAddress` from it. Otherwise, it creates an `Inet6Address`. This change ensures that IPv6 mapped IPv4 addresses are correctly handled, aligning with the commit message's intention to fix a bug. The answer is **Buggy**.
"
netty,15758.json,f0a3f849f7d0e028eb97e672a3cbf52648b98617,"@@ -1,26 +1,27 @@
     public ByteBuf writeZero(int length) {
         if (length == 0) {
             return this;
         }
 
+        ensureWritable(length);
         checkIndex(writerIndex, length);
 
         int nLong = length >>> 3;
         int nBytes = length & 7;
         for (int i = nLong; i > 0; i --) {
             writeLong(0);
         }
         if (nBytes == 4) {
             writeInt(0);
         } else if (nBytes < 4) {
             for (int i = nBytes; i > 0; i --) {
                 writeByte((byte) 0);
             }
         } else {
             writeInt(0);
             for (int i = nBytes - 4; i > 0; i --) {
                 writeByte((byte) 0);
             }
         }
         return this;
     }",Buggy,"Fix a bug in AbstractByteBuf.writeZero() where the capacity is not auto-expanded
",Buggy,"The commit message indicates a bug fix in `AbstractByteBuf.writeZero()` related to capacity auto-expansion. The diff adds `ensureWritable(length)` at the beginning of the method. This ensures that the buffer has enough capacity to write the specified number of zero bytes. Without this check, the method might fail if the current buffer capacity is insufficient, leading to a bug. The added line directly addresses the bug described in the commit message. The answer is **Buggy**."
netty,14984.json,dcc39e5b21eda7bdd9563ed86253a693923f6432,"@@ -1,13 +1,13 @@
     protected String format(ChannelHandlerContext ctx, String eventName, Object firstArg, Object secondArg) {
         if (secondArg == null) {
             return formatSimple(ctx, eventName, firstArg);
         }
 
         String chStr = ctx.channel().toString();
         String arg1Str = String.valueOf(firstArg);
         String arg2Str = secondArg.toString();
         StringBuilder buf = new StringBuilder(
-                chStr.length() + 1 + eventName + 2 + arg1Str.length() + 2 + arg2Str.length());
+                chStr.length() + 1 + eventName.length() + 2 + arg1Str.length() + 2 + arg2Str.length());
         buf.append(chStr).append(' ').append(eventName).append("": "").append(arg1Str).append("", "").append(arg2Str);
         return buf.toString();
     }",Buggy,"Fixes a LoggingHandler#format method with two arguments

Motivation:
Bug in capacity calculation: occurs auto convert to string instead of sum up.

Modifications:
Use `eventName.length()` in sum.

Result:
Less trash in logs.
",Buggy,"The commit message indicates a bug fix in the `LoggingHandler#format` method related to capacity calculation. The motivation explains that the bug caused incorrect string conversion instead of summing up lengths, leading to excessive logging. The modification involves using `eventName.length()` in the StringBuilder's capacity calculation. This aligns with the commit message and suggests a correction to a bug where the buffer size was miscalculated, potentially leading to performance issues or truncated logs. The answer is **Buggy**."
netty,2881.json,338e1a991c5d15695bd654a7b8d2bd4008e868ae,"@@ -1,51 +1,51 @@
         public void run() {
             if (!state.compareAndSet(SUBMITTED, RUNNING)) {
                 return;
             }
             for (;;) {
                 int i = 0;
                 try {
                     for (; i < maxTaskExecutePerRun; i++) {
                         Runnable task = tasks.poll();
                         if (task == null) {
                             break;
                         }
                         safeExecute(task);
                     }
                 } finally {
                     if (i == maxTaskExecutePerRun) {
                         try {
                             state.set(SUBMITTED);
                             executor.execute(this);
                             return; // done
                         } catch (Throwable ignore) {
                             // Reset the state back to running as we will keep on executing tasks.
                             state.set(RUNNING);
                             // if an error happened we should just ignore it and let the loop run again as there is not
                             // much else we can do. Most likely this was triggered by a full task queue. In this case
                             // we just will run more tasks and try again later.
                         }
                     } else {
                         state.set(NONE);
                         // After setting the state to NONE, look at the tasks queue one more time.
                         // If it is empty, then we can return from this method.
                         // Otherwise, it means the producer thread has called execute(Runnable)
                         // and enqueued a task in between the tasks.poll() above and the state.set(NONE) here.
                         // There are two possible scenarios when this happen
                         //
                         // 1. The producer thread sees state == NONE, hence the compareAndSet(NONE, SUBMITTED)
                         //    is successfully setting the state to SUBMITTED. This mean the producer
                         //    will call / has called executor.execute(this). In this case, we can just return.
                         // 2. The producer thread don't see the state change, hence the compareAndSet(NONE, SUBMITTED)
                         //    returns false. In this case, the producer thread won't call executor.execute.
                         //    In this case, we need to change the state to RUNNING and keeps running.
                         //
                         // The above cases can be distinguished by performing a
                         // compareAndSet(NONE, RUNNING). If it returns ""false"", it is case 1; otherwise it is case 2.
-                        if (tasks.peek() == null || !state.compareAndSet(NONE, RUNNING)) {
+                        if (tasks.isEmpty() || !state.compareAndSet(NONE, RUNNING)) {
                             return; // done
                         }
                     }
                 }
             }
         }",Buggy,"Fix a bug introduced by 79706357c73ded02615d0445db7503b646ff9547 which can cause thread to spin in an infinite loop. (#9579)

Motivation:
peek() is implemented in a similar way to poll() for the mpsc queue, thus it is more like a consumer call.
It is possible that we could have multiple thread call peek() and possibly one thread calls poll() at at the same time.
This lead to multiple consumer scenario, which violates the multiple producer single consumer condition and could lead to spin in an infinite loop in peek()

Modification:
Use isEmpty() instead of peek() to check if task queue is empty

Result:
Dont violate the mpsc semantics.
",Buggy,"The commit message indicates a bug fix related to a potential infinite loop caused by concurrent `peek()` and `poll()` calls on the task queue. The motivation explains that `peek()` was being used in a way that violated the multiple-producer single-consumer (MPSC) semantics of the queue, leading to the spin. The modification replaces `tasks.peek() == null` with `tasks.isEmpty()`. This change avoids the consumer-like behavior of `peek()` and uses a more appropriate method to check for emptiness without potentially interfering with `poll()`. This resolves the concurrency issue and prevents the infinite loop. The answer is **Buggy**."
netty,2881.json,79706357c73ded02615d0445db7503b646ff9547,"@@ -1,34 +1,51 @@
         public void run() {
             if (!state.compareAndSet(SUBMITTED, RUNNING)) {
                 return;
             }
             for (;;) {
                 int i = 0;
                 try {
                     for (; i < maxTaskExecutePerRun; i++) {
                         Runnable task = tasks.poll();
                         if (task == null) {
                             break;
                         }
                         safeExecute(task);
                     }
                 } finally {
                     if (i == maxTaskExecutePerRun) {
                         try {
                             state.set(SUBMITTED);
                             executor.execute(this);
                             return; // done
                         } catch (Throwable ignore) {
                             // Reset the state back to running as we will keep on executing tasks.
                             state.set(RUNNING);
                             // if an error happened we should just ignore it and let the loop run again as there is not
                             // much else we can do. Most likely this was triggered by a full task queue. In this case
                             // we just will run more tasks and try again later.
                         }
                     } else {
                         state.set(NONE);
-                        return; // done
+                        // After setting the state to NONE, look at the tasks queue one more time.
+                        // If it is empty, then we can return from this method.
+                        // Otherwise, it means the producer thread has called execute(Runnable)
+                        // and enqueued a task in between the tasks.poll() above and the state.set(NONE) here.
+                        // There are two possible scenarios when this happen
+                        //
+                        // 1. The producer thread sees state == NONE, hence the compareAndSet(NONE, SUBMITTED)
+                        //    is successfully setting the state to SUBMITTED. This mean the producer
+                        //    will call / has called executor.execute(this). In this case, we can just return.
+                        // 2. The producer thread don't see the state change, hence the compareAndSet(NONE, SUBMITTED)
+                        //    returns false. In this case, the producer thread won't call executor.execute.
+                        //    In this case, we need to change the state to RUNNING and keeps running.
+                        //
+                        // The above cases can be distinguished by performing a
+                        // compareAndSet(NONE, RUNNING). If it returns ""false"", it is case 1; otherwise it is case 2.
+                        if (tasks.peek() == null || !state.compareAndSet(NONE, RUNNING)) {
+                            return; // done
+                        }
                     }
                 }
             }
         }",Buggy,"Fix race condition in the NonStickyEventExecutorGroup (#8232)

Motivation:

There was a race condition between the task submitter and task executor threads such that the last Runnable submitted may not get executed. 

Modifications:

The bug was fixed by checking the task queue and state in the task executor thread after it saw the task queue was empty.

Result:

Fixes #8230",Buggy,"The commit message indicates a fix for a race condition where the last submitted Runnable might not be executed. The code diff introduces a check after the task queue appears empty and the state is set to NONE. It peeks at the task queue again and attempts to change the state to RUNNING if the queue is not empty. This addresses the race condition by ensuring that if a task is added to the queue between the poll() and state.set(NONE), the executor continues processing tasks. The added logic specifically handles the scenario where the producer thread might enqueue a task after the executor thread has determined the queue is empty. This clearly indicates a bug fix. The answer is **Buggy**."
netty,17549.json,20894bc99e28191cc3926ae95a23f5c7d3969a33,"@@ -1,4 +1,3 @@
     public ByteBuffer internalNioBuffer(int index, int length) {
-        // Do not mess with the internal buffer's byte order.
-        return buf.nioBuffer(index, length).order(order);
+        return buf.internalNioBuffer(index, length).duplicate().order(order);
     }",Buggy,"Fix a bug in internalNioBuffer() implementations of derived buffers

- A user can create multiple duplicates of a buffer and access their internal NIO buffers. (e.g. write multiple duplicates to multiple channels assigned to different event loop.)  Because the derived buffers' internalNioBuffer() simply delegates the call to the original buffer, all derived buffers and the original buffer's internalNioBuffer() will return the same buffer, which will lead to a race condition.
- Fixes #1739
",Buggy,"The commit message indicates a bug fix related to race conditions when accessing internal NIO buffers of duplicated buffers. The `internalNioBuffer()` method in derived buffers was delegating the call to the original buffer, causing all duplicates to share the same underlying buffer. This led to race conditions when multiple threads accessed these buffers concurrently.

The diff modifies the `internalNioBuffer()` method to return a `duplicate()` of the underlying buffer before setting the order. This ensures that each derived buffer has its own independent NIO buffer, preventing the race condition.

The change directly addresses the bug described in the commit message by creating a duplicate of the underlying buffer, thus isolating the derived buffers and preventing race conditions. The answer is **Buggy**."
netty,2272.json,cd3254df88b60476dc04b39915d3d70c200eb6f4,"@@ -1,6 +1,7 @@
         private static void reachabilityFence0(Object ref) {
             if (ref != null) {
-                // Empty synchronized is ok: https://stackoverflow.com/a/31933260/1151521
-                synchronized (ref) { }
+                synchronized (ref) {
+                    // Empty synchronized is ok: https://stackoverflow.com/a/31933260/1151521
+                }
             }
         }",Buggy,"Update to new checkstyle plugin (#8777) (#8780)

Motivation:

We need to update to a new checkstyle plugin to allow the usage of lambdas.

Modifications:

- Update to new plugin version.
- Fix checkstyle problems.

Result:

Be able to use checkstyle plugin which supports new Java syntax.",NotBuggy,The commit message indicates an update to the checkstyle plugin to allow the usage of lambdas and fixing checkstyle problems. The diff adds an empty block inside the synchronized block. This change does not seem to be related to fixing a bug in the code's logic or functionality. It's more likely related to satisfying the requirements of the new checkstyle plugin. The answer is **NotBuggy**.
netty,1691.json,32746c53c1089550f79bd2d6eeba4d65dab1f2d3,"@@ -1,24 +1,24 @@
-    static byte padWithZeros(byte b, int lowOrderBitsToPreserve) {
+    private static byte padWithZeros(byte b, int lowOrderBitsToPreserve) {
         switch (lowOrderBitsToPreserve) {
         case 0:
             return 0;
         case 1:
-            return (byte) (0x01 & b);
+            return (byte) (0x80 & b);
         case 2:
-            return (byte) (0x03 & b);
+            return (byte) (0xC0 & b);
         case 3:
-            return (byte) (0x07 & b);
+            return (byte) (0xE0 & b);
         case 4:
-            return (byte) (0x0F & b);
+            return (byte) (0xF0 & b);
         case 5:
-            return (byte) (0x1F & b);
+            return (byte) (0xF8 & b);
         case 6:
-            return (byte) (0x3F & b);
+            return (byte) (0xFC & b);
         case 7:
-            return (byte) (0x7F & b);
+            return (byte) (0xFE & b);
         case 8:
             return b;
         default:
             throw new IllegalArgumentException(""lowOrderBitsToPreserve: "" + lowOrderBitsToPreserve);
         }
     }",Buggy,"EDNS Client Subnet is not encoded correctly when source prefix length is not a multiple of 8.

Motivation:
When the ECS source prefix length is not a mutiple of 8, the last byte the address inside the
ECS OPT record is not padded properly.

Modifications:
DefaultDnsRecordEncoder.padWithZeros(...) was modified to add padding from the least
significant bits.

Result:
ECS encoding bug fixed.
",Buggy,"The commit message clearly states a bug related to EDNS Client Subnet encoding when the source prefix length is not a multiple of 8. The provided code diff modifies the `padWithZeros` method to correctly pad the last byte of the address in the ECS OPT record. The original code used a bitmask that preserved the least significant bits, while the corrected code uses a bitmask that preserves the most significant bits, effectively padding with zeros from the least significant bits as described in the commit message. This resolves the encoding issue, and the commit message explicitly labels this as a bug fix. The answer is **Buggy**.
"
netty,13070.json,a29532df43c9db08db5dbc83f30bb7bd2e55a596,"@@ -1,12 +1,12 @@
     private static Method updateByteBuffer(Checksum checksum) {
         if (PlatformDependent.javaVersion() >= 8) {
             try {
                 Method method = checksum.getClass().getDeclaredMethod(""update"", ByteBuffer.class);
-                method.invoke(method, ByteBuffer.allocate(1));
+                method.invoke(checksum, ByteBuffer.allocate(1));
                 return method;
             } catch (Throwable ignore) {
                 return null;
             }
         }
         return null;
     }",Buggy,"Fix ByteBufChecksum optimisation for CRC32 and Adler32 (#9242)

Motivation:

Because of a simple bug in ByteBufChecksum#updateByteBuffer(Checksum),
ReflectiveByteBufChecksum is never used for CRC32 and Adler32, resulting
in direct ByteBuffers being checksummed byte by byte, which is
undesriable.

Modification:

Fix ByteBufChecksum#updateByteBuffer(Checksum) method to pass the
correct argument to Method#invoke(Checksum, ByteBuffer).

Result:

ReflectiveByteBufChecksum will now be used for Adler32 and CRC32 on
Java8+ and direct ByteBuffers will no longer be checksummed on slow
byte-by-byte basis.",Buggy,"The commit message indicates a bug fix related to the optimization of `ByteBufChecksum` for CRC32 and Adler32 algorithms. The bug caused `ReflectiveByteBufChecksum` to be unused, leading to inefficient byte-by-byte checksumming of direct ByteBuffers. The provided diff shows a correction in the `updateByteBuffer` method where the `invoke` method was called incorrectly. It was passing the method itself as the target object instead of the checksum object. The fix changes `method.invoke(method, ByteBuffer.allocate(1))` to `method.invoke(checksum, ByteBuffer.allocate(1))`, which passes the checksum object as the target. This correction ensures that `ReflectiveByteBufChecksum` is used for Adler32 and CRC32 on Java 8+, resolving the performance issue. The answer is **Buggy**."
netty,239.json,1cc104e1c02f5ee54979d8dfcba5aa4fb20d3503,"@@ -1,3 +1,3 @@
-    public static SctpServerChannel open() {
+    public static SctpServerChannel open() throws IOException {
         return null;
     }",Buggy,"Fix a compilation error
",Buggy,"The commit message indicates a fix for a compilation error. The diff adds `throws IOException` to the method signature. This resolves a compilation error that would occur if the method threw a checked exception that wasn't declared in its signature. The answer is **Buggy**.
"
netty,1341.json,a4c96483d1e83981a1e0860d3a6f71fbe21d500c,"@@ -1,18 +1,20 @@
     private static boolean anyInterfaceSupportsIpV6() {
         try {
             Enumeration<NetworkInterface> interfaces = NetworkInterface.getNetworkInterfaces();
             while (interfaces.hasMoreElements()) {
                 NetworkInterface iface = interfaces.nextElement();
                 Enumeration<InetAddress> addresses = iface.getInetAddresses();
                 while (addresses.hasMoreElements()) {
-                    if (addresses.nextElement() instanceof Inet6Address) {
+                    InetAddress inetAddress = addresses.nextElement();
+                    if (inetAddress instanceof Inet6Address && !inetAddress.isAnyLocalAddress() &&
+                        !inetAddress.isLoopbackAddress() && !inetAddress.isLinkLocalAddress()) {
                         return true;
                     }
                 }
             }
         } catch (SocketException e) {
             logger.debug(""Unable to detect if any interface supports IPv6, assuming IPv4-only"", e);
             // ignore
         }
         return false;
     }",Buggy,"Fix a bug where making IPv6 DnsQuestion when it's not supported (#10170)

Motivation:
Related https://github.com/line/armeria/issues/2463
Here is an example that an NIC has only link local address for IPv6.
```
$ ipaddr
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
3: eth0@if18692: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1460 qdisc noqueue
    link/ether 1a:5e:xx:xx:xx:xx brd ff:ff:ff:ff:ff:ff
    inet 10.xxx.xxx.xxx/24 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::xxxx:xxxx:xxxx:xxxx/64 scope link
       valid_lft forever preferred_lft forever
```
If the NICs have only local or link local addresses, We should not send IPv6 DNS queris.

Modification:
- Ignore link-local IPv6 addresses which may exist even on a machine without IPv6 network.

Result:
- `DnsNameResolver` does not send DNS queries for AAAA when IPv6 is not available.",Buggy,"The commit message indicates a bug fix related to IPv6 DNS queries being sent even when IPv6 is not properly supported. The provided `ipaddr` example demonstrates a scenario where a network interface only has a link-local IPv6 address. The code modification filters IPv6 addresses to exclude any local, loopback, and link-local addresses. This ensures that DNS queries for AAAA records are only sent if a valid, non-local IPv6 address is present. This change directly addresses the bug described in the commit message. The answer is **Buggy**.
"
netty,17654.json,c0bbde48b78163a394ae3fd63f98b6b6c56acb25,"@@ -1,3 +1,3 @@
     public ByteBuffer nioBuffer(int index, int length) {
-        return (ByteBuffer) buffer.duplicate().position(index).position(index + length);
+        return (ByteBuffer) buffer.duplicate().position(index).limit(index + length);
     }",Buggy,"[#1852] Fix bug in UnpooledDirectByteBuf.nioBuffer(...) implementation
",Buggy,"The commit message indicates a bug fix in the `nioBuffer` method of `UnpooledDirectByteBuf`. The diff shows a change from `position(index + length)` to `limit(index + length)`. The original code was incorrectly setting the position to the end of the desired buffer region, effectively creating an empty buffer. The corrected code sets the limit to the end of the region, which is the correct way to define the buffer's boundaries. This resolves the bug where the `nioBuffer` method was not returning the expected buffer content. The answer is **Buggy**."
netty,12409.json,6983f704c0882dd322bc1cf0c7c1c635e4b05ea5,"@@ -1,3 +1,5 @@
     public ByteBuf readBytes(ByteBuf dst) {
-        throw new UnreplayableOperationException();
+        checkReadableBytes(dst.writableBytes());
+        buffer.readBytes(dst);
+        return this;
     }",Buggy,"Fix unreplayble error
",Buggy,"The commit message indicates a fix for an ""unreplayable error."" The code diff shows that the `readBytes` method, which previously threw an `UnreplayableOperationException`, is now implemented with a check for readable bytes and a subsequent read operation on the underlying buffer. This change replaces the throwing of an exception with actual functionality, which strongly suggests a bug fix. The answer is **Buggy**."
netty,9557.json,97d07253e2933dbca0d08cd19072503506bda863,"@@ -1,12 +1,12 @@
     public static SpdySessionStatus valueOf(int code) {
         switch (code) {
         case 0:
             return OK;
         case 1:
             return PROTOCOL_ERROR;
-        case 11:
+        case 2:
             return INTERNAL_ERROR;
         }
 
         return new SpdySessionStatus(code, ""UNKNOWN ("" + code + ')');
     }",Buggy,"SPDY: fixed session status (internal error misdocumented)
",Buggy,"The commit message indicates a fix related to the SPDY session status, specifically addressing a misdocumented internal error. The code change modifies the `valueOf` method to correctly map the integer code `2` to `INTERNAL_ERROR`, whereas previously `11` was mapped to `INTERNAL_ERROR`. This correction aligns with the commit message, suggesting a bug fix where the internal error code was incorrectly assigned. The answer is **Buggy**."
argouml,1941.json,045f33e22637950982cc1193584f80301860683e,"@@ -1,7 +1,7 @@
     public void notationTextChanged(NotationTextEvent event) {
         this.setText(event.getText());
         this.setUnderline(event.isUnderlined());
-        this.setUnderline(event.isBold());
-        this.setItalic(event.isBold());
+        this.setBold(event.isBold());
+        this.setItalic(event.isItalic());
         this.damage();
     }",Buggy,"Fix some cut and paste errors

git-svn-id: http://argouml.tigris.org/svn/argouml/trunk@19262 a161b567-7d1e-0410-9ef9-912c70fedb3f
",Buggy,"The commit message indicates a fix for ""cut and paste errors."" The diff shows that `setUnderline` was called with `event.isBold()` when it should have been `setBold`, and `setItalic` was called with `event.isBold()` when it should have been `event.isItalic()`. This confirms the cut and paste error described in the commit message. The answer is **Buggy**."
argouml,1718.json,126f591fd6961fb320d035184050b65257130479,"@@ -1,6 +1,8 @@
     public boolean doesAccept(Object objectToAccept) {
         if (Model.getFacade().isAClassifier(objectToAccept)) {
             return true;
+        } else if (Model.getFacade().isAComment(objectToAccept)) {
+        	return true;
         }
         return false;
     }",Buggy,"Fixed a bug where comments could not be added in Seq2 diagrams.

git-svn-id: http://argouml.tigris.org/svn/argouml/trunk@15998 a161b567-7d1e-0410-9ef9-912c70fedb3f
",Buggy,The commit message indicates a bug fix related to adding comments in Seq2 diagrams. The diff shows that the `doesAccept` method in some class now also accepts objects that are comments (`Model.getFacade().isAComment(objectToAccept)`). This directly addresses the bug described in the commit message. The answer is **Buggy**.
docx4j,8366.json,9f08fd2eac1d042d1fe3b2e70b2918f551ea677e,"@@ -1,50 +1,50 @@
     public static Document CxnSpToSVG(CxnSp cxnSp) {
     	
     	// Geometrical transforms
     	CTTransform2D xfrm = cxnSp.getSpPr().getXfrm();
     	Box b = new Box(xfrm.getOff().getX(), xfrm.getOff().getY(),
-    			xfrm.getExt().getCx(), xfrm.getExt().getCx() );
+    			xfrm.getExt().getCx(), xfrm.getExt().getCy() );
     	
     	if (xfrm.getRot()!=0) {
     		b.rotate(xfrm.getRot());
     	}
     	if (xfrm.isFlipH() ) {
     		b.flipH();
     	}
     	if (xfrm.isFlipV() ) {
     		b.flipV();
     	}
     	
     	// Convert from EMU to pixels
     	b.toPixels();
 
     	// Wrap in a div positioning it on the page
     	Document document = createDocument();
 		Element xhtmlDiv = document.createElement(""div"");
 		// Firefox needs the following; Chrome doesn't
 		xhtmlDiv.setAttribute(""style"", 
 				""position: absolute; width:100%; height:100%; left:0px; top:0px;"");		
 		Node n = document.appendChild(xhtmlDiv);
     	
     	// Convert the object itself to SVG
 		Svg svg = oFactory.createSvg();
     	Line line = oFactory.createLine();
     	svg.getSVGDescriptionClassOrSVGAnimationClassOrSVGStructureClass().add(line);
     	
     	line.setX1(b.getOffset().getXAsString() );
     	line.setY1(b.getOffset().getYAsString() );
     	
     	Point otherEnd = b.getOtherCorner();
     	
     	line.setX2( otherEnd.getXAsString() );
     	line.setY2( otherEnd.getYAsString() );
 
     	line.setStyle(""stroke:rgb(99,99,99)"");
     	// You can't see the line in Midori, unless you specify the color.
     	// width eg stroke-width:2 is optional
     	
     	Document d2 = XmlUtils.marshaltoW3CDomDocument(svg, jcSVG);   
     	XmlUtils.treeCopy(d2, n);
     	return document;
     	
     }",Buggy,"Fix typo bug.
",Buggy,"The commit message indicates a typo fix. The diff shows a change from `xfrm.getExt().getCx()` to `xfrm.getExt().getCy()` in the `Box` constructor. This suggests that the original code had a typo, using the `Cx` value for both width and height, instead of using `Cy` for the height. This correction directly addresses a bug caused by the typo. The answer is **Buggy**.
"
docx4j,5289.json,e0638645ae07b75593d031844af2c84f5b7a4960,"@@ -1,50 +1,50 @@
 	public OpcPackage get() throws Docx4JException {
 		
 		long startTime = System.currentTimeMillis();				
 
 		// 1. Get [Content_Types].xml
 		ContentTypeManager ctm = new ContentTypeManager();
 		try {
 			InputStream is = partStore.loadPart(""[Content_Types].xml"");		
 			ctm.parseContentTypesFile(is);
 		} catch (Docx4JException e) {
 			throw new Docx4JException(""Couldn't get [Content_Types].xml from ZipFile"", e);
 		} catch (NullPointerException e) {
 			throw new Docx4JException(""Couldn't get [Content_Types].xml from ZipFile"", e);
 		}
 		
 		// .. now find the name of the main part
 		String partName = ""_rels/.rels"";
 		RelationshipsPart rp = getRelationshipsPartFromZip(null, partName);
 		if (rp==null) {
 			throw new Docx4JException(""_rels/.rels appears to be missing from this package!"");
 		}
 		
 		String mainPartName = PackageRelsUtil.getNameOfMainPart(rp);
 		String pkgContentType = ctm.getContentType(new PartName(""/"" + mainPartName));
 
 		// 2. Create a new Package; this'll return the appropriate subclass
 		OpcPackage p = ctm.createPackage(pkgContentType);
 		log.info(""Instantiated package of type "" + p.getClass().getName() );
 		p.setPartStore(partStore);
 
 		p.setRelationships(rp);
 		rp.setSourceP(p); //
 		
 		// 5. Now recursively 
 //		(i) create new Parts for each thing listed
 //		in the relationships
 //		(ii) add the new Part to the package
 //		(iii) cross the PartName off unusedZipEntries
 		addPartsFromRelationships(p, rp, ctm );
 
 		// 6.
 		registerCustomXmlDataStorageParts(p);
 		
-		partStore.finishLoad();
+		partStore.finishSave();
 		
 		long endTime = System.currentTimeMillis();
 		log.info(""package read;  elapsed time: "" + Math.round((endTime-startTime)) + "" ms"" );
 		 
 		 return p;
 	}",Buggy,"Fix compilation errors in some classes
",Buggy,"The commit message indicates a fix for compilation errors. The diff shows a change from `partStore.finishLoad()` to `partStore.finishSave()`. This suggests that the original code had an incorrect method call, which would indeed cause a compilation error if `finishLoad()` was not the correct method to call in that context. Therefore, this change fixes a bug that prevented the code from compiling correctly. The answer is **Buggy**."
docx4j,5988.json,e43c7ce6875ed0292eea114d1f12f5b9bed3e549,"@@ -1,30 +1,31 @@
 	public static Part newPartForContentType(String contentType, String partName)
 	throws InvalidFormatException, PartUnrecognisedException {
 		
-		if (contentType.equals(ContentTypes.PRESENTATIONML_MAIN)) {
+		if (contentType.equals(ContentTypes.PRESENTATIONML_MAIN)
+				|| contentType.equals(ContentTypes.PRESENTATIONML_TEMPLATE) ) {
 			return new MainPresentationPart(new PartName(partName));
 		} else if (contentType.equals(ContentTypes.PRESENTATIONML_SLIDE)) {
 			return new SlidePart(new PartName(partName));
 		} else if (contentType.equals(ContentTypes.PRESENTATIONML_SLIDE_MASTER)) {
 			return new SlideMasterPart(new PartName(partName));
 		} else if (contentType.equals(ContentTypes.PRESENTATIONML_SLIDE_LAYOUT)) {
 			return new SlideLayoutPart(new PartName(partName));
 		} else if (contentType.equals(ContentTypes.PRESENTATIONML_TABLE_STYLES)) {
 			return new TableStylesPart(new PartName(partName));
 		} else if (contentType.equals(ContentTypes.PRESENTATIONML_PRES_PROPS)) {
 			return new PresentationPropertiesPart(new PartName(partName));
 		} else if (contentType.equals(ContentTypes.PRESENTATIONML_VIEW_PROPS)) {
 			return new ViewPropertiesPart(new PartName(partName));
 		} else if (contentType.equals(ContentTypes.PRESENTATIONML_TAGS)) {
 			return new TagsPart(new PartName(partName));
 		} else if (contentType.equals(ContentTypes.PRESENTATIONML_HANDOUT_MASTER)) {
 			return new HandoutMasterPart(new PartName(partName));
 		} else if (contentType.equals(ContentTypes.PRESENTATIONML_NOTES_MASTER)) {
 			return new NotesMasterPart(new PartName(partName));
 		} else if (contentType.equals(ContentTypes.PRESENTATIONML_NOTES_SLIDE)) {
 			return new NotesSlidePart(new PartName(partName));
 		} else {
 			throw new PartUnrecognisedException(""No subclass found for "" 
 					+ partName + "" (content type '"" + contentType + ""')"");					
 		}
 	}",Buggy,"Arnaud Kleinpeter's patch of 10 April, fixing error when trying to open a .potx file (powerpoint
template).
",Buggy,"The commit message explicitly states that the patch fixes an error when trying to open a `.potx` file, which is a PowerPoint template. The code change adds `ContentTypes.PRESENTATIONML_TEMPLATE` to the condition that creates a `MainPresentationPart`. This indicates that the original code did not correctly handle PowerPoint template files, leading to an error. The change fixes this by correctly identifying and handling `.potx` files. The answer is **Buggy**."
docx4j,6677.json,e7a3747180c1121e45343a31225052ae7dc68b01,"@@ -1,7 +1,6 @@
 	public Node toNode(AbstractWmlConversionContext context, Model model, TransformState state, Document doc) throws TransformerException {
 	HyperlinkModel hyperlinkModel = (HyperlinkModel)model;
 	Node ret = null;
 		ret = toNode(context, hyperlinkModel, doc);
-		XmlUtils.treeCopy(hyperlinkModel.getContent().getChildNodes(), ret);
 		return ret;
 	}",Buggy,"Fixed several problems converting docx to pdf

Fixes:
- removed treeCopy from AbstractHyperlinkWriter.java to prevent double
links creation;
- added creation of TblPr to prevent NPE and moving of P created for
<caption> tag in nestedTableHierarchyFix() method in XHTMLimporter.java.
So if one generates docx from html and then pdf from this docx
everything will work.
",Buggy,"The commit message indicates fixes for issues encountered when converting docx to pdf, specifically addressing double link creation and null pointer exceptions. The diff removes `XmlUtils.treeCopy` from `AbstractHyperlinkWriter.java` to prevent double link creation. This aligns directly with the commit message's description of fixing double link creation. Therefore, the changes indicate a bug fix. The answer is **Buggy**."
docx4j,5744.json,ec6c723c0edc1ad95aa484b4da21adf68c9e5f49,"@@ -1,27 +1,27 @@
 	public void registerInContentTypeManager() {
 		
 		ContentTypeManager ctm = this.getPackage().getContentTypeManager(); 
 		if (type.equals(AltChunkType.Xhtml) ) {
 			ctm.addDefaultContentType(""xhtml"", ""application/xhtml+xml"");
 		} else if (type.equals(AltChunkType.Mht) ) {
-			ctm.addDefaultContentType(""mht"", ""multipart/related"");
+			ctm.addDefaultContentType(""mht"", ""message/rfc822"");
 		} else if (type.equals(AltChunkType.Xml) ) {
 			ctm.addDefaultContentType(""xml"", ""application/xml"");
 		} else if (type.equals(AltChunkType.TextPlain) ) {
 			ctm.addDefaultContentType(""txt"", ""text/plain"");
 		} else if (type.equals(AltChunkType.WordprocessingML) ) { //Docx
 			// In case we're being added to a docm/dotx/dotm
 			ctm.addDefaultContentType(""docx"", ContentTypes.WORDPROCESSINGML_DOCUMENT);
 		} else if (type.equals(AltChunkType.OfficeWordMacroEnabled) ) {
 			ctm.addDefaultContentType(""docm"", ContentTypes.WORDPROCESSINGML_DOCUMENT_MACROENABLED);
 		} else if (type.equals(AltChunkType.OfficeWordTemplate) ) {
 			ctm.addDefaultContentType(""dotx"", ContentTypes.WORDPROCESSINGML_TEMPLATE);
 		} else if (type.equals(AltChunkType.OfficeWordMacroEnabledTemplate) ) {
 			ctm.addDefaultContentType(""dotm"", ContentTypes.WORDPROCESSINGML_TEMPLATE_MACROENABLED);
 		} else if (type.equals(AltChunkType.Rtf) ) {
 			ctm.addDefaultContentType(""rtf"", ""text/rtf"");
 		} else if (type.equals(AltChunkType.Html) ) {
 			ctm.addDefaultContentType(""html"", ""text/html"");
 		}
 		
 	}",Buggy,"Bug fix: Change content type for mht to ""message/rfc822"" (Zoltan Luspai)
",Buggy,"The commit message explicitly states ""Bug fix"". The code changes the content type for ""mht"" from ""multipart/related"" to ""message/rfc822"". This indicates a correction of an incorrect content type assignment, which is a bug fix. The answer is **Buggy**."
docx4j,6106.json,e89bdbce6981005eb9b5b3c2c5893d0f1561ac1f,"@@ -1,20 +1,25 @@
 	public static String getNameOfMainPart(RelationshipsPart packageRels) throws Docx4JException  {
 		
 		// find rel of type officeDocument
 		for (Relationship rel : packageRels.getRelationships().getRelationship() ) {
 			
 			if (rel.getType().equals(
 					""http://schemas.openxmlformats.org/officeDocument/2006/relationships/officeDocument"") ) {
 				return rel.getTarget();
 			} 
 			else if (rel.getType().equals(
 					""http://schemas.microsoft.com/office/2006/relationships/graphicFrameDoc"") ) {
 				// v:shape/@o:gfxdata
 				return rel.getTarget();
 			} 
+			else if (rel.getType().equals(
+					""http://schemas.openxmlformats.org/officeDocument/2006/relationships/diagramLayout"") ) {
+				// Glox
+				return rel.getTarget();
+			} 
 //			else {
 //				System.out.println(rel.getType());
 //			}
 		}
 		throw new Docx4JException(""No relationship of type officeDocument"");
 	}",Buggy,"Bug fix: make ParseGlox sample work.
",Buggy,"The commit message indicates a bug fix related to making the ParseGlox sample work. The diff adds a new condition to the `getNameOfMainPart` method to handle relationships of type ""http://schemas.openxmlformats.org/officeDocument/2006/relationships/diagramLayout"". This suggests that the original code was not correctly processing Glox files, and the added condition fixes this issue. Therefore, the changes indicate a bug fix. The answer is **Buggy**."
docx4j,5455.json,8d2fa40ea092e0c98a6e8648a80184c0dfe88ed7,"@@ -1,62 +1,68 @@
 	public void removePart(PartName partName) {
 		
 		log.info(""trying to removePart "" + partName.getName() );
 		
 		if (partName == null)
 			throw new IllegalArgumentException(""partName was null"");
 		
 		Part part = getPackage().getParts().get(partName);
 		
 		if (part!=null) {
 
 			// Remove the relationship for which it is a target from here
 			// Throw an error if this can't be found!
 			Relationship relToBeRemoved = null;
 //			for (Relationship rel : relationshipsByID.values() ) {
 			for (Relationship rel : relationships.getRelationship() ) {
+				
+				if (rel.getTargetMode() !=null
+						&& rel.getTargetMode().equals(""External"") ) {
+					// This method can't be used to remove external resources
+					continue;
+				}
 								
 				URI resolvedTargetURI = null;
 
 				try {
 					resolvedTargetURI = org.docx4j.openpackaging.URIHelper
 							.resolvePartUri(sourceP.partName.getURI(), new URI(
 									rel.getTarget()));
 				} catch (URISyntaxException e) {
 					log.error(""Cannot convert "" + rel.getTarget()
 							+ "" in a valid relationship URI-> ignored"", e);
 				}		
 
 				log.debug(""Comparing "" + resolvedTargetURI + "" == "" + partName.getName());
 				
 				if (partName.getName().equals(resolvedTargetURI.toString()) ) { // was rel.getTargetURI()
 					
 					log.info(""True - will delete relationship with target "" + rel.getTarget());
 					relToBeRemoved = rel; // Avoid java.util.ConcurrentModificationException
 					break;
 				}
 				
 			}
 			if (relToBeRemoved==null) {
 				// The Part may be in the package somewhere, but its not
 				// a target of this relationships part!
 				throw new IllegalArgumentException(partName + "" is not a target of "" + this.partName );
 			} else {
 				removeRelationship(relToBeRemoved);				
 			}
 						
 			// Remove parts it references
 			if (part.getRelationshipsPart()!=null) {
 				part.getRelationshipsPart().removeParts();
 				
 				// part.setRelationships(null);  // Unnecessary
 			}			
 
 			// Remove from Content Type Manager
 				// TODO			
 			
 			// Delete the specified part from the package.
 			getPackage().getParts().remove(partName);						
 		}
 
 //		this.isDirty = true;
 	}",Buggy,"Bug fix in removePart - ignore external resources.
",Buggy,"The commit message indicates a bug fix related to ignoring external resources in the `removePart` method. The code diff confirms this by adding a check for `rel.getTargetMode().equals(""External"")`. If the target mode is external, the code skips the relationship removal process. This prevents the method from attempting to remove external resources, which aligns with the commit message. The answer is **Buggy**."
docx4j,3230.json,71277942a6f5c967ca96c8454953e8c6697dffbb,"@@ -1,9 +1,14 @@
 	public void setXslFO(Element foElement) {
 		
 		if (((Color)this.getObject()).getVal()!=null ) {
-			foElement.setAttribute(FO_NAME, ""#"" + ((Color)this.getObject()).getVal());
+			if (((Color)this.getObject()).getVal().equals(""auto"")) {
+				// set it to black
+				foElement.setAttribute(FO_NAME, ""black"");				
+			} else {
+				foElement.setAttribute(FO_NAME, ""#"" + ((Color)this.getObject()).getVal());
+			}
 		} else {
 			//
 		}
 
 	}",Buggy,"bug fix: XSL FO doesn't like font color #auto
",Buggy,"The commit message indicates a bug fix related to how XSL FO handles the font color ""#auto"". The code diff shows that when the color value is ""auto"", the code now explicitly sets the font color to ""black"" instead of ""#auto"". This change directly addresses the issue described in the commit message, indicating a bug fix. The answer is **Buggy**."
docx4j,18108.json,b7d10c89810f8fb9ba39216b7630f82f493bfdee,"@@ -1,126 +1,125 @@
 	public static void main(String[] args) throws Docx4JException {
 		
 		// Input file
 		String inputfilepath = System.getProperty(""user.dir"") + ""/sample-docs/pptx/pptx-chart.pptx"";
 		
 		// The names of the parts which will be edited
 		// Alter these to match what is in your input pptx
 		// .. the chart
 		String chartPartName = ""/ppt/charts/chart1.xml"";
 		// .. the xlsx
 		String xlsPartName = ""/ppt/embeddings/Microsoft_Excel_Sheet1.xlsx"";
-//		String xlsPartName = ""/ppt/embeddings/Microsoft_Office_Excel_Worksheet1.xlsx"";
 		
 		// Output file
 		String outputfilepath = System.getProperty(""user.dir"") 
 				+ ""/OUT_EditEmbeddedCharts-"" 
 				+ System.currentTimeMillis() + "".pptx"";
 		
 		// Values to change
 		Random rand = new Random();
 
 		String firstValue  = String.valueOf(rand.nextInt(99));
 		String secondValue = String.valueOf(rand.nextInt(99));
 		
 		// Open the PPT template file
 		PresentationMLPackage ppt = (PresentationMLPackage) OpcPackage
 			.load(new java.io.File(inputfilepath));
 
 		/*
 		 * Get the Chart object and update the values. Afterwards, we'll update 
 		 * the associated spreadsheet so that the data is synchronized.
 		 */
 		Chart chart = (Chart) ppt.getParts().get(new PartName(chartPartName));
 		
 		List<Object> objects = chart.getJaxbElement().getChart().getPlotArea()
 				.getAreaChartOrArea3DChartOrLineChart();
 		
 		for (Object object : objects) {
 			
 			if (object instanceof CTBarChart) {
 
 				List<CTBarSer> ctBarSers = ((CTBarChart) object).getSer();
 				
 				for (CTBarSer ctBarSer : ctBarSers)
 				{
 					List<CTNumVal> ctNumVals = ctBarSer.getVal().getNumRef().getNumCache().getPt();
 					for (CTNumVal ctNumVal : ctNumVals)
 					{
 						System.out.println(""ctNumVal Val BEFORE: "" + ctNumVal.getV());
 						if (ctNumVal.getIdx() == 0) {
 							ctNumVal.setV(firstValue);
 						}
 						else if (ctNumVal.getIdx() == 1) {
 							ctNumVal.setV(secondValue);	
 						}
 						System.out.println(""ctNumVal Val AFTER: "" + ctNumVal.getV());
 					}
 				}
 			}
 		}
 				
 		/*
 		 * Get the spreadsheet and find the cell values that need to be updated
 		 */
 		
 		EmbeddedPackagePart epp  = (EmbeddedPackagePart) ppt
 			.getParts().get(new PartName(xlsPartName));
 		
 		if (epp==null) {
 			throw new Docx4JException(""Could find EmbeddedPackagePart: "" + xlsPartName);
 		}
 		
 		InputStream is = BufferUtil.newInputStream(epp.getBuffer());
 		
 		SpreadsheetMLPackage spreadSheet = (SpreadsheetMLPackage) SpreadsheetMLPackage.load(is);
 
 		Map<PartName,Part> partsMap = spreadSheet.getParts().getParts();		 
 		Iterator<Entry<PartName, Part>> it = partsMap.entrySet().iterator();
 
 		while(it.hasNext()) {
 			Map.Entry<PartName, Part> pairs = it.next();
 			
 			if (partsMap.get(pairs.getKey()) instanceof WorksheetPart) {
 				
 				WorksheetPart wsp = (WorksheetPart) partsMap.get(pairs.getKey()) ;
 				
 				List<Row> rows = wsp.getJaxbElement().getSheetData().getRow();
 
 				for (Row row : rows) {
 					List<Cell> cells = row.getC();
 					for (Cell cell : cells)
 					{
 						if (cell.getR().equals(""B2"") && cell.getV() != null) {
 							System.out.println(""B2 CELL VAL: "" + cell.getV());
 							// change the B2 cell value
+							cell.setT(STCellType.STR);
 							cell.setV(firstValue);
 						}
 						else if (cell.getR().equals(""B3"") && cell.getV() != null) {
 							System.out.println(""B3 CELL VAL: "" + cell.getV());
 							// Change the B3 cell value
-							cell.setV(secondValue); 
+							cell.setT(STCellType.STR);
+							cell.setV(secondValue);
 						}
 					}					
 				}
 			}
 		}
 
 		/*
 		 * Convert the Spreadsheet to a binary format, set it on the 
 		 * EmbeddedPackagePart, add it back onto the deck and save to a file.
 		 *  
 		 */		
 		ByteArrayOutputStream baos = new ByteArrayOutputStream();
 		
 		SaveToZipFile saver = new SaveToZipFile(spreadSheet);
 
 		saver.save(baos);
 		epp.setBinaryData(baos.toByteArray());
 
-		ppt.addTargetPart(epp);
-
 		// Write the new file to disk
 		ppt.save(new java.io.File(outputfilepath));
 
 		System.out.println(""\n\n done .. saved "" + outputfilepath);
 	}",Buggy,EditEmbeddedCharts - bug fixes,Buggy,"The commit message ""EditEmbeddedCharts - bug fixes"" indicates that the changes in the diff are intended to fix bugs. Looking at the diff, the code modifies an embedded chart in a PPTX file and updates the corresponding spreadsheet data. The key change is the addition of `cell.setT(STCellType.STR);` before setting the cell value in the spreadsheet. This line sets the cell type to String, which is crucial for ensuring that the numeric values are correctly interpreted as strings in the spreadsheet. Without this, the spreadsheet might misinterpret the values, leading to display errors or calculation issues. This correction directly addresses a potential bug related to data type handling in the spreadsheet. The answer is **Buggy**."
docx4j,5328.json,2b240431a940e8529960d06423db48c7122090ec,"@@ -1,67 +1,70 @@
 	public void addPartsFromRelationships(ZipOutputStream out,  RelationshipsPart rp )
 	 throws Docx4JException {
 		
 //		for (Iterator it = rp.iterator(); it.hasNext(); ) {
 //			Relationship r = (Relationship)it.next();
 //			log.info(""For Relationship Id="" + r.getId() + "" Source is "" + r.getSource().getPartName() + "", Target is "" + r.getTargetURI() );
 		for ( Relationship r : rp.getRelationships().getRelationship() ) {
 			
 			log.debug(""For Relationship Id="" + r.getId() 
 					+ "" Source is "" + rp.getSourceP().getPartName() 
 					+ "", Target is "" + r.getTarget() );
 			
-//			if (!r.getTargetMode().equals(TargetMode.INTERNAL) ) {
+			if (r.getType().equals(Namespaces.HYPERLINK)) {				
+				continue;  // whether internal or external								
+			}
+			
 			if (r.getTargetMode() != null
 					&& r.getTargetMode().equals(""External"") ) {
 				
 				// ie its EXTERNAL
 				// As at 1 May 2008, we don't have a Part for these;
 				// there is just the relationship.
 
 				log.warn(""Encountered external resource "" + r.getTarget() 
 						   + "" of type "" + r.getType() );
 				
 				// So
 				continue;				
 			}
 			
 			try {
 				//String resolvedPartUri = URIHelper.resolvePartUri(r.getSourceURI(), r.getTargetURI() ).toString();
 
 				String resolvedPartUri = URIHelper.resolvePartUri(rp.getSourceURI(), new URI(r.getTarget() ) ).toString();		
 				
 				// Now drop leading ""/'
 				resolvedPartUri = resolvedPartUri.substring(1);				
 				
 				// Now normalise it .. ie abc/def/../ghi
 				// becomes abc/ghi
 				// Maybe this isn't necessary with a zip file,
 				// - ZipFile class may be smart enough to do it.
 				// But it is certainly necessary in the JCR case.
 //				target = (new java.net.URI(target)).normalize().toString();
 //				log.info(""Normalised, it is "" + target );				
 				
 //				Document contents = getDocumentFromZippedPart( zf,  target);
 				
 				if (!false) {
 					log.debug(""Getting part /"" + resolvedPartUri );
 					
 					Part part = p.getParts().get(new PartName(""/"" + resolvedPartUri));
 					
 					if (part==null) {
 						log.error(""Part "" + resolvedPartUri + "" not found!"");
 					} else {
 						log.debug(part.getClass().getName() );
 					}
 					
 					savePart(out, part);
 					
 				}
 					
 			} catch (Exception e) {
 				throw new Docx4JException(""Failed to add parts from relationships"", e);				
 			}
 		}
 		
 		
 	}",Buggy,"Bug fix: save file containing internal hyperlink rel
",Buggy,"The commit message indicates a bug fix related to saving files containing internal hyperlink relationships. The code diff introduces a condition to skip hyperlink relationships (`r.getType().equals(Namespaces.HYPERLINK)`). This suggests that the previous code was incorrectly processing or saving hyperlink relationships, leading to a bug. The added `continue` statement effectively skips these relationships, which aligns with the commit message's intention to fix a bug related to hyperlinks. The answer is **Buggy**."
docx4j,6285.json,712217fb5f02ee12b18281bf0abe099e8e2cfe55,"@@ -1,32 +1,29 @@
 	private RelationshipsPart getRelationshipsPartFromXmlPackage(Base p, String partName) 
 			throws Docx4JException {
 		
-		RelationshipsPart thePart = null;
+		RelationshipsPart rp = null;
 		
 		try {
 			
 			org.docx4j.xmlPackage.Part part = parts.get(partName);
 			
 			if (part == null) {
 				return null;
 			}
 			
 			org.w3c.dom.Element el = part.getXmlData().getAny();
 			
-			RelationshipsPart rp = new RelationshipsPart(new PartName(partName) );
+			rp = new RelationshipsPart(new PartName(partName) );
 			// PartName already starts with a '/', so no need to add it
 			rp.setSourceP(p);
-			rp.unmarshal(el);
 			
-//			// Convert it to a Dom4J element
-//			thePart = new RelationshipsPart( p, new PartName( partName), convertW3CtoDom4J(el) );
-//				// PartName already starts with a '/', so no need to add it
-			
+			rp.setRelationships( (Relationships)rp.unmarshal(el) );
+						
 		} catch (Exception e) {
 			e.printStackTrace();
 			throw new Docx4JException(""Error getting document from XmlPackage:"" + partName, e);
 			
 		} 
 		
-		return thePart;
+		return rp;
 	}",Buggy,"bug fix in getRelationshipsPartFromXmlPackage
",Buggy,"The commit message indicates a bug fix in the `getRelationshipsPartFromXmlPackage` method. The code changes involve initializing `RelationshipsPart rp` to null, then assigning a new `RelationshipsPart` object to it within a try block. The original code had commented-out lines that suggest an attempt to convert the XML data to a Dom4J element, which is now replaced with unmarshalling the XML data directly into the `RelationshipsPart` object. The unmarshalled Relationships object is then set to the RelationshipsPart. The catch block includes exception handling. These changes suggest a correction in how the RelationshipsPart is created and populated from the XML package, likely addressing a bug related to incorrect or incomplete data loading. The answer is **Buggy**."
mockito,494.json,881c1d9cace557e64641d9b104c59b2b0c86ee4c,"@@ -1,8 +1,7 @@
     public static IMockitoConfiguration getConfig() {
         if (!initialized) {
-            //TODO check email of mockito group
             throw new IllegalStateException(""Something went wrong. GlobalConfiguration should be initialised by now.\n"" +
                 ""Please report issue at http://mockito.org or write an email to mockito@googlegroups.com"");
         }
         return globalConfiguration;
     }",Buggy,"removed TODOs,
fixed the minor problem of breaking the invocation string when no args given

--HG--
extra : convert_revision : svn%3Aaa2aecf3-ea3e-0410-9d70-716747e7c967/trunk%401161
",NotBuggy,"The commit message indicates the removal of TODOs and fixing a minor problem related to the invocation string when no arguments are given. The provided diff shows the removal of a TODO comment within the `getConfig()` method. While the commit message mentions fixing a problem with the invocation string, there is no code related to that in the diff. The removal of a TODO suggests a cleanup rather than a bug fix. The answer is **NotBuggy**."
mongo-java-driver,3587.json,e53bde1953e3e4d12809ce6c27c69a1eb8a4b8dd,"@@ -1,3 +1,4 @@
     public Binary readBinaryData(final String name) {
-        return readBinaryData(name);
+        verifyName(name);
+        return readBinaryData();
     }",Buggy,"fixed infinite recursion error: test forthcoming
",Buggy,"The commit message indicates a fix for an infinite recursion error. The diff shows that the `readBinaryData(String name)` method was recursively calling itself, leading to the infinite recursion. The fix replaces the recursive call with `verifyName(name)` and `readBinaryData()`. This resolves the infinite recursion issue. The answer is **Buggy**."
mongo-java-driver,2935.json,6fa5b4b1073a1df6d2a70d2fef360622bae6fbea,"@@ -1,14 +1,15 @@
     public void apply(final ClassModelBuilder<?> classModelBuilder) {
         for (PropertyModelBuilder<?> propertyModelBuilder : classModelBuilder.getPropertyModelBuilders()) {
             if (!(propertyModelBuilder.getPropertyAccessor() instanceof PropertyAccessorImpl)) {
                 throw new CodecConfigurationException(format(""The SET_PRIVATE_FIELDS_CONVENTION is not compatible with ""
                         + ""propertyModelBuilder instance that have custom implementations of org.bson.codecs.pojo.PropertyAccessor: %s"",
                         propertyModelBuilder.getPropertyAccessor().getClass().getName()));
             }
             PropertyAccessorImpl<?> defaultAccessor = (PropertyAccessorImpl<?>) propertyModelBuilder.getPropertyAccessor();
             PropertyMetadata<?> propertyMetaData = defaultAccessor.getPropertyMetadata();
-            if (!propertyMetaData.isDeserializable() && isPrivate(propertyMetaData.getField().getModifiers())) {
+            if (!propertyMetaData.isDeserializable() && propertyMetaData.getField() != null
+                    && isPrivate(propertyMetaData.getField().getModifiers())) {
                 setPropertyAccessor(propertyModelBuilder);
             }
         }
     }",Buggy,"Fix NPE error with ConventionSetPrivateField

JAVA-2951
",Buggy,"The commit message indicates a fix for a NullPointerException (NPE) error. The code diff adds a null check for `propertyMetaData.getField()` before accessing its modifiers. This prevents an NPE if the field is null, which aligns with the commit message's intent to fix an NPE. The answer is **Buggy**."
mongo-java-driver,3321.json,9d17beae8f95df45c855e8284e96a6ddbdca07b7,"@@ -1,26 +1,28 @@
     public static UUID decodeBinaryToUuid(final byte[] data, final byte type, final UuidRepresentation uuidRepresentation) {
         if (data.length != 16) {
             throw new BsonSerializationException(String.format(""Expected length to be 16, not %d."", data.length));
         }
 
         if (type == BsonBinarySubType.UUID_LEGACY.getValue()) {
             switch(uuidRepresentation) {
                 case C_SHARP_LEGACY:
                     reverseByteArray(data, 0, 4);
                     reverseByteArray(data, 4, 2);
                     reverseByteArray(data, 6, 2);
                     break;
                 case JAVA_LEGACY:
                     reverseByteArray(data, 0, 8);
                     reverseByteArray(data, 8, 8);
                     break;
                 case PYTHON_LEGACY:
-                case STANDARD:
                     break;
+                case STANDARD:
+                    throw new BSONException(""Can not decode a subtype 3 (UUID legacy) BSON binary when the decoder is configured to use "" +
+                            ""the standard UUID representation"");
                 default:
                     throw new BSONException(""Unexpected UUID representation"");
             }
         }
 
         return new UUID(readLongFromArrayBigEndian(data, 0), readLongFromArrayBigEndian(data, 8));
     }",Buggy,"Fix UUID decoder bug

The UUID decoder now does NOT allow decoding of a subtype 3 (legacy UUID) BSON Binary value
when the UUID decoder's UUID representation is STANDARD.
",Buggy,"The commit message indicates a bug fix in the UUID decoder. The code diff shows that when the UUID type is `UUID_LEGACY` and the representation is `STANDARD`, a `BSONException` is thrown. This prevents the decoding of legacy UUIDs when the decoder is configured for standard UUIDs, which aligns with the commit message's intention to disallow decoding of subtype 3 (legacy UUID) BSON Binary values when the UUID representation is STANDARD. This is a bug fix because it addresses an incorrect or undesirable behavior in the UUID decoder. The answer is **Buggy**."
mongo-java-driver,2112.json,381a79da3eab8c3d819826af40b0ae39dd08ae08,"@@ -1,3 +1,3 @@
-    public void pipe(BSONReader reader) {
+    public void pipe(final BSONReader reader) {
         pipeDocument(reader);
     }",Buggy,"Fixed checkstyle errors
",NotBuggy,"The commit message indicates a fix for checkstyle errors. The diff shows the addition of the `final` keyword to the `reader` parameter in the `pipe` method. This change doesn't alter the functionality of the code but rather addresses a style issue. Therefore, it's not a bug fix. The answer is **NotBuggy**.
"
mongo-java-driver,685.json,0a759943e1d5a8b971798db82e8a89330545e558,"@@ -1,8 +1,3 @@
     public GridFSInputFile createFile(final File file) throws IOException {
-        FileInputStream fileInputStream = new FileInputStream(file);
-        try {
-            return createFile(fileInputStream, file.getName(), true);
-        } finally {
-            fileInputStream.close();
-        }
+        return createFile(new FileInputStream(file), file.getName(), true);
     }",Buggy,"Fixed GridFS.createFile bug

The underlying call handles the closure of the stream correctly

JAVA-1813
",Buggy,"The commit message indicates a bug fix in `GridFS.createFile`. The code diff simplifies the `createFile` method by directly passing the `FileInputStream` to another `createFile` method, relying on the underlying call to handle the stream closure. The original code explicitly opened and closed the `FileInputStream` within a try-finally block. The change suggests that the original implementation might have been redundant or potentially problematic if the underlying call already handled the stream closure correctly. This indicates a bug fix related to resource management. The answer is **Buggy**.
"
junit5,2294.json,01eb2e4e3ba37aedc2c8f578fcc77a55ef305cd5,"@@ -1,3 +1,9 @@
 	private static String toString(Object obj) {
-		return (obj instanceof Class ? getCanonicalName((Class<?>) obj) : String.valueOf(obj));
+		if (obj instanceof Class) {
+			return getCanonicalName((Class<?>) obj);
+		}
+		if (obj instanceof Object[]) {
+			return Arrays.toString((Object[]) obj);
+		}
+		return StringUtils.nullSafeToString(obj);
 	}",Buggy,"Fix bug in AssertionUtils.toString(Object)

Prior to this commit, AssertionUtils.toString(Object) printed an array via
the array's toString() method which resulted in non-user-friendly output.

This commit addresses this issue by printing arrays using Arrays.toString(),
which produces human readable output.

This is a prerequisite for #961.

Issue: #1030
",Buggy,"The commit message indicates a bug fix in `AssertionUtils.toString(Object)` where arrays were not printed in a user-friendly format. The diff shows that the code now checks if the object is an array (`Object[]`) and, if so, uses `Arrays.toString()` to print it. This aligns with the commit message and fixes the bug of non-user-friendly array output. The answer is **Buggy**.
"
Essentials,1046.json,880ec1b3d953f1aef72ac925fe524799e4df8898,"@@ -1,13 +1,13 @@
 	protected void run(Server server, CommandSender sender, String commandLabel, String[] args) throws Exception
 	{
 		charge(sender);
 		sender.sendMessage(Util.format(""gcmax"", (Runtime.getRuntime().maxMemory() / 1024 / 1024)));
 		sender.sendMessage(Util.format(""gcmin"", (Runtime.getRuntime().freeMemory() / 1024 / 1024)));
 		for (World w : server.getWorlds())
 		{
 			sender.sendMessage(
 					(w.getEnvironment() == World.Environment.NETHER ? ""Nether"" : ""World"") + "" \"""" + w.getName() + ""\"": ""
 					+ w.getLoadedChunks().length + Util.i18n(""gcchunks"")
-					+ w.getEntities().size() + Util.i18n(""entities""));
+					+ w.getEntities().size() + Util.i18n(""gcentities""));
 		}
 	}",Buggy,"Fix translation error in gc

git-svn-id: https://svn.java.net/svn/essentials~svn/trunk@1465 e251c2fe-e539-e718-e476-b85c1f46cddb
",Buggy,"The commit message indicates a fix for a translation error. The diff shows a change from `Util.i18n(""entities"")` to `Util.i18n(""gcentities"")`. This change directly addresses the translation error mentioned in the commit message, suggesting that the original translation was incorrect. Therefore, this is a bug fix. The answer is **Buggy**."
Essentials,2593.json,4c78ab4f2a3eb4d598564069aa5a21e0380de29d,"@@ -1,28 +1,29 @@
 	private List<String> sort(List<String> permList) {
 		
 		List<String> result = new ArrayList<String>();
 		
 		for (String key : permList) {
 			String a = key.charAt(0) == '-'? key.substring(1):key;
 			Map<String, Boolean> allchildren = GroupManager.BukkitPermissions.getAllChildren(a, new HashSet<String>());
 			if (allchildren != null) {
 
 				ListIterator<String> itr = result.listIterator();
 				
 				while (itr.hasNext()){
 					String node = (String) itr.next();
 					String b = node.charAt(0) == '-'? node.substring(1):node;
 					
+					// Insert the parent node before the child
 					if (allchildren.containsKey(b)) {
-						itr.previous();
-						itr.add(key);
+						itr.set(key);
+						itr.add(node);
 						break;
 					}
 				}
 			}
 			if (!result.contains(key))
 				result.add(key);
 		}
 		
 		return result;
 	}",Buggy,fix for an iterator error if there is only one element in the array.,Buggy,"The commit message indicates a fix for an iterator error when the array has only one element. The code iterates through `result` and checks if any existing element `b` is a child of the current element `a`. If `b` is a child of `a`, the code attempts to insert `a` before `b`. The original code had `itr.previous()` followed by `itr.add(key)`. The corrected code replaces the current element `b` with `a` using `itr.set(key)` and then adds `b` after `a` using `itr.add(node)`. This change fixes a bug where the iterator might not handle the single-element case correctly, potentially leading to incorrect ordering or iterator errors. The answer is **Buggy**."
Essentials,1458.json,4823712f47bfb7f64e22b8633b05e3f6801919c5,"@@ -1,13 +1,16 @@
 		public void onEntityDamageByEntity(final EntityDamageByEntityEvent event)
 		{
 			if (event.getCause() != DamageCause.ENTITY_ATTACK || event.getEntity().getType() != EntityType.PLAYER)
 			{
 				return;
 			}
 			final Entity damager = event.getDamager();
-			final User user = ess.getUser(damager);
-			if (user.isJailed())
+			if (damager.getType() == EntityType.PLAYER)
 			{
-				event.setCancelled(true);
+				final User user = ess.getUser(damager);
+				if (user != null && user.isJailed())
+				{
+					event.setCancelled(true);
+				}
 			}
 		}",Buggy,"Fixing up NPE bug in jails (implemented in Dev2.9.163)
",Buggy,"The commit message indicates a fix for a NullPointerException (NPE) bug in jails. The code diff adds a null check for the `user` object before calling `user.isJailed()`. This prevents an NPE if `ess.getUser(damager)` returns null, which could happen if the damager is not a valid user. The added check `user != null` directly addresses the potential NPE, confirming the bug fix. The answer is **Buggy**."
Essentials,832.json,67b5b4e06b5a5952d86afe241001c0f5ba589ed4,"@@ -1,4 +1,5 @@
 	private boolean isAuthor(BookMeta bmeta, String player)
 	{
-		return bmeta.getAuthor().equalsIgnoreCase(player);
+		String author = bmeta.getAuthor();
+		return author != null && author.equalsIgnoreCase(player);
 	}",Buggy,"Fix minor /book bug (Null author)
",Buggy,The commit message indicates a fix for a bug related to a null author in the `/book` command. The code diff introduces a null check for `bmeta.getAuthor()` before comparing it with the player's name. This prevents a potential `NullPointerException` if the author is null. The answer is **Buggy**.
wicket,4568.json,89c5acbf41b98ab890ed489bbe15a5702da4595c,"@@ -1,4 +1,4 @@
 	public IModel<T> getModel()
 	{
-		return null;
+		return model;
 	}",Buggy,"fix small bug

git-svn-id: https://svn.apache.org/repos/asf/wicket/trunk@1159660 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix. The diff shows that the `getModel()` method was returning `null`, which is likely incorrect. The change replaces `return null;` with `return model;`, which suggests that the method should return the actual model. This correction aligns with the commit message indicating a bug fix. The answer is **Buggy**.
"
wicket,10114.json,f78f8eb142782d7931d91aec9a77037a8582f6cd,"@@ -1,5 +1,6 @@
 	public void renderHead(IHeaderResponse response)
 	{
 		super.renderHead(response);
-		response.render(CssHeaderItem.forReference(PageViewCSSResourceReference.get()));
+		response.render(
+			CssHeaderItem.forReference(new CssResourceReference(PageView.class, ""pageview.css"")));
 	}",Buggy,"WICKET-6737: fixed compilation error after deleting class
",Buggy,"The commit message indicates a fix for a compilation error caused by deleting a class. The diff replaces `PageViewCSSResourceReference.get()` with `new CssResourceReference(PageView.class, ""pageview.css"")`. This change suggests that `PageViewCSSResourceReference` was the deleted class, and the code is updated to use a direct `CssResourceReference` instantiation instead. This resolves the compilation error, indicating a bug fix. The answer is **Buggy**.
"
wicket,7162.json,8c62ff32ea564e1efb3b9fc84b75fe783508732e,"@@ -1,39 +1,41 @@
 	public boolean equalTo(final MarkupStream that)
 	{
 		// While a has more markup elements
 		while (this.hasMore())
 		{
 			// Get an element from each
-			final MarkupElement thisElement = this.next();
-			final MarkupElement thatElement = that.next();
+			final MarkupElement thisElement = this.get();
+			final MarkupElement thatElement = that.get();
 			
 			// and if the elements are not equal
 			if (thisElement != null && thatElement != null)
 			{
 				if (!thisElement.equalTo(thatElement))
 				{
 					// fail the comparison
 					return false;
 				}
 			}
 			else
 			{
 				// If one element is null,
 				if (!(thisElement == null && thatElement == null))
 				{
 					// fail the comparison
 					return false;
 				}
 			}
+			this.next();
+			that.next();
 		}
 
 		// If we've run out of markup elements in b
 		if (!that.hasMore())
 		{
 			// then the two streams match perfectly
 			return true;
 		}
 
 		// Stream b had extra elements
 		return false;
 	}",Buggy,"fixed error in DiffUtil (MarkupStream.equalsTo). It failed to compare markup streams with just one MarkupElement.

git-svn-id: https://svn.apache.org/repos/asf/incubator/wicket/trunk@552187 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix in `DiffUtil` related to comparing `MarkupStream` objects, specifically when one of the streams contains only one `MarkupElement`. The diff modifies the `equalTo` method to correctly handle this case. The original code appears to have an issue where it might not properly iterate and compare elements when one of the streams is short. The changes include using `this.get()` and `that.get()` to retrieve the current element without advancing the stream, and then explicitly calling `this.next()` and `that.next()` to move to the next element after the comparison. This ensures that the streams are advanced correctly, even if they have only one element. The added `this.next()` and `that.next()` calls fix the bug. The answer is **Buggy**."
wicket,2888.json,e72ed7e22a7d002070ffe4a1d3e7daf514351c20,"@@ -1,14 +1,19 @@
 	public Serializable getCacheKey()
 	{
 		Class<?> scope = getScope();
 		String currentStyle = getCurrentStyle();
 		Locale currentLocale = getCurrentLocale();
 		
+		 IResourceStream packageResource = Application.get()
+			.getResourceSettings()
+			.getResourceStreamLocator()
+			.locate(scope, absolutePath, currentStyle, variation, currentLocale, null, false);
+		 
 		// if resource stream can not be found do not cache
-		if (exists(scope, absolutePath, currentLocale, currentStyle, variation))
+		if (packageResource != null)
 		{
 			return new CacheKey(scopeName, absolutePath, currentLocale, currentStyle, variation);
 		}
 
 		return null;
 	}",Buggy,"WICKET-6061 improved getChacheKey to fix problems with absolute path of package resources
",Buggy,"The commit message indicates an improvement to `getCacheKey` to fix problems with absolute paths of package resources. The code changes involve checking if a resource stream can be located using `Application.get().getResourceSettings().getResourceStreamLocator().locate(...)`. If the resource stream is found (i.e., `packageResource != null`), a cache key is created and returned. Otherwise, `null` is returned, indicating that the resource should not be cached. This change likely addresses a bug where resources with absolute paths were not being cached correctly, or were causing issues when cached with incorrect keys. The original code used `exists(...)` which is now replaced with a direct resource stream lookup. This suggests a more robust way of determining if a resource can be cached. The answer is **Buggy**."
wicket,6423.json,835db0bf267a6c1b713f9801237c68cba043e9af,"@@ -1,5 +1,15 @@
 	public Border remove(final Component component)
 	{
-		getBodyContainer().remove(component);
+		if (component == body)
+		{
+			// when the user calls foo.add(getBodyContainer()) this method will be called with it to
+			// clear body container's old parent, in which case we do not want to redirect to body
+			// container but to border's old remove.
+			super.remove(body);
+		}
+		else
+		{
+			getBodyContainer().remove(component);
+		}
 		return this;
 	}",Buggy,"fixes a bug where border's getbodycontainer is left as a child of border even if it was added to another container
",Buggy,"The commit message indicates a bug fix related to the `Border` component's handling of its body container. The diff shows that the `remove` method now includes a check to see if the component being removed is the body container itself. If it is, the removal is handled by the superclass's `remove` method instead of the body container's `remove` method. This prevents the body container from being left as a child of the border when it has been added to another container. This aligns with the commit message's description of the bug. The answer is **Buggy**."
wicket,9455.json,7480012fafc537e405b6c0a624c1b76f39d56f41,"@@ -1,29 +1,29 @@
 	public static Number getMinValue(Class<? extends Number> numberType)
 	{
 		Number result;
 		if (Integer.class == numberType || int.class == numberType)
 		{
 			result = Integer.MIN_VALUE;
 		}
 		else if (Long.class == numberType || long.class == numberType) {
 			result = Long.MIN_VALUE;
 		}
 		else if (Float.class == numberType || float.class == numberType) {
 			result = Float.MIN_VALUE;
 		}
 		else if (Double.class == numberType || double.class == numberType) {
 			result = Double.MIN_VALUE;
 		}
 		else if (Byte.class == numberType || byte.class == numberType) {
 			result = Byte.MIN_VALUE;
 		}
 		else if (Short.class == numberType || short.class == numberType) {
 			result = Short.MIN_VALUE;
 		}
 		else { // null of any other Number
-			LOG.debug(""'{}' has no maximum value. Falling back to Double.MIN_VALUE."", numberType);
+			LOG.debug(""'{}' has no minimum value. Falling back to Double.MIN_VALUE."", numberType);
 			result = Double.MIN_VALUE;
 		}
 
 		return result;
 	}",Buggy,"Fix a copy/paste error
",Buggy,"The commit message indicates a fix for a copy/paste error. The diff changes the log message from ""'{}' has no maximum value"" to ""'{}' has no minimum value"". This correction aligns with the function's purpose of retrieving the minimum value for a given number type. The original log message was misleading due to the copy/paste error. The answer is **Buggy**."
vraptor4,95.json,d57d1be4185c180ec3a99dab938a0cfaec66516c,"@@ -1,33 +1,33 @@
 	public <T> T forwardTo(final Class<T> type) {
 		return proxifier.proxify(type, new MethodInvocation<T>() {
 
 			@Override
 			public Object intercept(T proxy, Method method, Object[] args, SuperMethod superMethod) {
 				try {
 					logger.debug(""Executing {}"", method);
 					ControllerMethod old = methodInfo.getControllerMethod();
 					methodInfo.setControllerMethod(DefaultControllerMethod.instanceFor(type, method));
 					Object methodResult = method.invoke(container.instanceFor(type), args);
 					methodInfo.setControllerMethod(old);
 
 					Type returnType = method.getGenericReturnType();
 					if (!(returnType == void.class)) {
 						request.setAttribute(extractor.nameFor(returnType), methodResult);
 					}
-					if (response.isCommitted() || result.used()) {
+					if (response.isCommitted()) {
 						logger.debug(""Response already commited, not forwarding."");
 						return null;
 					}
 					String path = resolver.pathFor(DefaultControllerMethod.instanceFor(type, method));
 					logger.debug(""Forwarding to {}"", path);
 					request.getRequestDispatcher(path).forward(request, response);
 					return null;
 				} catch (InvocationTargetException e) {
 					propagateIfPossible(e.getCause());
 					throw new ProxyInvocationException(e);
 				} catch (Exception e) {
 					throw new ProxyInvocationException(e);
 				}
 			}
 		});
 	}",Buggy,"Fixing problem with forward.
",Buggy,"The commit message ""Fixing problem with forward"" suggests a bug fix related to forwarding functionality. The code diff modifies the `forwardTo` method, specifically addressing a condition where the response is already committed. The original code had a condition `response.isCommitted() || result.used()`, which seems to be checking if the response is committed or if some `result` object has been used. The modified code removes the `result.used()` check, leaving only `response.isCommitted()`. This suggests that the `result.used()` condition was causing incorrect behavior, potentially preventing forwarding when it should have occurred. The removal of this condition likely resolves a bug where forwarding was being incorrectly skipped. Therefore, the changes indicate a bug fix. The answer is **Buggy**."
vraptor4,1079.json,cff51ec6e04f0bcb011c05510d72b8c4c22b1b66,"@@ -1,4 +1,13 @@
 	public <T extends View> T use(Class<T> view) {
+		if(view.isAssignableFrom(Results.json()) && validator.hasErrors()) {
+	        throw new IllegalStateException(
+	                ""There are validation errors and you forgot to specify where to go. Please add in your method ""
+	                        + ""something like:\n""
+	                        + ""validator.onErrorUse(page()).of(AnyController.class).anyMethod();\n""
+	                        + ""or any view that you like.\n""
+	                        + ""If you didn't add any validation error, it is possible that a conversion error had happened."");
+	    }
+		
 		responseCommitted = true;
 		return container.instanceFor(view);
 	}",Buggy,"Fixing problem: validation ignored on redirects.
",Buggy,"The commit message indicates a fix related to validation being ignored on redirects. The code diff introduces a check for validation errors specifically when the view being used is JSON. If validation errors exist, it throws an IllegalStateException, providing guidance on how to handle the errors. This change ensures that validation errors are properly handled, which directly addresses the issue described in the commit message. The answer is **Buggy**."
flink,4079.json,1863bb88e4f2388a2f91b8000bcb3ca1e44d352b,"@@ -1,9 +1,8 @@
 	static SupplierWithException<BufferPool, IOException> createBufferPoolFactory(
 			BufferPoolFactory bufferPoolFactory,
 			int networkBuffersPerChannel,
 			int floatingNetworkBuffersPerGate,
 			int size,
 			ResultPartitionType type) {
-		int maxNumberOfMemorySegments = type.isBounded() ? floatingNetworkBuffersPerGate : Integer.MAX_VALUE;
-		return () -> bufferPoolFactory.createBufferPool(0, maxNumberOfMemorySegments);
+		return () -> bufferPoolFactory.createBufferPool(0, floatingNetworkBuffersPerGate);
 	}",Buggy,"[FLINK-14872][runtime] Temporary fix for potential deadlock problem when tasks read from blocking ResultPartitions. (#10472)

This commit implements a temporary fix for the potential deadlock problem reported in FLINK-14872. The problem itself is not solved completely, however the possibility of deadlock is largely reduced. We leave the proper fix of this problem to the future version.",Buggy,"The commit message indicates a temporary fix for a potential deadlock problem. The code change modifies the `createBufferPoolFactory` method, specifically how the maximum number of memory segments is determined. The original code used `Integer.MAX_VALUE` when the `ResultPartitionType` was not bounded, potentially leading to resource exhaustion and deadlocks. The modified code removes this condition and always uses `floatingNetworkBuffersPerGate` for the maximum number of memory segments. This change directly addresses the potential deadlock issue by limiting the number of memory segments, aligning with the commit message's intention to reduce the possibility of deadlock. The answer is **Buggy**."
flink,18562.json,d8c74d20970e6cf379165ffafe8550a5b98d7c4d,"@@ -1,16 +1,19 @@
 		public boolean equals(final Object obj) {
+			if (obj == this) {
+				return true;
+			}
 
 			if (obj instanceof FSKey) {
 				final FSKey key = (FSKey) obj;
 
 				if (!this.scheme.equals(key.scheme)) {
 					return false;
 				}
 
 				if ((this.authority == null) || (key.authority == null)) {
 					return this.authority == null && key.authority == null;
 				}
 				return this.authority.equals(key.authority);
 			}
 			return false;
 		}",Buggy,"[FLINK-1766]Fix the bug of equals function of FSKey

The equals function in org.apache.flink.core.fs.FileSystem.FSKey should first confirm whether obj == this, if obj is the same object.It should return true

Author: hongsibao <hongsibao@huawei.com>

Closes #511 from matadorhong/Bug_Flink_1776 and squashes the following commits:

1ad06d7 [hongsibao] Fix the code format problem
431fc4b [hongsibao] Fix the code format problem
ab0ae5e [hongsibao] [FLINK-1766]Fix the bug of equals function of FSKey
",Buggy,"The commit message indicates a bug fix in the `equals` function of `FSKey`. The diff adds a check at the beginning of the `equals` method to determine if the input object `obj` is the same as the current object (`this`). If they are the same, the method immediately returns `true`. This is a standard optimization and correctness check for `equals` methods, preventing unnecessary comparisons and ensuring that an object is considered equal to itself. This change directly addresses the bug described in the commit message. The answer is **Buggy**.
"
flink,2817.json,cbfb807d65b68b2b6157e1b1d42606123ea499ad,"@@ -1,8 +1,7 @@
 	public void stop() throws Exception {
 		LOG.info(""Stopping ZooKeeperLeaderRetrievalService."");
 
 		client.getConnectionStateListenable().removeListener(connectionStateListener);
 
 		cache.close();
-		client.close();
 	}",Buggy,"[FLINK-5190] [runtime] fix ZooKeeperLeaderRetrievalService close the zk client when stopping bug
",Buggy,"The commit message indicates a bug fix related to closing the ZooKeeper client in `ZooKeeperLeaderRetrievalService`. The diff shows that the `client.close()` call has been removed from the `stop()` method. This suggests that the original code was incorrectly closing the ZooKeeper client during the stopping of the service, potentially leading to issues if other components were still relying on the client connection. Removing the `client.close()` call likely resolves this bug by preventing premature closure of the client. The answer is **Buggy**."
flink,43722.json,f0ed58c6a580db0966104b81491d08d25d1ff57e,"@@ -1,32 +1,33 @@
 		public void reduce(Iterable<T> records, Collector<T> out) {
 			final AggregationFunction<Object>[] aggFunctions = this.aggFunctions;
 			final int[] fieldPositions = this.fieldPositions;
 
 			// aggregators are initialized from before
 
 			T current = null;
 			final Iterator<T> values = records.iterator();
 			while (values.hasNext()) {
 				current = values.next();
 
 				for (int i = 0; i < fieldPositions.length; i++) {
 					Object val = current.productElement(fieldPositions[i]);
 					aggFunctions[i].aggregate(val);
 				}
 			}
 
 			Object[] fields = new Object[serializer.getArity()];
+			int length = serializer.getArity();
 			// First copy all tuple fields, then overwrite the aggregated ones
-			for (int i = 0; i < fieldPositions.length; i++) {
-				fields[0] = current.productElement(i);
+			for (int i = 0; i < length; i++) {
+				fields[i] = current.productElement(i);
 			}
 			for (int i = 0; i < fieldPositions.length; i++) {
 				Object aggVal = aggFunctions[i].getAggregate();
 				fields[fieldPositions[i]] = aggVal;
 				aggFunctions[i].initializeAggregate();
 			}
 
 			T result = serializer.createInstance(fields);
 
 			out.collect(result);
 		}",Buggy,"Fix Bug in ScalaAggregate Operator and add ITCase
",Buggy,"The commit message indicates a bug fix in the ScalaAggregate operator. The code diff modifies the `reduce` method within the operator.

The original code has a loop `for (int i = 0; i < fieldPositions.length; i++)` where it copies tuple fields into the `fields` array, but it accesses `current.productElement(i)`. This is incorrect because it should iterate through *all* fields of the tuple, not just the aggregated ones. The corrected code introduces `length = serializer.getArity()` and changes the loop to `for (int i = 0; i < length; i++)`, which iterates through all fields. This fixes a bug where non-aggregated fields were not being correctly copied into the result tuple.

The answer is **Buggy**.
"
flink,25849.json,0081fb2ef2bd03d06a786dd8988865d2ff6168c2,"@@ -1,5 +1,5 @@
 		public void combine(Iterable<Tuple3<K1, K2, IN>> values, Collector<Tuple3<K1, K2, IN>> out) throws Exception {
 			iter.set(values.iterator());
 			coll.set(out);
-			this.wrappedFunction.combine(iter, coll);
+			((GroupCombineFunction)this.wrappedFunction).combine(iter, coll);
 		}",Buggy,"[FLINK-2135] Fix faulty cast to GroupReduceFunction

This closes #769
",Buggy,"The commit message indicates a fix for a faulty cast to `GroupReduceFunction`. The diff shows a change where `this.wrappedFunction.combine(iter, coll)` is cast to `((GroupCombineFunction)this.wrappedFunction).combine(iter, coll)`. This explicit cast suggests that the original code was incorrectly assuming the type of `wrappedFunction`, leading to potential runtime errors or incorrect behavior. The cast is likely correcting a type mismatch, which is a bug fix. The answer is **Buggy**.
"
flink,17423.json,6624562982c9d57bebba8cb4b574b8ed28640a0d,"@@ -1,15 +1,17 @@
 	public static DeweyNumber fromString(final String deweyNumberString) {
 		String[] splits = deweyNumberString.split(""\\."");
 
-		if (splits.length == 0) {
+		if (splits.length == 1) {
 			return new DeweyNumber(Integer.parseInt(deweyNumberString));
-		} else {
+		} else if (splits.length > 0) {
 			int[] deweyNumber = new int[splits.length];
 
 			for (int i = 0; i < splits.length; i++) {
 				deweyNumber[i] = Integer.parseInt(splits[i]);
 			}
 
 			return new DeweyNumber(deweyNumber);
+		} else {
+			throw new IllegalArgumentException(""Failed to parse "" + deweyNumberString + "" as a Dewey number"");
 		}
 	}",Buggy,"[FLINK-13043] [Library / CEP] Fix the bug of parsing Dewey number from string

This closes #8936
",Buggy,"The commit message indicates a bug fix related to parsing Dewey numbers from strings. The code changes modify the logic for handling different lengths of the `splits` array after splitting the input string by ""."".

- The original code had a condition `if (splits.length == 0)`, which would only be true if the input string was empty. In this case, the code would proceed to `else` block, which would cause `ArrayIndexOutOfBoundsException` because `splits` would be an empty array.
- The modified code changes the condition to `if (splits.length == 1)`. This handles the case where the input string contains only one integer (e.g., ""123"").
- The `else` block is changed to `else if (splits.length > 0)` to handle cases where the input string has multiple parts separated by dots.
- An additional `else` block is added to throw an `IllegalArgumentException` if `splits.length` is not greater than 0 or equal to 1. This handles the case where the input string is empty or contains invalid characters.

These changes address a potential `ArrayIndexOutOfBoundsException` and improve the handling of different input formats, indicating a bug fix. The answer is **Buggy**."
flink,18090.json,624cb64a462765419bb0f81e2a50b3cec1c877f6,"@@ -1,25 +1,22 @@
 	public void copyNormalizedKey(MemorySegment target, int offset, int len) {
+		// note that the char is an unsigned data type in java and consequently needs
+		// no code that transforms the signed representation to an offsetted representation
+		// that is equivalent to unsigned, when compared byte by byte
 		if (len == 2) {
 			// default case, full normalized key
-			int highByte = ((value >>> 8) & 0xff);
-			highByte -= Byte.MIN_VALUE;
-			target.put(offset, (byte) highByte);
-			target.put(offset + 1, (byte) ((value) & 0xff));
+			target.put(offset,     (byte) ((value >>> 8) & 0xff));
+			target.put(offset + 1, (byte) ((value      ) & 0xff));
 		}
 		else if (len <= 0) {
 		}
 		else if (len == 1) {
-			int highByte = ((value >>> 8) & 0xff);
-			highByte -= Byte.MIN_VALUE;
-			target.put(offset, (byte) highByte);
+			target.put(offset,     (byte) ((value >>> 8) & 0xff));
 		}
 		else {
-			int highByte = ((value >>> 8) & 0xff);
-			highByte -= Byte.MIN_VALUE;
-			target.put(offset, (byte) highByte);
-			target.put(offset + 1, (byte) ((value) & 0xff));
+			target.put(offset,     (byte) ((value >>> 8) & 0xff));
+			target.put(offset + 1, (byte) ((value      ) & 0xff));
 			for (int i = 2; i < len; i++) {
 				target.put(offset + i, (byte) 0);
 			}
 		}
 	}",Buggy,"Fixed erroneous normalized key generation in PactCharacter.
",Buggy,"The commit message indicates a fix for an erroneous normalized key generation in `PactCharacter`. The diff removes the subtraction of `Byte.MIN_VALUE` from the high byte. The original code was attempting to treat the `char` as signed and offset it to an unsigned representation, but since `char` is already unsigned in Java, this operation was incorrect and led to erroneous key generation. Removing this offset corrects the key generation logic. The answer is **Buggy**."
flink,18559.json,f04b32b02900181db82c6c518ffbd6430a0a369a,"@@ -1,40 +1,40 @@
 	private static HashMap<String, FileSystemFactory> loadFileSystems() {
 		final HashMap<String, FileSystemFactory> map = new HashMap<>();
 
 		// by default, we always have the local file system factory
 		map.put(""file"", new LocalFileSystemFactory());
 
 		LOG.debug(""Loading extension file systems via services"");
 
 		try {
 			ServiceLoader<FileSystemFactory> serviceLoader = ServiceLoader.load(FileSystemFactory.class);
 			Iterator<FileSystemFactory> iter = serviceLoader.iterator();
 
 			// we explicitly use an iterator here (rather than for-each) because that way
 			// we can catch errors in individual service instantiations
 
 			//noinspection WhileLoopReplaceableByForEach
 			while (iter.hasNext()) {
 				try {
 					FileSystemFactory factory = iter.next();
 					String scheme = factory.getScheme();
 					map.put(scheme, factory);
 					LOG.debug(""Added file system {}:{}"", scheme, factory.getClass().getName());
 				}
 				catch (Throwable t) {
 					// catching Throwable here to handle various forms of class loading
 					// and initialization errors
 					ExceptionUtils.rethrowIfFatalErrorOrOOM(t);
-					LOG.error(""Failed to load a file systems via services"", t);
+					LOG.error(""Failed to load a file system via services"", t);
 				}
 			}
 		}
 		catch (Throwable t) {
 			// catching Throwable here to handle various forms of class loading
 			// and initialization errors
 			ExceptionUtils.rethrowIfFatalErrorOrOOM(t);
 			LOG.error(""Failed to load additional file systems via services"", t);
 		}
 
 		return map;
 	}",Buggy,"[hotfix] [core] Fix lots of checkstyle errors in core.fs
",Buggy,"The commit message indicates a hotfix addressing checkstyle errors. However, the diff reveals changes to the logging message within the catch block from ""Failed to load a file systems via services"" to ""Failed to load a file system via services"". This correction suggests a bug related to the accuracy of the logging message, indicating a bug fix. The answer is **Buggy**."
flink,11393.json,84b39dcb50167368d45882a88842760237fa3560,"@@ -1,19 +1,23 @@
 	public static String getUserRunning() {
 		try {
 			return UserGroupInformation.getCurrentUser().getShortUserName();
 		}
+		catch (LinkageError e) {
+			// hadoop classes are not in the classpath
+			LOG.debug(""Cannot determine user/group information using Hadoop utils. "" +
+					""Hadoop classes not loaded or compatible"", e);
+		}
 		catch (Throwable t) {
-			if (LOG.isDebugEnabled() && !(t instanceof ClassNotFoundException)) {
-				LOG.debug(""Cannot determine user/group information using Hadoop utils."", t);
-			}
+			// some other error occurred that we should log and make known
+			LOG.warn(""Error while accessing user/group information via Hadoop utils."", t);
 		}
 		
 		String user = System.getProperty(""user.name"");
 		if (user == null) {
 			user = UNKNOWN;
 			if (LOG.isDebugEnabled()) {
 				LOG.debug(""Cannot determine user/group information for the current user."");
 			}
 		}
 		return user;
 	}",Buggy,"[hotfix] Fix error messages in EnvironmentInformation when accessing user information
",Buggy,"The commit message indicates a fix to error messages in EnvironmentInformation related to user information access. The code diff shows changes in the `getUserRunning()` method. Specifically, it adds a `catch` block for `LinkageError`, which is likely to occur when Hadoop classes are not in the classpath. The original `catch` block for `Throwable` is modified to log errors at the `WARN` level instead of `DEBUG`, and the condition to ignore `ClassNotFoundException` is removed. These changes suggest that the original error handling was insufficient, leading to either unhandled exceptions or misleading log messages. The updated code provides more robust error handling and clearer logging, which aligns with the commit message's intention to fix error messages. The answer is **Buggy**."
flink,11393.json,e45534cf2275f6918294c81de5377146c243904c,"@@ -1,12 +1,19 @@
 	public static String getUserRunning() {
 		try {
 			return UserGroupInformation.getCurrentUser().getShortUserName();
-		} catch (Throwable t) {
-			if (LOG.isDebugEnabled()) {
-				LOG.debug(""Cannot determine user/group information for the current user."", t);
-			} else {
-				LOG.info(""Cannot determine user/group information for the current user."");
-			}
-			return UNKNOWN;
 		}
+		catch (Throwable t) {
+			if (LOG.isDebugEnabled() && !(t instanceof ClassNotFoundException)) {
+				LOG.debug(""Cannot determine user/group information using Hadoop utils."", t);
+			}
+		}
+		
+		String user = System.getProperty(""user.name"");
+		if (user == null) {
+			user = UNKNOWN;
+			if (LOG.isDebugEnabled()) {
+				LOG.debug(""Cannot determine user/group information for the current user."");
+			}
+		}
+		return user;
 	}",Buggy,"Fix problems with missing files/dependencies with cdh-4 distribution.
",Buggy,"The original code attempts to get the user's short name using `UserGroupInformation.getCurrentUser().getShortUserName()`. If this fails (due to a `Throwable`), it logs a message and returns `UNKNOWN`. The modified code now catches the `Throwable` but only logs the exception if it's not a `ClassNotFoundException`. It then attempts to get the username from the `user.name` system property. If this also fails, it logs a message and returns `UNKNOWN`.

The commit message indicates problems with missing files/dependencies, which could lead to `ClassNotFoundException`. The code change addresses this by attempting to retrieve the username using an alternative method (system property) if the Hadoop-related method fails, specifically avoiding logging a debug message if the exception is a `ClassNotFoundException`. This suggests that the original code was failing in certain environments due to missing Hadoop dependencies, and the fix provides a fallback mechanism. This is a bug fix.

**Buggy**
"
flink,3887.json,4d693c4fbc5e6f3ff34ccb3cb3a1d9f35d6bbd76,"@@ -1,25 +1,27 @@
 	private void writeAndCloseBufferConsumer(BufferConsumer bufferConsumer) throws IOException {
 		try {
 			final Buffer buffer = bufferConsumer.build();
 			try {
 				if (canBeCompressed(buffer)) {
 					final Buffer compressedBuffer = parent.bufferCompressor.compressToIntermediateBuffer(buffer);
 					data.writeBuffer(compressedBuffer);
-					compressedBuffer.recycleBuffer();
+					if (compressedBuffer != buffer) {
+						compressedBuffer.recycleBuffer();
+					}
 				} else {
 					data.writeBuffer(buffer);
 				}
 
 				numBuffersAndEventsWritten++;
 				if (buffer.isBuffer()) {
 					numDataBuffersWritten++;
 				}
 			}
 			finally {
 				buffer.recycleBuffer();
 			}
 		}
 		finally {
 			bufferConsumer.close();
 		}
 	}",Buggy,"[FLINK-15166][runtime] Fix the bug of wrongly recycling uncompressed buffer

For blocking shuffle data compression, the compressed intermediate buffer is recycled after it is written out. However when the data can not be compressed, the returned buffer is the original buffer which should not be recycled.

This commit fixes the bug of wrongly recycling uncompressed buffer by comparing the returned buffer with the original buffer.",Buggy,"The commit message indicates a bug where an uncompressed buffer was being incorrectly recycled after being written. The code diff shows that the `compressedBuffer` is only recycled if it is different from the original `buffer`. This check prevents the original, uncompressed buffer from being recycled when compression is not applied. This aligns with the commit message's description of the bug fix. The answer is **Buggy**."
flink,18028.json,0f28095f9f79b49d45905e61709ff4fab6fedd54,"@@ -1,8 +1,8 @@
 	public Appendable append(CharSequence csq, int start, int end) {
 		final int otherLen = end - start;
 		grow(this.len + otherLen);
-		for (int pos = start; pos < len; pos++)
+		for (int pos = start; pos < end; pos++)
 			this.value[this.len + pos] = csq.charAt(pos);
 		this.len += otherLen;
 		return this;
 	}",Buggy,"Fixed bug in PactString append.
",Buggy,"The commit message indicates a bug fix in the `PactString append` method. The code diff shows a change in the loop condition of the `append` method. The original code had `pos < len`, which would iterate up to the current length of the `PactString` instead of iterating through the provided `CharSequence` from `start` to `end`. This would lead to incorrect characters being appended and potentially an `ArrayIndexOutOfBoundsException`. The corrected code `pos < end` fixes this bug by iterating through the correct range of the input `CharSequence`. The answer is **Buggy**.
"
flink,40862.json,6cf15c29b1bf3bb3ddcf69157079f9e5b185d9c8,"@@ -1,5 +1,4 @@
 	public SingleOutputStreamOperator<OUT, ?> reduce(ReduceFunction<OUT> reducer) {
 		return dataStream.addFunction(""groupReduce"", reducer, getTypeWrapper(reducer),
-				getTypeWrapper(reducer), new GroupReduceInvokable<OUT>(reducer, keyPosition))
-				.partitionBy(keyPosition);
+				getTypeWrapper(reducer), new GroupReduceInvokable<OUT>(reducer, keyPosition));
 	}",Buggy,"[streaming] Minor bug and license header fixes
",Buggy,"The commit message indicates a minor bug fix. The diff removes `.partitionBy(keyPosition)` from the return statement. This suggests that the partitioning was either unnecessary or causing an issue, thus indicating a bug fix. The answer is **Buggy**."
flink,18045.json,6e9b2848d5fabace5c6ef491c87c562eed9b5f43,"@@ -1,27 +1,26 @@
 	public void copy(DataInputView in, DataOutputView target) throws IOException {
 		int len = in.readUnsignedByte();
 		target.writeByte(len);
 
 		if (len >= HIGH_BIT) {
 			int shift = 7;
 			int curr;
 			len = len & 0x7f;
 			while ((curr = in.readUnsignedByte()) >= HIGH_BIT) {
 				len |= (curr & 0x7f) << shift;
 				shift += 7;
 				target.writeByte(curr);
 			}
 			len |= curr << shift;
+			target.writeByte(curr);
 		}
 
 		for (int i = 0; i < len; i++) {
 			int c = in.readUnsignedByte();
 			target.writeByte(c);
-			if (c >= HIGH_BIT) {
-				int curr;
-				while ((curr = in.readUnsignedByte()) >= HIGH_BIT) {
-					target.writeByte(curr);
-				}
+			while (c >= HIGH_BIT) {
+				c = in.readUnsignedByte();
+				target.writeByte(c);
 			}
 		}
 	}",Buggy,"[FLINK-1336] [core] Fix bug in StringValue binary copy method
",Buggy,"The commit message indicates a bug fix in the `StringValue` binary copy method. The diff shows changes in the `copy` method, specifically in how it reads and writes data from the `DataInputView` to the `DataOutputView`. The original code had a potential issue where it might not correctly copy all bytes when dealing with characters greater than or equal to `HIGH_BIT`. The corrected code ensures that all bytes are properly copied, resolving the bug. The answer is **Buggy**."
flink,26584.json,32440227eceb122008906878136156ffa839efa6,"@@ -1,3 +1,4 @@
 	public String toString() {
-		return ""Local Environment (DOP = "" + (getDegreeOfParallelism() == -1 ? ""default"" : getDegreeOfParallelism()) + "") : "" + getIdString();
+		return ""Local Environment (DOP = "" + (getDegreeOfParallelism() == -1 ? ""default"" : getDegreeOfParallelism())
+				+ ""Number task manager = "" + getNumTaskManager() + "") : "" + getIdString();
 	}",Buggy,"Fixed free memory calculation bug in case where multiple task manager run on the same jvm.
",NotBuggy,"The commit message indicates a fix for a bug in free memory calculation when multiple task managers run on the same JVM. The provided diff modifies the `toString()` method of a `LocalEnvironment` class to include the number of task managers. While this change provides more information in the string representation of the environment, it doesn't directly address the free memory calculation bug mentioned in the commit message. The diff doesn't show any changes related to memory calculation or resource management. Therefore, based on the provided diff, it's difficult to confirm that the bug related to free memory calculation has been fixed. The answer is **NotBuggy**."
flink,19363.json,7e10e2b7ba967332740e1058efa7310fe2834123,"@@ -1,22 +1,24 @@
 	public String toString()
 	{
 		if (this.indexes.size() == 0) {
 			return ""(none)"";
 		}
 		final StringBuffer buf = new StringBuffer();
 		for (int i = 0; i < indexes.size(); i++) {
 			if (buf.length() == 0) {
 				buf.append(""["");
 			}
 			else {
 				buf.append("","");
 			}
 			buf.append(this.indexes.get(i));
-			buf.append("":"");
-			buf.append(this.types.get(i).getName());
+			if (this.types.get(i) != null) {
+				buf.append("":"");
+				buf.append(this.types.get(i).getName());
+			}
 			buf.append("":"");
 			buf.append(this.orders.get(i).name());
 		}
 		buf.append(""]"");
 		return buf.toString();
 	}",Buggy,"Fixed miscellaneous minor bugs in compiler.
",Buggy,"The commit message indicates that minor bugs in the compiler have been fixed. The diff modifies the `toString()` method. Specifically, it adds a null check for `this.types.get(i)` before calling `getName()`. This prevents a potential `NullPointerException` if the type at index `i` is null. This indicates a bug fix. The answer is **Buggy**."
flink,4441.json,81a7837a942668c23c795e5bd8a68c4d17009f85,"@@ -1,32 +1,35 @@
 	public void close() throws IOException {
 		// atomically set the close flag
 		synchronized (this.closeLock) {
 			if (this.closed) {
 				return;
 			}
 			this.closed = true;
 			
 			try {
 				// wait until as many buffers have been returned as were written
 				// only then is everything guaranteed to be consistent.
 				while (this.requestsNotReturned.get() > 0) {
 					try {
 						// we add a timeout here, because it is not guaranteed that the
 						// decrementing during buffer return and the check here are deadlock free.
 						// the deadlock situation is however unlikely and caught by the timeout
 						this.closeLock.wait(1000);
 						checkErroneous();
 					}
 					catch (InterruptedException iex) {
 						throw new IOException(""Closing of asynchronous file channel was interrupted."");
 					}
 				}
+
+				// Additional check because we might have skipped the while loop
+				checkErroneous();
 			}
 			finally {
 				// close the file
 				if (this.fileChannel.isOpen()) {
 					this.fileChannel.close();
 				}
 			}
 		}
 	}",Buggy,"[FLINK-1545] [runtime][tests] Fixes AsynchronousFileIOChannelsTest.testExceptionForwardsToClose by introducing additional error check in AsynchronousFileIOChannel.close method

This closes #399
",Buggy,"The commit message indicates a fix for `AsynchronousFileIOChannelsTest.testExceptionForwardsToClose` by adding an error check in the `AsynchronousFileIOChannel.close` method. The diff shows that an additional `checkErroneous()` call is added after the `while` loop inside the `close` method. This suggests that there was a scenario where the `while` loop could be skipped, leading to a missed error check and thus a bug. The added check ensures that errors are always checked before closing the file channel. Therefore, this change indicates a bug fix.
The answer is **Buggy**."
flink,25651.json,ce822bf7f5ec80df5d5a749b1439320af3fb8b18,"@@ -1,4 +1,7 @@
 	public <R> DeltaIteration<T, R> iterateDelta(DataSet<R> workset, int maxIterations, int... keyPositions) {
+		Preconditions.checkNotNull(workset);
+		Preconditions.checkNotNull(keyPositions);
+		
 		Keys.ExpressionKeys<T> keys = new Keys.ExpressionKeys<T>(keyPositions, getType(), false);
 		return new DeltaIteration<T, R>(getExecutionEnvironment(), getType(), this, workset, keys, maxIterations);
 	}",Buggy,"[FLINK-1254] [compiler] Fix compiler bug for pipeline breaker placement

This closes #216
",Buggy,"The commit message indicates a bug fix in the compiler related to pipeline breaker placement. The diff adds null checks for `workset` and `keyPositions` in the `iterateDelta` method. These checks prevent `NullPointerException`s if either of these parameters is null, which could lead to unexpected behavior or crashes during compilation or execution. The addition of these checks directly addresses a potential bug by preventing a specific type of error. The answer is **Buggy**.
"
flink,18032.json,1bfeeaead158051cb5f78f403c4878fc89596cc8,"@@ -1,23 +1,21 @@
 	public void write(final DataOutput out) throws IOException {
 		final int maxBit = 0x1 << 7;
 
 		int len = this.value.length();
 
 		while (len >= maxBit) {
 			out.write(len | maxBit);
 			len >>= 7;
 		}
 		out.write(len);
 
 		for (int i = 0; i < this.value.length(); i++) {
 			int c = this.value.charAt(i);
 
-			if (c < maxBit)
-				out.write(c);
-			else
-				while (c >= maxBit) {
-					out.write(c | maxBit);
-					c >>= 7;
-				}
+			while (c >= maxBit) {
+				out.write(c | maxBit);
+				c >>= 7;
+			}
+			out.write(c);
 		}
 	}",Buggy,"Fixed Serialization Bug in PactString
",Buggy,"The commit message indicates a fix for a serialization bug in `PactString`. The code diff modifies the `write` method, which is responsible for serializing the string. The original code had a conditional statement `if (c < maxBit)` that seems to have been removed. The loop `while (c >= maxBit)` now always executes, which suggests that the original code might have had a problem with characters greater than or equal to `maxBit`. The removal of the `if` statement and the consistent application of the `while` loop and `out.write(c)` at the end indicates a bug fix related to how characters are serialized. The answer is **Buggy**."
flink,22768.json,3854552ceefd2b2b9c0e2a9b6152a7fcb69153fe,"@@ -1,26 +1,32 @@
 	public static String unresolvedHostToNormalizedString(String host) {
 		// Return loopback interface address if host is null
 		// This represents the behavior of {@code InetAddress.getByName } and RFC 3330
 		if (host == null) {
 			host = InetAddress.getLoopbackAddress().getHostAddress();
 		} else {
 			host = host.trim().toLowerCase();
+			if (host.startsWith(""["") && host.endsWith(""]"")) {
+				String address = host.substring(1, host.length() - 1);
+				if (IPAddressUtil.isIPv6LiteralAddress(address)) {
+					host = address;
+				}
+			}
 		}
 
 		// normalize and valid address
 		if (IPAddressUtil.isIPv6LiteralAddress(host)) {
 			byte[] ipV6Address = IPAddressUtil.textToNumericFormatV6(host);
 			host = getIPv6UrlRepresentation(ipV6Address);
 		} else if (!IPAddressUtil.isIPv4LiteralAddress(host)) {
 			try {
 				// We don't allow these in hostnames
 				Preconditions.checkArgument(!host.startsWith("".""));
 				Preconditions.checkArgument(!host.endsWith("".""));
 				Preconditions.checkArgument(!host.contains("":""));
 			} catch (Exception e) {
 				throw new IllegalConfigurationException(""The configured hostname is not valid"", e);
 			}
 		}
 
 		return host;
 	}",Buggy,"[FLINK-12840] [core] Fix network utils to work with ipv6 correctly

  - Fixes problems around akka configuration parsing with some IPv6 literals
  - Fixes an issue with address parsing and validation with some Ipv6 literals

This closes #8734
",Buggy,"The commit message indicates fixes for IPv6 address handling in network utilities, specifically addressing issues with Akka configuration parsing and address validation related to IPv6 literals. The code diff modifies the `unresolvedHostToNormalizedString` method to handle IPv6 addresses enclosed in square brackets. It checks if the host string starts and ends with square brackets, and if the content within the brackets is a valid IPv6 literal address, it extracts the address. This addresses the parsing and validation issues mentioned in the commit message. The code also normalizes IPv6 addresses using `getIPv6UrlRepresentation`. The changes directly relate to fixing bugs in IPv6 address handling. The answer is **Buggy**.
"
flink,28054.json,6e5954e8a03ad5d440447a57098976b0250f4f72,"@@ -1,43 +1,45 @@
 	private RexNode convertOver(List<Expression> children) {
 		List<Expression> args = children;
 		Expression agg = args.get(0);
 		SqlAggFunction aggFunc = agg.accept(new SqlAggFunctionVisitor(typeFactory));
 		RelDataType aggResultType = typeFactory.createFieldTypeFromLogicalType(
 				fromDataTypeToLogicalType(((ResolvedExpression) agg).getOutputDataType()));
 
 		// assemble exprs by agg children
 		List<RexNode> aggExprs = agg.getChildren().stream().map(expr -> expr.accept(this))
 				.collect(Collectors.toList());
 
 		// assemble order by key
 		Expression orderKeyExpr = args.get(1);
 		Set<SqlKind> kinds = new HashSet<>();
 		RexNode collationRexNode = createCollation(orderKeyExpr.accept(this), RelFieldCollation.Direction.ASCENDING,
 				null, kinds);
 		ImmutableList<RexFieldCollation> orderKey = ImmutableList
 				.of(new RexFieldCollation(collationRexNode, kinds));
 
 		// assemble partition by keys
 		List<RexNode> partitionKeys = args.subList(4, args.size()).stream().map(expr -> expr.accept(this))
 				.collect(Collectors.toList());
 		// assemble bounds
 		Expression preceding = args.get(2);
-		boolean isPhysical = ((ResolvedExpression) preceding).getOutputDataType().equals(DataTypes.BIGINT());
+		boolean isPhysical = LogicalTypeChecks.hasRoot(
+				fromDataTypeToLogicalType(((ResolvedExpression) preceding).getOutputDataType()),
+				LogicalTypeRoot.BIGINT);
 		Expression following = args.get(3);
 		RexWindowBound lowerBound = createBound(preceding, SqlKind.PRECEDING);
 		RexWindowBound upperBound = createBound(following, SqlKind.FOLLOWING);
 
 		// build RexOver
 		return relBuilder.getRexBuilder().makeOver(
 				aggResultType,
 				aggFunc,
 				aggExprs,
 				partitionKeys,
 				orderKey,
 				lowerBound,
 				upperBound,
 				isPhysical,
 				true,
 				false,
 				false);
 	}",Buggy,"[FLINK-13107][table-planner-blink] Fix Bug to check whether OverCall is RowMode or RangeMode.
",Buggy,"The commit message indicates a bug fix related to checking whether an OverCall is in RowMode or RangeMode. The code change involves modifying the `convertOver` method. Specifically, the code that determines if the `preceding` expression represents a physical (RowMode) or logical (RangeMode) bound has been updated. The original code checked if the output data type of the `preceding` expression was equal to `DataTypes.BIGINT()`. The updated code uses `LogicalTypeChecks.hasRoot` to check if the root of the logical type derived from the `preceding` expression's output data type is `LogicalTypeRoot.BIGINT`. This change suggests that the original check was not robust enough and could have led to incorrect determination of RowMode or RangeMode, which is a bug. The updated code provides a more reliable check using `LogicalTypeChecks.hasRoot`. The answer is **Buggy**."
flink,22909.json,b01641bcc13631b0db82c54143670613babb7c0c,"@@ -1,79 +1,79 @@
 	public NumberSequenceIterator[] split(int numPartitions) {
 		if (numPartitions < 1) {
 			throw new IllegalArgumentException(""The number of partitions must be at least 1."");
 		}
 		
 		if (numPartitions == 1) {
 			return new NumberSequenceIterator[] { new NumberSequenceIterator(current, to) };
 		}
 		
 		// here, numPartitions >= 2 !!!
 		
 		long elementsPerSplit;
 		
-		if (to - current >= 0) {
-			elementsPerSplit = (to - current) / numPartitions;
+		if (to - current + 1 >= 0) {
+			elementsPerSplit = (to - current + 1) / numPartitions;
 		}
 		else {
 			// long overflow of the range.
 			// we compute based on half the distance, to prevent the overflow.
 			// in most cases it holds that: current < 0 and to > 0, except for: to == 0 and current == Long.MIN_VALUE
 			// the later needs a special case
 			final long halfDiff; // must be positive
 			
 			if (current == Long.MIN_VALUE) {
 				// this means to >= 0
 				halfDiff = (Long.MAX_VALUE/2+1) + to/2;
 			} else {
 				long posFrom = -current;
 				if (posFrom > to) {
 					halfDiff = to + ((posFrom - to) / 2);
 				} else {
 					halfDiff = posFrom + ((to - posFrom) / 2);
 				}
 			}
 			elementsPerSplit = halfDiff / numPartitions * 2;
 		}
 		
 		if (elementsPerSplit < Long.MAX_VALUE) {
 			// figure out how many get one in addition
-			long numWithExtra = -(elementsPerSplit * numPartitions) + to - current;
+			long numWithExtra = -(elementsPerSplit * numPartitions) + to - current + 1;
 			
 			// based on rounding errors, we may have lost one)
 			if (numWithExtra > numPartitions) {
 				elementsPerSplit++;
 				numWithExtra -= numPartitions;
 				
 				if (numWithExtra > numPartitions) {
 					throw new RuntimeException(""Bug in splitting logic. To much rounding loss."");
 				}
 			}
 			
 			NumberSequenceIterator[] iters = new NumberSequenceIterator[numPartitions];
 			long curr = current;
 			int i = 0;
 			for (; i < numWithExtra; i++) {
 				long next = curr + elementsPerSplit + 1;
-				iters[i] = new NumberSequenceIterator(curr, next);
+				iters[i] = new NumberSequenceIterator(curr, next-1);
 				curr = next;
 			}
 			for (; i < numPartitions; i++) {
 				long next = curr + elementsPerSplit;
-				iters[i] = new NumberSequenceIterator(curr, next);
+				iters[i] = new NumberSequenceIterator(curr, next-1, true);
 				curr = next;
 			}
 			
 			return iters;
 		}
 		else {
 			// this can only be the case when there are two partitions
 			if (numPartitions != 2) {
 				throw new RuntimeException(""Bug in splitting logic."");
 			}
 			
 			return new NumberSequenceIterator[] {
 				new NumberSequenceIterator(current, current + elementsPerSplit),
 				new NumberSequenceIterator(current + elementsPerSplit, to)
 			};
 		}
 	}",Buggy,"Fixed bug in splitting logic of number sequence iterator.
",Buggy,"The commit message states ""Fixed bug in splitting logic of number sequence iterator."" The diff modifies the calculation of `elementsPerSplit` and `numWithExtra`, and adjusts the upper bound of the `NumberSequenceIterator` in the loops. Specifically, it adds 1 to the range calculation `to - current` and subtracts 1 from the `next` value when creating the iterators. These changes suggest an attempt to correct an off-by-one error in the splitting logic, which is a common type of bug. The addition of the `true` parameter in the second iterator constructor also indicates a change in behavior, likely to address a specific edge case or bug. Therefore, the changes indicate a bug fix. The answer is **Buggy**."
flink,36898.json,6dcf74f022f4a127260b0e8d52b772fb28fa2249,"@@ -1,5 +1,10 @@
 	public void setCosts(Costs nodeCosts) {
-		// do not account for any cost, regardless of what the estimator
-		// calculates for our shipping strategies
-		super.setCosts(new Costs());
+		// the plan enumeration logic works as for regular two-input-operators, which is important
+		// because of the branch handling logic. it does pick redistributing network channels
+		// between the sink and the sink joiner, because sinks joiner has a different DOP than the sink.
+		// we discard any cost and simply use the sum of the costs from the two children.
+		
+		Costs totalCosts = getInput1().getSource().getCumulativeCosts().clone();
+		totalCosts.addCosts(getInput2().getSource().getCumulativeCosts());
+		super.setCosts(totalCosts);
 	}",Buggy,"Fixed erroneous costs handling in sink joiner.
",Buggy,"The commit message indicates a fix related to erroneous costs handling in a sink joiner. The diff shows that the `setCosts` method was modified. Previously, the method was setting the costs to a new empty `Costs` object, effectively ignoring any calculated costs. The updated code now calculates the total costs by summing the cumulative costs from the two input sources and setting the superclass's costs to this total. This change directly addresses the erroneous costs handling mentioned in the commit message, indicating a bug fix. The answer is **Buggy**.
"
flink,9963.json,750325e74c1cd91006b251f18f3ed8acf90c2a91,"@@ -1,3 +1,8 @@
 		public void jobLeaderLostLeadership(final JobID jobId, final UUID oldJobLeaderId) {
-			ResourceManager.this.jobLeaderLostLeadership(jobId, oldJobLeaderId);
+			runAsync(new Runnable() {
+				@Override
+				public void run() {
+					ResourceManager.this.jobLeaderLostLeadership(jobId, oldJobLeaderId);
+				}
+			});
 		}",Buggy,"[FLINK-5893] [RM] Fix the bug of race condition for removing previous JobManagerRegistration in ResourceManager

This closes #3399.
",Buggy,"The commit message indicates a fix for a race condition when removing a JobManagerRegistration in the ResourceManager. The diff shows that the call to `ResourceManager.this.jobLeaderLostLeadership(jobId, oldJobLeaderId)` is now executed asynchronously using `runAsync`. This change likely addresses a scenario where the removal operation was interfering with other operations, leading to the race condition. By making the call asynchronous, the operation is deferred, potentially avoiding the conflict. This clearly indicates a bug fix. The answer is **Buggy**."
flink,14028.json,0f88c392b6ad6f91aed33e157bdc9df6f613f09d,"@@ -1,3 +1,3 @@
 	public TableSource<Row> projectFields(int[] fields) {
-		return new ParquetTableSource(path, parquetSchema, parquetConfig, recursiveEnumeration, fields, null);
+		return new ParquetTableSource(path, parquetSchema, parquetConfig, recursiveEnumeration, fields, predicate);
 	}",Buggy,"[FLINK-15361][parquet] ParquetTableSource should pass predicate in projectFields

fix the problem, when after projectFields, ParquetTableSource will loose predicates.

this closes #10660.
",Buggy,"The commit message indicates that the `ParquetTableSource` was losing predicates after a `projectFields` operation. The diff shows that the `projectFields` method in `ParquetTableSource` is modified to pass the `predicate` to the new `ParquetTableSource` instance. This ensures that the predicate is preserved after the projection, which fixes the reported issue. The answer is **Buggy**."
flink,27787.json,37f67b7d394ccb3355ebc995af1c3fee04ce060f,"@@ -1,8 +1,8 @@
 	public Expression[] accumulateExpressions() {
 		Expression[] accExpressions = new Expression[1 + operands().length];
-		// sequence = if (lastValues equalTo orderKeys) sequence else sequence + 1
-		accExpressions[0] = ifThenElse(orderKeyEqualsExpression(), sequence, plus(sequence, literal(1L)));
+		// sequence = if (lastValues equalTo orderKeys and sequence != 0) sequence else sequence + 1
+		accExpressions[0] = ifThenElse(and(orderKeyEqualsExpression(), not(equalTo(sequence, literal(0L)))), sequence, plus(sequence, literal(1L)));
 		Expression[] operands = operands();
 		System.arraycopy(operands, 0, accExpressions, 1, operands.length);
 		return accExpressions;
 	}",Buggy,"[FLINK-14053][table-planner-blink] Fix DenseRankAggFunction first row bug.

We should consider the possibility that first row's order by key is equal to the initial last value.

This closes #9966
",Buggy,"The commit message indicates a bug fix in `DenseRankAggFunction` related to the first row. The diff modifies the `accumulateExpressions` method, specifically the logic for incrementing the sequence. The original code increments the sequence if the `orderKeyEqualsExpression` is false. The modified code adds an additional condition: `and(orderKeyEqualsExpression(), not(equalTo(sequence, literal(0L))))`. This condition checks if the order keys are equal to the last values AND the sequence is not 0. This suggests that the original code had a bug where the sequence was incorrectly incremented for the first row when the order keys were equal to the initial last value. The added condition prevents this incorrect increment, thus fixing the bug. The answer is **Buggy**.
"
flink,7316.json,c59f4836fecc4069b2cffeae8dd81e50ea5d5e73,"@@ -1,4 +1,4 @@
 	public String getDescription() {
-		return ""String value that specifies the termination mode. Supported values are: "" +
-			StringUtils.toQuotedListString(TerminationMode.values()) + '.';
+		return ""String value that specifies the termination mode. The only supported value is: \"""" +
+			TerminationMode.CANCEL.name().toLowerCase() + ""\""."";
 	}",Buggy,"[FLINK-13136][docs] Fix documentation error about stopping job with restful api

This closes #9013.
",Buggy,"The commit message indicates a fix to a documentation error regarding the termination mode of a job via the RESTful API. The diff shows a change in the `getDescription` method, which now specifies that only the ""cancel"" termination mode is supported. This aligns with the commit message, as it corrects a documentation error. The change clarifies the supported termination mode, indicating a bug fix in the documentation. The answer is **Buggy**."
flink,13283.json,2cb7bb96001f9780a27a880245382958448151c4,"@@ -1,3 +1,5 @@
 	public void setItemCount(long itemCount) throws IOException {
-		writeVarLongCount(out, itemCount);
+		if (itemCount > 0) {
+			writeVarLongCount(out, itemCount);
+		}
 	}",Buggy,"Fix bug in avro serialization for empty collections.
Add additional test for avro.
",Buggy,"The commit message indicates a bug fix related to Avro serialization of empty collections. The diff shows a modification to the `setItemCount` method, where a check is added to ensure that `itemCount` is greater than 0 before writing it using `writeVarLongCount`. This suggests that the original code might have had an issue when handling empty collections (i.e., when `itemCount` is 0), potentially leading to incorrect serialization or errors. The added `if` condition prevents writing a non-positive `itemCount`, which likely addresses the bug described in the commit message. The answer is **Buggy**."
flink,34466.json,a86bce5a176144e06d0120b804f3af986c325ebf,"@@ -1,18 +1,18 @@
 	private static int getMillis(String dateStr) {
 		int length = dateStr.length();
 		if (length == 19) {
 			// ""1999-12-31 12:34:56"", no milli second left
 			return 0;
 		} else if (length == 21) {
 			// ""1999-12-31 12:34:56.7"", return 7
 			return Integer.parseInt(dateStr.substring(20)) * 100;
 		} else if (length == 22) {
 			// ""1999-12-31 12:34:56.78"", return 78
 			return Integer.parseInt(dateStr.substring(20)) * 10;
 		} else if (length >= 23 && length <= 26) {
 			// ""1999-12-31 12:34:56.123"" ~ ""1999-12-31 12:34:56.123456""
-			return Integer.parseInt(dateStr.substring(20, 23)) * 10;
+			return Integer.parseInt(dateStr.substring(20, 23));
 		} else {
 			return 0;
 		}
 	}",Buggy,"[FLINK-12553][table-runtime-blink] Fix bug that SqlDateTimeUtils#parseToTimeMillis doesn't parse millisecond correctly

This closes #8483
",Buggy,"The commit message indicates a bug fix in `SqlDateTimeUtils#parseToTimeMillis` related to parsing milliseconds. The code diff shows a change in the `getMillis` method, specifically in the case where the input string length is between 23 and 26. The original code multiplies the parsed millisecond value by 10, which is incorrect. The corrected code removes this multiplication, indicating a fix for the incorrect parsing of milliseconds. The answer is **Buggy**.
"
flink,22688.json,7b6b5a2e019866bf8fdd993775eab410e22f0f5d,"@@ -1,11 +1,14 @@
 	public static Object deserializeObject(byte[] bytes, ClassLoader cl) throws IOException, ClassNotFoundException {
 		ObjectInputStream oois = null;
+		final ClassLoader old = Thread.currentThread().getContextClassLoader();
 		try {
+			Thread.currentThread().setContextClassLoader(cl);
 			oois = new ClassLoaderObjectInputStream(new ByteArrayInputStream(bytes), cl);
 			return oois.readObject();
 		} finally {
+			Thread.currentThread().setContextClassLoader(old);
 			if (oois != null) {
 				oois.close();
 			}
 		}
 	}",Buggy,"Fixes a bug where Thread.currentThread().getContextClassLoader() does not return the user code class loader within object deserialization.
",Buggy,"The commit message indicates a bug related to the context class loader during object deserialization. The diff shows that the code now explicitly sets the context class loader to the provided class loader `cl` before deserialization and restores the original context class loader in a `finally` block. This ensures that the deserialization process uses the correct class loader, addressing the bug described in the commit message. The answer is **Buggy**."
flink,22189.json,e28b62e0e2973b01ad5b08ce319aaf0e7ce4c087,"@@ -1,11 +1,11 @@
 	public MethodVisitor visitMethod(int access, String name, String desc, String sig, String[] exceptions) {
-		return new MethodVisitor(Opcodes.ASM4) {
+		return new MethodVisitor(Opcodes.ASM5) {
 
 			@Override
 			public void visitFieldInsn(int op, String owner, String name, String desc) {
 				if (op == Opcodes.GETFIELD && name.equals(this0Name)) {
 					isThis0Accessed = true;
 				}
 			}
 		};
 	}",Buggy,"[FLINK-3143] update Closure Cleaner's ASM references to ASM5

- This solves errors with reflectasm using Scala 2.11 and Java 8

This closes #1445.
",Buggy,"The commit message indicates a fix for errors encountered when using reflectasm with Scala 2.11 and Java 8. The diff updates the ASM version used by Closure Cleaner from ASM4 to ASM5. This change directly addresses compatibility issues and resolves errors, indicating a bug fix. The answer is **Buggy**."
flink,36539.json,73b5b3dd81e2a146592d9623f44ceff3d8c035fa,"@@ -1,6 +1,6 @@
 	public void setDegreeOfParallelism(int degreeOfParallelism) {
 		if (degreeOfParallelism < 1) {
-			throw new IllegalArgumentException();
+			throw new IllegalArgumentException(""Degree of parallelism of "" + degreeOfParallelism + "" is invalid."");
 		}
 		this.degreeOfParallelism = degreeOfParallelism;
 	}",Buggy,"Fix error with invalid config values for degree of parallelism.
",Buggy,"The commit message indicates a fix for invalid configuration values related to the degree of parallelism. The diff shows that an `IllegalArgumentException` is thrown when the `degreeOfParallelism` is less than 1. The change adds a descriptive message to the exception, clarifying why the argument is invalid. This change improves error handling by providing more context to the user when an invalid configuration value is provided. The answer is **Buggy**."
flink,1954.json,f28c28643df43633439b2c99c383e1ed01319ea3,"@@ -1,9 +1,9 @@
 	public <T extends Value> Class<? extends ConvergenceCriterion<T>> getConvergenceCriterion() {
 		@SuppressWarnings(""unchecked"")
-		Class<? extends ConvergenceCriterion<T>> clazz = (Class<? extends ConvergenceCriterion<T>>) 
+		Class<? extends ConvergenceCriterion<T>> clazz = (Class<? extends ConvergenceCriterion<T>>) (Class<?>) 
 							this.config.getClass(ITERATION_CONVERGENCE_CRITERION, null, ConvergenceCriterion.class);
 		if (clazz == null) {
 			throw new NullPointerException();
 		}
 		return clazz;
 	}",Buggy,"Fixed compiler error in maven for TaskConfig.java
",Buggy,"The commit message indicates a fix for a compiler error in `TaskConfig.java`. The diff involves a change to a type cast. Specifically, `(Class<? extends ConvergenceCriterion<T>>)` is changed to `(Class<? extends ConvergenceCriterion<T>>) (Class<?>)`. This kind of change is often used to suppress compiler warnings or errors related to type safety, especially when dealing with generics and reflection. This suggests the original code had a type-related issue that prevented compilation. The addition of `(Class<?>)` is a type erasure, which circumvents the compiler's type checking. This is a bug fix. The answer is **Buggy**.
"
flink,2537.json,5e498dc9e763a5daa3867456f657e19cd08fbb66,"@@ -1,13 +1,11 @@
-		private void handleCompletedFuture(T value, Throwable throwable) {
+		private void handleCompletedFuture(int index, T value, Throwable throwable) {
 			if (throwable != null) {
 				completeExceptionally(throwable);
 			} else {
-				int index = nextIndex.getAndIncrement();
-
 				results[index] = value;
 
 				if (numCompleted.incrementAndGet() == numTotal) {
 					complete(Arrays.asList(results));
 				}
 			}
 		}",Buggy,"[FLINK-12021] Deploy execution in topological sorted order

Due to changes how the slot futures are completed and due to the fact that the
ResultConjunctFuture does not maintain the order in which the futures were specified,
it could happen that executions were not deployed in topological order. This commit
fixes this problem by changing the ResultConjunctFuture so that it maintains the order
of the specified futures in its result collection.

This closes #8060.
",Buggy,"The commit message indicates a bug fix related to the order of execution deployment in Flink. The problem arises from the `ResultConjunctFuture` not maintaining the order of futures, leading to out-of-order deployments. The diff shows a change in the `handleCompletedFuture` method, which is responsible for processing completed futures. The change involves passing an `index` to the method, suggesting that the order of futures is now being tracked and used during completion. This aligns with the commit message's claim of fixing the topological order issue. The answer is **Buggy**."
eclipseJdt,3793.json,2f6e64cbbbc83dfd41e2988704a66b1a58193e8d,"@@ -1,3 +1,3 @@
 	public boolean isTypeAccess() {
-		return false;
+		return !this.haveReceiver;
 	}",Buggy,"Fixed Bug 424226 - [1.8] Cannot use static method from an interface in
static method reference ",Buggy,The commit message indicates a bug fix related to static method references in interfaces. The code change modifies the `isTypeAccess()` method to return the negation of `this.haveReceiver`. This change likely addresses the bug described in the commit message by correctly determining if a method access is a type access based on the presence of a receiver. The answer is **Buggy**.
eclipseJdt,651.json,969e112c287880aaa166b5c77a56bd62cba0db4b,"@@ -1,7 +1,11 @@
 private static IJavaProject getJavaProject(IPath path, IJavaModel model) {
-	IJavaProject project = model.getJavaProject(path.lastSegment());
+	String lastSeg = path.lastSegment();
+	if (lastSeg == null) {
+		lastSeg = path.toOSString();
+	}
+	IJavaProject project = model.getJavaProject(lastSeg);
 	if (project.exists()) {
 		return project;
 	}
 	return null;
 }",Buggy,"Fix for Bug 464339 When finding references, Java Search fails with
NullPointerException",Buggy,"The commit message indicates a fix for a NullPointerException in Java Search. The code change addresses a scenario where `path.lastSegment()` might return null, leading to a NullPointerException when passed to `model.getJavaProject()`. The fix adds a null check for `path.lastSegment()` and uses `path.toOSString()` as a fallback. This clearly resolves a bug. The answer is **Buggy**."
eclipseJdt,20760.json,5666cb170a5113b959f09f9a106548eceb94f3e9,"@@ -1,17 +1,23 @@
 	private boolean isPrimaryType(String name, IType type, boolean partialMatch) {
 		/*
 		 * Please have a look at: NameLookup#NameLookup
 		 * The HashTable this.typesInWorkingCopies contains values which are HashTables themselves.
 		 * The values of these HashTables are either of IType or IType[].
 		 * These values are types belonging to a compilation unit. Please check:
 		 * CompilationUnit#getTypes().
 		 * Therefore the parents of these types would be compilation units.
 		 */
 		ICompilationUnit cu = (ICompilationUnit) type.getParent();
 		String cuName = cu.getElementName().substring(0, cu.getElementName().lastIndexOf('.'));
+		/*
+		 * Secondary types along with primary types have their parent as the compilation unit.
+		 * The names of the primary type would match with their compilation unit.
+		 */
+		if (!cuName.equals(type.getElementName()))
+			return false;
 		if (partialMatch) {
 			return cuName.regionMatches(0, name, 0, name.length());
 		} else {
 			return cuName.equals(name);
 		}
 	}",Buggy,"Fixed Bug 431501 - NameLookup#findType API finds secondary types with
secondaryType and partial match set

Change-Id: Iad7904647729b2f282d039856b6ecee5325ebedb
Signed-off-by: Shankha Banerjee <shankhba@in.ibm.com>
",Buggy,"The commit message indicates a fix for bug 431501, specifically addressing an issue where `NameLookup#findType` API incorrectly identifies secondary types when using `secondaryType` and `partialMatch` settings. The code diff modifies the `isPrimaryType` method to check if the compilation unit name matches the type's element name. This check ensures that only primary types are returned when the names match, filtering out secondary types. Before the fix, secondary types were incorrectly identified as primary types under certain conditions. Therefore, the change directly addresses the bug described in the commit message. The answer is **Buggy**."
eclipseJdt,1127.json,fe3ac1852503ee5f3d26919dd323775bfa197648,"@@ -1,14 +1,17 @@
 public PossibleMatch[] getPossibleMatches(IPackageFragmentRoot[] roots) {
 	PossibleMatch[] result = new PossibleMatch[this.elementCount];
 	int index = 0;
+	HashSet<IPath> processedHash = new HashSet<>();
 	for (int i = 0, length = roots.length; i < length; i++) {
-		ObjectVector possibleMatches = (ObjectVector) this.rootsToPossibleMatches.get(roots[i].getPath());
-		if (possibleMatches != null) {
+		IPath path = roots[i].getPath();
+		ObjectVector possibleMatches = (ObjectVector) this.rootsToPossibleMatches.get(path);
+		if (possibleMatches != null && !processedHash.contains(path)) {
 			possibleMatches.copyInto(result, index);
 			index += possibleMatches.size();
+			processedHash.add(path);
 		}
 	}
 	if (index < this.elementCount)
 		System.arraycopy(result, 0, result = new PossibleMatch[index], 0, index);
 	return result;
 }",Buggy,"Fix for bug 478360 - [1.9][search] AIOOBE  while searching for java.base
methods",Buggy,"The commit message indicates a fix for an `ArrayIndexOutOfBoundsException` (AIOOBE) that occurs during a search for `java.base` methods. The code diff introduces a `HashSet` called `processedHash` to keep track of the paths of `IPackageFragmentRoot` objects that have already been processed. The code now checks if the path of the current root has already been processed before copying its possible matches into the result array. This prevents the same root from being processed multiple times, which could lead to an AIOOBE if the `elementCount` is not large enough to accommodate all the possible matches. The addition of the `processedHash` and the check `!processedHash.contains(path)` clearly indicates a bug fix to prevent duplicate processing of package fragment roots, which could lead to an `ArrayIndexOutOfBoundsException`. The answer is **Buggy**."
eclipseJdt,3485.json,5df55b1d083f2bad66acc93a2ff019e72423c2c9,"@@ -1,12 +1,20 @@
 	public static IndexLocation createIndexLocation(URL url) {
 		URL localUrl;
 		try {
 			localUrl = FileLocator.resolve(url);
 		} catch (IOException e) {
 			return null;
 		}
 		if (localUrl.getProtocol().equals(""file"")) { //$NON-NLS-1$
-			return new FileIndexLocation(url, new File(localUrl.getPath()));
+			File localFile = null;
+			try {
+				URI localFileURI = new URI(localUrl.toExternalForm());
+				localFile = new File(localFileURI);
+			}
+			catch(Exception ex) {
+				localFile = new File(localUrl.getPath());
+			}
+			return new FileIndexLocation(url, localFile);
 		}
 		return new JarIndexLocation(url, localUrl);
 	}",Buggy,"Fix for bug 397818 - An IndexLocation is not created properly if the
pre-build indexes are located in a directory with a space
",Buggy,"The commit message indicates a fix for bug 397818, which relates to creating an IndexLocation when pre-built indexes are in a directory with a space. The code changes involve handling URLs with spaces in their paths when creating a FileIndexLocation. The original code directly used `localUrl.getPath()` to create a File object. This approach can fail when the URL contains spaces, as the `getPath()` method might not correctly decode the URL. The corrected code attempts to create a URI from the URL and then create a File object from the URI. If this fails (due to malformed URI, etc.), it falls back to the original method of using `localUrl.getPath()`. This change addresses the bug by properly handling URLs with spaces in the file path. The answer is **Buggy**."
eclipseJdt,3849.json,e14a67b89e92e257eae6e75fc0c7b3046b4c9f63,"@@ -1,5 +1,6 @@
 public Object reusableJSRTarget() {
-	if (this.constant != Constant.NotAConstant)
+	if (this.constant != Constant.NotAConstant && (this.implicitConversion & TypeIds.BOXING) == 0) {
 		return this.constant;
+	}
 	return null;
 }",Buggy,Fix for bug 394718 - VerifyError: Inconsistent stackmap frames,Buggy,"The commit message indicates a fix for a `VerifyError` related to inconsistent stackmap frames, which is a clear sign of a bug fix. The code change adds a condition `(this.implicitConversion & TypeIds.BOXING) == 0` to the `if` statement within the `reusableJSRTarget()` method. This suggests that the original code was incorrectly handling cases involving boxing conversions, leading to the stackmap inconsistencies. The addition of this condition likely corrects the logic to properly handle boxing scenarios, thus resolving the `VerifyError`. The answer is **Buggy**."
eclipseJdt,2849.json,48e3ddc97a372ba17e3a8fb05bd8c143189dd168,"@@ -1,21 +1,21 @@
 	public static final char[] concat(char[] first, char[] second, char[] third, char[] fourth) {
 		if (first == null)
 			return concat(second, third, fourth);
 		if (second == null)
 			return concat(first, third, fourth);
 		if (third == null)
 			return concat(first, second, fourth);
 		if (fourth == null)
 			return concat(first, second, third);
 
 		int length1 = first.length;
 		int length2 = second.length;
 		int length3 = third.length;
 		int length4 = fourth.length;
 		char[] result = new char[length1 + length2 + length3 + length4];
 		System.arraycopy(first, 0, result, 0, length1);
 		System.arraycopy(second, 0, result, length1, length2);
 		System.arraycopy(third, 0, result, length1 + length2, length3);
-		System.arraycopy(third, 0, result, length1 + length2 + length3, length4);
+		System.arraycopy(fourth, 0, result, length1 + length2 + length3, length4);
 		return result;
 	}",Buggy,"Bug 481796 - Fix ArrayIndexOutOfBoundsException

Change-Id: Ib1cded7b613262929f0c906a04bd69a523311141
Signed-off-by: Stefan Xenos <sxenos@gmail.com>
",Buggy,"The commit message indicates a fix for an ArrayIndexOutOfBoundsException. The diff shows a change in the `concat` method where `System.arraycopy(third, 0, result, length1 + length2 + length3, length4);` is corrected to `System.arraycopy(fourth, 0, result, length1 + length2 + length3, length4);`. This correction ensures that the fourth array is copied into the correct position in the result array, preventing a potential ArrayIndexOutOfBoundsException if the third array's length is incorrectly used for the fourth array. The answer is **Buggy**."
eclipseJdt,5117.json,8f577b9f934a073bf0b3684c3935f2cd08a1660a,"@@ -1,8 +1,8 @@
 	public StringBuffer printExpression(int indent, StringBuffer output) {
-		output.append(""NAryStringLiteral{""); //$NON-NLS-1$
+		output.append(""StringLiteralConcatenation{""); //$NON-NLS-1$
 		for (int i = 0, max = this.counter; i < max; i++) {
 			this.literals[i].printExpression(indent, output);
 			output.append(""+\n"");//$NON-NLS-1$
 		}
 		return output.append('}');
 	}",Buggy,"HEAD - Fix bug in formatting string literal concatenation
",Buggy,"The commit message indicates a bug fix related to formatting string literal concatenation. The diff changes ""NAryStringLiteral{"" to ""StringLiteralConcatenation{"", which suggests that the class name in the output was incorrect, and this change corrects it. This is a bug fix because the original output was misleading or incorrect. The answer is **Buggy**.
"
eclipseJdt,20784.json,653036e82ad47971c36ae720862ebb4740be149f,"@@ -1,12 +1,13 @@
 char[][] fullExclusionPatternChars() {
 	try {
+		if (this.getKind() != IPackageFragmentRoot.K_SOURCE) return null;
 		ClasspathEntry entry = (ClasspathEntry)getRawClasspathEntry();
 		if (entry == null) {
 			return null;
 		} else {
 			return entry.fullExclusionPatternChars();
 		}
 	} catch (JavaModelException e) { 
 		return null;
 	}
 }",Buggy,"Fix for bug 28489
",Buggy,"The commit message indicates a bug fix. The code change adds a condition `if (this.getKind() != IPackageFragmentRoot.K_SOURCE) return null;` at the beginning of the `fullExclusionPatternChars()` method. This check likely addresses a scenario where the method was being called for non-source package fragment roots, leading to an error or unexpected behavior. The `try-catch` block suggests that `JavaModelException` could be thrown, and the added condition aims to prevent this exception from occurring in certain cases. The answer is **Buggy**."
eclipseJdt,1595.json,0a908cd3dc3279066d94660987f6e5450b72486e,"@@ -1,54 +1,55 @@
 private MethodBinding getMethodBinding(MethodPattern methodPattern, TypeBinding declaringTypeBinding) {
 	MethodBinding result;
 	char[][] parameterTypes = methodPattern.parameterSimpleNames;
 	if (parameterTypes == null) return null;
 	int paramTypeslength = parameterTypes.length;
 	ReferenceBinding referenceBinding = (ReferenceBinding) declaringTypeBinding;
 	MethodBinding[] methods = referenceBinding.getMethods(methodPattern.selector);
 	int methodsLength = methods.length;
 	TypeVariableBinding[] refTypeVariables = referenceBinding.typeVariables();
 	int typeVarLength = refTypeVariables==null ? 0 : refTypeVariables.length;
 	List <MethodBinding> possibleMethods = new ArrayList<MethodBinding>(methodsLength);
 	for (int i=0; i<methodsLength; i++) {
 		TypeBinding[] methodParameters = methods[i].parameters;
 		int paramLength = methodParameters==null ? 0 : methodParameters.length;
 		TypeVariableBinding[] methodTypeVariables = methods[i].typeVariables;
 		int methTypeVarLength = methodTypeVariables==null ? 0 : methodTypeVariables.length;
 		boolean found = false;
 		if (methodParameters != null && paramLength == paramTypeslength) {
 			for (int p=0; p<paramLength; p++) {
-				if (CharOperation.equals(methodParameters[p].sourceName(), parameterTypes[p])) {
+				TypeBinding parameter = methodParameters[p];
+				if (matchParams(methodPattern, p, parameter)) {
 					// param erasure match
 					found = true;
 				} else {
 					// type variable
 					found = false;
 					if (refTypeVariables != null) {
 						for (int v=0; v<typeVarLength; v++) {
 							if (!CharOperation.equals(refTypeVariables[v].sourceName, parameterTypes[p])) {
 								found = false;
 								break;
 							}
 							found = true;
 						}
 					}
 					if (!found && methodTypeVariables != null) {
 						for (int v=0; v<methTypeVarLength; v++) {
 							if (!CharOperation.equals(methodTypeVariables[v].sourceName, parameterTypes[p])) {
 								found = false;
 								break;
 							}
 							found = true;
 						}
 					}
 					if (!found) break;
 				}
 			}
 		}
 		if (found) {
 			possibleMethods.add(methods[i]);
 		}
 	}
 	result =  getMostApplicableMethod(possibleMethods, methodPattern);
 	return result;
 }",Buggy,Fix for Bug 521240: Search ignores the qualifier of the parameter type,Buggy,"The commit message indicates a fix for a bug where the search functionality was ignoring the qualifier of the parameter type. The code diff replaces the direct comparison of parameter names with a new `matchParams` method. This suggests that the original code was not correctly handling qualified parameter types, leading to incorrect search results. The introduction of `matchParams` likely addresses this issue by properly considering the qualifier of the parameter type during the search process. The answer is **Buggy**."
eclipseJdt,1134.json,b7bd88d9bc859369d4d17c37d109bd3331837ba7,"@@ -1,7 +1,8 @@
 public int match(MessageSend msgSend, MatchingNodeSet nodeSet)  {
 	if ((msgSend.bits & ASTNode.InsideJavadoc) == 0) return IMPOSSIBLE_MATCH;
+	if (!this.pattern.findReferences) return IMPOSSIBLE_MATCH;
 	if (this.pattern.declaringSimpleName == null || CharOperation.equals(msgSend.selector, this.pattern.declaringSimpleName)) {
 		return nodeSet.addMatch(msgSend, this.pattern.mustResolve ? POSSIBLE_MATCH : ACCURATE_MATCH);
 	}
 	return IMPOSSIBLE_MATCH;
 }",Buggy,"Fixed bug 381567: [search] Unexpected results from SearchEngine#search
Conflicts:
",Buggy,"The commit message indicates a bug fix related to search results. The code change adds a condition `if (!this.pattern.findReferences) return IMPOSSIBLE_MATCH;` at the beginning of the `match` method. This suggests that the search was not behaving as expected when `findReferences` was false, and this change corrects that behavior. The answer is **Buggy**."
eclipseJdt,5656.json,905dec82e916c38704735fea38fad8cba693cc45,"@@ -1,3 +1,3 @@
 public TypeBinding clone(TypeBinding outerType) {
-	throw new IllegalStateException(); // shouldn't get here.
+	return this; // shouldn't get here.
 }",Buggy,"Fixed Bug 427105 - [1.8][builder] Differences between incremental and
full builds in method contract verification in the presence of type
annotations",Buggy,"The commit message indicates a bug fix related to incremental vs. full builds with type annotations. The code change replaces throwing an `IllegalStateException` with returning `this`. This suggests that the original code was throwing an exception in a situation where it shouldn't have, leading to incorrect behavior during incremental builds. The change effectively disables the exception throwing, likely resolving the discrepancy between incremental and full builds. The answer is **Buggy**."
eclipseJdt,1327.json,84f1aab618cdc74f8c6aecebdc0fb845b80d6368,"@@ -1,4 +1,6 @@
 protected void consumeMemberValuePair() {
 	super.consumeMemberValuePair();
-	this.patternLocator.match((MemberValuePair) this.astStack[this.astPtr], this.nodeSet);
+	if ((this.patternFineGrain & ~IJavaSearchConstants.METHOD_REFERENCE_EXPRESSION) != 0) {
+		this.patternLocator.match((MemberValuePair) this.astStack[this.astPtr], this.nodeSet);
+	}
 }",Buggy,"Fix for Bug 435480 [1.8][search] search in method reference expressions
finds annotation element name",Buggy,"The commit message indicates a fix for a bug where search results incorrectly include annotation element names within method reference expressions. The code change introduces a check `(this.patternFineGrain & ~IJavaSearchConstants.METHOD_REFERENCE_EXPRESSION) != 0` before proceeding with the match. This check seems to filter out method reference expressions, preventing the incorrect matching of annotation element names within them. This aligns with the bug fix described in the commit message. The answer is **Buggy**."
eclipseJdt,165.json,fe024fd7797ce73b0a83969f685073606863b79e,"@@ -1,23 +1,23 @@
 	public PrimitiveTypeImpl getPrimitiveType(TypeKind kind)
 	{
 		switch (kind) {
 		case BOOLEAN:
 			return PrimitiveTypeImpl.BOOLEAN;
 		case BYTE:
 			return PrimitiveTypeImpl.BYTE;
 		case CHAR:
 			return PrimitiveTypeImpl.CHAR;
 		case DOUBLE:
 			return PrimitiveTypeImpl.DOUBLE;
 		case FLOAT:
 			return PrimitiveTypeImpl.FLOAT;
 		case INT:
 			return PrimitiveTypeImpl.INT;
 		case LONG:
 			return PrimitiveTypeImpl.LONG;
 		case SHORT:
 			return PrimitiveTypeImpl.SHORT;
 		default:
-			throw new IllegalStateException();
+			throw new IllegalArgumentException();
 		}
 	}",Buggy,"Fixes Bug 427943 - The method
org.eclipse.jdt.internal.compiler.apt.model.Factory.getPrimitiveType
does not throw IllegalArgumentException

Change-Id: I652c422b80ef6192955ef508387f9fd0fac31a7e
Signed-off-by: Harry Terkelsen <het@google.com>",Buggy,"The commit message indicates a fix for Bug 427943, specifically addressing an issue where the `getPrimitiveType` method was not throwing `IllegalArgumentException` as expected. The diff shows a change from throwing `IllegalStateException` to `IllegalArgumentException` in the default case of the switch statement. This aligns with the commit message, indicating a bug fix related to exception handling. The answer is **Buggy**."
eclipseJdt,1253.json,025c833d41053b8599d25c53d05e92e37975873f,"@@ -1,18 +1,29 @@
 protected int resolveLevelForType(char[] qualifiedPattern, TypeBinding type) {
 	if (qualifiedPattern == null) return ACCURATE_MATCH;
 	if (type == null || !type.isValidBinding()) return INACCURATE_MATCH;
 
 	// Type variable cannot be specified through pattern => this kind of binding cannot match it (see bug 79803)
 	if (type.isTypeVariable()) return IMPOSSIBLE_MATCH;
 
+	if (type instanceof IntersectionTypeBinding18) {
+		int result = IMPOSSIBLE_MATCH, prev = IMPOSSIBLE_MATCH;
+		IntersectionTypeBinding18 i18 = (IntersectionTypeBinding18) type;
+		for (ReferenceBinding ref : i18.intersectingTypes) {
+			result = resolveLevelForType(qualifiedPattern, ref);
+			if (result == ACCURATE_MATCH) return result; 
+			if (result == IMPOSSIBLE_MATCH) continue;
+			if (prev == IMPOSSIBLE_MATCH) prev = result;
+		}
+		return prev;
+	}
 	// NOTE: if case insensitive search then qualifiedPattern is assumed to be lowercase
 
 	char[] qualifiedPackageName = type.qualifiedPackageName();
 	char[] qualifiedSourceName = qualifiedSourceName(type);
 	char[] fullyQualifiedTypeName = qualifiedPackageName.length == 0
 		? qualifiedSourceName
 		: CharOperation.concat(qualifiedPackageName, qualifiedSourceName, '.');
 	return CharOperation.match(qualifiedPattern, fullyQualifiedTypeName, this.isCaseSensitive)
 		? ACCURATE_MATCH
 		: IMPOSSIBLE_MATCH;
 }",Buggy,"Fix for Bug 485805 [1.8][search] Search engine throws
UnsupportedOperationException when searching for subclass implementors

Change-Id: I28b09ffc6e446e2967c4dbf077816ef2df83d2ab",Buggy,"The commit message indicates a fix for a bug where the search engine throws an `UnsupportedOperationException` when searching for subclass implementors in Java 1.8. The code diff introduces a special handling for `IntersectionTypeBinding18`, which is a feature of Java 8. The code iterates through the intersecting types of the `IntersectionTypeBinding18` and resolves the level for each type. This suggests that the original code did not handle intersection types correctly, leading to the exception. The added code addresses this by recursively calling `resolveLevelForType` for each intersecting type and returning an appropriate match level. This aligns with the commit message indicating a bug fix related to Java 8 features. The answer is **Buggy**.
"
eclipseJdt,2496.json,7d2b09ebfd4cb99d1f345eedcae879729e8aff7e,"@@ -1,51 +1,53 @@
 	public MemoryAccessLog getReportFor(long address, int size) {
 		List<Tag> tags = new ArrayList<>();
 		tags.addAll(this.operationStack);
-		int pointerToStart = (this.insertionPosition + this.buffer0.length - this.currentEntries) % this.buffer0.length;
-		int currentPosition = (this.insertionPosition + this.buffer0.length - 1) % this.buffer0.length;
-		long currentWrite = this.timer;
 
 		List<MemoryOperation> operations = new ArrayList<>();
-		do {
-			long nextAddress = this.buffer0[currentPosition];
-			int nextArgument = this.buffer1[currentPosition];
-			byte nextOp = this.operation[currentPosition];
-
-			switch (nextOp) {
-				case POP_OPERATION: {
-					tags.add(getTagForId(nextArgument));
-					break;
-				}
-				case PUSH_OPERATION: {
-					tags.remove(tags.size() - 1);
-					break;
-				}
-				default: {
-					boolean isMatch = false;
-					if (address < nextAddress) {
-						long diff = nextAddress - address;
-						if (diff < size) {
-							isMatch = true;
-						}
-					} else {
-						long diff = address - nextAddress;
-						if (diff < nextArgument) {
-							isMatch = true;
-						}
+		if (this.buffer0 != null) {
+			int pointerToStart = (this.insertionPosition + this.buffer0.length - this.currentEntries) % this.buffer0.length;
+			int currentPosition = (this.insertionPosition + this.buffer0.length - 1) % this.buffer0.length;
+			long currentWrite = this.timer;
+			do {
+				long nextAddress = this.buffer0[currentPosition];
+				int nextArgument = this.buffer1[currentPosition];
+				byte nextOp = this.operation[currentPosition];
+	
+				switch (nextOp) {
+					case POP_OPERATION: {
+						tags.add(getTagForId(nextArgument));
+						break;
 					}
-
-					if (isMatch) {
-						List<Tag> stack = new ArrayList<>();
-						stack.addAll(tags);
-						MemoryOperation nextOperation = new MemoryOperation(nextOp, currentWrite, nextAddress,
-								nextArgument, stack);
-						operations.add(nextOperation);
+					case PUSH_OPERATION: {
+						tags.remove(tags.size() - 1);
+						break;
 					}
-
-					currentWrite--;
+					default: {
+						boolean isMatch = false;
+						if (address < nextAddress) {
+							long diff = nextAddress - address;
+							if (diff < size) {
+								isMatch = true;
+							}
+						} else {
+							long diff = address - nextAddress;
+							if (diff < nextArgument) {
+								isMatch = true;
+							}
+						}
+	
+						if (isMatch) {
+							List<Tag> stack = new ArrayList<>();
+							stack.addAll(tags);
+							MemoryOperation nextOperation = new MemoryOperation(nextOp, currentWrite, nextAddress,
+									nextArgument, stack);
+							operations.add(nextOperation);
+						}
+	
+						currentWrite--;
+					}
 				}
-			}
-			currentPosition = (currentPosition + this.buffer0.length - 1) % this.buffer0.length;
-		} while (currentPosition != pointerToStart);
+				currentPosition = (currentPosition + this.buffer0.length - 1) % this.buffer0.length;
+			} while (currentPosition != pointerToStart);
+		}
 		return new MemoryAccessLog(operations);
 	}",Buggy,"Bug 514089 - Build a tool to help debug index corruption

Fix NPE when corruption is detected and the buffer size is 0.

Change-Id: I4163e9f294c96582b201bdf09593602ab1807d6e
",Buggy,"The commit message indicates a fix for a NullPointerException (NPE) that occurs when corruption is detected and the buffer size is 0. The code diff adds a null check for `this.buffer0` before proceeding with the memory access log processing. If `this.buffer0` is null, the code skips the processing and returns an empty `MemoryAccessLog`. This prevents the NPE that would occur when trying to access elements of a null array. The answer is **Buggy**."
eclipseJdt,15048.json,30961650a033c638f3a768401431bcd77f1a4058,"@@ -1,26 +1,29 @@
 	private void removeUnresolvedBindings(org.eclipse.jdt.internal.compiler.ast.TypeDeclaration type) {
 		final MemberTypeDeclaration[] memberTypes = type.memberTypes;
 		if (memberTypes != null) {
 			for (int i = 0, max = memberTypes.length; i < max; i++){
 				removeUnresolvedBindings(memberTypes[i]);
 			}
 		}
 		if (type.binding != null && (type.binding.modifiers & CompilerModifiers.AccUnresolved) != 0) {
 			type.binding = null;
-			final org.eclipse.jdt.internal.compiler.ast.FieldDeclaration[] fields = type.fields;
-			if (fields != null) {
-				for (int i = 0, max = fields.length; i < max; i++){
+		}
+		
+		final org.eclipse.jdt.internal.compiler.ast.FieldDeclaration[] fields = type.fields;
+		if (fields != null) {
+			for (int i = 0, max = fields.length; i < max; i++){
+				if (fields[i].binding != null && (fields[i].binding.modifiers & CompilerModifiers.AccUnresolved) != 0) {
 					fields[i].binding = null;
 				}
 			}
 		}
-
+	
 		final AbstractMethodDeclaration[] methods = type.methods;
 		if (methods != null) {
 			for (int i = 0, max = methods.length; i < max; i++){
 				if (methods[i].binding !=  null && (methods[i].binding.modifiers & CompilerModifiers.AccUnresolved) != 0) {
 					methods[i].binding = null;
 				}
 			}
 		}
 	}",Buggy,"Fix for bug 40804
",Buggy,"The commit message indicates a fix for bug 40804. The code diff modifies the `removeUnresolvedBindings` method to handle unresolved bindings within a type declaration. Specifically, it nullifies the bindings of fields and methods if they are marked as unresolved. The change ensures that unresolved bindings are properly removed, which is a bug fix. The condition `fields[i].binding != null && (fields[i].binding.modifiers & CompilerModifiers.AccUnresolved) != 0` added to the fields iteration indicates that the previous code was not correctly handling the case where a field binding was unresolved.
The answer is **Buggy**."
eclipseJdt,18985.json,4b7408bf1768e4fe08faa711e23ea5c6fe36d225,"@@ -1,6 +1,6 @@
 public String toString() {
-	return ""State for "" + this.javaProjectName
-		+ "" (#"" + this.buildNumber
+	return ""State for "" + this.javaProjectName //$NON-NLS-1$
+		+ "" (#"" + this.buildNumber //$NON-NLS-1$
 			+ "" @ "" + new Date(this.lastStructuralBuildTime) //$NON-NLS-1$
 				+ "")""; //$NON-NLS-1$
 }",Buggy,"HEAD - Fix Clean-up pass 1 problems
",NotBuggy,"The commit message indicates a cleanup pass. The diff adds `//$NON-NLS-1$` to the `toString()` method. This change is related to internationalization (i18n) and localization (l10n), which involves marking strings for translation. This doesn't fix a bug, but rather improves the code for i18n/l10n purposes. The answer is **NotBuggy**."
eclipseJdt,4719.json,6fe04df602475d9f13e955fcfd38124da359e84a,"@@ -1,28 +1,34 @@
 public void computeConversion(Scope scope, TypeBinding runtimeTimeType, TypeBinding compileTimeType) {
 	if (runtimeTimeType == null || compileTimeType == null)
 		return;
-	if ((this.bits & Binding.FIELD) != 0 && this.binding != null && this.binding.isValidBinding()) {
-		// set the generic cast after the fact, once the type expectation is fully known (no need for strict cast)
-		FieldBinding field = (FieldBinding) this.binding;
-		FieldBinding originalBinding = field.original();
-		TypeBinding originalType = originalBinding.type;
-		// extra cast needed if field type is type variable
-		if (originalType.leafComponentType().isTypeVariable()) {
-	    	TypeBinding targetType = (!compileTimeType.isBaseType() && runtimeTimeType.isBaseType())
-	    		? compileTimeType  // unboxing: checkcast before conversion
-	    		: runtimeTimeType;
-	        this.genericCast = originalType.genericCast(scope.boxing(targetType));
-	        if (this.genericCast instanceof ReferenceBinding) {
+	if (this.binding != null && this.binding.isValidBinding()) {
+		TypeBinding originalType = null;
+		if ((this.bits & Binding.FIELD) != 0) {
+			// set the generic cast after the fact, once the type expectation is fully known (no need for strict cast)
+			FieldBinding field = (FieldBinding) this.binding;
+			FieldBinding originalBinding = field.original();
+			originalType = originalBinding.type;
+		} else if ((this.bits & Binding.LOCAL) != 0) {
+			LocalVariableBinding local = (LocalVariableBinding) this.binding;
+			originalType = local.type;
+		}
+		// extra cast needed if field/local type is type variable
+		if (originalType != null && originalType.leafComponentType().isTypeVariable()) {
+			TypeBinding targetType = (!compileTimeType.isBaseType() && runtimeTimeType.isBaseType())
+					? compileTimeType  // unboxing: checkcast before conversion
+							: runtimeTimeType;
+			this.genericCast = originalType.genericCast(scope.boxing(targetType));
+			if (this.genericCast instanceof ReferenceBinding) {
 				ReferenceBinding referenceCast = (ReferenceBinding) this.genericCast;
 				if (!referenceCast.canBeSeenBy(scope)) {
-		        	scope.problemReporter().invalidType(this,
-		        			new ProblemReferenceBinding(
-								CharOperation.splitOn('.', referenceCast.shortReadableName()),
-								referenceCast,
-								ProblemReasons.NotVisible));
+					scope.problemReporter().invalidType(this,
+							new ProblemReferenceBinding(
+									CharOperation.splitOn('.', referenceCast.shortReadableName()),
+									referenceCast,
+									ProblemReasons.NotVisible));
 				}
-	        }
+			}
 		}
 	}
 	super.computeConversion(scope, runtimeTimeType, compileTimeType);
 }",Buggy,"Fixed Bug 416480 - Error in bytecode generated by ECJ compiler leads to
IncompatibleClassChangeError",Buggy,"The commit message indicates a fix for a bug that leads to an `IncompatibleClassChangeError` due to an error in the bytecode generated by the ECJ compiler. The diff modifies the `computeConversion` method. The original code only considered `FieldBinding` when setting the generic cast. The modified code now also considers `LocalVariableBinding`. This suggests that the bug was related to incorrect handling of local variables during type conversion, leading to the `IncompatibleClassChangeError`. The change ensures that both field and local variables are correctly handled when determining the generic cast, which aligns with the bug fix described in the commit message. The answer is **Buggy**."
eclipseJdt,25728.json,ab7dabe7a80a22f7a1c8f6edce715e69ee26d4d5,"@@ -1,34 +1,36 @@
 	protected static boolean hasEmptyName(TypeReference reference, ASTNode assistNode) {
 		if (reference == null) return false;
 
-		if (reference.sourceStart <= assistNode.sourceStart && assistNode.sourceEnd <= reference.sourceEnd) return false;
+		// https://bugs.eclipse.org/bugs/show_bug.cgi?id=397070
+		if (reference != assistNode &&
+				reference.sourceStart <= assistNode.sourceStart && assistNode.sourceEnd <= reference.sourceEnd) return false;
 
 		if (reference instanceof CompletionOnSingleTypeReference ||
 				reference instanceof CompletionOnQualifiedTypeReference ||
 				reference instanceof CompletionOnParameterizedQualifiedTypeReference) {
 			char[][] typeName = reference.getTypeName();
 			if (typeName[typeName.length - 1].length == 0) return true;
 		}
 		if (reference instanceof ParameterizedSingleTypeReference) {
 			ParameterizedSingleTypeReference parameterizedReference = (ParameterizedSingleTypeReference) reference;
 			TypeReference[] typeArguments = parameterizedReference.typeArguments;
 			if (typeArguments != null) {
 				for (int i = 0; i < typeArguments.length; i++) {
 					if (hasEmptyName(typeArguments[i], assistNode)) return true;
 				}
 			}
 		} else if (reference instanceof ParameterizedQualifiedTypeReference) {
 			ParameterizedQualifiedTypeReference parameterizedReference = (ParameterizedQualifiedTypeReference) reference;
 			TypeReference[][] typeArguments = parameterizedReference.typeArguments;
 			if (typeArguments != null) {
 				for (int i = 0; i < typeArguments.length; i++) {
 					if (typeArguments[i] != null) {
 						for (int j = 0; j < typeArguments[i].length; j++) {
 							if (hasEmptyName(typeArguments[i][j], assistNode)) return true;
 						}
 					}
 				}
 			}
 		}
 		return false;
 	}",Buggy,"Fix for bug 397070 - On completion in implements/extends clause,
supertype/interface-signature(s) can not be retrieved from enclosing
type",Buggy,"The commit message explicitly states ""Fix for bug 397070"". The code change adds a condition `reference != assistNode` to the if statement `if (reference.sourceStart <= assistNode.sourceStart && assistNode.sourceEnd <= reference.sourceEnd)`. This suggests that the original code was incorrectly excluding a case where `reference` and `assistNode` were the same, leading to incorrect behavior. The added condition addresses this issue, indicating a bug fix. The answer is **Buggy**.
"
eclipseJdt,20900.json,c79924988b83a4410dc68ac4bbc9ed2e5112cc61,"@@ -1,3 +1,3 @@
-	public static IClasspathContainer getUserLibrary(String name) {
-		return (IClasspathContainer) getLibraryMap().get(name);
+	public static UserLibrary getUserLibrary(String name) {
+		return (UserLibrary) getLibraryMap().get(name);
 	}",Buggy,"Fix bug: invalid getUserLibrary return
",Buggy,"The commit message indicates a bug fix related to the return type of the `getUserLibrary` method. The diff shows a change in the return type from `IClasspathContainer` to `UserLibrary`. This suggests that the original code was incorrectly casting the object retrieved from the `getLibraryMap()` to `IClasspathContainer`, which could lead to runtime errors if the object was not actually an instance of `IClasspathContainer`. The change to `UserLibrary` implies that the method is now correctly returning the expected type, thus fixing a bug. The answer is **Buggy**.
"
eclipseJdt,14646.json,8bf6b68ac4b26f2f99e3e6e3e77b19496440917e,"@@ -1,12 +1,11 @@
 	public ITypeBinding getGenericTypeOfWildcardType() {
 		switch (this.binding.kind()) {
 			case Binding.WILDCARD_TYPE :
-			case Binding.INTERSECTION_TYPE :
 				WildcardBinding wildcardBinding = (WildcardBinding) this.binding;
 				if (wildcardBinding.genericType != null) {
 					return this.resolver.getTypeBinding(wildcardBinding.genericType);
 				}
 				break;
 		}
 		return null;
 	}",Buggy,"HEAD - Fixed bug 341759: NPE in ITypeBinding#getName() for intersection type
",Buggy,"The commit message indicates a fix for a NullPointerException (NPE) in `ITypeBinding#getName()` when dealing with intersection types. The diff removes `Binding.INTERSECTION_TYPE` from the switch statement in `getGenericTypeOfWildcardType()`. This suggests that the original code was incorrectly trying to handle intersection types within the `getGenericTypeOfWildcardType()` method, leading to the NPE. By removing this case, the code avoids the problematic handling of intersection types, thus fixing the bug. The answer is **Buggy**.
"
eclipseJdt,19389.json,ae30fd6df4fe6e776883eda11e8cb81aefe2c34c,"@@ -1,29 +1,26 @@
 protected IBuffer openBuffer(IProgressMonitor pm, Object info) throws JavaModelException {
 
 	// create buffer
-	IBuffer buffer = this.owner.createBuffer(this);
-	if (buffer == null) return null;
+	IBuffer buffer = BufferManager.createBuffer(this);
 
 	// set the buffer source
-	if (buffer.getCharacters() == null) {
-		IBuffer classFileBuffer = this.classFile.getBuffer();
-		if (classFileBuffer != null) {
-			buffer.setContents(classFileBuffer.getCharacters());
-		} else {
-			// Disassemble
-			IClassFileReader reader = ToolFactory.createDefaultClassFileReader(this.classFile, IClassFileReader.ALL);
-			Disassembler disassembler = new Disassembler();
-			String contents = disassembler.disassemble(reader, Util.getLineSeparator("""", getJavaProject()), ClassFileBytesDisassembler.WORKING_COPY); //$NON-NLS-1$
-			buffer.setContents(contents);
-		}
+	IBuffer classFileBuffer = this.classFile.getBuffer();
+	if (classFileBuffer != null) {
+		buffer.setContents(classFileBuffer.getCharacters());
+	} else {
+		// Disassemble
+		IClassFileReader reader = ToolFactory.createDefaultClassFileReader(this.classFile, IClassFileReader.ALL);
+		Disassembler disassembler = new Disassembler();
+		String contents = disassembler.disassemble(reader, Util.getLineSeparator("""", getJavaProject()), ClassFileBytesDisassembler.WORKING_COPY); //$NON-NLS-1$
+		buffer.setContents(contents);
 	}
 
 	// add buffer to buffer cache
 	BufferManager bufManager = getBufferManager();
 	bufManager.addBuffer(buffer);
 
 	// listen to buffer changes
 	buffer.addBufferChangedListener(this);
 
 	return buffer;
 }",Buggy,"HEAD - Fixed bug 337935: Test failures when run as an IDE (org.eclipse.sdk.ide)
",Buggy,"The commit message indicates a fix for test failures when run as an IDE. The code diff modifies the `openBuffer` method. The original code created a buffer using `this.owner.createBuffer(this)` and checked if the created buffer was null. If it was null, the method returned null. The modified code replaces `this.owner.createBuffer(this)` with `BufferManager.createBuffer(this)`. The null check is removed. This change suggests that the original buffer creation process was failing in certain IDE environments, leading to test failures. By using `BufferManager.createBuffer(this)` instead, the code likely avoids the problematic buffer creation scenario, resolving the test failures. The removal of the null check also implies a fix, as the code now assumes the buffer creation will always succeed, or that the `BufferManager` handles the failure internally. The answer is **Buggy**."
eclipseJdt,25842.json,4c94b3d7ef7a5503607858476a6477496a59c95b,"@@ -1,5 +1,5 @@
-protected void consumeLambdaHeader() {
-	super.consumeLambdaHeader();
+protected void consumeNestedLambda() {
+	super.consumeNestedLambda();
 	LambdaExpression lexp = (LambdaExpression) this.astStack[this.astPtr];
 	pushOnElementStack(K_LAMBDA_EXPRESSION_DELIMITER, EXPRESSION_BODY, lexp);
 }",Buggy,"Fixed Bug 430026 - [1.8] Lambda parameter has wrong parent if it
declares its type",Buggy,"The commit message indicates a bug fix related to lambda parameters having the wrong parent when their type is declared. The code change replaces `consumeLambdaHeader()` with `consumeNestedLambda()`. This suggests a correction in how lambda expressions are processed, likely to address the reported issue of incorrect parent assignment. The change directly relates to the bug described in the commit message. The answer is **Buggy**."
eclipseJdt,27032.json,b308074bc5b31f3a5857e47f87565296fbba4a15,"@@ -1,34 +1,39 @@
     public static EclipseDeclarationImpl createDeclaration(IBinding binding, BaseProcessorEnv env)
     {
         if(binding == null) return null;
        
         switch(binding.getKind())
         {
     	case IBinding.TYPE:
     		final ITypeBinding typeBinding = (ITypeBinding)binding;
         	if( typeBinding.isAnonymous() || typeBinding.isArray() || 
     			typeBinding.isWildcardType() || typeBinding.isPrimitive() )       
                 throw new IllegalStateException(""failed to create declaration from "" + binding); //$NON-NLS-1$
         	if( typeBinding.isTypeVariable() )
 	        	return new TypeParameterDeclarationImpl(typeBinding, env);
         	else
         		return createReferenceType(typeBinding, env);
         case IBinding.VARIABLE:
         	final IVariableBinding varBinding = (IVariableBinding)binding;            
             if(varBinding.isEnumConstant())
                 return new EnumConstantDeclarationImpl(varBinding, env);
             else
                 return new FieldDeclarationImpl(varBinding, env);
         case IBinding.METHOD:
             final IMethodBinding method = (IMethodBinding)binding;
             if( method.isConstructor() )
                 return new ConstructorDeclarationImpl(method, env);
             final ITypeBinding declaringType = method.getDeclaringClass();
             if( declaringType != null && declaringType.isAnnotation() )
                 return new AnnotationElementDeclarationImpl(method, env);
             else
-                return new MethodDeclarationImpl(method, env);             
+                return new MethodDeclarationImpl(method, env);
+        case IBinding.PACKAGE:
+            // https://bugs.eclipse.org/bugs/show_bug.cgi?id=352949
+            // Don't throw an exception, but just return null. 
+        	// apt also doesn't return a value
+        	return null;
         default:
             throw new IllegalStateException(""failed to create declaration from "" + binding); //$NON-NLS-1$
         }     
     }",Buggy,"Fix for bug 352949: Impossible to process package-level annotations with
an AnnotationProcessor",Buggy,"The commit message indicates a fix for bug 352949, which involves processing package-level annotations with an AnnotationProcessor. The code changes add a case for `IBinding.PACKAGE` in the switch statement. Instead of throwing an exception, it now returns `null` when encountering a package binding. This aligns with the bug fix described in the commit message. The answer is **Buggy**."
eclipseJdt,14260.json,ea916b0462fec6996e619f84c30f348e2877686a,"@@ -1,15 +1,23 @@
 	protected int retrieveEndOfRightParenthesisPosition(int start, int end) {
 		this.scanner.resetTo(start, end);
 		try {
 			int token;
+			int count = 0;
 			while ((token = this.scanner.getNextToken()) != TerminalTokens.TokenNameEOF) {
 				switch(token) {
 					case TerminalTokens.TokenNameRPAREN:
-						return this.scanner.currentPosition;
+						count--;
+						if (count <= 0) return this.scanner.currentPosition;
+						 break;
+					case TerminalTokens.TokenNameLPAREN:
+						count++;
+						//$FALL-THROUGH$
+					default:
+						break;
 				}
 			}
 		} catch(InvalidInputException e) {
 			// ignore
 		}
 		return -1;
 	}",Buggy,"Fix for bug 399600 - ASTConverter#retrieveEndOfRightParenthesisPosition
fails for certain cases",Buggy,The commit message indicates a fix for bug 399600 related to `ASTConverter#retrieveEndOfRightParenthesisPosition`. The code diff introduces a counter to keep track of nested parentheses. The logic now correctly identifies the matching right parenthesis by decrementing the counter for each right parenthesis and returning the position only when the counter reaches zero or less. The counter is incremented when a left parenthesis is encountered. This change addresses the issue where the method would incorrectly return the position of a right parenthesis within a nested expression. The answer is **Buggy**.
eclipseJdt,11577.json,3d11e595fd95f6b5ceb0fd10d1d7aa7d98828a7b,"@@ -1,56 +1,68 @@
 public void generateSyntheticEnclosingInstanceValues(BlockScope currentScope, ReferenceBinding targetType, Expression enclosingInstance, ASTNode invocationSite) {
 	// supplying enclosing instance for the anonymous type's superclass
 	ReferenceBinding checkedTargetType = targetType.isAnonymousType() ? (ReferenceBinding)targetType.superclass().erasure() : targetType;
 	boolean hasExtraEnclosingInstance = enclosingInstance != null;
 	if (hasExtraEnclosingInstance
 			&& (!checkedTargetType.isNestedType() || checkedTargetType.isStatic())) {
 		currentScope.problemReporter().unnecessaryEnclosingInstanceSpecification(enclosingInstance, checkedTargetType);
 		return;
 	}
 
 	// perform some emulation work in case there is some and we are inside a local type only
 	ReferenceBinding[] syntheticArgumentTypes;
 	if ((syntheticArgumentTypes = targetType.syntheticEnclosingInstanceTypes()) != null) {
 
 		ReferenceBinding targetEnclosingType = checkedTargetType.enclosingType();
 		long compliance = currentScope.compilerOptions().complianceLevel;
 
 		// deny access to enclosing instance argument for allocation and super constructor call (if 1.4)
 		// always consider it if complying to 1.5
 		boolean denyEnclosingArgInConstructorCall;
 		if (compliance <= ClassFileConstants.JDK1_3) {
 			denyEnclosingArgInConstructorCall = invocationSite instanceof AllocationExpression;
 		} else if (compliance == ClassFileConstants.JDK1_4){
 			denyEnclosingArgInConstructorCall = invocationSite instanceof AllocationExpression
 				|| invocationSite instanceof ExplicitConstructorCall && ((ExplicitConstructorCall)invocationSite).isSuperAccess();
-		} else {
+		} else if (compliance < ClassFileConstants.JDK1_7) {
 			//compliance >= JDK1_5
 			denyEnclosingArgInConstructorCall = (invocationSite instanceof AllocationExpression
 					|| invocationSite instanceof ExplicitConstructorCall && ((ExplicitConstructorCall)invocationSite).isSuperAccess())
 				&& !targetType.isLocalType();
+		} else {
+			//compliance >= JDK1_7
+			if (invocationSite instanceof AllocationExpression) {
+				denyEnclosingArgInConstructorCall = !targetType.isLocalType();
+			} else if (invocationSite instanceof ExplicitConstructorCall && 
+					((ExplicitConstructorCall)invocationSite).isSuperAccess()) {
+				MethodScope enclosingMethodScope = currentScope.enclosingMethodScope();
+				denyEnclosingArgInConstructorCall = !targetType.isLocalType() && enclosingMethodScope != null
+						&& enclosingMethodScope.isConstructorCall; 
+			} else {
+				denyEnclosingArgInConstructorCall = false;
+			}
 		}
 
 		boolean complyTo14 = compliance >= ClassFileConstants.JDK1_4;
 		for (int i = 0, max = syntheticArgumentTypes.length; i < max; i++) {
 			ReferenceBinding syntheticArgType = syntheticArgumentTypes[i];
 			if (hasExtraEnclosingInstance && TypeBinding.equalsEquals(syntheticArgType, targetEnclosingType)) {
 				hasExtraEnclosingInstance = false;
 				enclosingInstance.generateCode(currentScope, this, true);
 				if (complyTo14){
 					dup();
 					invokeObjectGetClass(); // will perform null check
 					pop();
 				}
 			} else {
 				Object[] emulationPath = currentScope.getEmulationPath(
 						syntheticArgType,
 						false /*not only exact match (that is, allow compatible)*/,
 						denyEnclosingArgInConstructorCall);
 				generateOuterAccess(emulationPath, invocationSite, syntheticArgType, currentScope);
 			}
 		}
 		if (hasExtraEnclosingInstance){
 			currentScope.problemReporter().unnecessaryEnclosingInstanceSpecification(enclosingInstance, checkedTargetType);
 		}
 	}
 }",Buggy,"Fixed bug 373371 - [compiler] JDT Compiler reports an error whereas
javac compiles without problem

Change-Id: I1ca89bd73228aa19ee351e12ef64c474cbc58175
",Buggy,"The commit message indicates a bug fix where the JDT compiler incorrectly reports an error compared to javac. The code changes involve adjustments to the logic that determines when enclosing instance arguments are denied in constructor calls, specifically for compliance levels JDK1_7 and above. The changes introduce additional checks based on the type of invocation site (AllocationExpression or ExplicitConstructorCall) and the scope of the enclosing method. This suggests that the original logic had a flaw that caused incorrect error reporting, which is now addressed by the updated conditions. The answer is **Buggy**."
eclipseJdt,877.json,48d4662ccc94304b71283abf02dc80106ec8ea24,"@@ -1,21 +1,22 @@
 private void readIndexMap() {
 	try {
 		char[] indexMaps = org.eclipse.jdt.internal.compiler.util.Util.getFileCharContent(this.indexNamesMapFile, null);
 		char[][] names = CharOperation.splitOn('\n', indexMaps);
 		if (names.length >= 3) {
 			// First line is DiskIndex signature (see writeIndexMapFile())
 			String savedSignature = DiskIndex.SIGNATURE;
 			if (savedSignature.equals(new String(names[0]))) {
 				for (int i = 1, l = names.length-1 ; i < l ; i+=2) {
 					IndexLocation indexPath = IndexLocation.createIndexLocation(new URL(new String(names[i])));
+					if (indexPath == null) continue;
 					this.indexLocations.put(new Path(new String(names[i+1])), indexPath );
 					this.indexStates.put(indexPath, REUSE_STATE);
 				}
 			}		
 		}
 	} catch (IOException ignored) {
 		if (VERBOSE)
 			Util.verbose(""Failed to read saved index file names""); //$NON-NLS-1$
 	}
 	return;
 }",Buggy,"Fix for Bug 420590
JavaIndexTests.testExistentIndexAfterRestart failed with error: NPE in
SimpleLookupTable.put
",Buggy,"The commit message indicates a fix for a specific bug (420590) that causes a NullPointerException (NPE) in `SimpleLookupTable.put` during the `JavaIndexTests.testExistentIndexAfterRestart` test. The code diff introduces a null check for `indexPath` after it's created using `IndexLocation.createIndexLocation`. If `createIndexLocation` returns null, the code now skips adding the null `indexPath` to the `indexLocations` and `indexStates` maps, preventing the potential NPE when `put` is called on these maps with a null value. This directly addresses the bug described in the commit message. The answer is **Buggy**."
eclipseJdt,1436.json,0f1f2b20ab9c02179f9ab0cd2826219f39372374,"@@ -1,6 +1,8 @@
 public int match(ReferenceExpression node, MatchingNodeSet nodeSet) {
 	if (!this.pattern.findReferences) return IMPOSSIBLE_MATCH;
 	if (!matchesName(this.pattern.selector, node.selector)) return IMPOSSIBLE_MATCH;
+	if (node.selector != null &&  Arrays.equals(node.selector, org.eclipse.jdt.internal.compiler.codegen.ConstantPool.Init))
+		return IMPOSSIBLE_MATCH; // :: new
 	nodeSet.mustResolve = true;
 	return nodeSet.addMatch(node, this.pattern.mustResolve ? POSSIBLE_MATCH : ACCURATE_MATCH);
 }",Buggy,"Fix for [Bug][458614][1.8][search] Constructor reference not found in
search",Buggy,"The commit message indicates a bug fix related to constructor references in search functionality. The code diff adds a condition to `match` method that checks if the `node.selector` is equal to `org.eclipse.jdt.internal.compiler.codegen.ConstantPool.Init`. If it is, the method returns `IMPOSSIBLE_MATCH`. This suggests that the original code was incorrectly identifying constructor references ("":: new"") as matches in certain search scenarios. The added condition filters out these incorrect matches, thus fixing the bug. The answer is **Buggy**."
eclipseJdt,23118.json,01fe9b211e794ff01834ebe0e40bd02cb1110d75,"@@ -1,30 +1,39 @@
 public static char[] getSignatureSimpleName(char[] typeSignature) {
 	if(typeSignature == null) return CharOperation.NO_CHAR;
 
 	char[] qualifiedType = Signature.toCharArray(typeSignature);
 
 	int dotCount = 0;
 	indexFound: for(int i = 0; i < typeSignature.length; i++) {
 		switch(typeSignature[i]) {
 			case C_DOT:
 				dotCount++;
 				break;
 			case C_GENERIC_START:
 				break indexFound;
 			case C_DOLLAR:
 				break indexFound;
 		}
 	}
 
 	if(dotCount > 0) {
+		int typeStart = 0;
 		for(int i = 0; i < qualifiedType.length; i++) {
-			if(qualifiedType[i] == '.') {
-				dotCount--;
+			switch (qualifiedType[i]) {
+				case '.':
+					dotCount--;
+					break;
+				case ' ':
+					typeStart = i+1;
+					break;
 			}
 			if(dotCount <= 0) {
-				return CharOperation.subarray(qualifiedType, i + 1, qualifiedType.length);
+				char[] simpleName = CharOperation.subarray(qualifiedType, i + 1, qualifiedType.length);
+				if (typeStart > 0 && typeStart < qualifiedType.length)
+					return CharOperation.concat(CharOperation.subarray(qualifiedType, 0, typeStart), simpleName);
+				return simpleName;
 			}
 		}
 	}
 	return qualifiedType;
 }",Buggy,"Bug 463533 - Signature.getSignatureSimpleName() returns different
results for resolved and unresolved extends

Change-Id: Iaa8b4273dab596b84734326ad99f775bbe74a330
Signed-off-by: Stephan Herrmann <stephan.herrmann@berlin.de>",Buggy,"The commit message indicates a bug fix related to `Signature.getSignatureSimpleName()` returning different results for resolved and unresolved extends. The code changes involve adjusting the logic for extracting the simple name from a type signature, specifically when dots are present. The addition of handling spaces in qualifiedType suggests a scenario where spaces might be included, affecting the simple name extraction. The concatenation logic ensures that any leading type information before the space is correctly combined with the extracted simple name. This indicates a bug fix. The answer is **Buggy**."
eclipseJdt,26368.json,04a3935ecc2f9e24622bcfddda7aa95a0643fdd7,"@@ -1 +1,10 @@
-    public String toString(){ return _arrayBinding.toString(); }

+    public String toString(){ 
+    	final ITypeBinding elementType = _arrayBinding.getElementType();
+    	final StringBuilder buffer = new StringBuilder();
+    	String name = elementType.getQualifiedName();
+    	buffer.append(name);
+		for( int i=0, dim = _arrayBinding.getDimensions(); i<dim; i++ )
+			buffer.append(""[]""); //$NON-NLS-1$
+		
+		return buffer.toString();
+    }",Buggy,"CR244832 | Annotation member that returns Class[] produces error in generated ControlBean

fixed ArrayType toString()

drt: apt.core
",Buggy,"The commit message indicates a fix for an error related to annotation members returning `Class[]` and causing issues in generated `ControlBean`. The diff shows a change in the `toString()` method of `ArrayType`. The original method simply returned the `toString()` representation of `_arrayBinding`. The modified method now constructs a string representation of the array type by getting the element type's qualified name and appending ""[]"" for each dimension of the array. This change likely addresses a bug where the original `toString()` method was not correctly representing array types, especially `Class[]`, leading to errors in the generated `ControlBean`. The new implementation provides a more accurate and informative string representation of the array type. The answer is **Buggy**."
eclipseJdt,1082.json,41e3b1756b128250deb0f9b82843aa520278c691,"@@ -1,53 +1,53 @@
 void matchReportReference(Expression expr, int lastIndex, TypeBinding refBinding, MatchLocator locator) throws CoreException {
 
 	// Look if there's a need to special report for parameterized type
 	if (refBinding.isParameterizedType() || refBinding.isRawType()) {
 
 		// Try to refine accuracy
 		ParameterizedTypeBinding parameterizedBinding = (ParameterizedTypeBinding)refBinding;
 		updateMatch(parameterizedBinding, this.pattern.getTypeArguments(), this.pattern.hasTypeParameters(), 0, locator);
 		
 		// See whether it is necessary to report or not
 		if (match.getRule() == 0) return; // impossible match
 		boolean report = (this.isErasureMatch && match.isErasure()) || (this.isEquivalentMatch && match.isEquivalent()) || match.isExact();
 		if (!report) return;
 
 		// Make a special report for parameterized types if necessary
 		 if (refBinding.isParameterizedType() && this.pattern.hasTypeArguments())  {
 			TypeReference typeRef = null;
 			TypeReference[] typeArguments = null;
 			if (expr instanceof ParameterizedQualifiedTypeReference) {
 				typeRef = (ParameterizedQualifiedTypeReference) expr;
 				typeArguments = ((ParameterizedQualifiedTypeReference) expr).typeArguments[lastIndex];
 			}
 			else if (expr instanceof ParameterizedSingleTypeReference) {
 				typeRef = (ParameterizedSingleTypeReference) expr;
 				typeArguments = ((ParameterizedSingleTypeReference) expr).typeArguments;
 			}
 			if (typeRef != null) {
 				locator.reportAccurateParameterizedTypeReference(match, typeRef, lastIndex, typeArguments);
 				return;
 			}
 		}
 	} else if (this.pattern.hasTypeArguments()) { // binding has no type params, compatible erasure if pattern does
 		match.setRule(SearchPattern.R_ERASURE_MATCH);
 	}
 
 	// Report match
 	if (expr instanceof ArrayTypeReference) {
 		locator.reportAccurateTypeReference(match, expr, this.pattern.simpleName);
 		return;
 	}
 	if (refBinding.isLocalType()) {
-		// see bug https://bugs.eclipse.org/bugs/show_bug.cgi?id=84049
+		// see bug https://bugs.eclipse.org/bugs/show_bug.cgi?id=82673
 		LocalTypeBinding local = (LocalTypeBinding) refBinding;
 		IJavaElement focus = ((InternalSearchPattern)pattern).focus;
 		if (focus != null && local.enclosingMethod != null && focus.getParent().getElementType() == IJavaElement.METHOD) {
 			IMethod method = (IMethod) focus.getParent();
 			if (!CharOperation.equals(local.enclosingMethod.selector, method.getElementName().toCharArray())) {
 				return;
 			}
 		}
 	}
 	locator.report(match);
 }",Buggy,"Fix bug reference
",Buggy,"The commit message ""Fix bug reference"" indicates a bug fix. The code diff includes a comment referencing a bug ID (""see bug https://bugs.eclipse.org/bugs/show_bug.cgi?id=82673""). The code within the `if (refBinding.isLocalType())` block appears to be addressing a specific scenario related to local types and method names, likely to resolve the issue described in the linked bug report. Therefore, the changes are related to fixing a bug. The answer is **Buggy**."
eclipseJdt,25078.json,71d81627399572ca242308eb9b1f8436f445d7df,"@@ -1,9 +1,9 @@
 	public TypeBinding resolveType(BlockScope scope) {
 		TypeBinding type = super.resolveType(scope);
-		if (type instanceof PolyTypeBinding)
+		if (type == null || type instanceof ProblemReferenceBinding || type instanceof PolyTypeBinding)
 			return type;
 		MethodBinding method = getMethodBinding();
 		if (method != null && method.isValidBinding() && !method.isSynthetic())
 			throw new SelectionNodeFound(this.actualMethodBinding);
 		throw new SelectionNodeFound();
 	}",Buggy,"Fixed Bug 440731 - [1.8][code select] Hover, F3 doesn't work for method
reference in method invocation of overloaded method

Signed-off-by: shankha banerjee <shankhba@in.ibm.com>",Buggy,"The commit message indicates a fix for a bug where hover and F3 functionality (likely related to code selection and navigation in an IDE) were not working correctly for method references in method invocations of overloaded methods. The code diff modifies the `resolveType` method. The change adds a null check `type == null` and a check for `type instanceof ProblemReferenceBinding` before proceeding. If `type` is null or a `ProblemReferenceBinding`, the method returns `type` immediately. This suggests that the original code was throwing an exception when `type` was null or a `ProblemReferenceBinding`, which would prevent the hover/F3 functionality from working correctly. The addition of these checks prevents the exception from being thrown in these cases, thus fixing the bug. The answer is **Buggy**."
eclipseJdt,25078.json,a5e431ef5dd3e4ca66d16fb63b6f792ceeb0643b,"@@ -1,7 +1,9 @@
 	public TypeBinding resolveType(BlockScope scope) {
-		super.resolveType(scope);
+		TypeBinding type = super.resolveType(scope);
+		if (type instanceof PolyTypeBinding)
+			return type;
 		MethodBinding method = getMethodBinding();
 		if (method != null && method.isValidBinding() && !method.isSynthetic())
 			throw new SelectionNodeFound(this.actualMethodBinding);
 		throw new SelectionNodeFound();
 	}",Buggy,"Fixed Bug 424110 - [1.8][hovering] Hover, F3 does not work for method
reference in method invocation",Buggy,"The commit message indicates a bug fix related to hovering and F3 functionality for method references in method invocations. The code diff modifies the `resolveType` method. It adds a check to see if the resolved type is a `PolyTypeBinding`. If it is, the method returns early. This suggests that the original code was not correctly handling `PolyTypeBinding`s, leading to the bug described in the commit message. The fix ensures that `PolyTypeBinding`s are handled correctly, likely resolving the issue with hovering and F3 functionality. The answer is **Buggy**.
"
eclipseJdt,3790.json,293a53a19363893dc72bcd9c4bf117c4e143553f,"@@ -1,3 +1,3 @@
 	public TypeBinding[] genericTypeArguments() {
-		return null;
+		return this.resolvedTypeArguments;
 	}",Buggy,"Fixed Bug 420580 - [1.8][compiler] ReferenceExpression drops explicit
type arguments",Buggy,"The commit message indicates a bug fix related to ReferenceExpression dropping explicit type arguments in the compiler, specifically for Java 1.8. The diff shows that the `genericTypeArguments()` method, which previously always returned `null`, is now returning `this.resolvedTypeArguments`. This change directly addresses the bug described in the commit message by providing the correct type arguments instead of dropping them. The answer is **Buggy**.
"
eclipseJdt,144.json,63326b7a3054b32190c64b8268487d927040f016,"@@ -1,4 +1,4 @@
 	public Element getEnclosingElement()
 	{
-		return null;
+		return getGenericElement();
 	}",Buggy,"HEAD - Fixed bug 342470: javax.lang.model.element.Element.getEnclosingElement() doesn't return null for type parameter
",Buggy,"The commit message indicates a bug fix related to the `getEnclosingElement()` method in `javax.lang.model.element.Element`. The original implementation always returned `null`, which was incorrect for type parameters. The corrected code now returns `getGenericElement()`, suggesting that the enclosing element should be the generic element associated with the type parameter. This change addresses the bug described in the commit message. The answer is **Buggy**."
eclipseJdt,11825.json,ee5e3843269e8af70318f7535175e87d19c752b4,"@@ -1,10 +1,11 @@
 	public int literalIndexForMethodHandle(MethodBinding binding) {
 		boolean isInterface = binding.declaringClass.isInterface();
 		int referenceKind =
 			isInterface ? binding.isStatic() ? MethodHandleRefKindInvokeStatic : binding.isPrivate() ? MethodHandleRefKindInvokeSpecial : MethodHandleRefKindInvokeInterface
 			: binding.isConstructor() ? MethodHandleRefKindNewInvokeSpecial
 			: binding.isStatic() ? MethodHandleRefKindInvokeStatic
+			: binding.isPrivate() ? MethodHandleRefKindInvokeSpecial
 			: MethodHandleRefKindInvokeVirtual;
 		
 		return literalIndexForMethodHandle(referenceKind, binding.declaringClass, binding.selector, binding.signature(), isInterface);
 	}",Buggy,"Fixed Bug 439889 - [1.8][compiler] [lambda] Deserializing lambda fails
with IllegalArgumentException: ""Invalid lambda deserialization""",Buggy,"The commit message indicates a bug fix related to lambda deserialization in the compiler. The code change introduces a new case for `binding.isPrivate()` within the non-interface condition. This addition suggests a missing condition that was causing incorrect method handle reference kinds to be assigned during lambda deserialization, leading to the reported `IllegalArgumentException`. This correction addresses a logical error in the original code. The answer is **Buggy**.
"
eclipseJdt,11825.json,6a493caae74ed7f294c44634abfb786fa2111d3d,"@@ -1,10 +1,10 @@
 	public int literalIndexForMethodHandle(MethodBinding binding) {
 		boolean isInterface = binding.declaringClass.isInterface();
 		int referenceKind =
-			isInterface ? binding.isStatic() ? MethodHandleRefKindInvokeStatic : MethodHandleRefKindInvokeInterface
+			isInterface ? binding.isStatic() ? MethodHandleRefKindInvokeStatic : binding.isPrivate() ? MethodHandleRefKindInvokeSpecial : MethodHandleRefKindInvokeInterface
 			: binding.isConstructor() ? MethodHandleRefKindNewInvokeSpecial
 			: binding.isStatic() ? MethodHandleRefKindInvokeStatic
 			: MethodHandleRefKindInvokeVirtual;
 		
 		return literalIndexForMethodHandle(referenceKind, binding.declaringClass, binding.selector, binding.signature(), isInterface);
 	}",Buggy,"Fixed Bug 421797 - [1.8][compiler] ClassFormatError with default 
methods & I.super.foo() syntax ",Buggy,"The commit message indicates a bug fix related to `ClassFormatError` in the compiler when using default methods and the `I.super.foo()` syntax. The code change modifies the `literalIndexForMethodHandle` method to correctly determine the `referenceKind` for method handles, specifically when dealing with interfaces. The original code only considered whether the method was static to determine the reference kind for interfaces, but the updated code now also checks if the method is private. If it is, `MethodHandleRefKindInvokeSpecial` is used; otherwise, `MethodHandleRefKindInvokeInterface` is used. This distinction is crucial for correctly handling default methods called via `I.super.foo()`, which are treated as special invocations. This change directly addresses the bug described in the commit message. The answer is **Buggy**."
eclipseJdt,11825.json,ffd96339198fcdfc38a0c44d4da9dfd0a59d21fc,"@@ -1,10 +1,10 @@
 	public int literalIndexForMethodHandle(MethodBinding binding) {
 		boolean isInterface = binding.declaringClass.isInterface();
 		int referenceKind =
-			isInterface ? MethodHandleRefKindInvokeInterface
+			isInterface ? binding.isStatic() ? MethodHandleRefKindInvokeStatic : MethodHandleRefKindInvokeInterface
 			: binding.isConstructor() ? MethodHandleRefKindNewInvokeSpecial
 			: binding.isStatic() ? MethodHandleRefKindInvokeStatic
 			: MethodHandleRefKindInvokeVirtual;
 		
 		return literalIndexForMethodHandle(referenceKind, binding.declaringClass, binding.selector, binding.signature(), isInterface);
 	}",Buggy,"Fixed Bug 421712 - [1.8][compiler] java.lang.NoSuchMethodError with
lambda expression in interface default method.",Buggy,"The commit message indicates a fix for a `NoSuchMethodError` related to lambda expressions in interface default methods. The code change modifies the `literalIndexForMethodHandle` method to correctly determine the `referenceKind` for method handles. Specifically, it adds a check to see if the method is static when the declaring class is an interface. If the method is static, it uses `MethodHandleRefKindInvokeStatic`; otherwise, it uses `MethodHandleRefKindInvokeInterface`. This distinction is crucial for correctly resolving method handles for lambda expressions in interfaces, which could lead to a `NoSuchMethodError` if the wrong reference kind is used. The answer is **Buggy**."
eclipseJdt,27787.json,6c76d390a016d2bc042688766f03adca637f8318,"@@ -1,4 +1,9 @@
 	public int run(InputStream in, OutputStream out, OutputStream err, String... arguments) {
-		boolean succeed = new Main(new PrintWriter(new OutputStreamWriter(out)), new PrintWriter(new OutputStreamWriter(err)), true/*systemExit*/, null/*options*/, null/*progress*/).compile(arguments);
+		boolean succeed = new Main(
+				new PrintWriter(new OutputStreamWriter(out != null ? out : System.out)),
+				new PrintWriter(new OutputStreamWriter(err != null ? err : System.err)),
+				true/* systemExit */,
+				null/* options */,
+				null/* progress */).compile(arguments);
 		return succeed ? 0 : -1;
 	}",Buggy,"Fixed Bug 426434 - EclipseCompiler#run() crashes when null out/err
passed by client",Buggy,The commit message indicates a fix for a crash in `EclipseCompiler#run()` when null `out` or `err` streams are passed. The diff modifies the constructor call to `Main` to handle null `out` and `err` streams by defaulting to `System.out` and `System.err` respectively. This prevents a `NullPointerException` and resolves the crash. The answer is **Buggy**.
eclipseJdt,16316.json,f372eb4169324cad87b7cff6d25b5939cf1574c9,"@@ -1,4 +1,4 @@
 	public boolean isLocalTypeDeclaration() {
 		ASTNode parent = getParent();
-		return (parent instanceof Block);
+		return (parent instanceof TypeDeclarationStatement);
 	}",Buggy,"Fix TypeDeclaration.isLocalTypeDeclaration (bug #10468)
",Buggy,"The commit message indicates a bug fix in `TypeDeclaration.isLocalTypeDeclaration`. The diff shows that the method `isLocalTypeDeclaration` is changed to return true if the parent is an instance of `TypeDeclarationStatement` instead of `Block`. This change likely corrects the logic for determining if a type declaration is local, indicating a bug fix. The answer is **Buggy**.
"
eclipseJdt,5374.json,8739fac6b0362a32fbe52c714ff661e202a84e1e,"@@ -1,31 +1,31 @@
 public void resolve(BlockScope upperScope) {
 	// special scope for secret locals optimization.
 	this.scope = new BlockScope(upperScope);
 	TypeBinding type = this.expression.resolveType(this.scope);
-	if (type == null)
-		return;
-	switch (type.id) {
-		case T_boolean :
-		case T_char :
-		case T_float :
-		case T_double :
-		case T_byte :
-		case T_short :
-		case T_int :
-		case T_long :
-			this.scope.problemReporter().invalidTypeToSynchronize(this.expression, type);
-			break;
-		case T_void :
-			this.scope.problemReporter().illegalVoidExpression(this.expression);
-			break;
-		case T_null :
-			this.scope.problemReporter().invalidNullToSynchronize(this.expression);
-			break;
+	if (type != null) {
+		switch (type.id) {
+			case T_boolean :
+			case T_char :
+			case T_float :
+			case T_double :
+			case T_byte :
+			case T_short :
+			case T_int :
+			case T_long :
+				this.scope.problemReporter().invalidTypeToSynchronize(this.expression, type);
+				break;
+			case T_void :
+				this.scope.problemReporter().illegalVoidExpression(this.expression);
+				break;
+			case T_null :
+				this.scope.problemReporter().invalidNullToSynchronize(this.expression);
+				break;
+			}
+			//continue even on errors in order to have the TC done into the statements
+			this.synchroVariable = new LocalVariableBinding(SecretLocalDeclarationName, type, ClassFileConstants.AccDefault, false);
+			this.scope.addLocalVariable(this.synchroVariable);
+			this.synchroVariable.setConstant(Constant.NotAConstant); // not inlinable
+			this.expression.computeConversion(this.scope, type, type);
 	}
-	//continue even on errors in order to have the TC done into the statements
-	this.synchroVariable = new LocalVariableBinding(SecretLocalDeclarationName, type, ClassFileConstants.AccDefault, false);
-	this.scope.addLocalVariable(this.synchroVariable);
-	this.synchroVariable.setConstant(Constant.NotAConstant); // not inlinable
-	this.expression.computeConversion(this.scope, type, type);
 	this.block.resolveUsing(this.scope);
 }",Buggy,"Bug 465048 - Bindings are null inside synchronized blocks

Resolve the block-part of a synchronized block, even if the synchronized
variable can't be resolved.

Change-Id: I3ed513fa21b10ae9c7fa39f1a94fe85fb98e5eb9
Signed-off-by: Carmi Grushko <carmi@google.com>
",Buggy,"The commit message indicates a bug fix where bindings were null inside synchronized blocks. The code change modifies the `resolve` method to handle cases where the synchronized variable cannot be resolved. Specifically, it moves the type checking and synchroVariable creation inside an `if (type != null)` block. This ensures that the block part of the synchronized block is resolved even if the synchronized variable cannot be resolved, which aligns with the bug fix described in the commit message. The answer is **Buggy**."
eclipseJdt,25918.json,8986600df161cd7584749fb6a28497a308fa862c,"@@ -1,5 +1,3 @@
 public boolean requireExtendedRecovery() {
-	if (this.assistNode instanceof TypeReference || this.assistNode instanceof CompletionOnKeyword2)
-		return false;
 	return lastIndexOfElement(K_LAMBDA_EXPRESSION_DELIMITER) >= 0;
 }",Buggy,"Fixed Bug 427463 - [1.8][content assist] No completions available in
throw statement within lambda body ",Buggy,"The commit message indicates a bug fix related to content assist in throw statements within lambda bodies. The code diff removes a condition that checks if the assistNode is an instance of TypeReference or CompletionOnKeyword2. This change likely enables content assist in throw statements within lambda expressions by removing a restriction based on the type of the assist node. The answer is **Buggy**.
"
eclipseJdt,17024.json,c554caa948bbe051a5aeee635338c74832d8067f,"@@ -1,16 +1,15 @@
 	public static Token fromCurrent(Scanner scanner, int currentToken) {
 		int start = scanner.getCurrentTokenStartPosition();
 		int end = scanner.getCurrentTokenEndPosition();
 		if (currentToken == TokenNameCOMMENT_LINE) {
 			// don't include line separator
-			String source = scanner.getCurrentTokenString();
-			for (int i = source.length() - 1; i > 0; i--) {
-				char c = source.charAt(i);
+			while(end >= start) {
+				char c = scanner.source[end];
 				if (c != '\r' && c != '\n')
 					break;
 				end--;
 			}
 		}
 		Token token = new Token(start, end, currentToken);
 		return token;
 	}",Buggy,"Fixed bug 471090: Java Code Formatter breaks code if single line
comments contains unicode escape

Change-Id: Id2c901c7853e4062182163ef714afc0e43833baf
Signed-off-by: Till Brychcy <register.eclipse@brychcy.de>
",Buggy,"The commit message indicates a fix for bug 471090, where the Java code formatter incorrectly handles single-line comments containing Unicode escapes. The diff modifies the `fromCurrent` method to adjust the end position of the comment token. Instead of relying on the token string, the code now iterates backward from the original end position using the scanner's source array to exclude line separators (`\r` and `\n`). This suggests that the original code was incorrectly including line separators in the comment token, leading to formatting issues when Unicode escapes were present. The change ensures that only the comment text is included in the token, resolving the bug. The answer is **Buggy**."
eclipseJdt,1305.json,63969f0e42bcdfc0a055ac0f3d2f47fa48cf3e29,"@@ -1,7 +1,9 @@
 protected void consumeCastExpressionLL1WithBounds() {
 	super.consumeCastExpressionLL1WithBounds();
 	if ((this.patternFineGrain & IJavaSearchConstants.CAST_TYPE_REFERENCE) != 0) {
 		CastExpression castExpression = (CastExpression) this.expressionStack[this.expressionPtr];
-		this.patternLocator.match(castExpression.type, this.nodeSet);
+		TypeReference[] typeReferences = ((IntersectionCastTypeReference) castExpression.type).typeReferences;
+		for (int i = 0, length = typeReferences.length; i < length; i++)
+			this.patternLocator.match(typeReferences[i], this.nodeSet);
 	}
 }",Buggy,"Fixed Bug 427537 - [1.8][search] CCE with search match location set to
cast type and intersection casts",Buggy,"The commit message indicates a bug fix related to a ClassCastException (CCE) when searching for cast type references, specifically with intersection casts in Java 1.8. The code diff shows a modification within the `consumeCastExpressionLL1WithBounds` method. The original code attempts to directly match the cast expression's type. However, the corrected code now handles `IntersectionCastTypeReference` by iterating through its `typeReferences` array and matching each individual type reference. This suggests that the original code failed to properly handle intersection casts, leading to the CCE. The fix addresses this by correctly processing the individual type references within the intersection cast. The answer is **Buggy**."
eclipseJdt,17644.json,f7608e255b7a2e7cde81de854a6575f0fb055925,"@@ -1,20 +1,20 @@
 public char[] getMainTypeName() {
 	if (this.mainTypeName == null) {
 		int start = CharOperation.lastIndexOf('/', this.fileName) + 1;
 		if (start == 0 || start < CharOperation.lastIndexOf('\\', this.fileName))
 			start = CharOperation.lastIndexOf('\\', this.fileName) + 1;
-		int separator = CharOperation.indexOf('|', this.fileName) + 1;
+		int separator = CharOperation.lastIndexOf('|', this.fileName) + 1;
 		if (separator > start) // case of a .class file in a default package in a jar
 			start = separator;
 
 		int end = CharOperation.lastIndexOf('$', this.fileName);
 		if (end == -1 || !Util.isClassFileName(this.fileName)) {
 			end = CharOperation.lastIndexOf('.', this.fileName);
 			if (end == -1)
 				end = this.fileName.length;
 		}
 
 		this.mainTypeName = CharOperation.subarray(this.fileName, start, end);
 	}
 	return this.mainTypeName;
 }",Buggy,"Fix for bug 515484 [1.9] [dom ast] ast not generated for
module-info.class  ",Buggy,"The commit message indicates a fix for bug 515484, specifically related to the AST generation for `module-info.class` files. The code diff modifies the `getMainTypeName()` method, which extracts the main type name from a file name. The change involves using `lastIndexOf` instead of `indexOf` to locate the separator `|`. This suggests that the original code might have incorrectly identified the separator in certain cases, leading to an incorrect type name being extracted, and thus a bug in AST generation. The answer is **Buggy**.
"
eclipseJdt,19098.json,302894c7bc98fd880776f5e317788c7b3da88ebe,"@@ -1,23 +1,23 @@
 protected void recordParticipantResult(CompilationParticipantResult result) {
 	// any added/changed/deleted generated files have already been taken care
 	// just record the problems and dependencies - do not expect there to be many
 	// must be called after we're finished with the compilation unit results but before incremental loop adds affected files
 	CategorizedProblem[] problems = result.problems;
 	if (problems != null && problems.length > 0) {
 		// existing problems have already been removed so just add these as new problems
 		this.notifier.updateProblemCounts(problems);
 		try {
 			storeProblemsFor(result.sourceFile, problems);
 		} catch (CoreException e) {
 			// must continue with compile loop so just log the CoreException
 			Util.log(e, ""JavaBuilder logging CompilationParticipant's CoreException to help debugging""); //$NON-NLS-1$
 		}
 	}
 
 	String[] dependencies = result.dependencies;
 	if (dependencies != null) {
-		ReferenceCollection refs = (ReferenceCollection) this.newState.references.get(result.sourceFile.typeLocator());
+		ReferenceCollection refs = this.newState.references.get(result.sourceFile.typeLocator());
 		if (refs != null)
 			refs.addDependencies(dependencies);
 	}
 }",Buggy,"Bug 563030 - builder.State: SimpleLookupTable performance does not
scale

- resolve new warning

Change-Id: I20cd500a7872eaa5cbda532f8f316d1a4168a3d9
",Buggy,"The commit message indicates a bug fix related to performance scaling in `builder.State: SimpleLookupTable`. The diff shows a change in the `recordParticipantResult` method where a cast to `ReferenceCollection` is removed. This suggests that the type of `this.newState.references.get(result.sourceFile.typeLocator())` was not always `ReferenceCollection`, leading to a potential `ClassCastException` or incorrect behavior. Removing the cast likely resolves this type mismatch, thus fixing a bug. The answer is **Buggy**."
eclipseJdt,25968.json,15950deb058261ec048eb3ddbd1c2ef13df604e6,"@@ -1,29 +1,30 @@
 private CodeSnippetToCuMapper getMapper() {
 	if (this.mapper == null) {
 		char[] varClassName = null;
 		VariablesInfo installedVars = this.context.installedVars;
 		if (installedVars != null) {
 			char[] superPackageName = installedVars.packageName;
 			if (superPackageName != null && superPackageName.length != 0) {
 				varClassName = CharOperation.concat(superPackageName, installedVars.className, '.');
 			} else {
 				varClassName = installedVars.className;
 			}
 
 		}
 		this.mapper = new CodeSnippetToCuMapper(
 			this.codeSnippet,
 			this.context.packageName,
 			this.context.imports,
 			getClassName(),
 			varClassName,
 			this.context.localVariableNames,
 			this.context.localVariableTypeNames,
 			this.context.localVariableModifiers,
 			this.context.declaringTypeName,
-			this.context.lineSeparator
+			this.context.lineSeparator,
+			CompilerOptions.versionToJdkLevel(this.options.get(JavaCore.COMPILER_COMPLIANCE))
 		);
 
 	}
 	return this.mapper;
 }",Buggy,"HEAD - Fixed bug 345334: CodeSnippet's run method is missing @Override annotation
",NotBuggy,"The commit message indicates a bug fix related to a missing `@Override` annotation in the `run` method of the `CodeSnippet` class. However, the provided diff does not show any changes to the `run` method or the addition of the `@Override` annotation. Instead, the diff focuses on the `getMapper` method, which seems unrelated to the stated bug fix. Therefore, based on the provided diff, it's impossible to confirm the bug fix described in the commit message. The answer is **NotBuggy**."
eclipseJdt,4105.json,35e2019a40c179f5dd02638e5bff541c41227721,"@@ -1,6 +1,9 @@
 	void recordResolution(LookupEnvironment env, TypeBinding typeFound) {
-		if (typeFound != null && typeFound.isValidBinding())
-			for (int i = 0; i < env.resolutionListeners.length; i++) {
-				env.resolutionListeners[i].recordResolution(this, typeFound);
+		if (typeFound != null && typeFound.isValidBinding()) {
+			synchronized (env.root) {
+				for (int i = 0; i < env.root.resolutionListeners.length; i++) {
+					env.root.resolutionListeners[i].recordResolution(this, typeFound);
+				}
 			}
+		}
 	}",Buggy,"Fix for bug 519980 [9][search] search for a type does not return the
matches in provides 

Change-Id: I0059fa02a1040f10a8f4a949c07806018dee73b5",Buggy,"The commit message indicates a fix for bug 519980 related to search functionality not returning matches in provides. The code diff modifies the `recordResolution` method. It adds a synchronization block around the loop that iterates through `resolutionListeners`. Also, it changes `env.resolutionListeners` to `env.root.resolutionListeners`. This change suggests that the original code might have been accessing the listeners in a non-thread-safe manner, potentially leading to incorrect or incomplete search results. The synchronization block ensures that only one thread can access and modify the listeners at a time, preventing race conditions. The change from `env.resolutionListeners` to `env.root.resolutionListeners` suggests that the listeners were not being accessed from the correct context. These changes indicate a bug fix related to thread safety and incorrect context. The answer is **Buggy**."
eclipseJdt,11564.json,266fb0d2bc58e2f4f179d2c1279dee095ed53e45,"@@ -1,6 +1,6 @@
 public void generateSyntheticBodyForArrayConstructor(SyntheticMethodBinding methodBinding) {
 	initializeMaxLocals(methodBinding);
 	iload_0();
-	anewarray(((ArrayBinding) methodBinding.returnType).elementsType());
+	newArray(null, null, (ArrayBinding) methodBinding.returnType);
 	areturn();
 }",Buggy,"Fixed Bug 424444 - [1.8] VerifyError when constructor reference used
with primitive array",Buggy,"The commit message indicates a fix for a `VerifyError` related to constructor references with primitive arrays. The code diff replaces `anewarray` with `newArray(null, null, (ArrayBinding) methodBinding.returnType)`. The `anewarray` instruction is used to create a new array of references, while `newArray` is more general and can handle primitive arrays as well. This change likely addresses the `VerifyError` by using the correct bytecode instruction for primitive array construction in the context of a constructor reference. The answer is **Buggy**."
eclipseJdt,6346.json,409121a5eb3d3ef99ff5c31121bd10011631e82f,"@@ -1,61 +1,65 @@
 	private void checkAndSetModifiersForField(FieldBinding fieldBinding, FieldDeclaration fieldDecl) {
 		int modifiers = fieldBinding.modifiers;
 		final ReferenceBinding declaringClass = fieldBinding.declaringClass;
 		if ((modifiers & ExtraCompilerModifiers.AccAlternateModifierProblem) != 0)
 			problemReporter().duplicateModifierForField(declaringClass, fieldDecl);
 
 		if (declaringClass.isInterface()) {
 			final int IMPLICIT_MODIFIERS = ClassFileConstants.AccPublic | ClassFileConstants.AccStatic | ClassFileConstants.AccFinal;
 			// set the modifiers
 			modifiers |= IMPLICIT_MODIFIERS;
 
 			// and then check that they are the only ones
 			if ((modifiers & ExtraCompilerModifiers.AccJustFlag) != IMPLICIT_MODIFIERS) {
 				if ((declaringClass.modifiers  & ClassFileConstants.AccAnnotation) != 0)
 					problemReporter().illegalModifierForAnnotationField(fieldDecl);
 				else
 					problemReporter().illegalModifierForInterfaceField(fieldDecl);
 			}
 			fieldBinding.modifiers = modifiers;
 			return;
 		} else if (fieldDecl.getKind() == AbstractVariableDeclaration.ENUM_CONSTANT) {
 			// check that they are not modifiers in source
 			if ((modifiers & ExtraCompilerModifiers.AccJustFlag) != 0)
 				problemReporter().illegalModifierForEnumConstant(declaringClass, fieldDecl);
 
 			// set the modifiers
-			final int IMPLICIT_MODIFIERS = ClassFileConstants.AccPublic | ClassFileConstants.AccStatic | ClassFileConstants.AccFinal | ClassFileConstants.AccEnum;
+			// https://bugs.eclipse.org/bugs/show_bug.cgi?id=267670. Force all enumerators to be marked
+			// as used locally. We are unable to track the usage of these reliably as they could be used
+			// in non obvious ways via the synthesized methods values() and valueOf(String) or by using 
+			// Enum.valueOf(Class<T>, String).
+			final int IMPLICIT_MODIFIERS = ClassFileConstants.AccPublic | ClassFileConstants.AccStatic | ClassFileConstants.AccFinal | ClassFileConstants.AccEnum | ExtraCompilerModifiers.AccLocallyUsed;
 			fieldBinding.modifiers|= IMPLICIT_MODIFIERS;
 			return;
 		}
 
 		// after this point, tests on the 16 bits reserved.
 		int realModifiers = modifiers & ExtraCompilerModifiers.AccJustFlag;
 		final int UNEXPECTED_MODIFIERS = ~(ClassFileConstants.AccPublic | ClassFileConstants.AccPrivate | ClassFileConstants.AccProtected | ClassFileConstants.AccFinal | ClassFileConstants.AccStatic | ClassFileConstants.AccTransient | ClassFileConstants.AccVolatile);
 		if ((realModifiers & UNEXPECTED_MODIFIERS) != 0) {
 			problemReporter().illegalModifierForField(declaringClass, fieldDecl);
 			modifiers &= ~ExtraCompilerModifiers.AccJustFlag | ~UNEXPECTED_MODIFIERS;
 		}
 
 		int accessorBits = realModifiers & (ClassFileConstants.AccPublic | ClassFileConstants.AccProtected | ClassFileConstants.AccPrivate);
 		if ((accessorBits & (accessorBits - 1)) > 1) {
 			problemReporter().illegalVisibilityModifierCombinationForField(declaringClass, fieldDecl);
 
 			// need to keep the less restrictive so disable Protected/Private as necessary
 			if ((accessorBits & ClassFileConstants.AccPublic) != 0) {
 				if ((accessorBits & ClassFileConstants.AccProtected) != 0)
 					modifiers &= ~ClassFileConstants.AccProtected;
 				if ((accessorBits & ClassFileConstants.AccPrivate) != 0)
 					modifiers &= ~ClassFileConstants.AccPrivate;
 			} else if ((accessorBits & ClassFileConstants.AccProtected) != 0 && (accessorBits & ClassFileConstants.AccPrivate) != 0) {
 				modifiers &= ~ClassFileConstants.AccPrivate;
 			}
 		}
 
 		if ((realModifiers & (ClassFileConstants.AccFinal | ClassFileConstants.AccVolatile)) == (ClassFileConstants.AccFinal | ClassFileConstants.AccVolatile))
 			problemReporter().illegalModifierCombinationFinalVolatileForField(declaringClass, fieldDecl);
 
 		if (fieldDecl.initialization == null && (modifiers & ClassFileConstants.AccFinal) != 0)
 			modifiers |= ExtraCompilerModifiers.AccBlankFinal;
 		fieldBinding.modifiers = modifiers;
 	}",Buggy,"Fix for bug #267670
",Buggy,"The commit message indicates a fix for bug #267670. The diff modifies the `checkAndSetModifiersForField` method, specifically when dealing with enum constants. It adds the `ExtraCompilerModifiers.AccLocallyUsed` flag to the implicit modifiers for enum constants. The comment explains the reason for this change: to force all enumerators to be marked as used locally because their usage cannot be reliably tracked due to synthesized methods and `Enum.valueOf`. This change directly addresses a bug related to the tracking of enum constant usage. The answer is **Buggy**."
eclipseJdt,4552.json,cc7009a12280dbb7a101bdbfbf1048948dc0093c,"@@ -1,66 +1,72 @@
 protected void verifyDuplicationAndOrder(int length, TypeBinding[] argumentTypes, boolean containsUnionTypes) {
 	// Verify that the catch clause are ordered in the right way:
 	// more specialized first.
 	if (containsUnionTypes) {
 		int totalCount = 0;
 		ReferenceBinding[][] allExceptionTypes = new ReferenceBinding[length][];
 		for (int i = 0; i < length; i++) {
+			if (argumentTypes[i] instanceof ArrayBinding)
+				continue;
 			ReferenceBinding currentExceptionType = (ReferenceBinding) argumentTypes[i];
 			TypeReference catchArgumentType = this.catchArguments[i].type;
 			if ((catchArgumentType.bits & ASTNode.IsUnionType) != 0) {
 				TypeReference[] typeReferences = ((UnionTypeReference) catchArgumentType).typeReferences;
 				int typeReferencesLength = typeReferences.length;
 				ReferenceBinding[] unionExceptionTypes = new ReferenceBinding[typeReferencesLength];
 				for (int j = 0; j < typeReferencesLength; j++) {
 					unionExceptionTypes[j] = (ReferenceBinding) typeReferences[j].resolvedType;
 				}
 				totalCount += typeReferencesLength;
 				allExceptionTypes[i] = unionExceptionTypes;
 			} else {
 				allExceptionTypes[i] = new ReferenceBinding[] { currentExceptionType };
 				totalCount++;
 			}
 		}
 		this.caughtExceptionTypes = new ReferenceBinding[totalCount];
 		this.caughtExceptionsCatchBlocks  = new int[totalCount];
 		for (int i = 0, l = 0; i < length; i++) {
 			ReferenceBinding[] currentExceptions = allExceptionTypes[i];
+			if (currentExceptions == null) continue;
 			loop: for (int j = 0, max = currentExceptions.length; j < max; j++) {
 				ReferenceBinding exception = currentExceptions[j];
 				this.caughtExceptionTypes[l] = exception;
 				this.caughtExceptionsCatchBlocks[l++] = i;
 				// now iterate over all previous exceptions
 				for (int k = 0; k < i; k++) {
 					ReferenceBinding[] exceptions = allExceptionTypes[k];
+					if (exceptions == null) continue;
 					for (int n = 0, max2 = exceptions.length; n < max2; n++) {
 						ReferenceBinding currentException = exceptions[n];
 						if (exception.isCompatibleWith(currentException)) {
 							TypeReference catchArgumentType = this.catchArguments[i].type;
 							if ((catchArgumentType.bits & ASTNode.IsUnionType) != 0) {
 								catchArgumentType = ((UnionTypeReference) catchArgumentType).typeReferences[j];
 							}
 							this.scope.problemReporter().wrongSequenceOfExceptionTypesError(
 								catchArgumentType,
 								exception,
 								currentException);
 							break loop;
 						}
 					}
 				}
 			}
 		}
 	} else {
 		this.caughtExceptionTypes = new ReferenceBinding[length];
 		for (int i = 0; i < length; i++) {
+			if (argumentTypes[i] instanceof ArrayBinding)
+				continue;
 			this.caughtExceptionTypes[i] = (ReferenceBinding) argumentTypes[i];
 			for (int j = 0; j < i; j++) {
 				if (this.caughtExceptionTypes[i].isCompatibleWith(argumentTypes[j])) {
 					this.scope.problemReporter().wrongSequenceOfExceptionTypesError(
 						this.catchArguments[i].type,
 						this.caughtExceptionTypes[i],
 						argumentTypes[j]);
 				}
 			}
 		}
 	}
 }",Buggy,"Fixed bug 433879 - ArrayBinding cannot be cast to ReferenceBinding

Signed-off-by: Shankha Banerjee <shankhba@in.ibm.com>",Buggy,"The commit message indicates a bug fix: ""Fixed bug 433879 - ArrayBinding cannot be cast to ReferenceBinding"". The code diff adds a check `if (argumentTypes[i] instanceof ArrayBinding) continue;` at the beginning of both the `if (containsUnionTypes)` and `else` blocks. This check prevents the code from attempting to cast an `ArrayBinding` to a `ReferenceBinding`, which was causing the reported bug. The code also adds null checks for `currentExceptions` and `exceptions` to avoid potential NullPointerExceptions after skipping ArrayBindings. Therefore, the changes directly address the bug described in the commit message. The answer is **Buggy**."
eclipseJdt,6311.json,c36a6a2b662267e56067d121b7f34ae48cbcb692,"@@ -1,16 +1,18 @@
 void checkInheritedMethods(MethodBinding inheritedMethod, MethodBinding otherInheritedMethod) {
 
 	// the 2 inherited methods clash because of a parameterized type overrides a raw type
 	//		interface I { void foo(A a); }
 	//		class Y { void foo(A<String> a) {} }
 	//		abstract class X extends Y implements I { }
 	//		class A<T> {}
 	// in this case the 2 inherited methods clash because of type variables
 	//		interface I { <T, S> void foo(T t); }
 	//		class Y { <T> void foo(T t) {} }
 	//		abstract class X extends Y implements I {}
 
-	if (inheritedMethod.declaringClass.isInterface() || inheritedMethod.isStatic()) return;
+	if (inheritedMethod.isStatic()) return;
+	if (this.environment.globalOptions.complianceLevel < ClassFileConstants.JDK1_7 && inheritedMethod.declaringClass.isInterface())
+		return;  // JDK7 checks for name clashes in interface inheritance, while JDK6 and below don't. See https://bugs.eclipse.org/bugs/show_bug.cgi?id=354229
 
 	detectInheritedNameClash(inheritedMethod.original(), otherInheritedMethod.original());
 }",Buggy,"Fixed bug 354229: [compiler][1.7] Name clash error not being reported by
ecj.",Buggy,"The commit message indicates a bug fix related to name clash errors in the compiler, specifically concerning Java 1.7. The code diff introduces a conditional check based on the compliance level. If the compliance level is below JDK 1.7 and the inherited method is declared in an interface, the method returns early. This suggests that the bug fix addresses a change in how name clashes are handled in interface inheritance between JDK 1.7 and earlier versions. The added condition effectively disables the stricter name clash checking for older JDK versions, resolving the reported bug. The answer is **Buggy**."
eclipseJdt,4397.json,7089fcfd703b096f125599a900905c00e15535fa,"@@ -1,30 +1,36 @@
 	public void getAllAnnotationContexts(int targetType, int typeParameterIndex, List allAnnotationContexts) {
 		AnnotationCollector collector = new AnnotationCollector(this, targetType, typeParameterIndex, allAnnotationContexts);
 		if (this.annotations != null) {
 			int annotationsLength = this.annotations.length;
 			for (int i = 0; i < annotationsLength; i++)
 				this.annotations[i].traverse(collector, (BlockScope) null);
 		}
 		switch(collector.targetType) {
 			case AnnotationTargetTypeConstants.CLASS_TYPE_PARAMETER :
 				collector.targetType = AnnotationTargetTypeConstants.CLASS_TYPE_PARAMETER_BOUND;
 				break;
 			case AnnotationTargetTypeConstants.METHOD_TYPE_PARAMETER :
 				collector.targetType = AnnotationTargetTypeConstants.METHOD_TYPE_PARAMETER_BOUND;
 		}
-		if (this.type != null && ((this.type.bits & ASTNode.HasTypeAnnotations) != 0)) {
-			collector.info2 = 0;
-			this.type.traverse(collector, (BlockScope) null);
+		int boundIndex = 0;
+		if (this.type != null) {
+			// boundIndex 0 is always a class
+			if (this.type.resolvedType.isInterface())
+				boundIndex = 1;
+			if ((this.type.bits & ASTNode.HasTypeAnnotations) != 0) {
+				collector.info2 = boundIndex;
+				this.type.traverse(collector, (BlockScope) null);
+			}
 		}
 		if (this.bounds != null) {
 			int boundsLength = this.bounds.length;
 			for (int i = 0; i < boundsLength; i++) {
 				TypeReference bound = this.bounds[i];
 				if ((bound.bits & ASTNode.HasTypeAnnotations) == 0) {
 					continue;
 				}
-				collector.info2 = i + 1;
+				collector.info2 = ++boundIndex;
 				bound.traverse(collector, (BlockScope) null);
 			}
 		}
 	}",Buggy,"Fixed Bug 415543 - [1.8][compiler] Incorrect bound index in
RuntimeInvisibleTypeAnnotations attrribute

Signed-off-by: Andrew Clement <aclement@gopivotal.com>",Buggy,"The commit message indicates a fix for a bug related to incorrect bound index in RuntimeInvisibleTypeAnnotations. The code changes modify how the bound index is calculated and used when traversing type annotations, especially within the context of class types and interfaces. The original code appears to have been missing logic to correctly determine the bound index, leading to incorrect annotation processing. The updated code introduces a `boundIndex` variable and adjusts its value based on whether the type is an interface or a class. This adjustment ensures that the correct bound index is used when traversing annotations on type bounds. The answer is **Buggy**."
eclipseJdt,5447.json,3c8db8654fc8e2927c75863ec1232c9bc3800c9b,"@@ -1,31 +1,34 @@
 	public TypeBinding resolveType(BlockScope scope) {
 
 		if (this.compilerAnnotation != null)
 			return this.resolvedType;
 
 		this.constant = Constant.NotAConstant;
 
 		ReferenceBinding containerAnnotationType = (ReferenceBinding) this.resolvedType;
 		if (!containerAnnotationType.isValidBinding())
 			containerAnnotationType = (ReferenceBinding) containerAnnotationType.closestMatch();
 		Annotation repeatingAnnotation = this.containees[0];
 		ReferenceBinding repeatingAnnotationType = (ReferenceBinding) repeatingAnnotation.resolvedType;
+		if (!repeatingAnnotationType.isDeprecated() && isTypeUseDeprecated(containerAnnotationType, scope)) {
+			scope.problemReporter().deprecatedType(containerAnnotationType, repeatingAnnotation);
+		}
 		checkContainerAnnotationType(repeatingAnnotation, scope, containerAnnotationType, repeatingAnnotationType, true); // true => repeated *use* site error reporting requested.
 		this.resolvedType = containerAnnotationType = repeatingAnnotationType.containerAnnotationType();
 		if (!this.resolvedType.isValidBinding())
 			return this.resolvedType;
 		
 		// OK, the declaration site of the repeating annotation type as well as the use site where the annotations actually repeat pass muster. 
 		MethodBinding[] methods = containerAnnotationType.methods();
 		MemberValuePair pair = memberValuePairs()[0];
 		
 		for (int i = 0, length = methods.length; i < length; i++) {
 			MethodBinding method = methods[i];
 			if (CharOperation.equals(method.selector, TypeConstants.VALUE)) {
 				pair.binding = method;
 				pair.resolveTypeExpecting(scope, method.returnType);
 			}
 		}
 		this.compilerAnnotation = scope.environment().createAnnotation((ReferenceBinding) this.resolvedType, computeElementValuePairs());
 		return this.resolvedType;
 	}",Buggy,"Follow up fixes for Bug 412153 - [1.8][compiler] Check validity of
annotations which may be repeatable",Buggy,"The commit message indicates a follow-up fix for a bug related to repeatable annotations. The code changes involve checking for deprecated types and reporting deprecated type usage. Specifically, the code now checks if the container annotation type is deprecated and reports it as a problem if it is. This suggests that the original code had a bug where it wasn't properly handling or reporting deprecated repeatable annotations. The answer is **Buggy**.
"
eclipseJdt,3852.json,d15e79b3439bbaf42d22da90dbcfabf7ceac0248,"@@ -1,3 +1,3 @@
 public boolean isCompatibleWith(TypeBinding left, Scope scope) {
-	throw new UnsupportedOperationException(""Unexpected control flow, should not have reached Expression.isCompatibleWith""); //$NON-NLS-1$
+	return this.resolvedType != null && this.resolvedType.isCompatibleWith(left,  scope);
 }",Buggy,"Fixed Bug 426315 - [1.8][compiler] UnsupportedOperationException with
conditional expression ",Buggy,"The commit message indicates a fix for a bug (Bug 426315) that causes an UnsupportedOperationException when using a conditional expression. The code diff replaces an `UnsupportedOperationException` with a check for compatibility between the resolved type of the expression and the left-hand side type within the given scope. This change directly addresses the reported bug by providing a valid implementation instead of throwing an exception. The answer is **Buggy**.
"
eclipseJdt,22376.json,883d74a76cfedfa8b3e490bff988420af3184ead,"@@ -1,23 +1,24 @@
 	private void pushOnCommentsStack(int start, int end) {
 
 		for (int i=start; i<=end; i++) {
+			if (this.scanner.commentPtr < i) break;
 			// First see if comment hasn't been already stored
 			int scannerStart = this.scanner.commentStarts[i]<0 ? -this.scanner.commentStarts[i] : this.scanner.commentStarts[i];
 			int commentStart = this.commentPtr == -1 ? -1 : (this.commentStarts[this.commentPtr]<0 ? -this.commentStarts[this.commentPtr] : this.commentStarts[this.commentPtr]);
 			if (commentStart == -1 ||  scannerStart > commentStart) {
 				int stackLength = this.commentStarts.length;
 				if (++this.commentPtr >= stackLength) {
 					System.arraycopy(
 						this.commentStarts, 0,
 						this.commentStarts = new int[stackLength + CommentIncrement], 0,
 						stackLength);
 					System.arraycopy(
 						this.commentStops, 0,
 						this.commentStops = new int[stackLength + CommentIncrement], 0,
 						stackLength);
 				}
 				this.commentStarts[this.commentPtr] = this.scanner.commentStarts[i];
 				this.commentStops[this.commentPtr] = this.scanner.commentStops[i];
 			}
 		}
 	}",Buggy,"Fix for Bug 526996: CompilationUnit.setCommentTable(...) throws
IllegalStateException",Buggy,"The commit message indicates a fix for a bug where `CompilationUnit.setCommentTable(...)` throws `IllegalStateException`. The code diff introduces a check `if (this.scanner.commentPtr < i) break;` within the loop. This check likely prevents the code from accessing invalid indices in the `scanner.commentStarts` array, which could lead to the `IllegalStateException` if `this.scanner.commentPtr` is less than `i`. The added check ensures that the loop iterates only through valid comment indices, thus fixing the bug. The answer is **Buggy**."
eclipseJdt,27085.json,1793b84adc002b2d6c63dad898df1777f10b28a0,"@@ -1,89 +1,89 @@
 	public boolean containsAnnotations() throws IOException {
 		State state = NORMAL;
 		
 		// for escaping quotes -- need to ignore the next single character
 		// Since this applies to all states it's handled separately
 		boolean seenBackslash = false;
 		
 		int c = getNext();
 		while (c != -1) {
 			
 			if (seenBackslash) {
 				// Skip one character
 				seenBackslash = false;
 			}
 			else if (c == '\\') {
 				// Skip the next character
 				seenBackslash = true;
 			}
 			else {
 				// Handle the character based on state
 				switch (state) {
 				
-				case (NORMAL) :
+				case NORMAL :
 					if (c == '@')
 						return true;
 					if (c == '/') {
 						state = SEEN_SLASH;
 					}
 					else if (c == '\'') {
 						state = IN_SINGLE_QUOTE;
 					}
 					else if (c == '\""') {
 						state = IN_DOUBLE_QUOTE;
 					}
 					break;
 					
-				case (SEEN_SLASH) :
+				case SEEN_SLASH :
 					if (c == '*') {
 						state = IN_COMMENT;
 					}
 					else if (c == '/') {
 						state = IN_SINGLE_LINE_COMMENT;
 					}
 					else {
 						state = NORMAL;
 					}
 					break;
 				
-				case (IN_COMMENT) :
+				case IN_COMMENT :
 					if (c == '*') {
 						state = IN_COMMENT_SEEN_STAR;
 					}
 					break;
 				
-				case (IN_COMMENT_SEEN_STAR) :
+				case IN_COMMENT_SEEN_STAR :
 					if (c == '/') {
 						state = NORMAL;
 					}
 					else {
 						state = IN_COMMENT;
 					}
 					break;
 					
-				case (IN_SINGLE_LINE_COMMENT) :
+				case IN_SINGLE_LINE_COMMENT :
 					if (c == '\n' || c == '\r') {
 						state = NORMAL;
 					}
 					break;
 					
-				case (IN_SINGLE_QUOTE) :
+				case IN_SINGLE_QUOTE :
 					if (c == '\'') {
 						state = NORMAL;
 					}
 					break;
 					
-				case (IN_DOUBLE_QUOTE) :
+				case IN_DOUBLE_QUOTE :
 					if (c == '\""') {
 						state = NORMAL;
 					}
 					break;
 					
 				default :
 					throw new IllegalStateException(""Unhandled state: "" + state);  //$NON-NLS-1$
 				}
 			}
 			c = getNext();
 		}
 		return false;
 	}",Buggy,"jgarms: fix syntax error in switch/case usage that is not caught by eclipse. This allows compiling by javac.
",Buggy,"The commit message indicates a fix for a syntax error in a switch/case statement. The diff shows changes in the switch statement where the cases `NORMAL`, `SEEN_SLASH`, `IN_COMMENT`, `IN_COMMENT_SEEN_STAR`, `IN_SINGLE_LINE_COMMENT`, `IN_SINGLE_QUOTE`, and `IN_DOUBLE_QUOTE` had parentheses around them. Removing the parentheses resolves the syntax error. The answer is **Buggy**."
eclipseJdt,6097.json,9a8f94b0b98db0b5d6b8eb4d27d568dc18fc1309,"@@ -1,18 +1,20 @@
 public ReferenceBinding anonymousOriginalSuperType() {
 	if (!isPrototype())
 		return ((LocalTypeBinding) this.prototype).anonymousOriginalSuperType();
+	if (this.superclass == null && this.scope != null)
+		return this.scope.getJavaLangObject();
 	
 	if (this.superInterfaces != Binding.NO_SUPERINTERFACES) {
 		return this.superInterfaces[0];
 	}
 	if ((this.tagBits & TagBits.HierarchyHasProblems) == 0) {
 		return this.superclass;
 	}
 	if (this.scope != null) {
 		TypeReference typeReference = this.scope.referenceContext.allocation.type;
 		if (typeReference != null) {
 			return (ReferenceBinding) typeReference.resolvedType;
 		}
 	}
 	return this.superclass; // default answer
 }",Buggy,"Fixed Bug 435348 - [1.8][compiler] NPE in JDT Core during AST creation

Signed-off-by: shankha banerjee <shankhba@in.ibm.com>",Buggy,"The commit message indicates a fix for a NullPointerException (NPE) in the JDT Core compiler during AST creation, specifically related to bug 435348. The code diff introduces a null check for `this.superclass` and `this.scope` within the `anonymousOriginalSuperType()` method. If both are null, it returns `this.scope.getJavaLangObject()`. This suggests that the original code was susceptible to an NPE when `this.superclass` was null and the code proceeded to access members of `this.scope` without ensuring it was not null. The added null check prevents this scenario, thus fixing the reported bug. The answer is **Buggy**."
eclipseJdt,20759.json,c71a2dd3276b1c054fbb64a586af2db8d142bd3f,"@@ -1,61 +1,66 @@
 	protected void seekTypesInSourcePackage(
 			String name,
 			IPackageFragment pkg,
 			int firstDot,
 			boolean partialMatch,
 			String topLevelTypeName,
 			int acceptFlags,
 			IJavaElementRequestor requestor) {
 
 		long start = -1;
 		if (VERBOSE)
 			start = System.currentTimeMillis();
 		try {
 			if (!partialMatch) {
 				try {
 					IJavaElement[] compilationUnits = pkg.getChildren();
 					for (int i = 0, length = compilationUnits.length; i < length; i++) {
 						if (requestor.isCanceled())
 							return;
 						IJavaElement cu = compilationUnits[i];
 						String cuName = cu.getElementName();
 						int lastDot = cuName.lastIndexOf('.');
 						if (lastDot != topLevelTypeName.length() || !topLevelTypeName.regionMatches(0, cuName, 0, lastDot))
 							continue;
+
+						// https://bugs.eclipse.org/bugs/show_bug.cgi?id=351697
+						// If we are looking at source location, just ignore binary types
+						if (!(cu instanceof ICompilationUnit))
+							continue;
 						IType type = ((ICompilationUnit) cu).getType(topLevelTypeName);
 						type = getMemberType(type, name, firstDot);
 						if (acceptType(type, acceptFlags, true/*a source type*/)) { // accept type checks for existence
 							requestor.acceptType(type);
 							break;  // since an exact match was requested, no other matching type can exist
 						}
 					}
 				} catch (JavaModelException e) {
 					// package doesn't exist -> ignore
 				}
 			} else {
 				try {
 					String cuPrefix = firstDot == -1 ? name : name.substring(0, firstDot);
 					IJavaElement[] compilationUnits = pkg.getChildren();
 					for (int i = 0, length = compilationUnits.length; i < length; i++) {
 						if (requestor.isCanceled())
 							return;
 						IJavaElement cu = compilationUnits[i];
 						if (!cu.getElementName().toLowerCase().startsWith(cuPrefix))
 							continue;
 						try {
 							IType[] types = ((ICompilationUnit) cu).getTypes();
 							for (int j = 0, typeLength = types.length; j < typeLength; j++)
 								seekTypesInTopLevelType(name, firstDot, types[j], requestor, acceptFlags);
 						} catch (JavaModelException e) {
 							// cu doesn't exist -> ignore
 						}
 					}
 				} catch (JavaModelException e) {
 					// package doesn't exist -> ignore
 				}
 			}
 		} finally {
 			if (VERBOSE)
 				this.timeSpentInSeekTypesInSourcePackage += System.currentTimeMillis()-start;
 		}
 	}",Buggy,"master - Fix for bug 351697: ClassCastException while copying a .class file
to wrong source package
",Buggy,"The commit message indicates a fix for bug 351697, which involves a ClassCastException occurring when copying a .class file to the wrong source package. The code diff adds a check to ensure that the `cu` variable is an instance of `ICompilationUnit` before casting it. This prevents the ClassCastException when a binary file (which is not an `ICompilationUnit`) is encountered in a source package. The answer is **Buggy**."
eclipseJdt,17605.json,0aa34db9e617bf74b6393bf5508b95dcdaed80af,"@@ -1,10 +1,11 @@
 protected void consumeMethodHeaderName(boolean isAnnotationMethod) {
 	long selectorSourcePositions = this.identifierPositionStack[this.identifierPtr];
 	int selectorSourceEnd = (int) selectorSourcePositions;
 	int currentAstPtr = this.astPtr;
 	super.consumeMethodHeaderName(isAnnotationMethod);
 	if (this.astPtr > currentAstPtr) { // if ast node was pushed on the ast stack
 		this.sourceEnds.put(this.astStack[this.astPtr], selectorSourceEnd);
 		rememberCategories();
 	}
+	flushCommentsDefinedPriorTo(this.scanner.currentPosition);
 }",Buggy,"Fix for Bug 443942 Reconciler reports AST with wrong node range (with
comment after return type)",Buggy,"The commit message indicates a fix for a bug where the reconciler reports an AST with the wrong node range, specifically when a comment appears after the return type. The code change adds `flushCommentsDefinedPriorTo(this.scanner.currentPosition)` to the `consumeMethodHeaderName` method. This suggests that the bug was related to how comments were being handled in relation to the AST node range. Flushing comments defined prior to the current position likely ensures that comments are correctly associated with the appropriate AST nodes, thus fixing the reported bug. The answer is **Buggy**."
eclipseJdt,6321.json,93b912e3753e060958c1b01615e164e0662ca645,"@@ -1,10 +1,10 @@
 MethodBinding findReplacedMethod(MethodBinding specific, MethodBinding general) {
 	MethodBinding generalSubstitute = computeSubstituteMethod(general, specific);
 	if (generalSubstitute != null 
-			&& (!specific.isAbstract() || general.isAbstract())	// if (abstract(specific) => abstract(general)) check if 'specific' overrides 'general' 
+			&& (!specific.isAbstract() || general.isAbstract() || (general.isDefaultMethod() && specific.declaringClass.isClass()))	// if (abstract(specific) => abstract(general)) check if 'specific' overrides 'general' 
 			&& isSubstituteParameterSubsignature(specific, generalSubstitute)) 
 	{
 		return generalSubstitute;
 	} 
 	return null;
 }",Buggy,"Fixed Bug 426318 - [1.8][compiler] Bogus name clash error in the
presence of default methods and varargs",Buggy,"The commit message indicates a bug fix related to name clash errors in the compiler, specifically involving default methods and varargs. The code diff modifies the `findReplacedMethod` method, adding a condition to the if statement: `(general.isDefaultMethod() && specific.declaringClass.isClass())`. This condition seems to address the scenario where a specific method in a class overrides a default method, potentially resolving the bogus name clash error mentioned in the commit message. The added condition suggests a correction to the logic that determines method overriding, which is a bug fix. The answer is **Buggy**."
commons-lang,3021.json,e582456625cc8a7056cc9354d2a75913f4ceb393,"@@ -1,46 +1,48 @@
     private void init() {
         thisYear= Calendar.getInstance(timeZone, locale).get(Calendar.YEAR);
         
         nameValues= new ConcurrentHashMap<Integer, KeyValue[]>();
         
         StringBuilder regex= new StringBuilder();
         List<Strategy> collector = new ArrayList<Strategy>();
         
         Matcher patternMatcher= formatPattern.matcher(pattern);
         if(!patternMatcher.lookingAt()) {
             throw new IllegalArgumentException(""Invalid pattern"");
         }
 
-        String localeName = locale.toString();
         // These locales don't use the Gregorian calendar
         // See http://docs.oracle.com/javase/6/docs/technotes/guides/intl/calendar.doc.html
-        if (localeName.equals(""ja_JP_JP"") || localeName.startsWith(""th_TH"")) {
+        // Also, the getEras() methods don't return the correct era names.
+        // N.B. Not safe to use toString() comparison because that changes between Java versions
+        if (locale.equals(JAPANESE_IMPERIAL)
+        || (locale.getLanguage().equals(""th"") && locale.getCountry().equals(""TH""))) {
             collector.add(new SimpleDateFormatStrategy());
             strategies= collector.toArray(new Strategy[collector.size()]);
             parsePattern= Pattern.compile(""(.*+)"");
             return;
         }
 
         currentFormatField= patternMatcher.group();
         Strategy currentStrategy= getStrategy(currentFormatField);
         for(;;) {
             patternMatcher.region(patternMatcher.end(), patternMatcher.regionEnd());
             if(!patternMatcher.lookingAt()) {
                 nextStrategy = null;
                 break;
             }
             String nextFormatField= patternMatcher.group();
             nextStrategy = getStrategy(nextFormatField);
             if(currentStrategy.addRegex(this, regex)) {
                 collector.add(currentStrategy);                
             }
             currentFormatField= nextFormatField;
             currentStrategy= nextStrategy;
         }
         if(currentStrategy.addRegex(this, regex)) {
             collector.add(currentStrategy);                
         }
         currentFormatField= null;
         strategies= collector.toArray(new Strategy[collector.size()]);
         parsePattern= Pattern.compile(regex.toString());
     }",Buggy,"LANG-828 FastDateParser does not handle non-Gregorian calendars properly
Fix bug in Java 7 (Locale.toString() format has changed)

git-svn-id: https://svn.apache.org/repos/asf/commons/proper/lang/trunk@1390189 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for non-Gregorian calendars and a Java 7 locale issue. The code changes address these problems. Specifically, the code now checks for `JAPANESE_IMPERIAL` locale using `.equals()` instead of `.toString()` and also checks for Thai locale using `.getLanguage()` and `.getCountry()` rather than `.toString()`. This resolves the Java 7 locale issue. The code also handles non-Gregorian calendars by using a `SimpleDateFormatStrategy`. Therefore, the changes indicate a bug fix. The answer is **Buggy**."
commons-lang,734.json,fbb0f7f88c84001e0a92dae6a71b7e43bda65a56,"@@ -1,54 +1,58 @@
     public static String wrap(final String str, int wrapLength, String newLineStr, final boolean wrapLongWords) {
         if (str == null) {
             return null;
         }
         if (newLineStr == null) {
             newLineStr = SystemUtils.LINE_SEPARATOR;
         }
         if (wrapLength < 1) {
             wrapLength = 1;
         }
         final int inputLineLength = str.length();
         int offset = 0;
         final StringBuilder wrappedLine = new StringBuilder(inputLineLength + 32);
         
-        while (inputLineLength - offset > wrapLength) {
+        while (offset < inputLineLength) {
             if (str.charAt(offset) == ' ') {
                 offset++;
                 continue;
             }
+            // only last line without leading spaces is left
+            if(inputLineLength - offset <= wrapLength) {
+                break;
+            }
             int spaceToWrapAt = str.lastIndexOf(' ', wrapLength + offset);
 
             if (spaceToWrapAt >= offset) {
                 // normal case
                 wrappedLine.append(str.substring(offset, spaceToWrapAt));
                 wrappedLine.append(newLineStr);
                 offset = spaceToWrapAt + 1;
                 
             } else {
                 // really long word or URL
                 if (wrapLongWords) {
                     // wrap really long word one line at a time
                     wrappedLine.append(str.substring(offset, wrapLength + offset));
                     wrappedLine.append(newLineStr);
                     offset += wrapLength;
                 } else {
                     // do not wrap really long word, just extend beyond limit
                     spaceToWrapAt = str.indexOf(' ', wrapLength + offset);
                     if (spaceToWrapAt >= 0) {
                         wrappedLine.append(str.substring(offset, spaceToWrapAt));
                         wrappedLine.append(newLineStr);
                         offset = spaceToWrapAt + 1;
                     } else {
                         wrappedLine.append(str.substring(offset));
                         offset = inputLineLength;
                     }
                 }
             }
         }
 
         // Whatever is left in line is short enough to just pass through
         wrappedLine.append(str.substring(offset));
 
         return wrappedLine.toString();
     }",Buggy,"LANG-995: Fix bug with stripping spaces on last line in WordUtils.wrap(). This fixes #18 from github. Thanks to Andrey Khobnya

git-svn-id: https://svn.apache.org/repos/asf/commons/proper/lang/trunk@1586649 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,The commit message indicates a bug fix related to stripping spaces on the last line in `WordUtils.wrap()`. The diff modifies the `while` loop condition and adds a check for leading spaces on the last line. The original code had an issue where trailing spaces on the last line were not handled correctly. The new code addresses this by ensuring that the loop continues until the offset reaches the end of the input string and by adding a specific condition to handle the last line without leading spaces. This confirms that the changes are indeed a bug fix. The answer is **Buggy**.
commons-lang,1056.json,cd9922ef8b2c57ed6aa1390f89f94c1c40cdc67c,"@@ -1,6 +1,6 @@
     protected void setArrayEnd(String arrayEnd) {
-        if (arrayStart == null) {
-            arrayStart = """";
+        if (arrayEnd == null) {
+            arrayEnd = """";
         }
         this.arrayEnd = arrayEnd;
     }",Buggy,"ToStringStyle setArrayEnd handled null incorrectly
bug 31933, fix from Masato Tezuka


git-svn-id: https://svn.apache.org/repos/asf/jakarta/commons/proper/lang/trunk@138011 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,The commit message indicates a bug fix related to how `ToStringStyle` handles null values in `setArrayEnd`. The diff shows a correction where the code was incorrectly checking `arrayStart` for null instead of `arrayEnd`. This confirms the bug fix described in the commit message. The answer is **Buggy**.
commons-lang,816.json,2d17989e282e67c9c37153adc4074082d3a38b1c,"@@ -1,7 +1,7 @@
     public CompareToBuilder appendSuper(int superCompareTo) {
         if (comparison != 0) {
             return this;
         }
-        comparison = superHashCode;
+        comparison = superCompareTo;
         return this;
     }",Buggy,"Fix stupid compile error....


git-svn-id: https://svn.apache.org/repos/asf/jakarta/commons/proper/lang/trunk@137489 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,The commit message indicates a fix for a compilation error. The diff shows that `superHashCode` was incorrectly assigned to the `comparison` variable; it should have been `superCompareTo`. This is a clear bug fix. The answer is **Buggy**.
commons-lang,816.json,dbe97c09cad3ba2c7473c15778773fa4d071e7de,"@@ -1,7 +1,7 @@
-    public CompareToBuilder appendSuper(int superHashCode) {
+    public CompareToBuilder appendSuper(int superCompareTo) {
         if (comparison != 0) {
             return this;
         }
         comparison = superHashCode;
         return this;
     }",Buggy,"Javadoc fixes
bug 21758, from Pete Gieser


git-svn-id: https://svn.apache.org/repos/asf/jakarta/commons/proper/lang/trunk@137487 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates Javadoc fixes related to bug 21758. However, the diff shows a change in the parameter name from `superHashCode` to `superCompareTo` in the `appendSuper` method of `CompareToBuilder`. This suggests a correction in the parameter's semantic meaning, likely to align with its actual usage within the method. The original name `superHashCode` was misleading, as the value is used for comparison, not hashing. This is a bug fix. The answer is **Buggy**."
commons-lang,183.json,8cafd87c83f4197f7e8d14de8cba453c844266c0,"@@ -1,20 +1,20 @@
     public static List<Method> getMethodsListWithAnnotation(final Class<?> cls,
                                                             final Class<? extends Annotation> annotationCls,
                                                             boolean searchSupers, boolean ignoreAccess) {
 
         Validate.isTrue(cls != null, ""The class must not be null"");
         Validate.isTrue(annotationCls != null, ""The annotation class must not be null"");
         List<Class<?>> classes = (searchSupers ? getAllSuperclassesAndInterfaces(cls)
                 : new ArrayList<Class<?>>());
-        classes.add(cls);
+        classes.add(0, cls);
         final List<Method> annotatedMethods = new ArrayList<>();
         for (Class<?> acls : classes) {
             final Method[] methods = (ignoreAccess ? acls.getDeclaredMethods() : acls.getMethods());
             for (final Method method : methods) {
                 if (method.getAnnotation(annotationCls) != null) {
                     annotatedMethods.add(method);
                 }
             }
         }
         return annotatedMethods;
     }",Buggy,"LANG-1317: Add MethodUtils#findAnnotation and extend MethodUtils#getMethodsWithAnnotation for non-public, super-class and interface methods

fix bug introduced by last commit
",Buggy,"The commit message indicates a bug fix related to the previous commit, specifically addressing issues in `MethodUtils#getMethodsWithAnnotation`. The diff modifies the order in which classes are added to the `classes` list. Previously, the class `cls` was added to the end of the list, but now it's added to the beginning using `classes.add(0, cls)`. This change likely ensures that methods of the class `cls` are checked for the annotation before methods of its superclasses and interfaces. This adjustment suggests a bug where annotations in the class itself were not being properly detected or were being overshadowed by annotations in superclasses or interfaces. The answer is **Buggy**."
commons-lang,2681.json,82c5dada6f7b369b69c383aa995f45578325e139,"@@ -1,15 +1,15 @@
-    private static void removeCommonFrames(List causeFrames, List wrapperFrames) {
+    public static void removeCommonFrames(List causeFrames, List wrapperFrames) {
         int causeFrameIndex = causeFrames.size() - 1;
         int wrapperFrameIndex = wrapperFrames.size() - 1;
         while (causeFrameIndex >= 0 && wrapperFrameIndex >= 0) {
             // Remove the frame from the cause trace if it is the same
             // as in the wrapper trace
             String causeFrame = (String) causeFrames.get(causeFrameIndex);
             String wrapperFrame = (String) wrapperFrames.get(wrapperFrameIndex);
             if (causeFrame.equals(wrapperFrame)) {
                 causeFrames.remove(causeFrameIndex);
             }
             causeFrameIndex--;
             wrapperFrameIndex--;
         }
     }",Buggy,"Bug #14357 fixed. Mohan's patch makes removeCommonFrames public, and adds an
isThrowableNested to ExceptionUtils.

It adds static attributes to decide if the stack trace should be topDown
and if the stack traces should be trimmed on repeat. If running 1.4 or higher,
it uses the default stack trace, and the functionality of NestableError,
NestableException and NestableRuntimeException getMessage()s all change.
Accompanying these changes are numerous tests.

Submitted by:	Mohan Kishore


git-svn-id: https://svn.apache.org/repos/asf/jakarta/commons/proper/lang/trunk@137314 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix related to stack trace handling in exceptions. The diff modifies the `removeCommonFrames` method, changing its visibility from private to public. While this change in itself doesn't directly indicate a bug fix, the commit message explicitly states that this change is part of fixing a bug. The change likely makes the method accessible for use in other parts of the library or by external users, potentially addressing an issue related to stack trace analysis or manipulation. Therefore, the answer is **Buggy**."
commons-lang,2064.json,f1ba9c252d99b3adf2bb3750339b895b2c097bd2,"@@ -1,16 +1,17 @@
         protected Class<?> resolveClass(ObjectStreamClass desc) throws IOException, ClassNotFoundException {
             String name = desc.getName();
             try {
                 return Class.forName(name, false, classLoader);
             } catch (ClassNotFoundException ex) {
                 try {
                     return Class.forName(name, false, Thread.currentThread().getContextClassLoader());
                 } catch (ClassNotFoundException cnfe) {
                     Class<?> cls = primitiveTypes.get(name);
-                    if (cls != null)
+                    if (cls != null) {
                         return cls;
-                    else
+                    } else {
                         throw cnfe;
+                    }
                 }
             }
         }",Buggy,"Applying Benedikt Ritter's patch to fix the Checkstyle error in SerializationUtils - LANG-793

git-svn-id: https://svn.apache.org/repos/asf/commons/proper/lang/trunk@1301321 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates a fix for a Checkstyle error. The code changes involve adding braces to `if` and `else` blocks within the `resolveClass` method. While this change improves code style and readability, it does not directly address a functional bug. The code's logic remains the same, and there's no indication of error handling updates, logical corrections, or exception-handling improvements. The answer is **NotBuggy**."
elasticsearch,60742.json,f2411253026503954da172ddd0b1bf77789dd476,"@@ -1,3 +1,3 @@
     public boolean snapshot() {
-        return snapshot != null && snapshot;
+        return snapshot;
     }",Buggy,"Internal: Change snapshot state for unreleased versions and add
validation tests for constants

Currently the snapshot flag for Version constants is only set to true
for CURRENT.  However, this means that the snapshot state changes from
branch to branch.  Instead, snapshot should be ""is this version
released?"".  This change also adds a validation test checking that
ID -> constant and vice versa are correct, and fixes one bug found there
(for an unreleased version).
",Buggy,"The commit message indicates a change in how the snapshot state is determined for unreleased versions. The diff simplifies the `snapshot()` method by directly returning the `snapshot` field, removing the null check. This suggests that the logic for determining the snapshot state was flawed, and the change corrects this flaw. The commit message explicitly mentions fixing a bug found during validation tests. The answer is **Buggy**."
elasticsearch,52357.json,1a915446037ab83ae292df8733e7ba7d63bb667e,"@@ -1,15 +1,16 @@
     public RestStatus status() {
         if (shardFailures.length == 0) {
-            // if no successful shards, it means no active shards, so just return SERVICE_UNAVAILABLE
-            return RestStatus.SERVICE_UNAVAILABLE;
+            // if no successful shards, the failure can be due to EsRejectedExecutionException during fetch phase
+            // on coordinator node. so get the status from cause instead of returning SERVICE_UNAVAILABLE blindly
+            return getCause() == null ? RestStatus.SERVICE_UNAVAILABLE : ExceptionsHelper.status(getCause());
         }
         RestStatus status = shardFailures[0].status();
         if (shardFailures.length > 1) {
             for (int i = 1; i < shardFailures.length; i++) {
                 if (shardFailures[i].status().getStatus() >= 500) {
                     status = shardFailures[i].status();
                 }
             }
         }
         return status;
     }",Buggy,"Fixing 503 Service Unavailable errors during fetch phase  (#39086)

When ESRejectedExecutionException gets thrown on the coordinating node while trying to fetch hits, the resulting exception will hold no shard failures, hence `503` is used as the response status code. In that case, `429` should be returned instead. Also, the status code should be taken from the cause if available whenever there are no shard failures instead of blindly returning `503` like we currently do.

Closes #38586",Buggy,"The commit message indicates a fix for 503 Service Unavailable errors during the fetch phase. The code change modifies the `status()` method to handle cases where `ESRejectedExecutionException` is thrown on the coordinating node. Instead of blindly returning `SERVICE_UNAVAILABLE` when there are no shard failures, the code now retrieves the status code from the cause of the exception, if available. This aligns with the commit message's description of the problem and the intended solution. The change addresses a specific error scenario and improves the accuracy of the returned status code. The answer is **Buggy**."
elasticsearch,47411.json,f0f2b261595e2f5ea48736eac7a4e4121dd20b2c,"@@ -1,21 +1,21 @@
     public static int getRoutingFactor(int sourceNumberOfShards, int targetNumberOfShards) {
         final int factor;
         if (sourceNumberOfShards < targetNumberOfShards) { // split
             factor = targetNumberOfShards / sourceNumberOfShards;
             if (factor * sourceNumberOfShards != targetNumberOfShards || factor <= 1) {
-                throw new IllegalArgumentException(""the number of source shards ["" + sourceNumberOfShards + ""] must be a must be a "" +
+                throw new IllegalArgumentException(""the number of source shards ["" + sourceNumberOfShards + ""] must be a "" +
                     ""factor of [""
                     + targetNumberOfShards + ""]"");
             }
         } else if (sourceNumberOfShards > targetNumberOfShards) { // shrink
             factor = sourceNumberOfShards / targetNumberOfShards;
             if (factor * targetNumberOfShards != sourceNumberOfShards || factor <= 1) {
-                throw new IllegalArgumentException(""the number of source shards ["" + sourceNumberOfShards + ""] must be a must be a "" +
+                throw new IllegalArgumentException(""the number of source shards ["" + sourceNumberOfShards + ""] must be a "" +
                     ""multiple of [""
                     + targetNumberOfShards + ""]"");
             }
         } else {
             factor = 1;
         }
         return factor;
     }",Buggy,"Fix duplicate phrase in shrink/split error message (#36734)

This commit removes a duplicate ""must be a"" from the shrink/split error
messages.",Buggy,"The commit message indicates a fix for a duplicate phrase in an error message. The diff removes the duplicate ""must be a"" from the IllegalArgumentException messages in the `getRoutingFactor` method. This correction directly addresses the issue described in the commit message. The answer is **Buggy**."
elasticsearch,8045.json,92f24c967ae2ad22a728ff43de231cf351b92fd2,"@@ -1,39 +1,39 @@
         protected MultiSearchResponse shardOperation(Request request, ShardId shardId) throws IOException {
             final IndexService indexService = indicesService.indexService(shardId.getIndex());
             final IndexShard indexShard = indicesService.getShardOrNull(shardId);
             try (Engine.Searcher searcher = indexShard.acquireSearcher(""enrich_msearch"")) {
                 final FieldsVisitor visitor = new FieldsVisitor(true);
                 final QueryShardContext context = indexService.newQueryShardContext(shardId.id(),
-                    searcher.getIndexReader(), () -> {throw new UnsupportedOperationException();}, null);
+                    searcher, () -> {throw new UnsupportedOperationException();}, null);
                 final MapperService mapperService = context.getMapperService();
                 final Text typeText = mapperService.documentMapper().typeText();
 
                 final MultiSearchResponse.Item[] items = new MultiSearchResponse.Item[request.multiSearchRequest.requests().size()];
                 for (int i = 0; i < request.multiSearchRequest.requests().size(); i++) {
                     final SearchSourceBuilder searchSourceBuilder = request.multiSearchRequest.requests().get(i).source();
 
                     final QueryBuilder queryBuilder = searchSourceBuilder.query();
                     final int from = searchSourceBuilder.from();
                     final int size = searchSourceBuilder.size();
                     final FetchSourceContext fetchSourceContext = searchSourceBuilder.fetchSource();
 
                     final Query luceneQuery = queryBuilder.rewrite(context).toQuery(context);
                     final int n = from + size;
                     final TopDocs topDocs = searcher.search(luceneQuery, n, new Sort(SortField.FIELD_DOC));
 
                     final SearchHit[] hits = new SearchHit[topDocs.scoreDocs.length];
                     for (int j = 0; j < topDocs.scoreDocs.length; j++) {
                         final ScoreDoc scoreDoc = topDocs.scoreDocs[j];
 
                         visitor.reset();
                         searcher.doc(scoreDoc.doc, visitor);
                         visitor.postProcess(mapperService);
                         final SearchHit hit = new SearchHit(scoreDoc.doc, visitor.uid().id(), typeText, Map.of());
                         hit.sourceRef(filterSource(fetchSourceContext, visitor.source()));
                         hits[j] = hit;
                     }
                     items[i] = new MultiSearchResponse.Item(createSearchResponse(topDocs, hits), null);
                 }
                 return new MultiSearchResponse(items, 1L);
             }
         }",Buggy,"fixed compile errors after merging
",Buggy,"The commit message ""fixed compile errors after merging"" indicates that the changes are intended to resolve compilation issues that arose after a merge operation. The diff shows a change in the `shardOperation` method where `searcher.getIndexReader()` is replaced with `searcher` when creating the `QueryShardContext`. This suggests that the `newQueryShardContext` method's signature or expected argument type has changed, leading to a compile error that is now being fixed. This is a direct fix for a build issue, implying that the previous code was not compilable due to an API incompatibility after the merge. The answer is **Buggy**."
elasticsearch,32648.json,fa3d365ee83a089d461eacf66f2118f68dc1a8ed,"@@ -1,19 +1,25 @@
     public BytesReference slice(int from, int length) {
+        FutureObjects.checkFromIndexSize(from, length, this.length);
+
+        if (length == 0) {
+            return BytesArray.EMPTY;
+        }
+
         // for slices we only need to find the start and the end reference
         // adjust them and pass on the references in between as they are fully contained
         final int to = from + length;
-        final int limit = getOffsetIndex(from + length);
+        final int limit = getOffsetIndex(to - 1);
         final int start = getOffsetIndex(from);
         final BytesReference[] inSlice = new BytesReference[1 + (limit - start)];
         for (int i = 0, j = start; i < inSlice.length; i++) {
             inSlice[i] = references[j++];
         }
         int inSliceOffset = from - offsets[start];
         if (inSlice.length == 1) {
             return inSlice[0].slice(inSliceOffset, length);
         }
         // now adjust slices in front and at the end
         inSlice[0] = inSlice[0].slice(inSliceOffset, inSlice[0].length() - inSliceOffset);
         inSlice[inSlice.length-1] = inSlice[inSlice.length-1].slice(0, to - offsets[limit]);
         return new CompositeBytesReference(inSlice);
     }",Buggy,"Fix CompositeBytesReference#slice to not throw AIOOBE with legal offsets. (#35955)

CompositeBytesReference#slice has two bugs:
 - One that makes it fail if the reference is empty and an empty slice is
   created, this is #35950 and is fixed by special-casing empty-slices.
 - One performance bug that makes it always create a composite slice when
   creating a slice that ends on a boundary, this is fixed by computing `limit`
   as the index of the sub reference that holds the last element rather than
   the next element after the slice.

Closes #35950
",Buggy,"The commit message indicates a bug fix in `CompositeBytesReference#slice` that could throw an `AIOOBE` with legal offsets. The diff includes a fix for empty slices and a performance improvement by adjusting how the `limit` is calculated. The addition of `FutureObjects.checkFromIndexSize` suggests the addition of a check that was missing before, indicating a bug fix. The special casing of empty slices with `if (length == 0)` also indicates a bug fix. The change in the `limit` calculation from `getOffsetIndex(from + length)` to `getOffsetIndex(to - 1)` also indicates a bug fix. The answer is **Buggy**.
"
elasticsearch,53519.json,68c82cd113e0a79916ea9fce6546139de7753731,"@@ -1,15 +1,14 @@
     public static void writeDocumentRequest(StreamOutput out, DocumentWriteRequest request)  throws IOException {
-        assert request != null : ""request must not be null"";
         if (request instanceof IndexRequest) {
             out.writeByte((byte) 0);
         } else if (request instanceof DeleteRequest) {
             out.writeByte((byte) 1);
         } else if (request instanceof UpdateRequest) {
             out.writeByte((byte) 2);
         } else if (request instanceof UpdateReplicaRequest) {
             out.writeByte((byte) 3);
         } else {
             throw new IllegalStateException(""invalid request ["" + request.getClass().getSimpleName() + "" ]"");
         }
         request.writeTo(out);
     }",Buggy,"Revert ""fix bug in bulk replication for noop update operation""

This reverts commit 42bc2d15bedb0f3b457bbfe06247311b9667b7c9.
",Buggy,"The commit message indicates a revert of a previous bug fix related to bulk replication and noop update operations. The diff removes an assertion `assert request != null : ""request must not be null"";`. Reverting a bug fix suggests that the original fix introduced a new issue or was incorrect. Therefore, this revert is related to undoing a previous attempt to fix a bug. The answer is **Buggy**."
elasticsearch,53519.json,42bc2d15bedb0f3b457bbfe06247311b9667b7c9,"@@ -1,14 +1,15 @@
     public static void writeDocumentRequest(StreamOutput out, DocumentWriteRequest request)  throws IOException {
+        assert request != null : ""request must not be null"";
         if (request instanceof IndexRequest) {
             out.writeByte((byte) 0);
         } else if (request instanceof DeleteRequest) {
             out.writeByte((byte) 1);
         } else if (request instanceof UpdateRequest) {
             out.writeByte((byte) 2);
         } else if (request instanceof UpdateReplicaRequest) {
             out.writeByte((byte) 3);
         } else {
             throw new IllegalStateException(""invalid request ["" + request.getClass().getSimpleName() + "" ]"");
         }
         request.writeTo(out);
     }",Buggy,"fix bug in bulk replication for noop update operation
",Buggy,"The commit message indicates a bug fix related to bulk replication and no-op update operations. The code adds an assertion to check if the request is null. While this check might prevent a `NullPointerException`, it doesn't directly fix a bug in the update operation's logic itself. It's more of a defensive programming measure. However, the commit message explicitly states a bug fix, and the added assertion suggests a potential null pointer exception scenario that could occur during bulk replication with no-op updates, thus constituting a bug. The answer is **Buggy**."
elasticsearch,9986.json,7e12d5a7958eb9f62fdb073863110dbd09b79747,"@@ -1,48 +1,48 @@
     private void buildUserFromClaims(JWTClaimsSet claims, ActionListener<AuthenticationResult> authResultListener) {
         final String principal = principalAttribute.getClaimValue(claims);
         if (Strings.isNullOrEmpty(principal)) {
             authResultListener.onResponse(AuthenticationResult.unsuccessful(
                 principalAttribute + ""not found in "" + claims.toJSONObject(), null));
             return;
         }
 
         final Map<String, Object> tokenMetadata = new HashMap<>();
         tokenMetadata.put(""id_token_hint"", claims.getClaim(""id_token_hint""));
         ActionListener<AuthenticationResult> wrappedAuthResultListener = ActionListener.wrap(auth -> {
             if (auth.isAuthenticated()) {
                 // Add the ID Token as metadata on the authentication, so that it can be used for logout requests
                 Map<String, Object> metadata = new HashMap<>(auth.getMetadata());
                 metadata.put(CONTEXT_TOKEN_DATA, tokenMetadata);
                 auth = AuthenticationResult.success(auth.getUser(), metadata);
             }
             authResultListener.onResponse(auth);
         }, authResultListener::onFailure);
 
         if (delegatedRealms.hasDelegation()) {
             delegatedRealms.resolve(principal, wrappedAuthResultListener);
             return;
         }
 
         final Map<String, Object> userMetadata;
         if (populateUserMetadata) {
             userMetadata = claims.getClaims().entrySet().stream().filter(entry -> {
                 /*
                  * We whitelist the Types that we want to parse as metadata from the Claims, explicitly filtering out {@link Date}s
                  */
                 Object v = entry.getValue();
-                return (v instanceof String || v instanceof Boolean || v instanceof Number || v instanceof Collections);
+                return (v instanceof String || v instanceof Boolean || v instanceof Number || v instanceof Collection);
             }).collect(Collectors.toUnmodifiableMap(entry -> ""oidc("" + entry.getKey() + "")"", Map.Entry::getValue));
         } else {
             userMetadata = Map.of();
         }
         final List<String> groups = groupsAttribute.getClaimValues(claims);
         final String dn = dnAttribute.getClaimValue(claims);
         final String mail = mailAttribute.getClaimValue(claims);
         final String name = nameAttribute.getClaimValue(claims);
         UserRoleMapper.UserData userData = new UserRoleMapper.UserData(principal, dn, groups, userMetadata, config);
         roleMapper.resolveRoles(userData, ActionListener.wrap(roles -> {
             final User user = new User(principal, roles.toArray(Strings.EMPTY_ARRAY), name, mail, userMetadata, true);
             wrappedAuthResultListener.onResponse(AuthenticationResult.success(user));
         }, wrappedAuthResultListener::onFailure));
 
     }",Buggy,"Populate OpenIDConnect metadata collections (#50521)

The OpenIdConnectRealm had a bug which would cause it not to populate
User metadata for collections contained in the user JWT claims.

This commit fixes that bug.

Resolves: #50250",Buggy,"The commit message indicates a bug fix related to populating user metadata collections from JWT claims in the OpenIdConnectRealm. The diff focuses on the `buildUserFromClaims` method, specifically the part where user metadata is extracted from the JWT claims. The code filters claim values based on their type, and the original code had `v instanceof Collections` which should be `v instanceof Collection`. This correction ensures that collections are correctly identified and included in the user metadata, fixing the bug described in the commit message. The answer is **Buggy**."
elasticsearch,31859.json,cea2d21c50c5a680cbbaefba254866601b5b4608,"@@ -1,20 +1,20 @@
     private int findPlaceholderEndIndex(CharSequence buf, int startIndex) {
         int index = startIndex + this.placeholderPrefix.length();
         int withinNestedPlaceholder = 0;
         while (index < buf.length()) {
             if (Strings.substringMatch(buf, index, this.placeholderSuffix)) {
                 if (withinNestedPlaceholder > 0) {
                     withinNestedPlaceholder--;
-                    index = index + this.placeholderPrefix.length() - 1;
+                    index = index + this.placeholderSuffix.length();
                 } else {
                     return index;
                 }
             } else if (Strings.substringMatch(buf, index, this.placeholderPrefix)) {
                 withinNestedPlaceholder++;
                 index = index + this.placeholderPrefix.length();
             } else {
                 index++;
             }
         }
         return -1;
     }",Buggy,"Fix bug in PropertyPlaceholder and add unit tests.

Close #6034
",Buggy,"The commit message indicates a bug fix in `PropertyPlaceholder`. The diff modifies the `findPlaceholderEndIndex` method, specifically how the index is incremented when a nested placeholder suffix is encountered. The original code `index = index + this.placeholderPrefix.length() - 1;` seems incorrect, as it uses the prefix length instead of the suffix length and subtracts 1, potentially leading to an infinite loop or incorrect end index. The corrected code `index = index + this.placeholderSuffix.length();` correctly advances the index by the length of the suffix, resolving the bug. The answer is **Buggy**."
elasticsearch,52107.json,267cd65506ab22ceb7606243ef58979bde5dbfe5,"@@ -1,28 +1,28 @@
         public void start() {
             if (shardIt == null) {
                 // just execute it on the local node
                 transportService.sendRequest(clusterService.localNode(), transportShardAction, internalRequest.request(), new BaseTransportResponseHandler<Response>() {
                     @Override
                     public Response newInstance() {
                         return newResponse();
                     }
 
                     @Override
                     public String executor() {
                         return ThreadPool.Names.SAME;
                     }
 
                     @Override
                     public void handleResponse(final Response response) {
                         listener.onResponse(response);
                     }
 
                     @Override
                     public void handleException(TransportException exp) {
-                        perform(exp);
+                        listener.onFailure(exp);
                     }
                 });
             } else {
                 perform(null);
             }
         }",Buggy,"Analysis : Fix no response from Analyze API without specified index

Fix error handling in TransportSingleShardAction without shardIt

Closes #15148
",Buggy,"The commit message indicates a fix for an error handling issue in `TransportSingleShardAction` when `shardIt` is null. The diff shows that in the `handleException` method, instead of calling `perform(exp)`, which seems like an attempt to retry or handle the exception internally, the code now calls `listener.onFailure(exp)`, which correctly propagates the exception to the listener. This change ensures that failures are properly reported when the action is executed on the local node without shard information. This is a bug fix. The answer is **Buggy**."
elasticsearch,11277.json,3b739b9fd5da2323a84b242708b64c16660113cc,"@@ -1,6 +1,6 @@
         protected ShardsIterator shards(ClusterState state, InternalRequest request) {
-            return state.routingTable()
-                    .index(request.concreteIndex())
-                    .shard(request.request().getShard().id())
+            return state
+                    .routingTable()
+                    .shardRoutingTable(request.concreteIndex(), request.request().getShard().id())
                     .activeInitializingShardsRandomIt();
         }",Buggy,"Avoid NPE on shard changes action (#32630)

If a leader index is deleted while there is an active follower, the
follower will send shard changes requests bound for the leader
index. Today this will result in a null pointer exception because there
will not be an index routing table for the index. A null pointer
exception looks like a bug to a user so this commit addresses this by
throwing an index not found exception instead.",Buggy,"The commit message indicates a fix for a NullPointerException (NPE) that occurs when a leader index is deleted while a follower is still active. The follower sends shard changes requests to the deleted leader index, leading to an NPE because the index routing table is missing. The commit replaces the NPE with an `IndexNotFoundException`, which is a more appropriate and user-friendly error.

The code diff changes how the `shards` iterator is obtained. Instead of directly accessing the index routing table using `state.routingTable().index(request.concreteIndex())`, it now uses `state.routingTable().shardRoutingTable(request.concreteIndex(), request.request().getShard().id())`. This change likely avoids the NPE by checking if the index exists before attempting to access its routing table. If the index is not found, `shardRoutingTable` probably returns null or throws an exception, which is then handled to throw the `IndexNotFoundException` as described in the commit message.

The changes directly address the bug described in the commit message. The original code caused an unexpected NPE, and the modified code handles the situation more gracefully by throwing a more informative exception.

The answer is **Buggy**.
"
elasticsearch,50555.json,03e8734b04d841e31724b10cf0d9c41801699411,"@@ -1,24 +1,29 @@
     private void getMultipleReposSnapshotInfo(List<RepositoryMetaData> repos, String[] snapshots, boolean ignoreUnavailable,
                                               boolean verbose, ActionListener<GetSnapshotsResponse> listener) {
+        // short-circuit if there are no repos, because we can not create GroupedActionListener of size 0
+        if (repos.isEmpty()) {
+            listener.onResponse(new GetSnapshotsResponse(Collections.emptyList()));
+            return;
+        }
         final GroupedActionListener<GetSnapshotsResponse.Response> groupedActionListener =
                 new GroupedActionListener<>(
                         ActionListener.map(listener, responses -> {
                             assert repos.size() == responses.size();
                             return new GetSnapshotsResponse(responses);
                         }), repos.size());
 
         // run concurrently for all repos on GENERIC thread pool
         for (final RepositoryMetaData repo : repos) {
             threadPool.executor(ThreadPool.Names.GENERIC).execute(new ActionRunnable<>(groupedActionListener) {
                 @Override
                 protected void doRun() {
                     try {
                         groupedActionListener.onResponse(GetSnapshotsResponse.Response.snapshots(
                                 repo.name(), getSingleRepoSnapshotInfo(repo.name(), snapshots, ignoreUnavailable, verbose)));
                     } catch (ElasticsearchException e) {
                         groupedActionListener.onResponse(GetSnapshotsResponse.Response.error(repo.name(), e));
                     }
                 }
             });
         }
     }",Buggy,"Fix GET /_snapshot/_all/_all if there are no repos (#43558)

When there are no repositories, a request to GET /_snapshot/_all/_all
returns a 504 timeout error.
This happens because try to create GroupedActionListener with the
size of zero, which leads to an exception.
This commit short-circuits if there are no repos and adds a test to
verify the fix.

Closes #43547",Buggy,"The commit message indicates a bug where requesting snapshot information for all repositories when no repositories exist results in a 504 timeout error. The provided code diff introduces a check for an empty repository list (`repos.isEmpty()`). If the list is empty, the code now short-circuits and returns an empty `GetSnapshotsResponse`, preventing the creation of a `GroupedActionListener` with a size of zero, which was causing the exception. This aligns with the commit message's description of the bug and the fix. The answer is **Buggy**.
"
elasticsearch,35892.json,ba8ad9c2b727f9609e16aac6ed2df17f44c6b683,"@@ -1,3 +1,3 @@
     public long getAgeInMillis() {
-        return Math.max(0, (System.nanoTime() - creationDate) / 1000);
+        return TimeUnit.MILLISECONDS.convert(relativeTimeProvider.getAsLong() - creationDate, TimeUnit.NANOSECONDS);
     }",Buggy,"Fix calculation of age of pending tasks

This commit addresses a time unit conversion bug in calculating the age
of a PrioritizedRunnable. The issue was an incorrect conversion from
nanoseconds to milliseconds as instead the conversion was to
microseconds. This leads to the timeInQueue metric for pending tasks to
be off by three orders of magnitude.
",Buggy,"The commit message clearly states a bug fix related to time unit conversion in calculating the age of pending tasks. The diff confirms this by showing a change in the `getAgeInMillis()` method. The original code incorrectly converted nanoseconds to milliseconds by dividing by 1000, which is the conversion to microseconds. The corrected code now uses `TimeUnit.MILLISECONDS.convert()` to properly convert nanoseconds to milliseconds. This confirms the bug fix described in the commit message. The answer is **Buggy**.
"
elasticsearch,55497.json,53f6dcfd375169c9847a6059253a1990fb7c99f5,"@@ -1,11 +1,11 @@
         private SpanQuery newSpanQuery(Term[] terms, boolean isPrefix) {
             if (terms.length == 1) {
                 return isPrefix ? fieldType.spanPrefixQuery(terms[0].text(), spanRewriteMethod, context) : new SpanTermQuery(terms[0]);
             }
             SpanQuery[] spanQueries = new SpanQuery[terms.length];
             for (int i = 0; i < terms.length; i++) {
-                spanQueries[i] = isPrefix ? new SpanTermQuery(terms[i]) :
-                    fieldType.spanPrefixQuery(terms[i].text(), spanRewriteMethod, context);
+                spanQueries[i] = isPrefix ? fieldType.spanPrefixQuery(terms[i].text(), spanRewriteMethod, context) :
+                    new SpanTermQuery(terms[i]);
             }
             return new SpanOrQuery(spanQueries);
         }",Buggy,"Fix wrong logic in `match_phrase` query with multi-word synonyms (#43941)

Disjunction over two individual terms in a phrase query with multi-word synonyms
wrongly applies a prefix query to each of these terms. This change fixes this bug
by inversing the logic to use prefixes on `phrase_prefix` queries only.

Closes #43308",Buggy,"The commit message indicates a bug fix in the `match_phrase` query logic when dealing with multi-word synonyms. The bug caused prefix queries to be incorrectly applied to individual terms in a disjunction. The diff inverts the logic for applying prefixes, ensuring they are only used in `phrase_prefix` queries. This change directly addresses the bug described in the commit message. The answer is **Buggy**."
elasticsearch,58217.json,a46d2f21c6504c8ff8cb98a2180293994b857a03,"@@ -1,10 +1,11 @@
     private static int expandCommonMappers(List<ObjectMapper> parentMappers, String[] nameParts, int i) {
         ObjectMapper last = parentMappers.get(parentMappers.size() - 1);
         while (i < nameParts.length - 1 && last.getMapper(nameParts[i]) != null) {
             Mapper newLast = last.getMapper(nameParts[i]);
             assert newLast instanceof ObjectMapper;
-            parentMappers.add((ObjectMapper)newLast);
+            last = (ObjectMapper) newLast;
+            parentMappers.add(last);
             ++i;
         }
         return i;
     }",Buggy,"Fix dynamic mapper bug with deeply nested fields.
",Buggy,"The code modifies the `expandCommonMappers` method. The original code adds `newLast` to `parentMappers` and then increments `i`. The corrected code assigns `newLast` to `last`, then adds `last` to `parentMappers`, and increments `i`. The original code was not updating the `last` variable, so it was not traversing deeply nested fields correctly. This change fixes a bug where deeply nested fields were not being mapped correctly. The answer is **Buggy**."
elasticsearch,63178.json,c33f894846989694f6d6fdc250ae396a88545c55,"@@ -1,19 +1,4 @@
         public Set<Entry<String, List<String>>> entrySet() {
-            return httpHeaders.names().stream().map(k -> new Entry<String, List<String>>() {
-
-                @Override
-                public String getKey() {
-                    return k;
-                }
-
-                @Override
-                public List<String> getValue() {
-                    return httpHeaders.getAll(k);
-                }
-
-                @Override
-                public List<String> setValue(List<String> value) {
-                    throw new UnsupportedOperationException(""modifications are not supported"");
-                }
-            }).collect(Collectors.toSet());
+            return httpHeaders.names().stream().map(k -> new AbstractMap.SimpleImmutableEntry<>(k, httpHeaders.getAll(k)))
+                    .collect(Collectors.toSet());
         }",Buggy,"Fixing compilation problem in Eclipse (#22956)

",Buggy,"The commit message indicates a fix for a compilation problem in Eclipse. The diff replaces an anonymous class implementation of `Entry<String, List<String>>` with `AbstractMap.SimpleImmutableEntry`. Anonymous classes can sometimes cause issues with serialization or reflection in different environments, potentially leading to compilation problems in specific IDEs like Eclipse. The change replaces the anonymous class with a standard, well-defined class, resolving potential compatibility issues. The answer is **Buggy**."
elasticsearch,10846.json,70d524c979f90759901fd81f70c56172c311bd86,"@@ -1,7 +1,7 @@
     protected boolean shouldCollect() {
-        if (licensee.collectionEnabled()) {
+        if (!licensee.collectionEnabled()) {
             logger.trace(""collector [{}] can not collect data due to invalid license"", name());
             return false;
         }
         return true;
     }",Buggy,"Marvel: Fix bug that prevent collectors to collect when license is enabled

Original commit: elastic/x-pack-elasticsearch@9f22baa2ee8f873e4123681d36bcbe759da619f1
",Buggy,"The commit message indicates a bug fix related to data collection when a license is enabled. The diff shows a change in the `shouldCollect()` method. The original code had a condition `if (licensee.collectionEnabled())`, which would proceed with collection if the license was enabled. The corrected code has `if (!licensee.collectionEnabled())`, which now prevents collection if the license is *not* enabled, effectively fixing the bug where collectors were prevented from collecting when the license was enabled. The answer is **Buggy**."
elasticsearch,15635.json,00cecac86e03195a28f4bec92ccd8cc182a16132,"@@ -1,13 +1,15 @@
     public List<ExecutorBuilder<?>> getExecutorBuilders(final Settings settings) {
         if (enabled) {
-            final FixedExecutorBuilder builder =
-                    new FixedExecutorBuilder(
-                            settings,
+            final ScalingExecutorBuilder builder =
+                    new ScalingExecutorBuilder(
                             InternalWatchExecutor.THREAD_POOL_NAME,
+                            0,
+                            // watcher threads can block on I/O for a long time, so we let this
+                            // pool be large so that execution of unblocked watches can proceed
                             5 * EsExecutors.boundedNumberOfProcessors(settings),
-                            1000,
+                            TimeValue.timeValueMinutes(5),
                             ""xpack.watcher.thread_pool"");
             return Collections.singletonList(builder);
         }
         return Collections.emptyList();
     }",Buggy,"Change Watcher thread pool to be scaling

Watcher uses a custom thread pool. This is because executing watches can
be long-running tasks that often block on I/O and it is best to not
consume the core thread pools with these tasks. Today this thread pool
is fixed, and sized at five times the bounded number of cores (so 160 on
a 32-core box). It makes sense for there to possibly be so many threads,
again because these tasks can block on I/O and having excess capacity
lets unblocked watches execute. It's the fixed size that can cause
problem, all these threads are always consuming resources even when
there are no or not that many watches running. This commit changes this
thread pool to be a scaling thread pool.

Relates elastic/elasticsearch#3660

Original commit: elastic/x-pack-elasticsearch@3cafab6e8360b17ecaf99d37bc1e024a9c7ff256
",Buggy,"The commit message describes a change from a fixed-size thread pool to a scaling thread pool for Watcher tasks. The rationale is to reduce resource consumption when there are few or no watches running, as the fixed-size pool always consumes resources regardless of the workload. The diff confirms this change by replacing `FixedExecutorBuilder` with `ScalingExecutorBuilder`. This change addresses a potential performance issue or resource wastage, which can be considered a bug fix in a broader sense. The answer is **Buggy**.
"
elasticsearch,11653.json,bca4edcd56fa984b9e712f9298e6f91a19983710,"@@ -1,76 +1,77 @@
     static Tuple<String, String> overrideFormatToGrokAndRegex(String overrideFormat) {
 
         if (overrideFormat.indexOf('\n') >= 0 || overrideFormat.indexOf('\r') >= 0) {
             throw new IllegalArgumentException(""Multi-line timestamp formats ["" + overrideFormat + ""] not supported"");
         }
 
         if (overrideFormat.indexOf(INDETERMINATE_FIELD_PLACEHOLDER) >= 0) {
             throw new IllegalArgumentException(""Timestamp format ["" + overrideFormat + ""] not supported because it contains [""
                 + INDETERMINATE_FIELD_PLACEHOLDER + ""]"");
         }
 
         StringBuilder grokPatternBuilder = new StringBuilder();
         StringBuilder regexBuilder = new StringBuilder();
 
         boolean notQuoted = true;
         char prevChar = '\0';
         String prevLetterGroup = null;
         int pos = 0;
         while (pos < overrideFormat.length()) {
             char curChar = overrideFormat.charAt(pos);
 
             if (curChar == '\'') {
                 notQuoted = !notQuoted;
             } else if (notQuoted && Character.isLetter(curChar)) {
                 int startPos = pos;
                 int endPos = startPos + 1;
                 while (endPos < overrideFormat.length() && overrideFormat.charAt(endPos) == curChar) {
                     ++endPos;
                     ++pos;
                 }
                 String letterGroup = overrideFormat.substring(startPos, endPos);
                 Tuple<String, String> grokPatternAndRegexForGroup = VALID_LETTER_GROUPS.get(letterGroup);
                 if (grokPatternAndRegexForGroup == null) {
                     // Special case of fractional seconds
                     if (curChar != 'S' || FRACTIONAL_SECOND_SEPARATORS.indexOf(prevChar) == -1 ||
                         ""ss"".equals(prevLetterGroup) == false || endPos - startPos > 9) {
                         String msg = ""Letter group ["" + letterGroup + ""] in ["" + overrideFormat + ""] is not supported"";
                         if (curChar == 'S') {
                             msg += "" because it is not preceded by [ss] and a separator from ["" + FRACTIONAL_SECOND_SEPARATORS + ""]"";
                         }
                         throw new IllegalArgumentException(msg);
                     }
-                    // No need to append to the Grok pattern as %{SECOND} already allows for an optional
-                    // fraction, but we need to remove the separator that's included in %{SECOND}
-                    grokPatternBuilder.deleteCharAt(grokPatternBuilder.length() - 1);
+                    // No need to append to the Grok pattern as %{SECOND} already allows for an optional fraction,
+                    // but we need to remove the separator that's included in %{SECOND} (and that might be escaped)
+                    int numCharsToDelete = (PUNCTUATION_THAT_NEEDS_ESCAPING_IN_REGEX.indexOf(prevChar) >= 0) ? 2 : 1;
+                    grokPatternBuilder.delete(grokPatternBuilder.length() - numCharsToDelete, grokPatternBuilder.length());
                     regexBuilder.append(""\\d{"").append(endPos - startPos).append('}');
                 } else {
                     grokPatternBuilder.append(grokPatternAndRegexForGroup.v1());
                     if (regexBuilder.length() == 0) {
                         regexBuilder.append(""\\b"");
                     }
                     regexBuilder.append(grokPatternAndRegexForGroup.v2());
                 }
                 if (pos + 1 == overrideFormat.length()) {
                     regexBuilder.append(""\\b"");
                 }
                 prevLetterGroup = letterGroup;
             } else {
                 if (PUNCTUATION_THAT_NEEDS_ESCAPING_IN_REGEX.indexOf(curChar) >= 0) {
                     grokPatternBuilder.append('\\');
                     regexBuilder.append('\\');
                 }
                 grokPatternBuilder.append(curChar);
                 regexBuilder.append(curChar);
             }
 
             prevChar = curChar;
             ++pos;
         }
 
         if (prevLetterGroup == null) {
             throw new IllegalArgumentException(""No time format letter groups in override format ["" + overrideFormat + ""]"");
         }
 
         return new Tuple<>(grokPatternBuilder.toString(), regexBuilder.toString());
     }",Buggy,"[ML] Fix custom timestamp override with dot-separated fractional seconds (#44127)

Custom timestamp overrides provided to the find_file_structure
endpoint produced an invalid Grok pattern if the fractional
seconds separator was a dot rather than a comma or colon.
This commit fixes that problem and adds tests for this sort
of timestamp override.

Fixes #44110",Buggy,"The commit message indicates a bug fix related to custom timestamp overrides in the find_file_structure endpoint. Specifically, it addresses an issue where Grok patterns were generated incorrectly when the fractional seconds separator was a dot. The diff modifies the overrideFormatToGrokAndRegex method to correctly handle dot-separated fractional seconds. The change involves adjusting how the separator between seconds and fractional seconds is handled when constructing the Grok pattern. Instead of unconditionally deleting one character, the code now checks if the preceding character needs escaping in regex and deletes one or two characters accordingly. This suggests a bug fix related to incorrect Grok pattern generation. The answer is **Buggy**."
elasticsearch,26537.json,61f5c188e0a577555db382dd0fe9a4222da9df1a,"@@ -1,12 +1,12 @@
-    static Request openJob(OpenJobRequest openJobRequest) {
+    static Request openJob(OpenJobRequest openJobRequest) throws IOException {
         String endpoint = new EndpointBuilder()
                 .addPathPartAsIs(""_xpack"")
                 .addPathPartAsIs(""ml"")
                 .addPathPartAsIs(""anomaly_detectors"")
                 .addPathPart(openJobRequest.getJobId())
                 .addPathPartAsIs(""_open"")
                 .build();
         Request request = new Request(HttpPost.METHOD_NAME, endpoint);
         request.setEntity(createEntity(openJobRequest, REQUEST_BODY_CONTENT_TYPE));
         return request;
     }",Buggy,"HLRC: Fix Compile Error From Missing Throws (#33083)

* 50441f97ae745814db96c262e99d0f465aca5b2c#diff-53a95fe7ded21313483f1b2f15977395L72 removed the throws breaking compilation here",Buggy,"The commit message indicates a fix for a compile error caused by a missing `throws` declaration. The diff adds `throws IOException` to the `openJob` method signature. This aligns with the commit message, indicating that the change is a bug fix to resolve a compilation issue. The answer is **Buggy**."
elasticsearch,54800.json,2637e499ac88ce0aa14db62b13be052e7ca78f98,"@@ -1,8 +1,8 @@
     protected final DirectoryReader wrapReader(DirectoryReader reader,
                                                     Function<DirectoryReader, DirectoryReader> readerWrapperFunction) throws IOException {
-        reader = ElasticsearchDirectoryReader.wrap(reader, engineConfig.getShardId());
         if (engineConfig.getIndexSettings().isSoftDeleteEnabled()) {
             reader = new SoftDeletesDirectoryReaderWrapper(reader, Lucene.SOFT_DELETES_FIELD);
         }
-        return readerWrapperFunction.apply(reader);
+        reader = readerWrapperFunction.apply(reader);
+        return ElasticsearchDirectoryReader.wrap(reader, engineConfig.getShardId());
     }",Buggy,"Fix assertion error when caching the result of a search in a read-only index (#41900)

The ReadOnlyEngine wraps its reader with a SoftDeletesDirectoryReaderWrapper if soft deletes
are enabled. However the wrapping is done on top of the ElasticsearchDirectoryReader and that
trips assertion later on since the cache key of these directories are different. This commit
changes the order of the wrapping to put the ElasticsearchDirectoryReader first in order to
ensure that it is always retrieved first when we unwrap the directory.

Closes #41795
",Buggy,"The commit message indicates a fix for an assertion error related to caching search results in a read-only index when soft deletes are enabled. The issue arises from the order in which `DirectoryReader` instances are wrapped, specifically `ElasticsearchDirectoryReader` and `SoftDeletesDirectoryReaderWrapper`. The original order caused different cache keys, leading to assertion failures. The fix reorders the wrapping to ensure `ElasticsearchDirectoryReader` is always retrieved first. The code diff reflects this reordering. This clearly indicates a bug fix. The answer is **Buggy**."
elasticsearch,31501.json,0f93b7abdf65425344f5cedda3e6b04b31e910d4,"@@ -1,3 +1,3 @@
-    static boolean needsReassignment(final Assignment assignment, final DiscoveryNodes nodes) {
+    public static boolean needsReassignment(final Assignment assignment, final DiscoveryNodes nodes) {
         return (assignment.isAssigned() == false || nodes.nodeExists(assignment.getExecutorNode()) == false);
     }",Buggy,"Fix compilation errors in ML integration tests

After elastic/elasticsearch#29109, the `needsReassignment` method has
been moved to the PersistentTasksClusterService. This commit fixes
some compilation in tests I introduced.
",Buggy,"The commit message indicates that the change fixes compilation errors in ML integration tests due to a method being moved. The diff shows a change in the visibility of the `needsReassignment` method from package-private to public. This change likely resolves a compilation error where the test code could no longer access the method after it was moved to a different package. The change directly addresses a compilation issue, which is a type of bug. The answer is **Buggy**."
elasticsearch,63560.json,63f33e0f1e045f5c345b80bdbc4b6f367e72aaad,"@@ -1,16 +1,18 @@
     public void writeTo(StreamOutput out) throws IOException {
+        // marshall doc count
+        out.writeGenericValue(docCount);
         // marshall fieldSum
         out.writeGenericValue(fieldSum);
         // counts
         out.writeGenericValue(counts);
         // mean
         out.writeGenericValue(means);
         // variances
         out.writeGenericValue(variances);
         // skewness
         out.writeGenericValue(skewness);
         // kurtosis
         out.writeGenericValue(kurtosis);
         // covariances
         out.writeGenericValue(covariances);
     }",Buggy,"Serialize doc counts in Matrix-Stats module

This fixes a bug in the RunningStats class for the matrix stats aggregation module. doc counts were not being searlized which means they were only computed the first time the aggregation was computed. This was causing incorrect results when the aggregation was pulled from cache.
",Buggy,"The commit message clearly states a bug fix in the RunningStats class within the matrix-stats aggregation module. The bug was that document counts were not being serialized, leading to incorrect results when the aggregation was retrieved from the cache. The diff shows that `docCount` is now being serialized using `out.writeGenericValue(docCount)`. This aligns perfectly with the commit message, indicating a fix for the described bug. The answer is **Buggy**."
elasticsearch,62818.json,e6fb3a5d950c52ccba8cf1f0b0d6e819e70c243c,"@@ -1,15 +1,21 @@
     public void execute(IngestDocument document) {
         String oldVal = document.getFieldValue(field, String.class, ignoreMissing);
 
         if (oldVal == null && ignoreMissing) {
             return;
         } else if (oldVal == null) {
             throw new IllegalArgumentException(""field ["" + field + ""] is null, cannot extract key-value pairs."");
         }
 
         String fieldPathPrefix = (targetField == null) ? """" : targetField + ""."";
         Arrays.stream(oldVal.split(fieldSplit))
-            .map((f) -> f.split(valueSplit, 2))
+            .map((f) -> {
+                String[] kv = f.split(valueSplit, 2);
+                if (kv.length != 2) {
+                    throw new IllegalArgumentException(""field ["" + field + ""] does not contain value_split ["" + valueSplit + ""]"");
+                }
+                return kv;
+            })
             .filter((p) -> includeKeys == null || includeKeys.contains(p[0]))
             .forEach((p) -> append(document, fieldPathPrefix + p[0], p[1]));
     }",Buggy,"fix index out of bounds error in KV Processor (#22288)

- checks for index-out-of-bounds
- added unit tests for failed `field_split` and `value_split` scenarios

missed this test in #22272.",Buggy,"The commit message indicates a fix for an index out of bounds error in the KV Processor. The diff adds a check to ensure that the split result `kv` has a length of 2. If it doesn't, it throws an IllegalArgumentException. This directly addresses the potential index out of bounds error that could occur when accessing `p[0]` and `p[1]` if the split doesn't produce two elements. The added unit tests for failed `field_split` and `value_split` scenarios further support that this commit is a bug fix. The answer is **Buggy**.
"
elasticsearch,31383.json,76cd7b1eb2cf7f87081c24ae6362ae1d9dec6100,"@@ -1,3 +1,3 @@
-        public Params getParams() {
+        public P getParams() {
             return params;
         }",Buggy,"Fixes compile errors in Eclipse due to generics

PersistentTasksCustomMetadata was using a generic param named `Params`. This conflicted with the imported interface `ToXContent.Params`. The java compiler was preferring the generic param over the interface so everything was fine but Eclipse apparently prefers the interface int his case which was screwing up the Hierarchy and causing compile errors in Eclipse. This changes fixes it by renaming the Generic param to `P`
",Buggy,The commit message indicates a fix for compile errors in Eclipse due to a naming conflict with a generic parameter. The diff shows that the generic parameter `Params` was renamed to `P` to resolve the conflict with the imported interface `ToXContent.Params`. This change directly addresses the compilation issue described in the commit message. The answer is **Buggy**.
elasticsearch,18621.json,6236b3aee4c4ccf9a06ca94af4c4082b7fcf5cde,"@@ -1,32 +1,45 @@
     public ResourcePrivilegesMap checkResourcePrivileges(Set<String> checkForIndexPatterns, boolean allowRestrictedIndices,
                                                          Set<String> checkForPrivileges) {
         final ResourcePrivilegesMap.Builder resourcePrivilegesMapBuilder = ResourcePrivilegesMap.builder();
         final Map<IndicesPermission.Group, Automaton> predicateCache = new HashMap<>();
         for (String forIndexPattern : checkForIndexPatterns) {
-            final Automaton checkIndexAutomaton = IndicesPermission.Group.buildIndexMatcherAutomaton(allowRestrictedIndices,
-                    forIndexPattern);
-            Automaton allowedIndexPrivilegesAutomaton = null;
-            for (Group group : groups) {
-                final Automaton groupIndexAutomaton = predicateCache.computeIfAbsent(group,
-                        g -> IndicesPermission.Group.buildIndexMatcherAutomaton(g.allowRestrictedIndices(), g.indices()));
-                if (Operations.subsetOf(checkIndexAutomaton, groupIndexAutomaton)) {
-                    if (allowedIndexPrivilegesAutomaton != null) {
-                        allowedIndexPrivilegesAutomaton = Automatons
-                                .unionAndMinimize(Arrays.asList(allowedIndexPrivilegesAutomaton, group.privilege().getAutomaton()));
-                    } else {
-                        allowedIndexPrivilegesAutomaton = group.privilege().getAutomaton();
+            Automaton checkIndexAutomaton = Automatons.patterns(forIndexPattern);
+            if (false == allowRestrictedIndices && false == RestrictedIndicesNames.RESTRICTED_NAMES.contains(forIndexPattern)) {
+                checkIndexAutomaton = Automatons.minusAndMinimize(checkIndexAutomaton, RestrictedIndicesNames.NAMES_AUTOMATON);
+            }
+            if (false == Operations.isEmpty(checkIndexAutomaton)) {
+                Automaton allowedIndexPrivilegesAutomaton = null;
+                for (Group group : groups) {
+                    final Automaton groupIndexAutomaton = predicateCache.computeIfAbsent(group,
+                            g -> IndicesPermission.Group.buildIndexMatcherAutomaton(g.allowRestrictedIndices(), g.indices()));
+                    if (Operations.subsetOf(checkIndexAutomaton, groupIndexAutomaton)) {
+                        if (allowedIndexPrivilegesAutomaton != null) {
+                            allowedIndexPrivilegesAutomaton = Automatons
+                                    .unionAndMinimize(Arrays.asList(allowedIndexPrivilegesAutomaton, group.privilege().getAutomaton()));
+                        } else {
+                            allowedIndexPrivilegesAutomaton = group.privilege().getAutomaton();
+                        }
                     }
                 }
-            }
-            for (String privilege : checkForPrivileges) {
-                IndexPrivilege indexPrivilege = IndexPrivilege.get(Collections.singleton(privilege));
-                if (allowedIndexPrivilegesAutomaton != null
-                        && Operations.subsetOf(indexPrivilege.getAutomaton(), allowedIndexPrivilegesAutomaton)) {
-                    resourcePrivilegesMapBuilder.addResourcePrivilege(forIndexPattern, privilege, Boolean.TRUE);
-                } else {
+                for (String privilege : checkForPrivileges) {
+                    IndexPrivilege indexPrivilege = IndexPrivilege.get(Collections.singleton(privilege));
+                    if (allowedIndexPrivilegesAutomaton != null
+                            && Operations.subsetOf(indexPrivilege.getAutomaton(), allowedIndexPrivilegesAutomaton)) {
+                        resourcePrivilegesMapBuilder.addResourcePrivilege(forIndexPattern, privilege, Boolean.TRUE);
+                    } else {
+                        resourcePrivilegesMapBuilder.addResourcePrivilege(forIndexPattern, privilege, Boolean.FALSE);
+                    }
+                }
+            } else {
+                // the index pattern produced the empty automaton, presumably because the requested pattern expands exclusively inside the
+                // restricted indices namespace - a namespace of indices that are normally hidden when granting/checking privileges - and
+                // the pattern was not marked as `allowRestrictedIndices`. We try to anticipate this by considering _explicit_ restricted
+                // indices even if `allowRestrictedIndices` is false.
+                // TODO The `false` result is a _safe_ default but this is actually an error. Make it an error.
+                for (String privilege : checkForPrivileges) {
                     resourcePrivilegesMapBuilder.addResourcePrivilege(forIndexPattern, privilege, Boolean.FALSE);
                 }
             }
         }
         return resourcePrivilegesMapBuilder.build();
     }",Buggy,"Fix Has Privilege API check on restricted indices (#41226)

The Has Privileges API allows to tap into the authorization process, to validate
privileges without actually running the operations to be authorized. This commit
fixes a bug, in which the Has Privilege API returned spurious results when checking
for index privileges over restricted indices (currently .security, .security-6,
.security-7). The actual authorization process is not affected by the bug.",Buggy,"The commit message indicates a bug fix in the Has Privilege API related to restricted indices. The code diff modifies the logic for checking index privileges, specifically addressing the handling of restricted indices when `allowRestrictedIndices` is false. The changes involve constructing and manipulating automatons to represent index patterns and privileges. The new code explicitly excludes restricted indices from the check if `allowRestrictedIndices` is false and the index pattern is not a restricted name. The code also handles the case where the index pattern produces an empty automaton, which can occur when the pattern expands exclusively inside the restricted indices namespace. This suggests that the original code was not correctly handling restricted indices in certain scenarios, leading to spurious results. The addition of the `else` block with the comment ""// TODO The `false` result is a _safe_ default but this is actually an error. Make it an error."" further supports the presence of a bug. The answer is **Buggy**."
elasticsearch,52742.json,a36543531b4f547bbb7be8156d4c1e55f0d53cf9,"@@ -1,8 +1,11 @@
     private void onFailedFreedContext(Throwable e, DiscoveryNode node) {
         logger.warn(() -> new ParameterizedMessage(""Clear SC failed on node[{}]"", node), e);
+        /*
+         * We have to set the failure marker before we count down otherwise we can expose the failure marker before we have set it to a
+         * racing thread successfully freeing a context. This would lead to that thread responding that the clear scroll succeeded.
+         */
+        hasFailed.set(true);
         if (expectedOps.countDown()) {
             listener.onResponse(new ClearScrollResponse(false, freedSearchContexts.get()));
-        } else {
-            hasFailed.set(true);
         }
     }",Buggy,"Fix race in clear scroll (#31259)

Here is the problem: if two threads are racing and one hits a failure
freeing a context and the other succeeded, we can expose the value of
the has failure marker to the succeeding thread before the failing
thread has had a chance to set the failure marker. This is a problem if
the failing thread counted down the expected number of operations, then
be put to sleep by a gentle lullaby from the OS, and then the other
thread could count down to zero. Since the failing thread did not get to
set the failure marker, the succeeding thread would respond that the
clear scroll succeeded and that makes that thread a liar. This commit
addresses by first setting the failure marker before we potentially
expose its value to another thread.",Buggy,"The commit message describes a race condition where a thread might incorrectly report success due to the order of operations. The diff moves `hasFailed.set(true)` before `expectedOps.countDown()`. This ensures the failure marker is set before another thread can check it, preventing the race condition described in the commit message. The changes directly address the bug described. The answer is **Buggy**."
elasticsearch,9348.json,308ae98988fb9d7c5374a5b16dac810a7bd9715c,"@@ -1,20 +1,22 @@
         private List<Realm> getRealmList(String principal) {
             final List<Realm> orderedRealmList = this.defaultOrderedRealmList;
             if (lastSuccessfulAuthCache != null) {
                 final Realm lastSuccess = lastSuccessfulAuthCache.get(principal);
                 if (lastSuccess != null) {
                     final int index = orderedRealmList.indexOf(lastSuccess);
                     if (index > 0) {
                         final List<Realm> smartOrder = new ArrayList<>(orderedRealmList.size());
                         smartOrder.add(lastSuccess);
-                        for (int i = 1; i < orderedRealmList.size(); i++) {
+                        for (int i = 0; i < orderedRealmList.size(); i++) {
                             if (i != index) {
                                 smartOrder.add(orderedRealmList.get(i));
                             }
                         }
+                        assert smartOrder.size() == orderedRealmList.size() && smartOrder.containsAll(orderedRealmList)
+                            : ""Element mismatch between SmartOrder="" + smartOrder + "" and DefaultOrder="" + orderedRealmList;
                         return Collections.unmodifiableList(smartOrder);
                     }
                 }
             }
             return orderedRealmList;
         }",Buggy,"Fix iterate-from-1 bug in smart realm order (#49473)

The AuthenticationService has a feature to ""smart order"" the realm
chain so that whicherver realm was the last one to successfully
authenticate a given user will be tried first when that user tries to
authenticate again.

There was a bug where the building of this realm order would
incorrectly drop the first realm from the default chain unless that
realm was the ""last successful"" realm.

In most cases this didn't cause problems because the first realm is
the reserved realm and so it is unusual for a user that authenticated
against a different realm to later need to authenticate against the
resevered realm.

This commit fixes that bug and adds relevant asserts and tests.",Buggy,"The commit message clearly states a bug related to the ""smart realm order"" feature in the AuthenticationService. The bug caused the first realm in the default chain to be dropped incorrectly. The provided code diff modifies the loop in `getRealmList` from `i = 1` to `i = 0`. This change ensures that all realms from the `defaultOrderedRealmList` are considered when constructing the `smartOrder` list, thus fixing the bug where the first realm was dropped. The added assert confirms that the `smartOrder` list contains all elements from the `defaultOrderedRealmList`.

The answer is **Buggy**.
"
elasticsearch,38299.json,38085cf90af953cfcaf7807c2dfccc55b742cdff,"@@ -1,5 +1,4 @@
     public AggregationPath subPath(int offset, int length) {
-        PathElement[] subTokens = new PathElement[length];
-        System.arraycopy(pathElements, offset, subTokens, 0, length);
-        return new AggregationPath(pathElements);
+        List<PathElement> subTokens = new ArrayList<>(pathElements.subList(offset, offset + length));
+        return new AggregationPath(subTokens);
     }",Buggy,"Aggregation: Fix AggregationPath.subPath() to not throw ArrayStoreException

Aggregation.subPath() always threw an ArrayStoreException because we were trying to pass a List into System.arraycopy(). This change fixes that bug and adds a test to prevent regression
",Buggy,"The commit message indicates a bug fix in `AggregationPath.subPath()` that was throwing an `ArrayStoreException`. The diff shows that the code was indeed using `System.arraycopy()` with `PathElement[]` as the destination, but it was trying to copy from a `List<PathElement>`. The corrected code now uses `ArrayList<>(pathElements.subList(offset, offset + length))` to create a new list, resolving the type mismatch and fixing the bug. The commit message also mentions adding a test to prevent regression, further indicating that this change is a bug fix. The answer is **Buggy**."
elasticsearch,10639.json,8b201e64ffeffa8dfe8c5849a81decb2a62dbe06,"@@ -1,19 +1,20 @@
     public static String loadWatch(final ClusterService clusterService, final String watchId) {
         final String resource = String.format(Locale.ROOT, WATCH_FILE, watchId);
 
         try {
             final String clusterUuid = clusterService.state().metaData().clusterUUID();
             final String uniqueWatchId = createUniqueWatchId(clusterUuid, watchId);
 
             // load the resource as-is
             String source = loadResource(resource).utf8ToString();
 
             source = CLUSTER_UUID_PROPERTY.matcher(source).replaceAll(clusterUuid);
             source = WATCH_ID_PROPERTY.matcher(source).replaceAll(watchId);
             source = UNIQUE_WATCH_ID_PROPERTY.matcher(source).replaceAll(uniqueWatchId);
+            source = VERSION_CREATED_PROPERTY.matcher(source).replaceAll(Integer.toString(LAST_UPDATED_VERSION));
 
             return source;
         } catch (final IOException e) {
             throw new RuntimeException(""Unable to load Watch ["" + watchId + ""]"", e);
         }
     }",Buggy,"Fix cluster alert for watcher/monitoring IndexOutOfBoundsExcep (#45308)

If a cluster sending monitoring data is unhealthy and triggers an
alert, then stops sending data the following exception [1] can occur.

This exception stops the current Watch and the behavior is actually
correct in part due to the exception. Simply fixing the exception
introduces some incorrect behavior. Now that the Watch does not
error in the this case, it will result in an incorrectly ""resolved""
alert.  The fix here is two parts a) fix the exception b) fix the
following incorrect behavior.

a) fixing the exception is as easy as checking the size of the
array before accessing it.

b) fixing the following incorrect behavior is a bit more intrusive

- Note - the UI depends on the success/met state for each condition
to determine an ""OK"" or ""FIRING""

In this scenario, where an unhealthy cluster triggers an alert and
then goes silent, it should keep ""FIRING"" until it hears back that
the cluster is green. To keep the Watch ""FIRING"" either the index
action or the email action needs to fire. Since the Watch is neither
a ""new"" alert or a ""resolved"" alert, we do not want to keep sending
an email (that would be non-passive too). Without completely changing
the logic of how an alert is resolved allowing the index action to
take place would result in the alert being resolved. Since we can
not keep ""FIRING"" either the email or index action (since we don't
want to resolve the alert nor re-write the logic for alert resolution),
we will introduce a 3rd action. A logging action that WILL fire when
the cluster is unhealthy. Specifically will fire when there is an
unresolved alert and it can not find the cluster state.
This logging action is logged at debug, so it should be noticed much.
This logging action serves as an 'anchor' for the UI to keep the state
in an a ""FIRING"" status until the alert is resolved.

This presents a possible scenario where a cluster starts firing,
then goes completely silent forever, the Watch will be ""FIRING""
forever. This is an edge case that already exists in some scenarios
and requires manual intervention to remove that Watch.

This changes changes to use a template-like method to populate the 
version_created for the default monitoring watches. The version is 
set to 7.5 since that is where this is first introduced.

Fixes #43184




",Buggy,"The commit message indicates a bug fix for an `IndexOutOfBoundsException` in cluster alerts, which occurs when a cluster stops sending monitoring data. The fix involves checking the size of an array before accessing it to prevent the exception. Additionally, the commit addresses incorrect behavior where an alert could be incorrectly resolved when the unhealthy cluster goes silent. To maintain the ""FIRING"" status of the alert in such scenarios, a logging action is introduced. The diff replaces a template-like method to populate the version_created for the default monitoring watches. The version is set to 7.5 since that is where this is first introduced. The changes in the diff are related to setting the version for the monitoring watches.
The answer is **Buggy**."
elasticsearch,61886.json,aed30d6cc7761d12b929696774591ac0ccb989d0,"@@ -1,32 +1,32 @@
     public void checkInvalidPatterns() throws IOException {
         Pattern allPatterns = Pattern.compile(""("" + String.join("")|("", getPatterns().values()) + "")"");
         List<String> failures = new ArrayList<>();
         for (File f : files()) {
             List<String> lines;
             try(Stream<String> stream = Files.lines(f.toPath(), StandardCharsets.UTF_8)) {
                     lines = stream.collect(Collectors.toList());
             } catch (UncheckedIOException e) {
                 throw new IllegalArgumentException(""Failed to read "" + f + "" as UTF_8"", e);
             }
             List<Integer> invalidLines = IntStream.range(0, lines.size())
                 .filter(i -> allPatterns.matcher(lines.get(i)).find())
                 .boxed()
                 .collect(Collectors.toList());
 
             String path = getProject().getRootProject().getProjectDir().toURI().relativize(f.toURI()).toString();
-            failures = invalidLines.stream()
+            failures.addAll(invalidLines.stream()
                 .map(l -> new AbstractMap.SimpleEntry<>(l+1, lines.get(l)))
                 .flatMap(kv -> patterns.entrySet().stream()
                     .filter(p -> Pattern.compile(p.getValue()).matcher(kv.getValue()).find())
                     .map(p -> ""- "" + p.getKey() + "" on line "" + kv.getKey() + "" of "" + path)
                 )
-                .collect(Collectors.toList());
+                .collect(Collectors.toList()));
         }
         if (failures.isEmpty() == false) {
             throw new GradleException(""Found invalid patterns:\n"" + String.join(""\n"", failures));
         }
 
         File outputMarker = getOutputMarker();
         outputMarker.getParentFile().mkdirs();
         Files.write(outputMarker.toPath(), ""done"".getBytes(StandardCharsets.UTF_8));
     }",Buggy,"Don't replace forbidden pattern failures when found (#40710)

This commit fixes a bug in forbidden patterns where the failures for a
file replace the failures from the previous files instead of extending
them.
",Buggy,The commit message indicates a bug fix related to how forbidden pattern failures are handled across multiple files. The diff shows that the `failures = invalidLines.stream()...collect(Collectors.toList());` line has been changed to `failures.addAll(invalidLines.stream()...collect(Collectors.toList()));`. This change replaces the assignment of the new failures to the `failures` variable with adding the new failures to the existing `failures` list. This confirms that the bug where failures from previous files were being replaced has been fixed. The answer is **Buggy**.
elasticsearch,39934.json,8260138e5975ebcb588933d792d22374168c48cf,"@@ -1,21 +1,21 @@
         void toXContent(XContentBuilder builder, Params params, boolean keyed, @Nullable ValueFormatter formatter) throws IOException {
-            if (formatter != null) {
+            if (formatter != null && formatter != ValueFormatter.RAW) {
                 Text keyTxt = new StringText(formatter.format(key));
                 if (keyed) {
                     builder.startObject(keyTxt.string());
                 } else {
                     builder.startObject();
                 }
                 builder.field(CommonFields.KEY_AS_STRING, keyTxt);
             } else {
                 if (keyed) {
                     builder.startObject(String.valueOf(getKeyAsNumber()));
                 } else {
                     builder.startObject();
                 }
             }
             builder.field(CommonFields.KEY, key);
             builder.field(CommonFields.DOC_COUNT, docCount);
             aggregations.toXContentInternal(builder, params);
             builder.endObject();
         }",Buggy,"Aggregations: Fixed Histogram key_as_string bug

The key as string field in the response for the histogram aggregation will now only show if format is specified on the request.

Closes #6655
",Buggy,"The commit message indicates a bug fix related to the `key_as_string` field in the histogram aggregation response. The fix ensures that the `key_as_string` field is only included in the response if a format is specified in the request. The code diff shows that the `key_as_string` field is now only written to the XContentBuilder if the formatter is not null and is not equal to `ValueFormatter.RAW`. This aligns with the commit message, indicating that a bug where `key_as_string` was being displayed even when no format was specified has been fixed. The answer is **Buggy**."
elasticsearch,8075.json,ae4bfe99ecd6e298c56236c7abf9a53c6b490ab2,"@@ -1,30 +1,31 @@
     protected void masterOperation(Task task, DeleteEnrichPolicyAction.Request request, ClusterState state,
                                    ActionListener<AcknowledgedResponse> listener) throws Exception {
         List<PipelineConfiguration> pipelines = IngestService.getPipelines(state);
         EnrichPolicy policy = EnrichStore.getPolicy(request.getName(), state);
         List<String> pipelinesWithProcessors = new ArrayList<>();
 
         for (PipelineConfiguration pipelineConfiguration : pipelines) {
             List<AbstractEnrichProcessor> enrichProcessors =
                 ingestService.getProcessorsInPipeline(pipelineConfiguration.getId(), AbstractEnrichProcessor.class);
             for (AbstractEnrichProcessor processor: enrichProcessors) {
                 if (processor.getPolicyName().equals(request.getName())) {
                     pipelinesWithProcessors.add(pipelineConfiguration.getId());
                 }
             }
         }
 
         if (pipelinesWithProcessors.isEmpty() == false) {
             listener.onFailure(
                 new ElasticsearchStatusException(""Could not delete policy [{}] because a pipeline is referencing it {}"",
                     RestStatus.CONFLICT, request.getName(), pipelinesWithProcessors));
+            return;
         }
 
         EnrichStore.deletePolicy(request.getName(), clusterService, e -> {
            if (e == null) {
                listener.onResponse(new AcknowledgedResponse(true));
            } else {
                listener.onFailure(e);
            }
         });
     }",Buggy,"Fix policy removal bug in delete policy (#45573)

The delete policy had a subtle bug in that it would still delete the
policy if pipelines were accessing it, after giving the client back an
error. This commit fixes that and ensures it does not happen by adding
verification in the test.",Buggy,"The commit message indicates a bug where the delete policy would still proceed even after an error was returned to the client due to pipelines accessing it. The diff shows that a `return` statement was added after the `listener.onFailure` call within the `if (pipelinesWithProcessors.isEmpty() == false)` block. This ensures that the `EnrichStore.deletePolicy` method is not called when there are pipelines referencing the policy, thus fixing the bug. The answer is **Buggy**.
"
elasticsearch,51449.json,63fe3c6ed604d2aee3ae5266b35e87f35ff09bee,"@@ -1,40 +1,44 @@
     private void verifyThenSubmitUpdate(ClusterRerouteRequest request, ActionListener<ClusterRerouteResponse> listener,
         Map<String, List<AbstractAllocateAllocationCommand>> stalePrimaryAllocations) {
         transportService.sendRequest(transportService.getLocalNode(), IndicesShardStoresAction.NAME,
             new IndicesShardStoresRequest().indices(stalePrimaryAllocations.keySet().toArray(Strings.EMPTY_ARRAY)),
             new ActionListenerResponseHandler<>(
                 ActionListener.wrap(
                     response -> {
                         ImmutableOpenMap<String, ImmutableOpenIntMap<List<IndicesShardStoresResponse.StoreStatus>>> status =
                             response.getStoreStatuses();
                         Exception e = null;
                         for (Map.Entry<String, List<AbstractAllocateAllocationCommand>> entry : stalePrimaryAllocations.entrySet()) {
                             final String index = entry.getKey();
                             final ImmutableOpenIntMap<List<IndicesShardStoresResponse.StoreStatus>> indexStatus = status.get(index);
-                            assert indexStatus != null;
+                            if (indexStatus == null) {
+                                // The index in the stale primary allocation request was green and hence filtered out by the store status
+                                // request. We ignore it here since the relevant exception will be thrown by the reroute action later on.
+                                continue;
+                            }
                             for (AbstractAllocateAllocationCommand command : entry.getValue()) {
                                 final List<IndicesShardStoresResponse.StoreStatus> shardStatus =
                                     indexStatus.get(command.shardId());
                                 if (shardStatus == null || shardStatus.isEmpty()) {
                                     e = ExceptionsHelper.useOrSuppress(e, new IllegalArgumentException(
                                         ""No data for shard ["" + command.shardId() + ""] of index ["" + index + ""] found on any node"")
                                     );
                                 } else if (shardStatus.stream().noneMatch(storeStatus -> {
                                     final DiscoveryNode node = storeStatus.getNode();
                                     final String nodeInCommand = command.node();
                                     return nodeInCommand.equals(node.getName()) || nodeInCommand.equals(node.getId());
                                 })) {
                                     e = ExceptionsHelper.useOrSuppress(e, new IllegalArgumentException(
                                         ""No data for shard ["" + command.shardId() + ""] of index ["" + index + ""] found on node [""
                                             + command.node() + ']'));
                                 }
                             }
                         }
                         if (e == null) {
                             submitStateUpdate(request, listener);
                         } else {
                             listener.onFailure(e);
                         }
                     }, listener::onFailure
                 ), IndicesShardStoresResponse::new));
     }",Buggy,"Fix PrimaryAllocationIT Race Condition (#37355)

* Fix PrimaryAllocationIT Race Condition

* Forcing a stale primary allocation on a green index was tripping the assertion that was removed
   * Added a test that this case still errors out correctly
* Made the ability to wipe stopped datanode's data public on the internal test cluster and used it to ensure correct behaviour on the fixed test
   * Previously it simply passed because the test finished before the index went green and would NPE when the index was green at the time of the shard store status request, that would then come up empty
* Closes #37345
",Buggy,"The commit message indicates a fix for a race condition in `PrimaryAllocationIT`. The code diff shows a change in `verifyThenSubmitUpdate` where it now handles the case where `indexStatus` is null. This occurs when the index in the stale primary allocation request is green and filtered out by the store status request. The code now continues in this case, rather than throwing an assertion error. This change addresses a potential race condition where the test finishes before the index goes green, leading to an NPE. The addition of the `if (indexStatus == null)` block is a direct response to the race condition, preventing the test from failing prematurely. The answer is **Buggy**."
elasticsearch,47709.json,049e122e8f653fb54f12e0f0aa1eb86c5a1a629d,"@@ -1,18 +1,18 @@
     public static Map<Integer, List<String>> getAutoExpandReplicaChanges(MetaData metaData, DiscoveryNodes discoveryNodes) {
         // used for translating ""all"" to a number
         final int dataNodeCount = discoveryNodes.getDataNodes().size();
 
         Map<Integer, List<String>> nrReplicasChanged = new HashMap<>();
 
         for (final IndexMetaData indexMetaData : metaData) {
-            if (indexMetaData.getState() != IndexMetaData.State.CLOSE) {
+            if (indexMetaData.getState() == IndexMetaData.State.OPEN || isIndexVerifiedBeforeClosed(indexMetaData)) {
                 AutoExpandReplicas autoExpandReplicas = SETTING.get(indexMetaData.getSettings());
                 autoExpandReplicas.getDesiredNumberOfReplicas(dataNodeCount).ifPresent(numberOfReplicas -> {
                     if (numberOfReplicas != indexMetaData.getNumberOfReplicas()) {
                         nrReplicasChanged.computeIfAbsent(numberOfReplicas, ArrayList::new).add(indexMetaData.getIndex().getName());
                     }
                 });
             }
         }
         return nrReplicasChanged;
     }",Buggy,"Auto-expand replicated closed indices (#48973)

Fixes a bug where replicated closed indices were not being auto-expanded.",Buggy,"The commit message indicates a bug fix related to auto-expanding replicated closed indices. The code diff modifies the condition under which the auto-expansion logic is applied. The original code only considered indices that were not in the `CLOSE` state. The modified code now considers indices that are either in the `OPEN` state or have been verified before being closed (`isIndexVerifiedBeforeClosed`). This change suggests that the original logic was incorrectly preventing auto-expansion for replicated closed indices, which aligns with the bug described in the commit message. The answer is **Buggy**."
elasticsearch,46897.json,cdb482eaae0517f202efdfaf445e8847917182f1,"@@ -1,9 +1,13 @@
     boolean cancelCommittedPublication() {
         synchronized (mutex) {
-            if (currentPublication.isPresent() && currentPublication.get().isCommitted()) {
-                currentPublication.get().cancel(""cancelCommittedPublication"");
-                return true;
+            if (currentPublication.isPresent()) {
+                final CoordinatorPublication publication = currentPublication.get();
+                if (publication.isCommitted()) {
+                    publication.cancel(""cancelCommittedPublication"");
+                    logger.debug(""Cancelled publication of [{}]."", publication);
+                    return true;
+                }
             }
             return false;
         }
     }",Buggy,"Fix RareClusterStateIT (#42430)

* It looks like we might be cancelling a previous publication instead of
the one triggered by the given request with a very low likelihood.
   * Fixed by adding a wait for no in-progress publications
   * Also added debug logging that would've identified this problem
* Closes #36813",Buggy,"The commit message indicates a fix in RareClusterStateIT, potentially related to cancelling the wrong publication due to timing issues. The code change adds a debug log statement and checks if the current publication is present before attempting to cancel it. This suggests a bug where the code might have been trying to cancel a non-existent publication or a publication that wasn't yet committed, leading to unexpected behavior. The added logging would help diagnose such issues. Therefore, the answer is **Buggy**."
elasticsearch,19038.json,7597b7ce2bd280401fcbfbeb281dfbb205830d75,"@@ -1,34 +1,38 @@
     public ActionRequestValidationException validate() {
         ActionRequestValidationException validationException = null;
-        for (ApplicationPrivilegeDescriptor privilege : privileges) {
-            try {
-                ApplicationPrivilege.validateApplicationName(privilege.getApplication());
-            } catch (IllegalArgumentException e) {
-                validationException = addValidationError(e.getMessage(), validationException);
-            }
-            try {
-                ApplicationPrivilege.validatePrivilegeName(privilege.getName());
-            } catch (IllegalArgumentException e) {
-                validationException = addValidationError(e.getMessage(), validationException);
-            }
-            if (privilege.getActions().isEmpty()) {
-                validationException = addValidationError(""Application privileges must have at least one action"", validationException);
-            }
-            for (String action : privilege.getActions()) {
-                if (action.indexOf('/') == -1 && action.indexOf('*') == -1 && action.indexOf(':') == -1) {
-                    validationException = addValidationError(""action ["" + action + ""] must contain one of [ '/' , '*' , ':' ]"",
-                        validationException);
-                }
+        if (privileges.isEmpty()) {
+            validationException = addValidationError(""At least one application privilege must be provided"", validationException);
+        } else {
+            for (ApplicationPrivilegeDescriptor privilege : privileges) {
                 try {
-                    ApplicationPrivilege.validatePrivilegeOrActionName(action);
+                    ApplicationPrivilege.validateApplicationName(privilege.getApplication());
                 } catch (IllegalArgumentException e) {
                     validationException = addValidationError(e.getMessage(), validationException);
                 }
-            }
-            if (MetadataUtils.containsReservedMetadata(privilege.getMetadata())) {
-                validationException = addValidationError(""metadata keys may not start with ["" + MetadataUtils.RESERVED_PREFIX
-                    + ""] (in privilege "" + privilege.getApplication() + ' ' + privilege.getName() + "")"", validationException);
+                try {
+                    ApplicationPrivilege.validatePrivilegeName(privilege.getName());
+                } catch (IllegalArgumentException e) {
+                    validationException = addValidationError(e.getMessage(), validationException);
+                }
+                if (privilege.getActions().isEmpty()) {
+                    validationException = addValidationError(""Application privileges must have at least one action"", validationException);
+                }
+                for (String action : privilege.getActions()) {
+                    if (action.indexOf('/') == -1 && action.indexOf('*') == -1 && action.indexOf(':') == -1) {
+                        validationException = addValidationError(""action ["" + action + ""] must contain one of [ '/' , '*' , ':' ]"",
+                            validationException);
+                    }
+                    try {
+                        ApplicationPrivilege.validatePrivilegeOrActionName(action);
+                    } catch (IllegalArgumentException e) {
+                        validationException = addValidationError(e.getMessage(), validationException);
+                    }
+                }
+                if (MetadataUtils.containsReservedMetadata(privilege.getMetadata())) {
+                    validationException = addValidationError(""metadata keys may not start with ["" + MetadataUtils.RESERVED_PREFIX
+                        + ""] (in privilege "" + privilege.getApplication() + ' ' + privilege.getName() + "")"", validationException);
+                }
             }
         }
         return validationException;
     }",Buggy,"Add validation for empty PutPrivilegeRequest (#37569)

Return an error to the user if the put privilege api is called with
an empty body (no privileges)

Resolves: #37561",Buggy,"The commit message indicates that validation for empty `PutPrivilegeRequest` is added. The diff shows that the code now checks if the `privileges` list is empty. If it is, a validation error is added. Otherwise, the original validation logic is executed. This addresses the scenario where the API is called with an empty body, which was previously not handled, leading to unexpected behavior or errors. The answer is **Buggy**."
elasticsearch,49943.json,adc195e30c4e237372f168086f7a57b0c3b5e7f1,"@@ -1,10 +1,10 @@
     public ActionRequestValidationException validate() {
         ActionRequestValidationException validationException = null;
         if (name == null) {
             validationException = addValidationError(""name is missing"", validationException);
         }
         if (indexPatterns == null || indexPatterns.size() == 0) {
-            validationException = addValidationError(""pattern is missing"", validationException);
+            validationException = addValidationError(""index patterns are missing"", validationException);
         }
         return validationException;
     }",Buggy,"Fix error message for a put index template request without index_patterns (#27102)

Just correct the error message from ""Validation Failed: 1: pattern is
missing;"" to ""Validation Failed: 1: index_patterns is missing;"".

Closes #27100",Buggy,"The commit message indicates a fix to an error message. The diff changes the error message from ""pattern is missing"" to ""index patterns are missing"". This is a bug fix because the original error message was misleading or incorrect. The answer is **Buggy**."
elasticsearch,53233.json,a8bfa466b2f2a68c2384bc730a0b8c7c9ce7ea87,"@@ -1,12 +1,12 @@
     public void markAsCompleted(BulkItemResponse translatedResponse) {
         assertInvariants(ItemProcessingState.EXECUTED);
-        assert executionResult == null || translatedResponse.getItemId() == executionResult.getItemId();
+        assert executionResult != null && translatedResponse.getItemId() == executionResult.getItemId();
         assert translatedResponse.getItemId() == getCurrentItem().id();
 
-        if (translatedResponse.isFailed() == false && requestToExecute != getCurrent())  {
+        if (translatedResponse.isFailed() == false && requestToExecute != null && requestToExecute != getCurrent())  {
             request.items()[currentIndex] = new BulkItemRequest(request.items()[currentIndex].id(), requestToExecute);
         }
         getCurrentItem().setPrimaryResponse(translatedResponse);
         currentItemState = ItemProcessingState.COMPLETED;
         advance();
     }",Buggy,"Fix NOOP bulk updates (#32819)

#31821 introduced an unreleased bug where NOOP updates were incorrectly mutating the bulk
shard request, inserting null item to be replicated, which would result in NullPointerExceptions when
serializing the request to be shipped to the replicas.

Closes #32808",Buggy,"The commit message clearly states that the change fixes a bug introduced in a previous commit. The bug involves NOOP updates incorrectly mutating the bulk shard request, leading to NullPointerExceptions. The code diff modifies the `markAsCompleted` method. Specifically, the assertion `assert executionResult == null || translatedResponse.getItemId() == executionResult.getItemId();` is changed to `assert executionResult != null && translatedResponse.getItemId() == executionResult.getItemId();`. Also, the condition `if (translatedResponse.isFailed() == false && requestToExecute != getCurrent())` is changed to `if (translatedResponse.isFailed() == false && requestToExecute != null && requestToExecute != getCurrent())`. These changes likely address the issue of NOOP updates causing unexpected behavior, as described in the commit message. The added null check `requestToExecute != null` prevents a potential NullPointerException. The assertion change also seems to enforce that `executionResult` should not be null.

The answer is **Buggy**."
elasticsearch,265.json,c9dc55c3322b631f708867fdfd650e5ca27a30d0,"@@ -1,18 +1,20 @@
     static Tuple<Set<String>, Set<String>> findTasksWithoutConfig(ClusterState state, String transformId) {
         PersistentTasksCustomMetaData tasks = state.metaData().custom(PersistentTasksCustomMetaData.TYPE);
 
         Set<String> taskIds = new HashSet<>();
         Set<String> executorNodes = new HashSet<>();
 
-        Predicate<PersistentTask<?>> taskMatcher = Strings.isAllOrWildcard(new String[] { transformId }) ? t -> true : t -> {
-            TransformTaskParams transformParams = (TransformTaskParams) t.getParams();
-            return Regex.simpleMatch(transformId, transformParams.getId());
-        };
+        if (tasks != null) {
+            Predicate<PersistentTask<?>> taskMatcher = Strings.isAllOrWildcard(new String[] { transformId }) ? t -> true : t -> {
+                TransformTaskParams transformParams = (TransformTaskParams) t.getParams();
+                return Regex.simpleMatch(transformId, transformParams.getId());
+            };
 
-        for (PersistentTasksCustomMetaData.PersistentTask<?> pTask : tasks.findTasks(TransformField.TASK_NAME, taskMatcher)) {
-            executorNodes.add(pTask.getExecutorNode());
-            taskIds.add(pTask.getId());
+            for (PersistentTasksCustomMetaData.PersistentTask<?> pTask : tasks.findTasks(TransformField.TASK_NAME, taskMatcher)) {
+                executorNodes.add(pTask.getExecutorNode());
+                taskIds.add(pTask.getId());
+            }
         }
 
         return new Tuple<>(taskIds, executorNodes);
     }",Buggy,"check custom meta data to avoid NPE (#51163)

check custom meta data to avoid NPE, fixes a problem introduced in #51072

fixes #51153",Buggy,"The commit message indicates a fix for a NullPointerException (NPE) related to custom meta data. The code diff adds a null check for `tasks` before proceeding with task processing. This prevents the code from attempting to access members of a null `tasks` object, which would indeed cause an NPE. The commit message also mentions that the problem was introduced in a previous commit and fixes a specific issue. Therefore, the changes clearly address a bug. The answer is **Buggy**."
elasticsearch,7551.json,341006e9913e831408f5bbc7f8ad8c453a7f630e,"@@ -1,91 +1,91 @@
     private static List<EsIndex> buildIndices(String[] indexNames, String javaRegex, Map<String, Map<String, FieldCapabilities>> fieldCaps,
             Function<String, String> indexNameProcessor,
             BiFunction<String, Map<String, FieldCapabilities>, InvalidMappedField> validityVerifier) {
 
         if (indexNames == null || indexNames.length == 0) {
             return emptyList();
         }
 
         final List<String> resolvedIndices = asList(indexNames);
         Map<String, Fields> indices = new LinkedHashMap<>(resolvedIndices.size());
         Pattern pattern = javaRegex != null ? Pattern.compile(javaRegex) : null;
 
         // sort fields in reverse order to build the field hierarchy
         Set<Entry<String, Map<String, FieldCapabilities>>> sortedFields = new TreeSet<>(
                 Collections.reverseOrder(Comparator.comparing(Entry::getKey)));
 
         sortedFields.addAll(fieldCaps.entrySet());
 
         for (Entry<String, Map<String, FieldCapabilities>> entry : sortedFields) {
             String fieldName = entry.getKey();
             Map<String, FieldCapabilities> types = entry.getValue();
 
             // ignore size added by the mapper plugin
             if (FIELD_NAMES_BLACKLIST.contains(fieldName)) {
                 continue;
             }
 
             // apply verification
             final InvalidMappedField invalidField = validityVerifier.apply(fieldName, types);
 
             // filter meta fields and unmapped
             FieldCapabilities unmapped = types.get(UNMAPPED);
             Set<String> unmappedIndices = unmapped != null ? new HashSet<>(asList(unmapped.indices())) : emptySet();
 
             // check each type
             for (Entry<String, FieldCapabilities> typeEntry : types.entrySet()) {
                 FieldCapabilities typeCap = typeEntry.getValue();
                 String[] capIndices = typeCap.indices();
 
                 // Skip internal fields (name starting with underscore and its type reported by field_caps starts
                 // with underscore as well). A meta field named ""_version"", for example, has the type named ""_version"".
                 if (typeEntry.getKey().startsWith(""_"") && typeCap.getType().startsWith(""_"")) {
                     continue;
                 }
 
                 // compute the actual indices - if any are specified, take into account the unmapped indices
                 List<String> concreteIndices = null;
                 if (capIndices != null) {
                     if (unmappedIndices.isEmpty() == true) {
                         concreteIndices = asList(capIndices);
                     } else {
-                        concreteIndices = new ArrayList<>(capIndices.length - unmappedIndices.size() + 1);
+                        concreteIndices = new ArrayList<>(capIndices.length);
                         for (String capIndex : capIndices) {
                             // add only indices that have a mapping
                             if (unmappedIndices.contains(capIndex) == false) {
                                 concreteIndices.add(capIndex);
                             }
                         }
                     }
                 } else {
                     concreteIndices = resolvedIndices;
                 }
 
                 // put the field in their respective mappings
                 for (String index : concreteIndices) {
                     if (pattern == null || pattern.matcher(index).matches()) {
                         String indexName = indexNameProcessor.apply(index);
                         Fields indexFields = indices.get(indexName);
                         if (indexFields == null) {
                             indexFields = new Fields();
                             indices.put(indexName, indexFields);
                         }
                         EsField field = indexFields.flattedMapping.get(fieldName);
                         if (field == null || (invalidField != null && (field instanceof InvalidMappedField) == false)) {
                             createField(fieldName, fieldCaps, indexFields.hierarchicalMapping, indexFields.flattedMapping,
                                     s -> invalidField != null ? invalidField : createField(s, typeCap.getType(), emptyMap(),
                                             typeCap.isAggregatable()));
                         }
                     }
                 }
             }
         }
 
         // return indices in ascending order
         List<EsIndex> foundIndices = new ArrayList<>(indices.size());
         for (Entry<String, Fields> entry : indices.entrySet()) {
             foundIndices.add(new EsIndex(entry.getKey(), entry.getValue().hierarchicalMapping));
         }
         foundIndices.sort(Comparator.comparing(EsIndex::name));
         return foundIndices;
     }",Buggy,"SQL: concrete indices array size bug fix (#43878)

* The created array didn't have the correct initial size while attempting to resolve multiple indices
",Buggy,"The commit message indicates a bug fix related to the size of the `concreteIndices` array. The diff shows that the initial size of the `concreteIndices` list was incorrectly calculated when `unmappedIndices` were present. The original code `new ArrayList<>(capIndices.length - unmappedIndices.size() + 1)` could lead to an `IllegalArgumentException` if `capIndices.length - unmappedIndices.size() + 1` is negative or unnecessarily allocate an extra element. The corrected code `new ArrayList<>(capIndices.length)` allocates the list with the maximum possible size, avoiding the potential exception and improving efficiency. This change directly addresses the bug described in the commit message. The answer is **Buggy**."
elasticsearch,41046.json,0354825914ba79d7e3ec500aa9eab0532e43850f,"@@ -1,24 +1,22 @@
     private FieldLookup loadFieldData(String name) {
         FieldLookup data = cachedFieldData.get(name);
         if (data == null) {
             FieldMapper mapper = mapperService.smartNameFieldMapper(name, types);
             if (mapper == null) {
                 throw new ElasticSearchIllegalArgumentException(""No field found for ["" + name + ""] in mapping with types "" + Arrays.toString(types) + """");
             }
             data = new FieldLookup(mapper);
             cachedFieldData.put(name, data);
         }
         if (data.doc() == null) {
             fieldVisitor.name(data.mapper().names().indexName());
             try {
                 reader.document(docId, fieldVisitor);
                 // LUCENE 4 UPGRADE: Only one field we don't need document
                 data.doc(fieldVisitor.createDocument());
             } catch (IOException e) {
                 throw new ElasticSearchParseException(""failed to load field ["" + name + ""]"", e);
-            } finally {
-                fieldVisitor.reset();
             }
         }
         return data;
     }",Buggy,"lucene 4: Fixed compile error
",Buggy,"The commit message indicates a fix for a compile error. The diff removes `fieldVisitor.reset()` from the `finally` block. This suggests that the `reset()` method was causing a compile error, possibly due to changes in the Lucene 4 API. Removing it resolves the compilation issue. The answer is **Buggy**."
elasticsearch,11747.json,22415fa2de1d7d07cea7dd5e7263eb1ed4270503,"@@ -1,82 +1,88 @@
     CharsetMatch findCharset(List<String> explanation, InputStream inputStream) throws Exception {
 
         // We need an input stream that supports mark and reset, so wrap the argument
         // in a BufferedInputStream if it doesn't already support this feature
         if (inputStream.markSupported() == false) {
             inputStream = new BufferedInputStream(inputStream, BUFFER_SIZE);
         }
 
         // This is from ICU4J
         CharsetDetector charsetDetector = new CharsetDetector().setText(inputStream);
         CharsetMatch[] charsetMatches = charsetDetector.detectAll();
 
         // Determine some extra characteristics of the input to compensate for some deficiencies of ICU4J
         boolean pureAscii = true;
         boolean containsZeroBytes = false;
         inputStream.mark(BUFFER_SIZE);
         byte[] workspace = new byte[BUFFER_SIZE];
         int remainingLength = BUFFER_SIZE;
         do {
             int bytesRead = inputStream.read(workspace, 0, remainingLength);
             if (bytesRead <= 0) {
                 break;
             }
             for (int i = 0; i < bytesRead && containsZeroBytes == false; ++i) {
                 if (workspace[i] == 0) {
                     containsZeroBytes = true;
                     pureAscii = false;
                 } else {
                     pureAscii = pureAscii && workspace[i] > 0 && workspace[i] < 128;
                 }
             }
             remainingLength -= bytesRead;
         } while (containsZeroBytes == false && remainingLength > 0);
         inputStream.reset();
 
         if (pureAscii) {
             // If the input is pure ASCII then many single byte character sets will match.  We want to favour
             // UTF-8 in this case, as it avoids putting a bold declaration of a dubious character set choice
             // in the config files.
             Optional<CharsetMatch> utf8CharsetMatch = Arrays.stream(charsetMatches)
                 .filter(charsetMatch -> StandardCharsets.UTF_8.name().equals(charsetMatch.getName())).findFirst();
             if (utf8CharsetMatch.isPresent()) {
                 explanation.add(""Using character encoding ["" + StandardCharsets.UTF_8.name() +
                     ""], which matched the input with ["" + utf8CharsetMatch.get().getConfidence() + ""%] confidence - first ["" +
                     (BUFFER_SIZE / 1024) + ""kB] of input was pure ASCII"");
                 return utf8CharsetMatch.get();
             }
         }
 
         // Input wasn't pure ASCII, so use the best matching character set that's supported by both Java and Go.
         // Additionally, if the input contains zero bytes then avoid single byte character sets, as ICU4J will
         // suggest these for binary files but then
         for (CharsetMatch charsetMatch : charsetMatches) {
             String name = charsetMatch.getName();
             if (Charset.isSupported(name) && FILEBEAT_SUPPORTED_ENCODINGS.contains(name.toLowerCase(Locale.ROOT))) {
 
                 // This extra test is to avoid trying to read binary files as text.  Running the log config
                 // deduction algorithms on binary files is very slow as the binary files generally appear to
                 // have very long lines.
                 boolean spaceEncodingContainsZeroByte = false;
-                byte[] spaceBytes = "" "".getBytes(name);
-                for (int i = 0; i < spaceBytes.length && spaceEncodingContainsZeroByte == false; ++i) {
-                    spaceEncodingContainsZeroByte = (spaceBytes[i] == 0);
+                Charset charset = Charset.forName(name);
+                // Some character sets cannot be encoded.  These are extremely rare so it's likely that
+                // they've been chosen based on incorrectly provided binary data.  Therefore, err on
+                // the side of rejecting binary data.
+                if (charset.canEncode()) {
+                    byte[] spaceBytes = "" "".getBytes(charset);
+                    for (int i = 0; i < spaceBytes.length && spaceEncodingContainsZeroByte == false; ++i) {
+                        spaceEncodingContainsZeroByte = (spaceBytes[i] == 0);
+                    }
                 }
                 if (containsZeroBytes && spaceEncodingContainsZeroByte == false) {
                     explanation.add(""Character encoding ["" + name + ""] matched the input with ["" + charsetMatch.getConfidence() +
                         ""%] confidence but was rejected as the input contains zero bytes and the ["" + name + ""] encoding does not"");
                 } else {
                     explanation.add(""Using character encoding ["" + name + ""], which matched the input with ["" +
                         charsetMatch.getConfidence() + ""%] confidence"");
                     return charsetMatch;
                 }
             } else {
                 explanation.add(""Character encoding ["" + name + ""] matched the input with ["" + charsetMatch.getConfidence() +
                     ""%] confidence but was rejected as it is not supported by ["" +
                     (Charset.isSupported(name) ? ""Filebeat"" : ""the JVM"") + ""]"");
             }
         }
 
         throw new IllegalArgumentException(""Could not determine a usable character encoding for the input"" +
             (containsZeroBytes ? "" - could it be binary data?"" : """"));
     }",Buggy,"[ML] Fix character set finder bug with unencodable charsets (#33234)

Some character sets cannot be encoded and this was tripping
up the binary data check in the ML log structure character
set finder.

The fix is to assume that if ICU4J identifies that some bytes
correspond to a character set that cannot be encoded and those
bytes contain zeroes then the data is binary rather than text.

Fixes #33227",Buggy,"The commit message indicates a fix for a bug related to character set detection, specifically when dealing with unencodable character sets. The code changes introduce a check to determine if a character set can be encoded using `charset.canEncode()`. If a character set cannot be encoded and the input contains zero bytes, the code now assumes the data is binary and avoids using that character set. This aligns with the commit message's description of the bug and the fix. The answer is **Buggy**."
elasticsearch,54093.json,b3341da0779673c106db526ec58c4449bda59dc2,"@@ -1,9 +1,13 @@
     private static boolean usingBundledJdk() {
         /*
          * We are using the bundled JDK if java.home is the jdk sub-directory of our working directory. This is because we always set
          * the working directory of Elasticsearch to home, and the bundled JDK is in the jdk sub-directory there.
          */
         final String javaHome = System.getProperty(""java.home"");
         final String userDir = System.getProperty(""user.dir"");
-        return PathUtils.get(javaHome).equals(PathUtils.get(userDir).resolve(""jdk"").toAbsolutePath());
+        if (Constants.MAC_OS_X) {
+            return PathUtils.get(javaHome).equals(PathUtils.get(userDir).resolve(""jdk/Contents/Home"").toAbsolutePath());
+        } else {
+            return PathUtils.get(javaHome).equals(PathUtils.get(userDir).resolve(""jdk"").toAbsolutePath());
+        }
     }",Buggy,"Fix bug in detecting use of bundled JDK on macOS

This commit fixes a bug in detecting the use of the bundled JDK on
macOS. This bug arose because the path of Java home is different on
macOS.
",Buggy,"The commit message indicates a bug fix related to detecting the bundled JDK on macOS due to an incorrect Java home path. The code diff confirms this by adding a conditional check for `Constants.MAC_OS_X`. When running on macOS, the code now resolves the Java home path to `jdk/Contents/Home`, which is the correct path for bundled JDKs on macOS. Otherwise, it uses the original path. This change directly addresses the bug described in the commit message. The answer is **Buggy**."
elasticsearch,12667.json,7ae57d6e226bfc314ce31acc1a622fb0d111fa46,"@@ -1,36 +1,47 @@
     void refresh(PersistentTasksCustomMetaData persistentTasks, ActionListener<Void> onCompletion) {
 
         synchronized (fullRefreshCompletionListeners) {
             fullRefreshCompletionListeners.add(onCompletion);
             if (fullRefreshCompletionListeners.size() > 1) {
                 // A refresh is already in progress, so don't do another
                 return;
             }
         }
 
         ActionListener<Void> refreshComplete = ActionListener.wrap(aVoid -> {
             lastUpdateTime = Instant.now();
             synchronized (fullRefreshCompletionListeners) {
                 assert fullRefreshCompletionListeners.isEmpty() == false;
                 for (ActionListener<Void> listener : fullRefreshCompletionListeners) {
                     listener.onResponse(null);
                 }
                 fullRefreshCompletionListeners.clear();
             }
-        }, onCompletion::onFailure);
+        },
+        e -> {
+            synchronized (fullRefreshCompletionListeners) {
+                assert fullRefreshCompletionListeners.isEmpty() == false;
+                for (ActionListener<Void> listener : fullRefreshCompletionListeners) {
+                    listener.onFailure(e);
+                }
+                // It's critical that we empty out the current listener list on
+                // error otherwise subsequent retries to refresh will be ignored
+                fullRefreshCompletionListeners.clear();
+            }
+        });
 
         // persistentTasks will be null if there's never been a persistent task created in this cluster
         if (persistentTasks == null) {
             refreshComplete.onResponse(null);
         } else {
             List<PersistentTasksCustomMetaData.PersistentTask<?>> mlDataFrameAnalyticsJobTasks = persistentTasks.tasks().stream()
                 .filter(task -> MlTasks.DATA_FRAME_ANALYTICS_TASK_NAME.equals(task.getTaskName())).collect(Collectors.toList());
             ActionListener<Void> refreshDataFrameAnalyticsJobs =
                 ActionListener.wrap(aVoid -> refreshAllDataFrameAnalyticsJobTasks(mlDataFrameAnalyticsJobTasks, refreshComplete),
                     refreshComplete::onFailure);
 
             List<PersistentTasksCustomMetaData.PersistentTask<?>> mlAnomalyDetectorJobTasks = persistentTasks.tasks().stream()
                 .filter(task -> MlTasks.JOB_TASK_NAME.equals(task.getTaskName())).collect(Collectors.toList());
             iterateAnomalyDetectorJobTasks(mlAnomalyDetectorJobTasks.iterator(), refreshDataFrameAnalyticsJobs);
         }
     }",Buggy,"[ML] Fix ML memory tracker lockup when inner step fails (#44158)

When the ML memory tracker is refreshed and a refresh is
already in progress the idea is that the second and
subsequent refresh requests receive the same response as
the currently in progress refresh.

There was a bug that if a refresh failed then the ML
memory tracker's view of whether a refresh was in progress
was not reset, leading to every subsequent request being
registered to receive a response that would never come.

This change makes the ML memory tracker pass on failures
as well as successes to all interested parties and reset
the list of interested parties so that further refresh
attempts are possible after either a success or failure.

This fixes problem 1 of #44156",Buggy,"The commit message indicates a fix for a memory tracker lockup issue that occurs when an inner step fails during a refresh operation. The core problem was that the `fullRefreshCompletionListeners` were not cleared when a refresh failed, preventing subsequent refresh attempts from being processed.

The diff modifies the `refresh` method to handle failures correctly. Specifically, it updates the `onFailure` callback to iterate through the `fullRefreshCompletionListeners`, propagate the failure to each listener, and then clear the list. This ensures that subsequent refresh attempts can be registered and processed, resolving the lockup issue.

The changes directly address the bug described in the commit message by ensuring that the memory tracker's state is properly reset after a failed refresh. The addition of error handling and the clearing of the listener list are clear indicators of a bug fix.

The answer is **Buggy**."
elasticsearch,39911.json,4735e0a9d3c7c89f34100e4fd47f3de25e56e1ed,"@@ -1,11 +1,14 @@
     public HistogramAggregationBuilder extendedBounds(double minBound, double maxBound) {
-        if (minBound == Double.NEGATIVE_INFINITY) {
-            throw new IllegalArgumentException(""minBound must not be -Infinity, got: "" + minBound);
+        if (Double.isFinite(minBound) == false) {
+            throw new IllegalArgumentException(""minBound must be finite, got: "" + minBound);
         }
-        if (maxBound == Double.POSITIVE_INFINITY) {
-            throw new IllegalArgumentException(""maxBound must not be +Infinity, got: "" + maxBound);
+        if (Double.isFinite(maxBound) == false) {
+            throw new IllegalArgumentException(""maxBound must be finite, got: "" + maxBound);
+        }
+        if (maxBound < minBound) {
+            throw new IllegalArgumentException(""maxBound ["" + maxBound + ""] must be greater than minBound ["" + minBound + ""]"");
         }
         this.minBound = minBound;
         this.maxBound = maxBound;
         return this;
     }",Buggy,"Throw exception when maxBounds greater than minBounds

The recent changes to the Histogram Aggregator introduced a bug where
an exception would not be thrown if the maxBound of the extended bounds
is less that the minBound. This change fixes that bug.

Closes #19833
",Buggy,"The commit message indicates a bug fix related to the validation of `minBound` and `maxBound` in the `HistogramAggregationBuilder`. Specifically, it addresses a scenario where an exception was not thrown when `maxBound` was less than `minBound`.

The diff confirms this by adding a check `if (maxBound < minBound)` and throwing an `IllegalArgumentException` if the condition is met. Additionally, the code was changed to check if the bounds are finite instead of checking if they are equal to positive or negative infinity.

The changes directly address the bug described in the commit message by adding the missing validation and improving the bound checks. The answer is **Buggy**."
elasticsearch,45276.json,c5b6f52eccb2a663770e0eb9fee496432e1a8d5c,"@@ -1,21 +1,22 @@
-        private ImmutableOpenMap<String, List<ShardId>> findWaitingIndices(ImmutableOpenMap<ShardId, ShardSnapshotStatus> shards) {
+        ImmutableOpenMap<String, List<ShardId>> findWaitingIndices(ImmutableOpenMap<ShardId, ShardSnapshotStatus> shards) {
             Map<String, List<ShardId>> waitingIndicesMap = new HashMap<>();
             for (ObjectObjectCursor<ShardId, ShardSnapshotStatus> entry : shards) {
                 if (entry.value.state() == State.WAITING) {
-                    List<ShardId> waitingShards = waitingIndicesMap.get(entry.key.getIndex());
+                    final String indexName = entry.key.getIndexName();
+                    List<ShardId> waitingShards = waitingIndicesMap.get(indexName);
                     if (waitingShards == null) {
                         waitingShards = new ArrayList<>();
-                        waitingIndicesMap.put(entry.key.getIndexName(), waitingShards);
+                        waitingIndicesMap.put(indexName, waitingShards);
                     }
                     waitingShards.add(entry.key);
                 }
             }
             if (waitingIndicesMap.isEmpty()) {
                 return ImmutableOpenMap.of();
             }
             ImmutableOpenMap.Builder<String, List<ShardId>> waitingIndicesBuilder = ImmutableOpenMap.builder();
             for (Map.Entry<String, List<ShardId>> entry : waitingIndicesMap.entrySet()) {
                 waitingIndicesBuilder.put(entry.getKey(), Collections.unmodifiableList(entry.getValue()));
             }
             return waitingIndicesBuilder.build();
         }",Buggy,"Fixes maintaining the shards a snapshot is waiting on (#24289)

There was a bug in the calculation of the shards that a snapshot must
wait on, due to their relocating or initializing, before the snapshot
can proceed safely to snapshot the shard data.  In this bug, an
incorrect key was used to look up the index of the waiting shards,
resulting in the fact that each index would have at most one shard in
the waiting state causing the snapshot to pause.  This could be
problematic if there are more than one shard in the relocating or
initializing state, which would result in a snapshot prematurely
starting because it thinks its only waiting on one relocating or
initializing shard (when in fact there could be more than one).  While
not a common case and likely rare in practice, it is still problematic.

This commit fixes the issue by ensuring the correct key is used to look
up the waiting indices map as it is being built up, so the list of
waiting shards for each index (those shards that are relocating or
initializing) are aggregated for a given index instead of overwritten.
",Buggy,"The commit message clearly states a bug in the calculation of shards a snapshot waits on. The bug involved using an incorrect key to look up the index of waiting shards, leading to premature snapshot starts. The code diff shows that the key used to retrieve and store the list of waiting shards was `entry.key.getIndex()` which was incorrect. It has been corrected to `entry.key.getIndexName()`. This ensures that shards belonging to the same index are correctly grouped together in the `waitingIndicesMap`. Therefore, the code change fixes the bug described in the commit message. The answer is **Buggy**.
"
elasticsearch,60460.json,953a99c75cde29a18db58abde3fdee720fcddc4f,"@@ -1,6 +1,6 @@
     public boolean isSame(StoreFileMetaData other) {
-        if (checksum != null && other.checksum != null) {
-            return checksum.equals(other.checksum);
+        if (checksum == null || other.checksum == null) {
+            return false;
         }
-        return length == other.length;
+        return length == other.length && checksum.equals(other.checksum);
     }",Buggy,"fix a bug in new checksum mechanism that caused for replicas not to retain the _checksums file. Also, now that checksums are widely used, consider files without checksums as ones that need to be recovered
",Buggy,"The commit message indicates a bug fix related to checksums in replica handling. The diff modifies the `isSame` method, which compares `StoreFileMetaData` objects. The original code only checked checksums if both objects had them, otherwise relying solely on length. The corrected code now returns `false` immediately if either checksum is null, and only compares checksums if both are present, in addition to comparing lengths. This change addresses a scenario where replicas might not retain the `_checksums` file due to incorrect comparison logic, aligning with the commit message. The answer is **Buggy**."
elasticsearch,52327.json,fe5af6f34d1204b6238c46c3aff7ea2a056eeb7a,"@@ -1,3 +1,3 @@
     public String toString() {
-        return ""shard ["" + (shardTarget == null ? ""_na"" : shardTarget) + ""], reason ["" + reason + ""]"";
+        return ""shard ["" + (shardTarget == null ? ""_na"" : shardTarget) + ""], reason ["" + reason + ""], cause ["" + (cause == null ? ""_na"" : ExceptionsHelper.stackTrace(cause)) + ""]"";
     }",Buggy,"Tests: fix test bug (foo -> bar)
",NotBuggy,"The commit message indicates a test bug fix, but the provided diff modifies the `toString()` method to include the cause of an exception in the output. This change doesn't seem directly related to fixing a test bug. Instead, it enhances the debugging information provided by the `toString()` method. Therefore, the change is not related to a bug fix. The answer is **NotBuggy**."
elasticsearch,15357.json,368e5a1194c2d2f762c117dd84397bb65f835dec,"@@ -1,5 +1,6 @@
     public static boolean validate(ClusterState state) {
-        return state.getMetaData().getTemplates().containsKey(WatcherIndexTemplateRegistryField.HISTORY_TEMPLATE_NAME) &&
+        return (state.getMetaData().getTemplates().containsKey(WatcherIndexTemplateRegistryField.HISTORY_TEMPLATE_NAME) ||
+            state.getMetaData().getTemplates().containsKey(WatcherIndexTemplateRegistryField.HISTORY_TEMPLATE_NAME_NO_ILM)) &&
             state.getMetaData().getTemplates().containsKey(WatcherIndexTemplateRegistryField.TRIGGERED_TEMPLATE_NAME) &&
             state.getMetaData().getTemplates().containsKey(WatcherIndexTemplateRegistryField.WATCHES_TEMPLATE_NAME);
     }",Buggy,"fix unlikely bug that can prevent Watcher from restarting (#42030)

The bug fixed here is unlikely to happen. It requires ES to be started with
ILM disabled, Watcher enabled, and Watcher explicitly stopped and restarted.
Due to template validation Watcher does not fully start and can result in a
partially started state. This is an unlikely scenerio outside of the testing
framework.

Note - this bug was introduced while the test that would have caught it was
muted. The test remains muted since the underlying cuase of the random failures
has not been identified. When this test is un-muted it will now work.",Buggy,"The commit message indicates a bug fix related to Watcher's restart mechanism in Elasticsearch, specifically when ILM is disabled. The diff modifies the `validate` method to check for either `HISTORY_TEMPLATE_NAME` or `HISTORY_TEMPLATE_NAME_NO_ILM` in the cluster state's templates. This suggests that the original validation logic was too strict and didn't account for the case where ILM is disabled, leading to Watcher failing to fully start after a restart. The addition of the `HISTORY_TEMPLATE_NAME_NO_ILM` check addresses this issue. The answer is **Buggy**."
elasticsearch,53154.json,b7314c87211d99c45569ae0cdd8797636220b980,"@@ -1,11 +1,11 @@
     public String toString() {
         StringBuilder builder = new StringBuilder();
         builder.append(""IndexResponse["");
         builder.append(""index="").append(getIndex());
         builder.append("",type="").append(getType());
         builder.append("",id="").append(getId());
         builder.append("",version="").append(getVersion());
         builder.append("",result="").append(getResult().getLowercase());
-        builder.append("",shards="").append(getShardInfo());
+        builder.append("",shards="").append(Strings.toString(getShardInfo(), true));
         return builder.append(""]"").toString();
     }",Buggy,"fix IndexResponse#toString to print out shards info (#20562)

IndexResponse#toString method outputs an error caused by the shards object needing to be wrapped into another object. It is fixed by calling a different variant of Strings.toString(XContent) which accepts a second boolean argument that makes sure that a new object is created before outputting ShardInfo. I didn't change ShardInfo#toString directly as whether it needs a new object or not very much depends on where it is printed out. IndexResponse seemed a specific case as the rest of the info were not json, hence the shards object was the first one, but it is usually not the case.",Buggy,"The commit message indicates a bug in the `IndexResponse#toString` method related to how shard information is printed. The diff shows that the `getShardInfo()` output is now wrapped using `Strings.toString(getShardInfo(), true)`. This suggests that the original `getShardInfo()` output was not correctly formatted for inclusion in the `IndexResponse`'s string representation, and the `Strings.toString` method is used to ensure proper formatting, specifically by creating a new object before outputting `ShardInfo`. This aligns with the commit message's explanation of the fix. The answer is **Buggy**."
elasticsearch,45221.json,5c59b1206762df8461b7f827ce3922231250c49e,"@@ -1,33 +1,35 @@
         public void writeTo(StreamOutput out) throws IOException {
             out.writeVInt(deletes.size());
             for (K delete : deletes) {
                 keySerializer.writeKey(delete, out);
             }
             Version version = out.getVersion();
             // filter out custom states not supported by the other node
             int diffCount = 0;
             for (Diff<T> diff : diffs.values()) {
                 if(valueSerializer.supportsVersion(diff, version)) {
                     diffCount++;
                 }
             }
             out.writeVInt(diffCount);
             for (Map.Entry<K, Diff<T>> entry : diffs.entrySet()) {
                 if(valueSerializer.supportsVersion(entry.getValue(), version)) {
                     keySerializer.writeKey(entry.getKey(), out);
                     valueSerializer.writeDiff(entry.getValue(), out);
                 }
             }
             // filter out custom states not supported by the other node
             int upsertsCount = 0;
             for (T upsert : upserts.values()) {
                 if(valueSerializer.supportsVersion(upsert, version)) {
                     upsertsCount++;
                 }
             }
             out.writeVInt(upsertsCount);
             for (Map.Entry<K, T> entry : upserts.entrySet()) {
-                keySerializer.writeKey(entry.getKey(), out);
-                valueSerializer.write(entry.getValue(), out);
+                if(valueSerializer.supportsVersion(entry.getValue(), version)) {
+                    keySerializer.writeKey(entry.getKey(), out);
+                    valueSerializer.write(entry.getValue(), out);
+                }
             }
         }",Buggy,"Fixing the custom object serialization bug in diffable utils. (#39544)

While serializing custom objects, the length of the list is computed after
filtering out the unsupported objects but while writing objects the filter
is not applied thus resulting in writing unsupported objects which will fail
to deserialize by the receiever. Adding the condition to filter out unsupported
custom objects.",Buggy,"The commit message indicates a bug related to custom object serialization where unsupported objects were being written, leading to deserialization failures. The code diff confirms this by adding a check `if(valueSerializer.supportsVersion(entry.getValue(), version))` before writing the upserted values. This ensures that only supported objects are serialized, aligning with the bug fix described in the commit message. The answer is **Buggy**."
elasticsearch,56477.json,1792bd6b160993aae2d925ad4b7c0663d14a9e82,"@@ -1,14 +1,6 @@
     public static IndexShardState fromId(byte id) throws ElasticSearchIllegalArgumentException {
-        if (id == 0) {
-            return CREATED;
-        } else if (id == 1) {
-            return RECOVERING;
-        } else if (id == 2) {
-            return STARTED;
-        } else if (id == 3) {
-            return RELOCATED;
-        } else if (id == 4) {
-            return CLOSED;
+        if (id < ORDS[0].id && id > ORDS[ORDS.length - 1].id) {
+            throw new ElasticSearchIllegalArgumentException(""No mapping for id ["" + id + ""]"");
         }
-        throw new ElasticSearchIllegalArgumentException(""No mapping for id ["" + id + ""]"");
+        return ORDS[id];
     }",Buggy,"Fixed serialization error. POST_RECOVERY is now also serialized
",Buggy,"The commit message indicates a fix for a serialization error, specifically related to the `POST_RECOVERY` state. The diff replaces a series of `if/else if` statements with a check against an `ORDS` array. This suggests that the original code was missing a case for `POST_RECOVERY`, leading to a serialization issue when trying to serialize or deserialize an `IndexShardState` with the id of `POST_RECOVERY`. The new code uses an array lookup, which likely includes `POST_RECOVERY`, thus fixing the serialization error. The answer is **Buggy**.
"
checkstyle,1499.json,ca8fcd42047b2637e0027ff6aec79edd3656ba56,"@@ -1,3 +1,4 @@
-    public void setIgnoreEqualsIgnoreCase(boolean newValue) {
-        mIgnoreEqualsIgnoreCase = newValue;
+    public void setIgnoreEqualsIgnoreCase(boolean aNewValue)
+    {
+        mIgnoreEqualsIgnoreCase = aNewValue;
     }",Buggy,"fix up checkstyle errors.
",NotBuggy,"The commit message indicates a fix for checkstyle errors. The diff shows a change in the parameter name from `newValue` to `aNewValue` and the addition of braces around the method body. These changes are purely stylistic and do not affect the functionality of the code. Therefore, this is not a bug fix. The answer is **NotBuggy**.
"
checkstyle,1058.json,9d41bddb46e5fbe291be4cdb347dee18ea4424fe,"@@ -1,4 +1,8 @@
     public int[] getRequiredTokens()
     {
-        return getDefaultTokens();
+        return new int[] {
+            TokenTypes.CTOR_DEF,
+            TokenTypes.METHOD_DEF,
+            TokenTypes.EXPR,
+        };
     }",Buggy,"Fixed bug #1579227: Fixed docs, added unit tests and changed the check implementation to allow turning off the | and & operators
",Buggy,"The commit message indicates a bug fix related to token requirements. The diff modifies the `getRequiredTokens` method to return a specific set of token types (`CTOR_DEF`, `METHOD_DEF`, `EXPR`) instead of the default tokens. This change likely addresses a scenario where the original set of required tokens was insufficient or incorrect, leading to misbehavior or errors. The modification directly impacts the behavior of the code by altering the tokens that are considered necessary, indicating a bug fix. The answer is **Buggy**."
lombok,2739.json,aa40323ae8dde889f4bcf21304e5cffdc0003b4c,"@@ -1,21 +1,26 @@
 	public static TypeParameter[] copyTypeParams(TypeParameter[] params) {
 		if ( params == null ) return null;
 		TypeParameter[] out = new TypeParameter[params.length];
 		int idx = 0;
 		for ( TypeParameter param : params ) {
 			TypeParameter o = new TypeParameter();
 			o.annotations = param.annotations;
 			o.bits = param.bits;
 			o.modifiers = param.modifiers;
 			o.name = param.name;
 			o.type = copyType(param.type);
+			o.sourceStart = param.sourceStart;
+			o.sourceEnd = param.sourceEnd;
+			o.declarationEnd = param.declarationEnd;
+			o.declarationSourceStart = param.declarationSourceStart;
+			o.declarationSourceEnd = param.declarationSourceEnd;
 			if ( param.bounds != null ) {
 				TypeReference[] b = new TypeReference[param.bounds.length];
 				int idx2 = 0;
 				for ( TypeReference ref : param.bounds ) b[idx2++] = copyType(ref);
 				o.bounds = b;
 			}
 			out[idx++] = o;
 		}
 		return out;
 	}",Buggy,"Fixed a problem where @Data with a static constructor and generics params on the class would generate errors regarding IllegalArgumentException in setSourcePosition in ASTNode.
",Buggy,"The commit message indicates a fix for an IllegalArgumentException related to source positions in ASTNode when using `@Data` with a static constructor and generics. The diff shows that the `copyTypeParams` method now copies the source position attributes (`sourceStart`, `sourceEnd`, `declarationEnd`, `declarationSourceStart`, `declarationSourceEnd`) from the original `TypeParameter` to the copied `TypeParameter`. This suggests that the lack of copying these attributes was the cause of the IllegalArgumentException, as the AST node was missing necessary source position information. Therefore, the changes indicate a bug fix. The answer is **Buggy**."
lombok,3095.json,6fce13a8db45629a76e31c2a1c676ab9dac021dc,"@@ -1,7 +1,7 @@
-	private static int getReplacementOffset(IJavaCompletionProposal proposal) {
+	private static int getReplacementOffset(Object proposal) {
 		try {
 			return Reflection.replacementOffsetField.getInt(proposal);
 		} catch (Exception ignore) {
 			return 0;
 		}
 	}",Buggy,"Somehow in eclipse mars wanted us to pull in more ecj deps, fixed that problem with a trivial code tweak.
",Buggy,"The commit message indicates a fix related to Eclipse Mars dependencies, achieved through a ""trivial code tweak."" The diff shows a change in the parameter type of the `getReplacementOffset` method from `IJavaCompletionProposal` to `Object`. This suggests a modification to accommodate a broader range of proposal types or to avoid direct dependency on the `IJavaCompletionProposal` interface, which might be related to the Eclipse dependencies issue mentioned in the commit message. The try-catch block suggests that the original code might have thrown an exception, and this change might be related to handling that exception. The change is likely a workaround or a fix to resolve the dependency problem. The answer is **Buggy**."
lombok,1236.json,e4b61e1263eb0eb832eb6cfbd97ad92e869ca27e,"@@ -1,11 +1,11 @@
 	public static boolean typeMatches(String type, JavacNode node, JCTree typeNode) {
 		String typeName = typeNode == null ? null : typeNode.toString();
 		if (typeName == null || typeName.length() == 0) return false;
 		int lastIndexA = typeName.lastIndexOf('.') + 1;
-		int lastIndexB = type.lastIndexOf('.') + 1;
+		int lastIndexB = Math.max(type.lastIndexOf('.'), type.lastIndexOf('$')) + 1;
 		int len = typeName.length() - lastIndexA;
 		if (len != type.length() - lastIndexB) return false;
 		for (int i = 0; i < len; i++) if (typeName.charAt(i + lastIndexA) != type.charAt(i + lastIndexB)) return false;
 		TypeResolver resolver = node.getImportListAsTypeResolver();
 		return resolver.typeMatches(node, type, typeName);
 	}",Buggy,[performance] fixing bug in earlier performance commit on optimizing lookups of typeMatches.,Buggy,"The commit message indicates a bug fix related to performance optimizations. The diff modifies the `typeMatches` method, specifically the calculation of `lastIndexB`. The original code only considered the last index of '.' while the corrected code considers both '.' and '$' to handle nested classes correctly. This change addresses a potential issue where the type matching logic would fail for nested classes, indicating a bug fix. The answer is **Buggy**."
lombok,314.json,9ac86c8a234f2ecd280f27f0dbda15440b1bf155,"@@ -1,9 +1,12 @@
 	public static LombokOptions getDelombokOptions(Context context) {
+		Options rawOptions = Options.instance(context);
+		if (rawOptions instanceof LombokOptions) return (LombokOptions) rawOptions;
+		
 		LombokOptions options;
 		if (Javac.getJavaCompilerVersion() < 8) {
 			options = LombokOptionCompilerVersion.JDK7_AND_LOWER.createAndRegisterOptions(context);
 		} else {
 			options = LombokOptionCompilerVersion.JDK8.createAndRegisterOptions(context);
 		}
 		return options;
 	}",Buggy,"fixed a bug where called LombokOptionsFactory.getLombokOptions() would actually cause the old options to be wrapped continuously, thus wrapping LombokOptions into itself infinitely. Now it just wraps if needed, other wise returns what's already been done.
",Buggy,"The commit message indicates a bug where `LombokOptions` were being wrapped infinitely. The diff introduces a check to see if the `rawOptions` are already an instance of `LombokOptions`. If so, it returns the existing instance, preventing the infinite wrapping. This aligns with the commit message's description of the bug fix. The answer is **Buggy**.
"
hibernate-search,6397.json,58ae33f9813b56aa60ffb2d301bc1aca8f023f6c,"@@ -1,27 +1,41 @@
 	private void indexAllQueue(Session session) {
 		final InstanceInitializer sessionInitializer = new HibernateSessionLoadingInitializer(
-				(SessionImplementor) session );
+				(SessionImplementor) session
+		);
 		try {
 			ConversionContext contextualBridge = new ContextualExceptionBridgeHelper();
 			while ( true ) {
 				List<?> takeList = source.take();
 				if ( takeList == null ) {
 					break;
 				}
 				else {
 					log.tracef( ""received a list of objects to index: %s"", takeList );
-					for ( Object take : takeList ) {
+					for ( Object object : takeList ) {
 						//trick to attach the objects to session:
-						session.buildLockRequest( LockOptions.NONE ).lock( take );
-						index( take, session, sessionInitializer, contextualBridge );
-						monitor.documentsBuilt( 1 );
+						session.buildLockRequest( LockOptions.NONE ).lock( object );
+						try {
+							index( object, session, sessionInitializer, contextualBridge );
+							monitor.documentsBuilt( 1 );
+						}
+						catch (InterruptedException ie) {
+							// rethrowing the interrupted exception
+							throw ie;
+						}
+						catch (RuntimeException e) {
+							String errorMsg = log.massIndexerUnableToIndexInstance(
+									object.getClass().getName(),
+									object.toString()
+							);
+							errorHandler.handleException( errorMsg, e );
+						}
 						session.clear();
 					}
 				}
 			}
 		}
 		catch (InterruptedException e) {
 			// just quit
 			Thread.currentThread().interrupt();
 		}
 	}",Buggy,"HSEARCH-1354 Fixing error handling during indexing for mass indexer and adding test case
",Buggy,"The commit message indicates a fix for error handling during indexing and the addition of a test case. The diff shows that the code now includes a try-catch block within the loop that iterates through the objects to be indexed. This try-catch block specifically handles `RuntimeException` during the `index` operation, logging an error message and delegating the exception to an `errorHandler`. This addition addresses potential errors that might occur during the indexing of individual objects, preventing the entire mass indexing process from failing. The `InterruptedException` is rethrown, which is a good practice. The changes clearly indicate improved error handling, aligning with the commit message's claim of ""fixing error handling"". The answer is **Buggy**."
hibernate-search,1888.json,835d01200cf073c05220f435d7ec47dc2593dddb,"@@ -1,13 +1,13 @@
-	public Query filterOrPassthrough(Query filteredQuery) {
+	public Query filterOrPassthrough(Query queryToFilter) {
 		if ( isEmpty() ) {
-			return filteredQuery;
+			return queryToFilter;
 		}
 		else {
 			BooleanQuery.Builder boolQueryBuilder = new BooleanQuery.Builder();
-			boolQueryBuilder.add( filteredQuery, Occur.MUST );
+			boolQueryBuilder.add( queryToFilter, Occur.MUST );
 			for ( Query bc : filterQueries ) {
 				boolQueryBuilder.add( bc, BooleanClause.Occur.FILTER );
 			}
 			return boolQueryBuilder.build();
 		}
 	}",Buggy,"HSEARCH-2698 Fix incomplete javadoc for some methods

This does not address all the problems, I intentionally didn't address
the more controversial ones (like the useless @return in fluent APIs).
",NotBuggy,"The commit message indicates a fix for incomplete Javadoc, but the diff shows a change in the parameter name from `filteredQuery` to `queryToFilter` in the `filterOrPassthrough` method. While this change might improve readability, it doesn't directly address a bug or error in the code's functionality. It's more of a refactoring or code style improvement. Therefore, the changes do not indicate a bug fix. The answer is **NotBuggy**.
"
wildfly,13142.json,77807f20fe0a38e6b867210ebf1b305edc14cf6a,"@@ -1,55 +1,59 @@
     private Method getTimeoutMethod(TimeoutMethod timeoutMethodInfo) {
 
         String declaringClass = timeoutMethodInfo.getDeclaringClass();
         Class<?> timeoutMethodDeclaringClass = null;
         try {
             timeoutMethodDeclaringClass = Class.forName(declaringClass, false, timedObjectInvoker.getClassLoader());
         } catch (ClassNotFoundException cnfe) {
-            throw new RuntimeException(""Could not load declaring class: "" + declaringClass + "" of timeout method"");
+            throw new RuntimeException(""Could not load declaring class: "" + declaringClass + "" of timeout method"", cnfe);
         }
 
         String timeoutMethodName = timeoutMethodInfo.getMethodName();
         String[] timeoutMethodParams = timeoutMethodInfo.getMethodParams();
         // load the method param classes
         Class<?>[] timeoutMethodParamTypes = new Class<?>[]
                 {};
         if (timeoutMethodParams != null) {
             timeoutMethodParamTypes = new Class<?>[timeoutMethodParams.length];
             int i = 0;
             for (String paramClassName : timeoutMethodParams) {
                 Class<?> methodParamClass = null;
                 try {
                     methodParamClass = Class.forName(paramClassName, false, timedObjectInvoker.getClassLoader());
                 } catch (ClassNotFoundException cnfe) {
-                    throw new RuntimeException(""Could not load method param class: "" + paramClassName + "" of timeout method"");
+                    throw new RuntimeException(""Could not load method param class: "" + paramClassName + "" of timeout method"", cnfe);
                 }
                 timeoutMethodParamTypes[i++] = methodParamClass;
             }
         }
         // now start looking for the method
         Class<?> klass = timeoutMethodDeclaringClass;
         while (klass != null) {
             Method[] methods = klass.getDeclaredMethods();
             for (Method method : methods) {
                 if (method.getName().equals(timeoutMethodName)) {
                     Class<?>[] methodParamTypes = method.getParameterTypes();
                     // param length doesn't match
                     if (timeoutMethodParamTypes.length != methodParamTypes.length) {
                         continue;
                     }
+                    boolean match = true;
                     for (int i = 0; i < methodParamTypes.length; i++) {
                         // param type doesn't match
                         if (!timeoutMethodParamTypes[i].equals(methodParamTypes[i])) {
-                            continue;
+                            match = false;
+                            break;
                         }
                     }
-                    // match found
-                    return method;
+                    if (match) {
+                        // match found
+                        return method;
+                    }
                 }
             }
             klass = klass.getSuperclass();
 
         }
         // no match found
         return null;
     }",Buggy,"Fix minor timer service bug
",Buggy,"The commit message indicates a bug fix in the timer service. The diff includes two changes:

1.  In the `ClassNotFoundException` catch blocks, the code now includes the original exception (`cnfe`) as the cause when throwing a new `RuntimeException`. This provides more context for debugging.
2.  In the method matching logic, a `match` variable is introduced to track whether the parameter types match. If a mismatch is found, the `match` variable is set to `false`, and the inner loop breaks. After the inner loop, the code checks if `match` is still `true` before returning the method. This ensures that the method is only returned if all parameter types match.

The first change improves error handling by providing more context when a class cannot be loaded. The second change corrects a potential logical error in the method matching logic. The original code would continue searching for methods even after finding a mismatch in parameter types, which could lead to incorrect method selection. The updated code ensures that only methods with matching parameter types are considered.

Both changes indicate bug fixes. The first change fixes a bug related to error reporting, while the second change fixes a bug related to incorrect method selection.
The answer is **Buggy**."
wildfly,15035.json,162a90c311abfe554f09dc4b9c080098bd27ad89,"@@ -1,18 +1,18 @@
         public final T visit(final ContextNode contextNode) throws NamingException {
             if (isEmpty(currentName)) {
                 return found(contextNode);
             }
             final String childName = currentName.get(0);
             traversedName.add(childName);
             currentName = currentName.getSuffix(1);
             final TreeNode node = contextNode.children.get(childName);
             if (node == null) {
                 if (createIfMissing) {
-                    final NamingContext subContext = new NamingContext(traversedName, InMemoryNamingStore.this, new Hashtable<String, Object>());
-                    return contextNode.addOrGetChild(childName, new ContextNode(contextNode, childName, traversedName, subContext)).accept(this);
+                    final NamingContext subContext = new NamingContext((Name)traversedName.clone(), InMemoryNamingStore.this, new Hashtable<String, Object>());
+                    return contextNode.addOrGetChild(childName, new ContextNode(contextNode, childName, (Name)traversedName.clone(), subContext)).accept(this);
                 } else {
                     throw nameNotFoundException(childName, contextNode.fullName);
                 }
             }
             return node.accept(this);
         }",Buggy,"AS7-1407 - Fix bug in nested context binding
",Buggy,"The commit message indicates a bug fix related to nested context binding. The code diff modifies the creation of a `NamingContext` when a child node is missing and `createIfMissing` is true. Specifically, it clones the `traversedName` before passing it to the `NamingContext` constructor and the `ContextNode` constructor. Without cloning, modifications to `traversedName` in subsequent operations could affect the `NamingContext` and `ContextNode` created earlier, leading to incorrect state and potential bugs in nested context binding. This change ensures that each `NamingContext` and `ContextNode` receives a distinct copy of the name, preventing unintended side effects and resolving the bug. The answer is **Buggy**."
wildfly,11562.json,1af2b154ec4daa8436ba6f946ef54fe201062c6a,"@@ -1,6 +1,8 @@
     private synchronized void destroySingletonInstance() {
         if (this.singletonComponentInstance != null) {
-            this.destroyInstance(this.singletonComponentInstance);
+            // TODO: Implement destroying an instance
+            logger.warn(""Destorying of singleton instance not yet implemented"");
+            //this.destroyInstance(this.singletonComponentInstance);
             this.singletonComponentInstance = null;
         }
     }",Buggy,"Fix compilation errors in EJB3 module to start refactoring the EJB3 component
",Buggy,"The commit message indicates a fix for compilation errors and the start of refactoring. The diff shows a change in the `destroySingletonInstance` method where the original `destroyInstance` call is commented out and replaced with a warning log message. This suggests that the original code was causing compilation errors or other issues that prevented the module from building. By commenting out the problematic code and adding a warning, the compilation errors are likely resolved, allowing the refactoring to proceed. This clearly indicates a bug fix, even if it's a temporary workaround. The answer is **Buggy**.
"
wildfly,9528.json,3b47b9b23bf3f10d124f26f37dff3245fe09ec56,"@@ -1,52 +1,52 @@
         public void execute(final OperationContext context, final ModelNode operation) throws OperationFailedException {
             final ModelNode model = context.readResource(PathAddress.EMPTY_ADDRESS).getModel();
 
             final ModelNode coreThreads;
             final ModelNode maxThreads;
             final ModelNode queueLength;
 
             if (isRuntimeStage) {
                 coreThreads = CORE_THREADS_AD.resolveModelAttribute(context, model);
                 maxThreads = MAX_THREADS_AD.resolveModelAttribute(context, model);
                 queueLength = QUEUE_LENGTH_AD.resolveModelAttribute(context, model);
             } else {
                 coreThreads = model.get(CORE_THREADS);
                 maxThreads = model.get(MAX_THREADS);
                 queueLength = model.get(QUEUE_LENGTH);
             }
 
             if (coreThreads.getType() == ModelType.EXPRESSION || maxThreads.getType() == ModelType.EXPRESSION ||
                     queueLength.getType() == ModelType.EXPRESSION) {
                 context.addStep(new ExecutorQueueValidationStepHandler(true), OperationContext.Stage.RUNTIME, true);
                 return;
             }
 
             // Validate an unbounded queue
             if (!queueLength.isDefined() || queueLength.asInt() == Integer.MAX_VALUE) {
                 if (coreThreads.isDefined() && coreThreads.asInt() <= 0) {
-                    throw EeLogger.ROOT_LOGGER.invalidCoreThreadsSize(coreThreads.asString());
+                    throw EeLogger.ROOT_LOGGER.invalidCoreThreadsSize(queueLength.asString());
                 }
 
             }
 
             // Validate a hand-off queue
             if (queueLength.isDefined() && queueLength.asInt() == 0) {
                 if (coreThreads.isDefined() && coreThreads.asInt() <= 0) {
-                    throw EeLogger.ROOT_LOGGER.invalidCoreThreadsSize(coreThreads.asString());
+                    throw EeLogger.ROOT_LOGGER.invalidCoreThreadsSize(queueLength.asString());
                 }
             }
 
             // max-threads must be defined and greater than 0 if core-threads is 0
             if (coreThreads.isDefined() && coreThreads.asInt() == 0) {
                 if (!maxThreads.isDefined() || maxThreads.asInt() <= 0) {
                     throw EeLogger.ROOT_LOGGER.invalidMaxThreads(maxThreads.isDefined() ? maxThreads.asInt() : 0, coreThreads.asInt());
                 }
             }
 
             // max-threads must be greater than or equal to core-threads
             if (coreThreads.isDefined() && maxThreads.isDefined()) {
                 if (maxThreads.asInt() < coreThreads.asInt()) {
                     throw EeLogger.ROOT_LOGGER.invalidMaxThreads(maxThreads.asInt(), coreThreads.asInt());
                 }
             }
         }",Buggy,"[WFLY-8516] Update schema documentation and fix error messages for the managed-executor-service.
",Buggy,"The commit message indicates that error messages for the managed-executor-service have been fixed. The diff shows that in two places, `EeLogger.ROOT_LOGGER.invalidCoreThreadsSize(coreThreads.asString())` has been changed to `EeLogger.ROOT_LOGGER.invalidCoreThreadsSize(queueLength.asString())`. This indicates that the wrong variable was being used in the error message, which is a bug fix. The answer is **Buggy**."
wildfly,10266.json,2bb205f62a00e2d69781f4b468c299316f855764,"@@ -1,5 +1,5 @@
     public void start(StartContext context) throws StartException {
         this.value.setDefaultBootstrapContext(defaultBootstrapContext.getValue());
-        ROOT_LOGGER.startingSubsystem(""JCS"", Version.FULL_VERSION);
+        ROOT_LOGGER.startingSubsystem(""JCA"", Version.FULL_VERSION);
         ROOT_LOGGER.tracef(""config=%s"", value);
     }",Buggy,"Fix spelling error in a JCA subsystem log message.
",Buggy,"The commit message indicates a spelling error fix in a log message. The diff shows a change from ""JCS"" to ""JCA"" in the `startingSubsystem` log message. This aligns with the commit message and represents a correction of a typographical error, which can be considered a bug fix in the sense of correcting an error in the codebase. The answer is **Buggy**.
"
wildfly,18502.json,a10bd882e880725b709e6745e140931720629a95,"@@ -1,17 +1,20 @@
     public InputStream sanitize(InputStream in) throws Exception {
+        byte [] content = IOUtils.toByteArray(in);
         try {
-            Document doc = builder.parse(in);
+            // storing the entire file in memory in case we need to bail.
+            Document doc = builder.parse(new ByteArrayInputStream(content));
             Object result = expression.evaluate(doc, XPathConstants.NODESET);
             NodeList nodes = (NodeList) result;
             for (int i = 0; i < nodes.getLength(); i++) {
                 nodes.item(i).setTextContent("""");
             }
             DOMSource source = new DOMSource(doc);
             ByteArrayOutputStream output = new ByteArrayOutputStream();
             StreamResult outStream = new StreamResult(output);
             transformer.transform(source, outStream);
             return new ByteArrayInputStream(output.toByteArray());
         } catch (Exception e) {
-            return in;
+            ROOT_LOGGER.debug(""Error while sanitizing an xml document"", e);
+            return new ByteArrayInputStream(content);
         }
     }",Buggy,"Addressing AS7-6731 [bz920113]

Module dependency on xalan now uses services=import
Adding javadoc to various bits of the code
Fixes an issue where an error during sanitation causes
0 length files to be stored in the archive.
",Buggy,"The commit message indicates a fix related to sanitation, where errors could lead to zero-length files being stored. The code diff shows that the `sanitize` method now stores the entire input stream in memory as a byte array before parsing it. If an exception occurs during the sanitization process, the original content is returned. This prevents the creation of empty files when errors occur. The logging was also improved. The answer is **Buggy**."
wildfly,6365.json,7f64913774a8043da0b0cc545f4e1b2a8af1a082,"@@ -1,7 +1,13 @@
     public boolean isTransactionActive() {
         try {
-            return (injectedTransaction.getValue().getStatus() & Status.STATUS_ACTIVE) != 0;
+            final int status = injectedTransaction.getValue().getStatus();
+            return status == Status.STATUS_ACTIVE ||
+                    status == Status.STATUS_COMMITTING ||
+                    status == Status.STATUS_MARKED_ROLLBACK ||
+                    status == Status.STATUS_PREPARED ||
+                    status == Status.STATUS_PREPARING ||
+                    status == Status.STATUS_ROLLING_BACK;
         } catch (SystemException e) {
             throw new RuntimeException(""SystemException while getting transaction status"", e);
         }
     }",Buggy,"Fix bug in WeldTransactionServices
",Buggy,"The commit message indicates a bug fix in `WeldTransactionServices`. The code diff modifies the `isTransactionActive` method to check for various transaction statuses beyond just `Status.STATUS_ACTIVE`. The original code only checked if the transaction status had the `STATUS_ACTIVE` bit set, potentially missing other active states like `STATUS_COMMITTING`, `STATUS_MARKED_ROLLBACK`, etc. The updated code explicitly checks for all possible active states, indicating a correction of a logical error. The answer is **Buggy**.
"
wildfly,14704.json,59232bbc1b3a2808f2cbc125e1aca2647fa5abd5,"@@ -1,16 +1,11 @@
     public static void initializeNamingManager() {
         // Setup naming environment
         System.setProperty(Context.URL_PKG_PREFIXES, PACKAGE_PREFIXES);
         try {
             //If we are reusing the JVM. e.g. in tests we should not set this again
             if (!NamingManager.hasInitialContextFactoryBuilder())
                 NamingManager.setInitialContextFactoryBuilder(new InitialContextFactoryBuilder());
         } catch (NamingException e) {
             log.warn(""Failed to set InitialContextFactoryBuilder"", e);
         }
-        try {
-            NamingManager.setObjectFactoryBuilder(ObjectFactoryBuilder.INSTANCE);
-        } catch(Throwable t) {
-            log.warn(""Failed to set ObjectFactoryBuilder"", t);
-        }
     }",Buggy,"Fix problem where object factory builder can only be set once
",Buggy,"The commit message indicates a fix for a problem where the object factory builder could only be set once. The code diff removes the `NamingManager.setObjectFactoryBuilder` call from the `initializeNamingManager` method. The original code attempted to set the `ObjectFactoryBuilder` every time `initializeNamingManager` was called, which would fail after the first call because it can only be set once. Removing this line fixes the issue described in the commit message. The answer is **Buggy**.
"
wildfly,14735.json,603f475d6af9bc7abc7bcee74a897d4577977d27,"@@ -1,3 +1,3 @@
     public void close() throws NamingException {
-        namingStore.close();
+        // NO-OP
     }",Buggy,"Fix bug in naming context with Context.close removing entries from the naming store
",Buggy,"The commit message indicates a bug related to `Context.close` removing entries from the naming store. The diff shows that the `namingStore.close()` call within the `close()` method has been commented out (replaced with a NO-OP). This suggests that the original `namingStore.close()` operation was the cause of the bug, and removing it fixes the issue. Therefore, the change is a bug fix. The answer is **Buggy**."
wildfly,5832.json,8c3680a316585b462638de3f031affc527f5c723,"@@ -1,7 +1,7 @@
     public static <T> Class<T> loadClass(String className, ClassLoader classLoader) {
         try {
             return cast(classLoader.loadClass(className));
-        } catch (Exception e) {
+        } catch (Throwable e) {
             return null;
         }
     }",Buggy,"Capture RuntimeExceptions in dinamic class loading

Since 9.x, if you deploy an war with two (or more wars) with different libraries, deployment crash since it's trying to load classes from one WAR with the classloader from the other WAR. This cross load is intended (see BeanDeploymentArchiveImpl, method isAccesible, line 246, WFLY-4250), but it's waiting for a null load, not a RuntimeException.

With this fix, a warn is registered in log but application can be loaded without major problems.
",Buggy,"The commit message indicates a fix for a class loading issue in a multi-WAR deployment scenario. The problem arises when one WAR's classloader attempts to load classes from another WAR, resulting in a `RuntimeException`. The original code expected a `null` load in such cases, not an exception. The diff modifies the `loadClass` method to catch `Throwable` instead of `Exception`. This change allows the application to continue loading, albeit with a warning logged. By catching `Throwable`, the code now handles `RuntimeException` which was previously unhandled, indicating a bug fix. The answer is **Buggy**."
wildfly,10236.json,efb61994477cc0e8f0f1eca6da690a78c9c33699,"@@ -1,21 +1,21 @@
         private void writeWorkManagers(XMLExtendedStreamWriter writer, ModelNode parentNode) throws XMLStreamException {
             if (parentNode.hasDefined(WORKMANAGER) && parentNode.get(WORKMANAGER).asList().size() != 0) {
                 for (Property property : parentNode.get(WORKMANAGER).asPropertyList()) {
                     if (""default"".equals(property.getValue().get(NAME).asString())) {
                         writer.writeStartElement(Element.DEFAULT_WORKMANAGER.getLocalName());
                     } else {
                         writer.writeStartElement(Element.WORKMANAGER.getLocalName());
                         WorkManagerAdd.WmParameters.NAME.getAttribute().marshallAsAttribute(property.getValue(), writer);
                     }
-                    for (Property prop : property.getValue().asPropertyList()) {
+                    for (Property prop : property.getValue().asPropertyList() ) {
                         if (WORKMANAGER_LONG_RUNNING.equals(prop.getName()) && prop.getValue().isDefined() && prop.getValue().asPropertyList().size() != 0) {
                             ThreadsParser.getInstance().writeBoundedQueueThreadPool(writer, prop.getValue().asProperty().getValue(), Element.LONG_RUNNING_THREADS.getLocalName(), false);
                         }
-                        if (WORKMANAGER_SHORT_RUNNING.equals(prop.getName())) {
+                        if (WORKMANAGER_SHORT_RUNNING.equals(prop.getName()) && prop.getValue().isDefined() && prop.getValue().asPropertyList().size() != 0) {
                             ThreadsParser.getInstance().writeBoundedQueueThreadPool(writer, prop.getValue().asProperty().getValue(), Element.SHORT_RUNNING_THREADS.getLocalName(), false);
                         }
                     }
                     writer.writeEndElement();
                 }
             }
         }",Buggy,"Fixing persistemce problem when all thread executor config has been removed
",Buggy,"The commit message indicates a fix for a persistence problem that occurs when all thread executor configurations are removed. The code diff focuses on the `writeWorkManagers` method, which is responsible for writing work manager configurations to an XML stream. The changes involve adding checks `prop.getValue().isDefined() && prop.getValue().asPropertyList().size() != 0` before processing `WORKMANAGER_SHORT_RUNNING` and `WORKMANAGER_LONG_RUNNING` properties. These checks prevent the code from attempting to process these properties if they are not defined or have an empty list of properties, which could lead to errors or unexpected behavior during persistence. This suggests that the original code had a bug where it would try to process these properties even when they were not present, causing the persistence problem described in the commit message. The added checks are a direct fix for this bug. The answer is **Buggy**."
wildfly,10893.json,e8dd2b36ea6a8dd25b8cffc37791e862fb08aa45,"@@ -1,12 +1,12 @@
         public ResourceAdapter doDeploy(URL url, String deploymentName, File root, ClassLoader cl, Connector cmd,
                 IronJacamar ijmd) throws Throwable {
             // TODO
             this.setConfiguration(getConfig().getValue());
 
             this.start();
 
-            CommonDeployment dep = this.createObjectsAndInjectValue(url, deploymentName, root, null, cl, cmd, ijmd, null);
+            CommonDeployment dep = this.createObjectsAndInjectValue(url, deploymentName, root, cl, cmd, ijmd);
 
             return dep.getResourceAdapter();
 
         }",Buggy,"fixing problem with latests ironjacamar snapshots
",Buggy,"The commit message indicates a fix related to IronJacamar snapshots. The code diff shows a change in the `doDeploy` method where a `null` argument is removed from the `createObjectsAndInjectValue` method call. While it's not immediately clear what problem this resolves, the commit message suggests it's a fix, and the change involves modifying a method call, which could be related to resolving compatibility issues or errors with newer IronJacamar snapshots. Therefore, it's likely a bug fix. The answer is **Buggy**.
"
javaparser,3383.json,85b5cf5a98bc7fc59b3481d802bdff3b736ae8a1,"@@ -1,62 +1,65 @@
     public void accept(VarType node, ProblemReporter reporter) {
         // All allowed locations are within a VariableDeclaration inside a VariableDeclarationExpr inside something else.
         Optional<VariableDeclarator> variableDeclarator = node.findAncestor(VariableDeclarator.class);
         if (!variableDeclarator.isPresent()) {
             // Java 11's var in lambda's
             if (varAllowedInLambdaParameters) {
                 boolean valid = node
                         .findAncestor(Parameter.class)
                         .flatMap(Node::getParentNode)
                         .map((Node p) -> p instanceof LambdaExpr).orElse(false);
                 if (valid) {
                     return;
                 }
             }
             reportIllegalPosition(node, reporter);
             return;
         }
         variableDeclarator.ifPresent(vd -> {
+            if(vd.getType().isArrayType()){
+                reporter.report(vd, ""\""var\"" cannot have extra array brackets."");
+            }
             Optional<Node> variableDeclarationExpr = vd.getParentNode();
             if (!variableDeclarationExpr.isPresent()) {
                 reportIllegalPosition(node, reporter);
                 return;
             }
             variableDeclarationExpr.ifPresent(vdeNode -> {
                 if (!(vdeNode instanceof VariableDeclarationExpr)) {
                     reportIllegalPosition(node, reporter);
                     return;
                 }
                 VariableDeclarationExpr vde = (VariableDeclarationExpr) vdeNode;
                 if (vde.getVariables().size() > 1) {
                     reporter.report(vde, ""\""var\"" only takes a single variable."");
                 }
                 Optional<Node> container = vdeNode.getParentNode();
                 if (!container.isPresent()) {
                     reportIllegalPosition(node, reporter);
                     return;
                 }
                 container.ifPresent(c -> {
                     boolean positionIsFine = c instanceof ForStmt || c instanceof ForeachStmt || c instanceof ExpressionStmt;
                     if (!positionIsFine) {
                         reportIllegalPosition(node, reporter);
                     }
                     // A local variable declaration ends up inside an ExpressionStmt.
                     if (c instanceof ExpressionStmt) {
                         if (!vd.getInitializer().isPresent()) {
                             reporter.report(node, ""\""var\"" needs an initializer."");
                         }
                         vd.getInitializer().ifPresent(initializer -> {
                             if (initializer instanceof NullLiteralExpr) {
                                 reporter.report(node, ""\""var\"" cannot infer type from just null."");
                             }
-                            if (initializer instanceof ArrayCreationExpr) {
+                            if (initializer instanceof ArrayInitializerExpr) {
                                 reporter.report(node, ""\""var\"" cannot infer array types."");
                             }
                         });
 
                     }
                 });
             });
         });
 
     }",Buggy,"Fix mistake with var and array initializers.
",Buggy,"The commit message ""Fix mistake with var and array initializers"" suggests a bug fix related to the `var` keyword and array initializers. The diff adds a check to prevent `var` from having extra array brackets: `if(vd.getType().isArrayType()){ reporter.report(vd, ""\""var\"" cannot have extra array brackets.""); }`. It also changes the check for array initializers from `ArrayCreationExpr` to `ArrayInitializerExpr`: `if (initializer instanceof ArrayInitializerExpr) { reporter.report(node, ""\""var\"" cannot infer array types.""); }`. These changes indicate that the code previously had issues with handling array types and `var`, which are now being addressed. The answer is **Buggy**."
javaparser,6722.json,c620101966295e302b9ff94a73a0e2ab29c7ec6d,"@@ -1,3 +1,3 @@
     public boolean isWildcard() {
-        return getType().isArray();
+        return getType().isWildcard();
     }",Buggy,"Fix bug in LazyType
",Buggy,The commit message indicates a bug fix in `LazyType`. The diff shows a change in the `isWildcard()` method. The original code incorrectly checked if the type was an array (`getType().isArray()`) to determine if it was a wildcard. The corrected code now properly checks if the type is a wildcard (`getType().isWildcard()`). This indicates a logical error in the original code that has been fixed. The answer is **Buggy**.
javaparser,6689.json,a795de5cf63b37147e9efcd9ed89e486a613fd3c,"@@ -1,3 +1,3 @@
     public String getName() {
-        return constructor.getName();
+        return constructor.getDeclaringClass().getSimpleName();
     }",Buggy,"Fixed a bug in ReflectionConstructorDeclaration that caused the constructor's signature to be calculated incorrectly.
",Buggy,"The commit message indicates a bug fix related to the constructor's signature calculation in `ReflectionConstructorDeclaration`. The diff shows that the `getName()` method now returns the simple name of the declaring class instead of the constructor's name. This change aligns with the commit message, suggesting that the original implementation was indeed incorrect and led to an inaccurate constructor signature. The answer is **Buggy**."
javaparser,6349.json,dc99432270600b2bb2a445c86b329a77601005e6,"@@ -1,10 +1,10 @@
     public SymbolReference<ResolvedAnnotationDeclaration> solve(AnnotationExpr annotationExpr) {
         Context context = JavaParserFactory.getContext(annotationExpr, typeSolver);
         SymbolReference<ResolvedTypeDeclaration> typeDeclarationSymbolReference = context.solveType(annotationExpr.getNameAsString(), typeSolver);
-        ResolvedAnnotationDeclaration annotationDeclaration = (ResolvedAnnotationDeclaration) typeDeclarationSymbolReference.getCorrespondingDeclaration();
         if (typeDeclarationSymbolReference.isSolved()) {
+            ResolvedAnnotationDeclaration annotationDeclaration = (ResolvedAnnotationDeclaration) typeDeclarationSymbolReference.getCorrespondingDeclaration();
             return SymbolReference.solved(annotationDeclaration);
         } else {
             return SymbolReference.unsolved(ResolvedAnnotationDeclaration.class);
         }
     }",Buggy,"Fixed minor bug: a corresponding declaration of a SymbolReference is only available if the SymbolReference is solved, and one should not try to retrieve it otherwise, lest a NullPointerException be thrown.
",Buggy,"The commit message indicates a fix for a potential NullPointerException when retrieving a `SymbolReference`'s declaration before it's solved. The code diff confirms this: the `getCorrespondingDeclaration()` call is now within the `if (typeDeclarationSymbolReference.isSolved())` block. This prevents the code from attempting to access the declaration when it's not available, thus avoiding the NullPointerException. The answer is **Buggy**."
intellij-community,15168.json,8983e75f816afa5ca0d149e488a252e54d16f50f,"@@ -1,22 +1,22 @@
   private static void showWhatsNewNotification(@NotNull Project project) {
     PropertiesComponent properties = PropertiesComponent.getInstance();
     String updateHtmlMessage = properties.getValue(UPDATE_WHATS_NEW_MESSAGE);
     if (updateHtmlMessage == null) {
       LOG.warn(""Cannot show what's new notification: no content found."");
       return;
     }
 
     String title = IdeBundle.message(""update.whats.new.notification.title"", ApplicationNamesInfo.getInstance().getFullProductName());
     UpdateChecker.getNotificationGroup().createNotification(title, null, null, NotificationType.INFORMATION, null)
       .addAction(new NotificationAction(IdeBundle.message(""update.whats.new.notification.action"")) {
         @Override
         public void actionPerformed(@NotNull AnActionEvent e, @NotNull Notification notification) {
           LightVirtualFile file = new LightVirtualFile(IdeBundle.message(""update.whats.new.file.name"", ApplicationInfo.getInstance().getFullVersion()), updateHtmlMessage);
           file.putUserData(HTMLEditorProvider.Companion.getHTML_CONTENT_TYPE(), true);
           FileEditorManager.getInstance(project).openFile(file, true);
           IdeUpdateUsageTriggerCollector.trigger(""update.whats.new"");
           notification.expire();
         }
-      });
+      }).notify(project);
     properties.setValue(UPDATE_WHATS_NEW_MESSAGE, null);
   }",Buggy,"Fix stupid mistake: lost call notify()

GitOrigin-RevId: 954900084aa113c881590d721b66974c560d9d9f",Buggy,"The commit message ""Fix stupid mistake: lost call notify()"" indicates a bug fix where a notification was not being displayed. The diff shows that the `.notify(project)` call was missing from the `createNotification` chain. This omission would prevent the notification from being shown, which aligns with the commit message's description of a ""lost call"". The answer is **Buggy**."
intellij-community,24434.json,358dddc2d98464d660a801e43e3671cbd5b340ed,"@@ -1,5 +1,5 @@
   public static Icon loadApplicationIconImage(@NotNull ScaleContext ctx, int size) {
     String url = ApplicationInfoImpl.getShadowInstance().getApplicationSvgIconUrl();
     Image image = loadApplicationIconImage(url, ctx, size, null);
-    return image != null ? new JBImageIcon(ImageUtil.ensureHiDPI(image, ctx)) : null;
+    return image != null ? new JBImageIcon(image) : null;
   }",Buggy,"IDEA-217767 License dialog polishing

* App icon problems fixed

GitOrigin-RevId: babcb9bce8c091934088cc24bc7ce335170096a6",Buggy,"The commit message indicates a fix for application icon problems. The code change removes `ImageUtil.ensureHiDPI(image, ctx)` which suggests that the previous implementation was incorrectly handling HiDPI scaling, potentially leading to display issues with the application icon. Removing the HiDPI adjustment likely resolves a bug related to icon scaling or display. The answer is **Buggy**."
intellij-community,8042.json,08de853c05b9eb1944f79c1ea7df05ae1f103aa7,"@@ -1,3 +1,3 @@
   protected static String getUserHome() {
-    return SystemProperties.getUserHome();
+    return StringUtil.trimEnd(SystemProperties.getUserHome(), ""/"");
   }",Buggy,"save all paths in .iml relatively except for the ones under USER_HOME (IDEA-60906)
reason: fixes the issue and I don't see any problems it could cause
",Buggy,"The commit message indicates a fix related to saving paths in `.iml` files, specifically addressing an issue where paths under `USER_HOME` were not being handled correctly. The diff shows a change in the `getUserHome()` method, which now trims trailing slashes from the user's home directory path. This suggests that the original code might have had issues with paths containing trailing slashes, potentially causing incorrect path resolution or storage in the `.iml` file. The trimming of the trailing slash is a correction to ensure consistent path handling. The answer is **Buggy**.
"
intellij-community,32648.json,0922a97944f2ca7d388ba2807e0e64afb02fe427,"@@ -1,3 +1,3 @@
   public void load(Element element) {
-    setVagrantFolder(element.getAttributeValue(VAGRANT_FOLDER));
+    setVagrantFolder(StringUtil.notNullize(element.getAttributeValue(VAGRANT_FOLDER)));
   }",Buggy,"Vagrant error messaging fixed.
",Buggy,"The commit message indicates a fix to Vagrant error messaging. The diff shows a change in the `load` method where `setVagrantFolder` is called with the result of `StringUtil.notNullize` applied to the attribute value. This suggests that the original code might have been susceptible to null pointer exceptions or other issues when the `VAGRANT_FOLDER` attribute was null. By using `StringUtil.notNullize`, the code now ensures that a non-null value (likely an empty string) is passed to `setVagrantFolder`, preventing potential errors. This is a bug fix. The answer is **Buggy**."
intellij-community,23329.json,bcb3520ffa862b37e8147a7dac6cddb7d67c82b8,"@@ -1,29 +1,29 @@
     public void doExecute(Editor editor, @Nullable Caret caret, DataContext dataContext) {
       int endOffset = editor.getDocument().getTextLength();
       List<Caret> carets = editor.getCaretModel().getAllCarets();
       if (editor.isColumnMode() && editor.getCaretModel().supportsMultipleCarets()) {
         if (caret == null) { // normally we're always called with null caret
           caret = carets.get(0) == editor.getCaretModel().getPrimaryCaret() ? carets.get(carets.size() - 1) : carets.get(0);
         }
         LogicalPosition leadSelectionPosition = editor.visualToLogicalPosition(caret.getLeadSelectionPosition());
         LogicalPosition targetPosition = editor.offsetToLogicalPosition(endOffset).leanForward(true);
         editor.getSelectionModel().setBlockSelection(leadSelectionPosition, targetPosition);
       }
       else {
         if (caret == null) { // normally we're always called with null caret
           caret = carets.get(0);
         }
         int selectionStart = caret.getLeadSelectionOffset();
         if (editor instanceof EditorImpl && ((EditorImpl)editor).myUseNewRendering) {
-          editor.getCaretModel().moveToLogicalPosition(editor.offsetToLogicalPosition(endOffset).leanForward(true));
+          caret.moveToLogicalPosition(editor.offsetToLogicalPosition(endOffset).leanForward(true));
         }
         else {
           caret.moveToOffset(endOffset);
         }
         caret.setSelection(selectionStart, endOffset);
       }
       ScrollingModel scrollingModel = editor.getScrollingModel();
       scrollingModel.disableAnimation();
       scrollingModel.scrollToCaret(ScrollType.CENTER);
       scrollingModel.enableAnimation();
     }",Buggy,"editor.new.rendering: fix a mistake in TextEndWithSelectionAction
",Buggy,"The commit message indicates a fix in `TextEndWithSelectionAction`. The diff shows that inside the `else` block (when not in column mode), there's a change in how the caret is moved when `myUseNewRendering` is true. Instead of calling `editor.getCaretModel().moveToLogicalPosition(...)`, the code now calls `caret.moveToLogicalPosition(...)`. This suggests that the original code was incorrectly trying to move *all* carets to the end of the document instead of just the current caret, which would be a bug. The corrected code moves only the current caret. The answer is **Buggy**."
intellij-community,11452.json,8a97673aeb2e23f7a0bafdb1907b513cd3514810,"@@ -1,3 +1,3 @@
-  protected boolean atToken(final IElementType tokenType) {
+  protected boolean atToken(@Nullable final IElementType tokenType) {
     return myBuilder.getTokenType() == tokenType;
   }",Buggy,"better recovery for syntax errors in parameter list (PY-3635); fix old bug with tuple parameters parsing
",Buggy,"The commit message indicates a fix for syntax errors and tuple parameter parsing. The code change adds a `@Nullable` annotation to the `tokenType` parameter of the `atToken` method. This change suggests that the method might have been incorrectly handling null token types, potentially leading to errors or unexpected behavior. The addition of `@Nullable` implies that the method now explicitly accounts for the possibility of a null token type, which aligns with the commit message's mention of improved error recovery. The answer is **Buggy**."
intellij-community,28989.json,ac607f3f6a68a2dbccca5581d55264e5ecad1181,"@@ -1,4 +1,4 @@
     public void addNotify() {
       super.addNotify();
-      addMouseListenerToHierarchy(this, myEntranceListener);
+      ListenerUtil.addMouseListener(this, myEntranceListener);
     }",Buggy,"links in IDE Fatal errors fix
",Buggy,"The commit message indicates a fix for ""IDE Fatal errors"" related to links. The code change replaces `addMouseListenerToHierarchy` with `ListenerUtil.addMouseListener`. This suggests that the original method might have been causing errors, possibly due to incorrect handling of mouse listeners in the component hierarchy. The change likely addresses a bug related to how mouse listeners are added, potentially preventing fatal errors in the IDE. The answer is **Buggy**."
intellij-community,10790.json,e3fc0ee66ac0e1b3cff41e6cf7cf90b5c16318f5,"@@ -1,26 +1,29 @@
   private static int getExpressionPriority(PyElement expr) {
     int priority = 0;
-    if (expr instanceof PySubscriptionExpression || expr instanceof PySliceExpression || expr instanceof PyCallExpression) priority = 1;
+    if (expr instanceof PyReferenceExpression ||
+        expr instanceof PySubscriptionExpression ||
+        expr instanceof PySliceExpression ||
+        expr instanceof PyCallExpression) priority = 1;
     else if (expr instanceof PyPrefixExpression) {
       final IElementType opType = getOperationType(expr);
       if (opType == PLUS || opType == MINUS || opType == TILDE) priority = 2;
       if (opType == NOT_KEYWORD) priority = 11;
     }
     else if (expr instanceof PyBinaryExpression) {
       final IElementType opType = getOperationType(expr);
       if (opType == EXP) priority =  3;
       if (opType == MULT || opType == DIV || opType == PERC || opType == FLOORDIV) priority =  4;
       if (opType == PLUS || opType == MINUS) priority =  5;
       if (opType == LTLT || opType == GTGT) priority = 6;
       if (opType == AND) priority = 7;
       if (opType == XOR) priority = 8;
       if (opType == OR) priority = 9;
       if (COMPARISON_OPERATIONS.contains(opType)) priority = 10;
       if (opType == AND_KEYWORD) priority = 12;
       if (opType == OR_KEYWORD) priority = 13;
     }
     else if (expr instanceof PyConditionalExpression) priority = 14;
     else if (expr instanceof PyLambdaExpression) priority = 15;
 
     return -priority;
   }",Buggy,"PY-13114 Inlining of method call does not add extra parenthesis

Fix prioritization problem: reference expressions should have the same
priority as indexing, slicing and function calls.
",Buggy,"The commit message indicates a fix for prioritization issues related to inlining method calls, specifically addressing the need for extra parentheses. The code change modifies the `getExpressionPriority` method to include `PyReferenceExpression` in the same priority group as `PySubscriptionExpression`, `PySliceExpression`, and `PyCallExpression`. This adjustment suggests that `PyReferenceExpression` was previously not considered with the correct priority, leading to incorrect inlining behavior and the potential omission of necessary parentheses. Therefore, the change fixes a bug related to operator precedence during inlining. The answer is **Buggy**."
intellij-community,11366.json,aa7434f6b1f2853d50f940681697db30743e0f2e,"@@ -1,12 +1,13 @@
   static void deleteAdjacentComma(ASTDelegatePsiElement pyImportStatement, ASTNode child, final PyElement[] elements) {
     if (ArrayUtil.contains(child.getPsi(), elements)) {
       ASTNode next = getNextComma(child);
       if (next == null) {
         next = getPrevComma(child);
       }
       if (next != null) {
-        removeSlash(pyImportStatement, next);
+        final ASTNode prev = next.getTreePrev();
         pyImportStatement.deleteChildInternal(next);
+        removeSlash(pyImportStatement, prev);
       }
     }
   }",Buggy,"fixed PY-5489 Refactor/Move: leads to syntax error with multi-line imports
",Buggy,"The commit message indicates a fix for a syntax error caused by refactoring/moving multi-line imports. The code diff modifies the `deleteAdjacentComma` method, which is responsible for removing commas adjacent to import elements during refactoring. The change involves storing the previous node of the comma to be deleted in a variable `prev` and then passing this `prev` node to the `removeSlash` method. This suggests that the original code might have been incorrectly handling the removal of slashes associated with commas in multi-line imports, potentially leading to syntax errors. The modification ensures that the correct node is passed to `removeSlash`, which is likely a bug fix. The answer is **Buggy**."
intellij-community,11366.json,319662174c861f6c1472a6d644db4513907f4c74,"@@ -1,11 +1,12 @@
   static void deleteAdjacentComma(ASTDelegatePsiElement pyImportStatement, ASTNode child, final PyElement[] elements) {
     if (ArrayUtil.contains(child.getPsi(), elements)) {
       ASTNode next = getNextComma(child);
       if (next == null) {
         next = getPrevComma(child);
       }
       if (next != null) {
+        removeSlash(pyImportStatement, next);
         pyImportStatement.deleteChildInternal(next);
       }
     }
   }",Buggy,"fixed PY-5489 Refactor/Move: leads to syntax error with multi-line imports
",Buggy,"The commit message indicates a fix for a bug (PY-5489) related to refactoring/moving multi-line imports, which leads to syntax errors. The code diff shows a change in the `deleteAdjacentComma` method, where a `removeSlash` method is called before deleting a comma. This suggests that the original code was not correctly handling multi-line imports, potentially leaving dangling slashes that caused syntax errors after refactoring/moving. The addition of `removeSlash` likely addresses this issue. The answer is **Buggy**.
"
intellij-community,36036.json,84dd250e37af6f1a98d160eaae6a59022d24c7dd,"@@ -1,8 +1,11 @@
   public void revalidate(@NotNull PositionTracker<Balloon> tracker) {
+    if (ApplicationManager.getApplication().isDisposeInProgress()) {
+      return;
+    }
     RelativePoint newPosition = tracker.recalculateLocation(this);
 
     if (newPosition != null) {
       myTargetPoint = myPosition.getShiftedPoint(newPosition.getPoint(myLayeredPane), myCalloutShift);
       myPosition.updateBounds(this);
     }
   }",Buggy,"Fix for ""Application being disposed"" error
",Buggy,"The commit message indicates a fix for an ""Application being disposed"" error. The code diff adds a check at the beginning of the `revalidate` method to see if the application is in the process of being disposed. If it is, the method returns immediately, preventing further actions that could lead to the error. This is a defensive programming technique to avoid issues when the application is shutting down. The answer is **Buggy**."
intellij-community,29938.json,d5022fa9bef92fcb2e21a87b4a03cb0938a1f961,"@@ -1,12 +1,20 @@
   private void doCloseCommunication() {
     if (myPydevConsoleCommunication != null) {
-      try {
-        myPydevConsoleCommunication.close();
-        // waiting for REPL communication before destroying process handler
-        Thread.sleep(300);
-      }
-      catch (Exception e1) {
-        // Ignore
-      }
+
+      UIUtil.invokeAndWaitIfNeeded(new Runnable() {
+        @Override
+        public void run() {
+          try {
+            myPydevConsoleCommunication.close();
+            Thread.sleep(300);
+          }
+          catch (Exception e1) {
+            // Ignore
+          }
+        }
+      });
+
+      // waiting for REPL communication before destroying process handler
+
     }
   }",Buggy,"Fixed EDT bug (PY-3434).
",Buggy,"The commit message indicates a fix for an EDT (Event Dispatch Thread) bug. The code diff wraps the `myPydevConsoleCommunication.close()` and `Thread.sleep(300)` calls within `UIUtil.invokeAndWaitIfNeeded()`. This ensures that the code is executed on the EDT, which is crucial for UI-related operations and prevents potential threading issues or deadlocks that could arise from performing these operations on a different thread. The original code might have been causing issues by directly closing the communication and sleeping on a non-EDT thread. Therefore, this change addresses a bug related to thread management and UI responsiveness. The answer is **Buggy**."
intellij-community,12571.json,d9a69b15763b0da7a03c07d0f9c8e5a8f7e3d7f7,"@@ -1,25 +1,25 @@
     private static void appendDescriptors(ASTNode node,
                                           List<FoldingDescriptor> descriptors) {
         if (node.getElementType() == PyElementTypes.STATEMENT_LIST) {
             IElementType elType = node.getTreeParent().getElementType();
             if (elType == PyElementTypes.FUNCTION_DECLARATION || elType == PyElementTypes.CLASS_DECLARATION) {
                 ASTNode colon = node.getTreeParent().findChildByType(PyTokenTypes.COLON);
-                if (colon != null && colon.getStartOffset() + 1 < node.getStartOffset() + node.getTextLength()) {
+                if (colon != null && colon.getStartOffset() + 1 < colon.getTextRange().getEndOffset()) {
                     descriptors.add(new FoldingDescriptor(node,
                             new TextRange(colon.getStartOffset() + 1, node.getStartOffset() + node.getTextLength())));
                 }
                 else {
                     TextRange range = node.getTextRange();
                     if (range.getStartOffset() < range.getEndOffset()-1) { // only for ranges at leas 1 char wide
                         descriptors.add(new FoldingDescriptor(node, range));
                     }
                 }
             }
         }
 
         ASTNode child = node.getFirstChildNode();
         while (child != null) {
             appendDescriptors(child, descriptors);
             child = child.getTreeNext();
         }
     }",Buggy,"Fixed an assertion error when indentation is wrong. (This whole thing begs to be rewritten.)
",Buggy,"The commit message indicates a fix for an assertion error related to incorrect indentation. The code diff modifies a condition that checks the position of a colon relative to a node's start offset. Specifically, it changes `node.getStartOffset() + node.getTextLength()` to `colon.getTextRange().getEndOffset()`. This change likely addresses a scenario where the original condition was too lenient or incorrect, leading to the assertion error when the indentation was not as expected. The change ensures that the folding descriptor is added only when the colon is properly positioned before the indented block, thus fixing the bug. The answer is **Buggy**."
intellij-community,12571.json,df6834a4b25a87ff95c28760a761cae01c53874f,"@@ -1,25 +1,25 @@
     private static void appendDescriptors(ASTNode node,
                                           List<FoldingDescriptor> descriptors) {
         if (node.getElementType() == PyElementTypes.STATEMENT_LIST) {
             IElementType elType = node.getTreeParent().getElementType();
             if (elType == PyElementTypes.FUNCTION_DECLARATION || elType == PyElementTypes.CLASS_DECLARATION) {
                 ASTNode colon = node.getTreeParent().findChildByType(PyTokenTypes.COLON);
-                if (colon != null) {
+                if (colon != null && colon.getStartOffset() + 1 < node.getStartOffset() + node.getTextLength()) {
                     descriptors.add(new FoldingDescriptor(node,
                             new TextRange(colon.getStartOffset() + 1, node.getStartOffset() + node.getTextLength())));
                 }
                 else {
                     TextRange range = node.getTextRange();
                     if (range.getStartOffset() < range.getEndOffset()-1) { // only for ranges at leas 1 char wide
                         descriptors.add(new FoldingDescriptor(node, range));
                     }
                 }
             }
         }
 
         ASTNode child = node.getFirstChildNode();
         while (child != null) {
             appendDescriptors(child, descriptors);
             child = child.getTreeNext();
         }
     }",Buggy,"Fixed an assertion error when a function is being defined and a colon is not yet typed in.
",Buggy,"The commit message indicates a fix for an assertion error that occurs when a colon is missing during function definition. The code change adds a condition `colon.getStartOffset() + 1 < node.getStartOffset() + node.getTextLength()` to ensure that the colon's position is within the expected range before creating a folding descriptor. This check prevents the assertion error when the colon is not yet typed in, aligning with the commit message. The answer is **Buggy**."
intellij-community,679.json,5dc1df3c0d442dd294ec203a212d97f07a9fb175,"@@ -1,80 +1,80 @@
   private static void compileCythonExtension(@NotNull Project project) {
     try {
       final RunManager runManager = RunManager.getInstance(project);
       final RunnerAndConfigurationSettings selectedConfiguration = runManager.getSelectedConfiguration();
       if (selectedConfiguration == null) {
         throw new ExecutionException(""Python Run Configuration should be selected"");
       }
       final RunConfiguration configuration = selectedConfiguration.getConfiguration();
       if (!(configuration instanceof AbstractPythonRunConfiguration)) {
         throw new ExecutionException(""Python Run Configuration should be selected"");
       }
       AbstractPythonRunConfiguration runConfiguration = (AbstractPythonRunConfiguration)configuration;
-      final String sdkPath = runConfiguration.getSdkHome();
+      final String interpreterPath = runConfiguration.getInterpreterPath();
       final String helpersPath = PythonHelpersLocator.getHelpersRoot().getPath();
 
       final String cythonExtensionsDir = PyDebugRunner.CYTHON_EXTENSIONS_DIR;
       final String[] cythonArgs =
         {""build_ext"", ""--build-lib"", cythonExtensionsDir, ""--build-temp"", String.format(""%s%sbuild"", cythonExtensionsDir, File.separator)};
 
       final List<String> cmdline = new ArrayList<>();
-      cmdline.add(sdkPath);
+      cmdline.add(interpreterPath);
       cmdline.add(FileUtil.join(helpersPath, FileUtil.toSystemDependentName(SETUP_CYTHON_PATH)));
       cmdline.addAll(Arrays.asList(cythonArgs));
       LOG.info(""Compile Cython Extensions "" + StringUtil.join(cmdline, "" ""));
 
       final Map<String, String> environment = new HashMap<>(System.getenv());
       PythonEnvUtil.addToPythonPath(environment, cythonExtensionsDir);
       PythonEnvUtil.setPythonUnbuffered(environment);
       PythonEnvUtil.setPythonDontWriteBytecode(environment);
-      if (sdkPath != null) {
-        PythonEnvUtil.resetHomePathChanges(sdkPath, environment);
+      if (interpreterPath != null) {
+        PythonEnvUtil.resetHomePathChanges(interpreterPath, environment);
       }
       GeneralCommandLine commandLine = new GeneralCommandLine(cmdline).withEnvironment(environment);
 
       final boolean canCreate = FileUtil.ensureCanCreateFile(new File(helpersPath));
       final boolean useSudo = !canCreate && !SystemInfo.isWindows;
       Process process;
       if (useSudo) {
         process = ExecUtil.sudo(commandLine, ""Please enter your password to compile cython extensions: "");
       }
       else {
         process = commandLine.createProcess();
       }
 
       ProgressManager.getInstance().run(new Task.Backgroundable(project, ""Compile Cython Extensions"") {
         @Override
         public void run(@NotNull ProgressIndicator indicator) {
           final CapturingProcessHandler handler =
             new CapturingProcessHandler(process, commandLine.getCharset(), commandLine.getCommandLineString());
           handler.addProcessListener(new ProcessAdapter() {
             @Override
             public void onTextAvailable(@NotNull ProcessEvent event, @NotNull Key outputType) {
               if (outputType == ProcessOutputTypes.STDOUT || outputType == ProcessOutputTypes.STDERR) {
                 for (String line : StringUtil.splitByLines(event.getText())) {
                   if (isSignificantOutput(line)) {
                     indicator.setText2(line.trim());
                   }
                 }
               }
             }
 
             private boolean isSignificantOutput(String line) {
               return line.trim().length() > 3;
             }
           });
           final ProcessOutput result = handler.runProcessWithProgressIndicator(indicator);
           final int exitCode = result.getExitCode();
           if (exitCode != 0) {
             final String message = StringUtil.isEmptyOrSpaces(result.getStdout()) && StringUtil.isEmptyOrSpaces(result.getStderr())
                                    ? ""Permission denied""
                                    : ""Non-zero exit code ("" + exitCode + ""): \n"" + result.getStderr();
             UIUtil.invokeLaterIfNeeded(() -> showErrorDialog(project, message));
           }
         }
       });
     }
     catch (IOException | ExecutionException e) {
       showErrorDialog(project, e.getMessage());
     }
   }",Buggy,"Fixes after review (PY-26101)

Fix stepping after error jump, fix tests, move code modification to an appropriate function and make it more general
",Buggy,"The commit message ""Fixes after review (PY-26101)"" indicates that the changes are a result of a review process, and includes ""fix stepping after error jump, fix tests, move code modification to an appropriate function and make it more general"". The diff replaces `sdkPath` with `interpreterPath`, which suggests a correction in how the Python interpreter is located or used. The code also includes exception handling and UI updates to show error messages. These changes suggest that the original code had issues with error handling or incorrect path resolution, which are now being addressed. The answer is **Buggy**."
intellij-community,6419.json,39a18a8cfd9b96c783c9a408ad36fee10452c418,"@@ -1,24 +1,30 @@
-  private static String quoteItemsInFragments(@NotNull PyNewStyleStringFormatParser.Field field, char hostStringQuote) {
+  private static String quoteItemsInFragments(@NotNull PyNewStyleStringFormatParser.Field field, @NotNull StringNodeInfo hostStringInfo) {
     List<String> escaped = new ArrayList<>();
     for (String part : field.getAttributesAndLookups()) {
       if (part.startsWith(""."")) {
         escaped.add(part);
       }
       else if (part.startsWith(""["")) {
         if (part.contains(""\\"")) {
           return null;
         }
         final String indexText = part.substring(1, part.length() - 1);
         if (indexText.matches(""\\d+"")) {
           escaped.add(part);
           continue;
         }
-        final char quote = flipQuote(hostStringQuote);
-        if (indexText.indexOf(hostStringQuote) >= 0 || indexText.indexOf(quote) >= 0) {
-          return null;
+        final char originalQuote = hostStringInfo.getSingleQuote();
+        char targetQuote = flipQuote(originalQuote);
+        // there are no escapes inside the fragment, so the lookup key cannot contain 
+        // the host string quote unless it's a multiline string literal
+        if (indexText.indexOf(targetQuote) >= 0) {
+          if (!hostStringInfo.isTripleQuoted() || indexText.indexOf(originalQuote) >= 0) {
+            return null;
+          }
+          targetQuote = originalQuote;
         }
-        escaped.add(""["" + quote + indexText + quote + ""]"");
+        escaped.add(""["" + targetQuote + indexText + targetQuote + ""]"");
       }
     }
     return StringUtil.join(escaped, """");
   }",Buggy,"PY-21245 Handle quotes in lookup items inside multiline string literals better

Namely, allow lookup items that contain quotes in some cases if the
containing string literal is multiline, hence there is still a suitable
type of quotes to wrap injected string literal into.

The orignal error described in the issue has been fixed earlier as
part of other changes for PY-21161.
",Buggy,"The commit message indicates a fix for handling quotes in lookup items within multiline string literals. The code changes reflect this by:

1.  Replacing `hostStringQuote` (a single character) with `hostStringInfo` (an object containing information about the host string, including whether it's triple-quoted).
2.  Introducing logic to determine the appropriate quote character to use for the lookup item based on whether the host string is multiline (triple-quoted) and whether the lookup item contains the host string's quote character.
3.  Adding a check to see if the index text contains the flipped quote. If it does, and the host string is not triple quoted, or the index text contains the original quote, then the method returns null.
4.  Using the determined `targetQuote` to wrap the index text.

These changes address the bug described in the commit message, where quotes within lookup items were not being handled correctly in multiline string literals. The code now correctly determines the appropriate quote character to use, ensuring that the lookup item is properly escaped.

The answer is **Buggy**.
"
intellij-community,3353.json,234eea4ef406b48f114d35e7385b2ebfeece9e6d,"@@ -1,10 +1,3 @@
-  private static AndroidFacet getFacet(@NotNull Module module) {
-    AndroidFacet androidFacet = null;
-
-    for (Facet facet : module.getFacets().values()) {
-      if (facet instanceof AndroidFacet) {
-        androidFacet = (AndroidFacet)facet;
-      }
-    }
-    return androidFacet;
-  }

+    public AndroidFacet getFacet() {
+      return myFacet;
+    }",Buggy,"android-jps: fix some bugs, support aapt and dex compilation
",Buggy,"The commit message indicates bug fixes and support for aapt and dex compilation. The code diff removes the original `getFacet` method, which iterated through all facets of a module to find an `AndroidFacet`, and replaces it with a simple getter method that returns `myFacet`. This change suggests that the original method might have had issues, such as returning null or an incorrect facet under certain conditions, which would constitute a bug. The new method directly accesses the `myFacet` field, implying that the facet is now stored directly within the class, likely during initialization. This is a more direct and reliable way to access the facet, suggesting a bug fix. The answer is **Buggy**."
intellij-community,23124.json,295d3d1dd1470e62a6748a0baeded3ad4ca1d508,"@@ -1,13 +1,14 @@
-  public void beginObject() {
+  public JsonReaderEx beginObject() {
     int p = peeked;
     if (p == PEEKED_NONE) {
       p = doPeek();
     }
     if (p == PEEKED_BEGIN_OBJECT) {
       push(JsonScope.EMPTY_OBJECT);
       peeked = PEEKED_NONE;
     }
     else {
       throw createParseError(""Expected BEGIN_OBJECT but was "" + peek());
     }
+    return this;
   }",Buggy,"cleanup, fix WEB-14597 Breakpoints being ignored if functions called from live console

AsynPromise  don't ignore error if no rejected handler
",Buggy,"The commit message indicates a fix for a breakpoint issue and mentions ""don't ignore error if no rejected handler"". The diff modifies the `beginObject()` method in `JsonReaderEx` to return `this`. While the change itself doesn't scream ""bug fix"" in the traditional sense (no explicit error handling or logic correction), the commit message suggests that this change is related to a broader fix where errors were being ignored. The return of `this` is likely part of a chain of calls that were previously broken due to the ignored error. Therefore, this change is likely part of a bug fix. The answer is **Buggy**.
"
intellij-community,35717.json,ac323a030f4b82bc3a41f4bb970f770de5a10462,"@@ -1,10 +1,10 @@
     private void updateFile(final VirtualFile file) {
-      myQueue.queue(new Update(""ProblemUpdate"") {
+      myQueue.queue(new Update(file) {
         public void run() {
           if (isFileOpen(file)) {
             updateFileIcon(file);
             updateFileColor(file);
           }
         }
       });
     }",Buggy,"fixed bug with file staying red underwaved in editor tab
",Buggy,"The commit message indicates a bug fix related to a file's red-underwaved appearance in the editor tab. The diff changes the `Update` constructor argument from a generic string ""ProblemUpdate"" to the `VirtualFile` object itself. This suggests that the previous update mechanism might not have been correctly identifying or associating updates with specific files, potentially leading to the red-underwaved status not being cleared or updated properly. By using the `VirtualFile` as the ID for the update, the code ensures that updates are correctly associated with the file, which likely resolves the bug. The answer is **Buggy**.
"
intellij-community,26650.json,aca5c269d7a12badc8892d6a204c4496583cd882,"@@ -1,4 +1,4 @@
   protected void buildPayload(Payload payload) {
     super.buildPayload(payload);
-    payload.add(myThreadId).add(myFrameId).add(""FRAME"").add(myExpression);
+    payload.add(""FRAME"").add(myExpression);
   }",Buggy,"fixed bug after refactoring
",Buggy,"The commit message indicates a bug fix that occurred after refactoring. The diff removes `myThreadId` and `myFrameId` from the payload. This suggests that these values were either incorrect or unnecessary after the refactoring, indicating a bug in how the payload was being constructed. The answer is **Buggy**.
"
intellij-community,15806.json,1a1325592a39001a011e5c0ece3664a582fc1fff,"@@ -1,9 +1,12 @@
-  public EditorWithProviderComposite findFileComposite(final VirtualFile file) {
+  public EditorWithProviderComposite findFileComposite(VirtualFile file) {
+    if (file instanceof BackedVirtualFile)
+      file = ((BackedVirtualFile)file).getOriginFile();
+
     for (int i = 0; i != getTabCount(); ++i) {
       final EditorWithProviderComposite editor = getEditorAt(i);
       if (editor.getFile().equals(file)) {
         return editor;
       }
     }
     return null;
   }",Buggy,"Add check of BackedVirtualFile in findFileComposite in EditorWindow.java
to fix problem with openedEditorWindow resolving.

Now if you try go to Usage of variables in Jupyter, Zeppelin Notebooks (BackedVirtualFile)
new window will be opened. We need to get original file for correct search of File Composite

GitOrigin-RevId: 5a3577cb3964fc0a7fa0c7fa91a4fed137fe2ebf",Buggy,"The commit message indicates a fix for a problem with resolving opened editor windows, specifically related to `BackedVirtualFile` in Jupyter and Zeppelin notebooks. The diff adds a check to see if the input `file` is an instance of `BackedVirtualFile`. If it is, it retrieves the original file using `getOriginFile()` and uses that for further processing. This change addresses the issue described in the commit message, where the original file was needed for correct search of `FileComposite`. The answer is **Buggy**."
intellij-community,6447.json,f96c4ad9fa786eb5990bebe12f45dbbe4f698642,"@@ -1,16 +1,17 @@
   public void beforeCharDeleted(char c, PsiFile file, Editor editor) {
     isTripleQuote = false;
     if (c == '""' || c == '\'' && CodeInsightSettings.getInstance().AUTOINSERT_PAIR_QUOTE) {
       final QuoteHandler quoteHandler = TypedHandler.getQuoteHandler(file, editor);
       if (quoteHandler == null || !(quoteHandler instanceof BaseQuoteHandler)) return;
 
       final int offset = editor.getCaretModel().getCurrentCaret().getOffset();
       String text = editor.getDocument().getText();
       boolean mayBeTripleQuote = offset >= 3 && offset + 2 < text.length();
       if (mayBeTripleQuote) {
         HighlighterIterator iterator = ((EditorEx)editor).getHighlighter().createIterator(offset);
-        boolean hasTripleQuoteAfter = text.charAt(offset - 1) == c && text.charAt(offset - 2) == c && text.charAt(offset - 3) == c;
+        boolean hasTripleQuoteAfter = offset + 2 < text.length() &&
+                                      text.charAt(offset) == c && text.charAt(offset + 1) == c && text.charAt(offset + 2) == c;
         isTripleQuote = quoteHandler.isOpeningQuote(iterator, offset - 1) && hasTripleQuoteAfter;
       }
     }
   }",Buggy,"PY-1779 Fix problem with removing pair of triple quotes

Check if there is triple quote to the right of the cursor
",Buggy,"The commit message indicates a fix for removing pairs of triple quotes. The code diff modifies the `beforeCharDeleted` method to correctly identify and handle triple quotes when a character is deleted. Specifically, the change adds a check to ensure that there are actually three quotes to the *right* of the cursor before considering it a triple quote. This fixes a potential issue where the code might incorrectly identify a triple quote if there aren't enough characters available after the cursor. The original code only checked for triple quotes to the left. The answer is **Buggy**."
intellij-community,11190.json,bf7f424b6599b6b662a69c63d5028263dcbee88b,"@@ -1,10 +1,10 @@
   public static String constructPythonPathCommand(Collection<String> pythonPath) {
     final String path = Joiner.on("", "").join(Collections2.transform(pythonPath, new Function<String, String>() {
       @Override
       public String apply(String input) {
-        return ""'"" + input.replace(""\\"", ""\\\\"") + ""'"";
+        return ""'"" + input.replace(""\\"", ""\\\\"").replace(""'"", ""\\'"") + ""'"";
       }
     }));
 
     return ""sys.path.extend(["" + path + ""])"";
   }",Buggy,"fix problem with single quote in project path
http://youtrack.jetbrains.com/issue/PY-10179
",Buggy,"The commit message indicates a fix for a problem with single quotes in the project path. The diff shows that the code now escapes single quotes within the project path strings by replacing them with `\'`. This prevents single quotes in the path from prematurely terminating the string literal in the generated Python code, which would cause errors. Therefore, this change addresses a bug related to handling special characters in project paths. The answer is **Buggy**.
"
intellij-community,7728.json,9421b80596d7526f1637a84fb4bf457977dd635e,"@@ -1,16 +1,16 @@
     public boolean accept(File file) {
       final JavaSourceRootDescriptor rd = myBuildRootIndex.findJavaRootDescriptor(myContext, file);
       if (rd == null) {
         return true;
       }
-      final JpsModule moduleOfFile = rd.target.getModule();
-      if (myChunkModules.contains(moduleOfFile)) {
+      final ModuleBuildTarget targetOfFile = rd.target;
+      if (myChunkTargets.contains(targetOfFile)) {
         return true;
       }
-      Set<JpsModule> moduleOfFileWithDependencies = myCache.get(moduleOfFile);
-      if (moduleOfFileWithDependencies == null) {
-        moduleOfFileWithDependencies = ProjectPaths.getModulesWithDependentsRecursively(moduleOfFile, true);
-        myCache.put(moduleOfFile, moduleOfFileWithDependencies);
+      Set<BuildTarget<?>> targetOfFileWithDependencies = myCache.get(targetOfFile);
+      if (targetOfFileWithDependencies == null) {
+        targetOfFileWithDependencies = myBuildTargetIndex.getDependenciesRecursively(targetOfFile, myContext);
+        myCache.put(targetOfFile, targetOfFileWithDependencies);
       }
-      return Utils.intersects(moduleOfFileWithDependencies, myChunkModules);
+      return ContainerUtil.intersects(targetOfFileWithDependencies, myChunkTargets);
     }",Buggy,"external build: fixed problem with files remaining dirty after 'Make Project'
",Buggy,"The commit message indicates a fix for a problem where files remained dirty after a 'Make Project' operation. The diff shows changes in how the code determines whether a file should be processed during the build. Specifically, it replaces `JpsModule` with `ModuleBuildTarget` and updates the logic for identifying modules/targets with dependencies. This suggests a bug fix related to incorrect dependency analysis or module identification, which caused files to be incorrectly marked as dirty. The change from `ProjectPaths.getModulesWithDependentsRecursively` to `myBuildTargetIndex.getDependenciesRecursively` further supports this, as it indicates a shift to a more accurate or comprehensive dependency resolution mechanism. The answer is **Buggy**."
intellij-community,10628.json,cfedbcc1be3a6aec0e05d57995ce6945e112168c,"@@ -1,15 +1,21 @@
   protected JPanel createAdvancedSettings() {
     JComponent advancedSettings = null;
     if (myProjectGenerator instanceof PythonProjectGenerator)
       advancedSettings = ((PythonProjectGenerator)myProjectGenerator).getSettingsPanel(myProjectDirectory);
     else if (myProjectGenerator instanceof WebProjectTemplate) {
       advancedSettings = ((WebProjectTemplate)myProjectGenerator).getPeer().getComponent();
     }
     if (advancedSettings != null) {
       final JPanel jPanel = new JPanel(new VerticalFlowLayout());
       final HideableDecorator deco = new HideableDecorator(jPanel, ""Mor&e Settings"", false);
+      boolean isValid = checkValid();
+      deco.setOn(!isValid);
+      if (myProjectGenerator instanceof PythonProjectGenerator && !deco.isExpanded()) {
+        final ValidationResult result = ((PythonProjectGenerator)myProjectGenerator).warningValidation(getSdk());
+        deco.setOn(!result.isOk());
+      }
       deco.setContentComponent(advancedSettings);
       return jPanel;
     }
     return null;
   }",Buggy,"fixed PY-13503 Create New Project: Expand more setting by default if there is an error in configuration
",Buggy,"The commit message indicates a fix for a bug (PY-13503) related to the ""Create New Project"" functionality. Specifically, it mentions that the ""More Settings"" section should be expanded by default if there's an error in the configuration.

The diff shows changes within the `createAdvancedSettings` method. A `HideableDecorator` is used to encapsulate advanced settings. The key change is the addition of the following lines:

```java
boolean isValid = checkValid();
deco.setOn(!isValid);
if (myProjectGenerator instanceof PythonProjectGenerator && !deco.isExpanded()) {
  final ValidationResult result = ((PythonProjectGenerator)myProjectGenerator).warningValidation(getSdk());
  deco.setOn(!result.isOk());
}
```

This code checks if the current configuration is valid using `checkValid()`. If it's not valid (`!isValid`), the `deco.setOn(!isValid)` line expands the ""More Settings"" section. Additionally, for Python projects, it performs a warning validation and expands the settings if the validation result is not OK. This ensures that users are presented with the advanced settings when there are configuration issues, aligning with the commit message's intention to expand the settings on error.

The changes directly address the bug described in the commit message, ensuring the ""More Settings"" section expands when there are configuration errors.

The answer is **Buggy**.
"
intellij-community,35455.json,04eb55fb65bb931432ac5b51786825882d53027a,"@@ -1,21 +1,26 @@
     private boolean runNextEvent() {
       long startedAt = System.currentTimeMillis();
       final RunnableInfo lastInfo = getNextEvent(true);
       myLastInfo = lastInfo;
 
       if (lastInfo != null) {
         try {
           doRun(lastInfo);
           lastInfo.markDone();
         }
-        catch (ProcessCanceledException ignored) { }
+        catch (ProcessCanceledException ignored) {
+
+        }
         catch (Throwable t) {
+          if (ApplicationManager.getApplication().isUnitTestMode()) {
+            ExceptionUtil.rethrow(t);
+          }
           LOG.error(t);
         }
         finally {
           if (!DEBUG) myLastInfo = null;
           TransactionGuardImpl.logTimeMillis(startedAt, lastInfo.runnable);
         }
       }
       return lastInfo != null;
     }",Buggy,"make exceptions thrown in invokeLater() and friends visible in tests to help fix errors in abandoned futures

GitOrigin-RevId: aec5c2221220e3567f67c9a35d050a61f06a3a2e",Buggy,"The commit message indicates a change to make exceptions thrown in `invokeLater()` and related methods visible in tests, aiming to help fix errors in abandoned futures. The diff shows that in the `runNextEvent()` method, a `try-catch` block is modified. Specifically, a new condition is added within the `catch(Throwable t)` block: `if (ApplicationManager.getApplication().isUnitTestMode()) { ExceptionUtil.rethrow(t); }`. This means that if the application is running in unit test mode, any caught `Throwable` will be re-thrown, making it visible to the test. This change directly addresses the commit message's goal of making exceptions visible in tests to aid in debugging. This is a bug fix because it exposes previously hidden exceptions during testing, which can help identify and resolve underlying issues. The answer is **Buggy**."
intellij-community,18837.json,7e6fe7d6c4d89fc902a471879c0244f8b19597db,"@@ -1,6 +1,8 @@
   public boolean isDumb() {
-    if (!ApplicationManager.getApplication().isReadAccessAllowed() && REPORTED_EXECUTIONS.add(ExceptionUtil.currentStackTrace())) {
+    if (!ApplicationManager.getApplication().isReadAccessAllowed() &&
+        Registry.is(""ide.check.is.dumb.contract"") &&
+        REPORTED_EXECUTIONS.add(ExceptionUtil.currentStackTrace())) {
       LOG.error(""To avoid race conditions isDumb method should be used only under read action or in EDT thread."");
     }
     return myState.get() != State.SMART;
   }",Buggy,"IDEA-CR-47405: IDEA-214525 Disable isDumb check by default

Need to fix found frequent problems and switch on it back

GitOrigin-RevId: 2a916fbfe43bb077042adf0e0515253c3aa7ce55
",Buggy,"The commit message indicates a fix for frequent problems related to the `isDumb` check. The diff introduces a registry setting (`ide.check.is.dumb.contract`) to control whether the check is enabled. This suggests that the original implementation of `isDumb` was causing issues, and the change allows disabling it to mitigate those problems. This is a bug fix. The answer is **Buggy**.
"
intellij-community,2087.json,90a381b2f133397b7cbbfac2a4966e528b3e91cf,"@@ -1,10 +1,10 @@
   public boolean isAsyncAllowed() {
     final LanguageLevel languageLevel = LanguageLevel.forElement(this);
     final String functionName = getName();
 
     return languageLevel.isAtLeast(LanguageLevel.PYTHON35) && (
       functionName == null ||
-      ArrayUtil.contains(functionName, PyNames.AITER, PyNames.ANEXT, PyNames.AENTER, PyNames.AEXIT) ||
+      ArrayUtil.contains(functionName, PyNames.AITER, PyNames.ANEXT, PyNames.AENTER, PyNames.AEXIT, PyNames.CALL) ||
       !PyNames.getBuiltinMethods(languageLevel).containsKey(functionName)
     );
   }",Buggy,"PY-20662 Fixed: Making any magic method async displays an error, even though it is sometimes legal

Mark __call__ builtin as allowed to be async
",Buggy,"The commit message indicates a fix for a false error being displayed when making certain magic methods async. The code change adds `PyNames.CALL` to the list of allowed async methods. This directly addresses the issue described in the commit message, indicating a bug fix. The answer is **Buggy**."
intellij-community,10885.json,2b57b19667f12b0967107bbb6dd4052825fed484,"@@ -1,4 +1,7 @@
   public String getValue() {
+    if (myListPopUp == null || !myListPopUp.isVisible()) {
+      return null; // Nothing is selected if list is invisible
+    }
     final Object value = myList.getSelectedValue();
     return ((value == null) ? """" : getElement(value).mySuggestion.getText());
   }",Buggy,"PY-11855 Run manage.py task improvements

Tests enabled, completion bug fixed
",Buggy,"The commit message indicates a bug fix related to completion. The code diff adds a check to ensure the `myListPopUp` is both not null and visible before attempting to retrieve the selected value. If the popup is null or not visible, it returns null, indicating that nothing is selected. This prevents a potential `NullPointerException` or incorrect value being returned when the popup is not properly initialized or visible, which aligns with the ""completion bug fixed"" part of the commit message. The answer is **Buggy**."
intellij-community,38709.json,1e53ffa3e0e7fe08387b61f9aba2b48275685b41,"@@ -1,6 +1,8 @@
   public void setCancelButtonText(String text){
-    if (myDialog != null)
+    if (myDialog != null) {
       myDialog.changeCancelButtonText(text);
-    else
+    }
+    else {
       myCancelText = text;
+    }
   }",Buggy,"Fixing problem of progress window appearing over modal dialogs shown during runProcessWithProgressSynchronously().
",Buggy,"The commit message indicates a fix related to the appearance of a progress window over modal dialogs. The code change introduces an `else` block to handle the case where `myDialog` is null, assigning the `text` to `myCancelText`. This suggests that the original code might have been missing a condition to set the cancel button text when the dialog was not yet initialized, potentially leading to incorrect behavior or a visual bug. The addition of the `else` block addresses this issue, indicating a bug fix. The answer is **Buggy**.
"
intellij-community,18404.json,87fa91dd700f6b1cfc0885f0857cf501df46649f,"@@ -1,6 +1,7 @@
     private JComponent createErrorsLink() {
       IdeMessagePanel panel = new IdeMessagePanel(null, MessagePool.getInstance());
       panel.setBorder(JBUI.Borders.emptyRight(13));
+      panel.setOpaque(false);
       Disposer.register(this, panel);
       return panel;
     }",Buggy,"Fix error icon background on welcome screen

GitOrigin-RevId: 6412c1adb4627ebdd5895f3319a3878f7a68a44d
",Buggy,"The commit message indicates a fix to the error icon background on the welcome screen. The diff shows that `panel.setOpaque(false)` is added to the `createErrorsLink` method. This change likely makes the background of the error message panel transparent, addressing the issue described in the commit message. The answer is **Buggy**."
intellij-community,17737.json,29ea6cca1f95c474adfc2715fe7d091e26d043d7,"@@ -1,104 +1,109 @@
     private void processModules(final DiffState state, File fileName) {
       final Difference.Specifier<ModuleRepr, ModuleRepr.Diff> modulesDiff = state.myModulesDiff;
       if (modulesDiff.unchanged()) {
         return;
       }
+
+      for (ModuleRepr moduleRepr : modulesDiff.added()) {
+        myDelta.addChangedClass(moduleRepr.name); // need this for integrate
+      }
+      
       for (ModuleRepr removedModule : modulesDiff.removed()) {
-        myDelta.addDeletedClass(removedModule, fileName);
+        myDelta.addDeletedClass(removedModule, fileName); // need this for integrate
         myPresent.affectDependentModules(state, removedModule.name, null, true);
       }
 
       for (Pair<ModuleRepr, ModuleRepr.Diff> pair : modulesDiff.changed()) {
         final ModuleRepr moduleRepr = pair.first;
         final ModuleRepr.Diff d = pair.second;
         boolean affectSelf = false;
         boolean affectDeps = false;
         UsageConstraint constraint = null;
 
-        myDelta.addChangedClass(moduleRepr.name);
+        myDelta.addChangedClass(moduleRepr.name); // need this for integrate
 
         if (d.versionChanged()) {
           final int version = moduleRepr.getVersion();
           myPresent.affectDependentModules(state, moduleRepr.name, new UsageConstraint() {
             public boolean checkResidence(int dep) {
               final ModuleRepr depModule = myPresent.moduleReprByName(dep);
               if (depModule != null) {
                 for (ModuleRequiresRepr requires : depModule.getRequires()) {
                   if (requires.name == moduleRepr.name && requires.getVersion() == version) {
                     return true;
                   }
                 }
               }
               return false;
             }
           }, false);
         }
 
         final Difference.Specifier<ModuleRequiresRepr, ModuleRequiresRepr.Diff> requiresDiff = d.requires();
         for (ModuleRequiresRepr removed : requiresDiff.removed()) {
           affectSelf = true;
           if (removed.isTransitive()) {
             affectDeps = true;
             constraint = UsageConstraint.ANY;
             break;
           }
         }
         for (Pair<ModuleRequiresRepr, ModuleRequiresRepr.Diff> changed : requiresDiff.changed()) {
           affectSelf |= changed.second.versionChanged();
           if (changed.second.becameNonTransitive()) {
             affectDeps = true;
             // we could have created more precise constraint here: analyze if required module (recursively)
             // has only qualified exports that include given module's name. But this seems to be excessive since
             // in most cases module's exports are unqualified, so that any other module can access the exported API.
             constraint = UsageConstraint.ANY;
           }
         }
 
         final Difference.Specifier<ModulePackageRepr, ModulePackageRepr.Diff> exportsDiff = d.exports();
 
         if (!affectDeps) {
           for (ModulePackageRepr removedPackage : exportsDiff.removed()) {
             affectDeps = true;
             if (!removedPackage.isQualified()) {
               constraint = UsageConstraint.ANY;
               break;
             }
             for (Integer name : removedPackage.getModuleNames()) {
               final UsageConstraint matchName = UsageConstraint.exactMatch(name);
               if (constraint == null) {
                 constraint = matchName;
               }
               else {
                 constraint = constraint.or(matchName);
               }
             }
           }
         }
 
         if (!affectDeps || constraint != UsageConstraint.ANY) {
           for (Pair<ModulePackageRepr, ModulePackageRepr.Diff> p : exportsDiff.changed()) {
             final Collection<Integer> removedModuleNames = p.second.targetModules().removed();
             affectDeps |= !removedModuleNames.isEmpty();
             if (!removedModuleNames.isEmpty()) {
               affectDeps = true;
               for (Integer name : removedModuleNames) {
                 final UsageConstraint matchName = UsageConstraint.exactMatch(name);
                 if (constraint == null) {
                   constraint = matchName;
                 }
                 else {
                   constraint = constraint.or(matchName);
                 }
               }
             }
           }
         }
 
         if (affectSelf) {
           myPresent.affectModule(moduleRepr, myAffectedFiles);
         }
         if (affectDeps) {
           myPresent.affectDependentModules(state, moduleRepr.name, constraint, true);
         }
       }
     }",Buggy,"fix module-info integrate after errors (IDEA-181208)
",Buggy,"The commit message ""fix module-info integrate after errors (IDEA-181208)"" indicates a bug fix related to module-info integration, likely addressing errors encountered during the process. The diff shows modifications within the `processModules` method, specifically adding `myDelta.addChangedClass(moduleRepr.name)` for added modules. The comments ""// need this for integrate"" strongly suggest that these changes are directly related to fixing the integration issue mentioned in the commit message. The addition of this line ensures that changed classes are correctly processed during module integration, which was previously not happening, leading to errors. The answer is **Buggy**."
intellij-community,29881.json,89451925c29655bb8771859c792c18d6b28c9e55,"@@ -1,19 +1,19 @@
     public void visitPyFunction(final PyFunction node) {
       PyDecoratorList decolist = node.getDecoratorList();
       if (decolist != null) {
         PyDecorator[] decos = decolist.getDecorators();
         if (decos.length > 1) {
           for (int i = decos.length - 1; i >= 1; i -= 1) {
             PyDecorator deco = decos[i];
             String deconame = deco.getName();
             if ((PyNames.CLASSMETHOD.equals(deconame) || PyNames.STATICMETHOD.equals(deconame)) && deco.isBuiltin()) {
               registerProblem(
                 decos[i-1],
                 PyBundle.message(""INSP.decorator.receives.unexpected.builtin""),
-                ProblemHighlightType.GENERIC_ERROR_OR_WARNING
+                ProblemHighlightType.GENERIC_ERROR_OR_WARNING, null, new RemoveDecoratorQuickFix()
               );
             }
           }
         }
       }
     }",Buggy,"added Remove decorator quick fix to the Problematic nesting of decorators inspection
",NotBuggy,"The commit message indicates the addition of a quick fix for a specific inspection related to decorator nesting. The diff shows that a `RemoveDecoratorQuickFix` is being added to the `registerProblem` call within the `visitPyFunction` method, specifically when a classmethod or staticmethod decorator is encountered in a problematic nesting scenario. This addition directly addresses the issue highlighted by the inspection, providing a way to automatically remove the problematic decorator. This is not a bug fix, but a feature enhancement. The answer is **NotBuggy**.
"
intellij-community,27903.json,0b9404bccbc274086537d9c49bf8a31a26ba4d7a,"@@ -1,9 +1,11 @@
   private static boolean distinctTokens(@Nullable IElementType token1, @Nullable IElementType token2) {
     if (token1 == token2) return false;
     if (token1 == null || token2 == null) return true;
     if (StringEscapesTokenTypes.STRING_LITERAL_ESCAPES.contains(token1) ||
         StringEscapesTokenTypes.STRING_LITERAL_ESCAPES.contains(token2)) return false;
-    if (!token1.getLanguage().is(token2.getLanguage())) return true;
-    BidiRegionsSeparator separator = LanguageBidiRegionsSeparator.INSTANCE.forLanguage(token1.getLanguage());
+    if (token1 != TokenType.WHITE_SPACE && token2 != TokenType.WHITE_SPACE && !token1.getLanguage().is(token2.getLanguage())) return true;
+    Language language = token1.getLanguage();
+    if (language == Language.ANY) language = token2.getLanguage();
+    BidiRegionsSeparator separator = LanguageBidiRegionsSeparator.INSTANCE.forLanguage(language);
     return separator.createBorderBetweenTokens(token1, token2);
   }",Buggy,"IDEA-169157 Right-to-left bug - fix for JSX files
",Buggy,"The commit message indicates a fix for a right-to-left bug specifically in JSX files. The code changes in `distinctTokens` method modify the logic for determining token boundaries, especially concerning whitespace and language differences. The original code directly checked language equality, but the modified version adds a condition to exclude whitespace tokens from this check. Additionally, it handles cases where one of the tokens has `Language.ANY`, defaulting to the other token's language. These changes likely address issues where the bidirectional algorithm was incorrectly splitting or merging regions in JSX files, leading to the reported right-to-left bug. The answer is **Buggy**.
"
intellij-community,35807.json,6555125d8e28088da575e0b10ca50780c50642d0,"@@ -1,22 +1,23 @@
   public static ScriptOutput executeScriptInConsoleWithFullOutput(String exePathString,
                                                                   @Nullable VirtualFile scriptFile,
                                                                   @Nullable String workingDirectory,
                                                                   long timeout,
                                                                   ScriptOutputType scriptOutputType,
                                                                   @NonNls String... parameters)
     throws ExecutionException {
     final OSProcessHandler processHandler = execute(exePathString, workingDirectory, scriptFile, parameters);
 
     final StringBuilder standardOutput = scriptOutputType.readStandardOutput() ? new StringBuilder() : null;
     final StringBuilder errorOutput = scriptOutputType.readErrorOutput() ? new StringBuilder() : null;
     final StringBuilder mergedOutput =
       (scriptOutputType.readStandardOutput() && scriptOutputType.readErrorOutput()) ? new StringBuilder() : null;
     addReadingProcessListener(scriptOutputType, processHandler, standardOutput, errorOutput, mergedOutput);
+    processHandler.startNotify();
 
     if (!processHandler.waitFor(timeout)) {
       LOG.warn(""Process did not complete in "" + timeout / 1000 + ""s"");
       throw new ExecutionException(ExecutionBundle.message(""script.execution.timeout"", String.valueOf(timeout / 1000)));
     }
     LOG.debug(""script output: "" + standardOutput);
     return new ScriptOutput(scriptOutputType, standardOutput, errorOutput, mergedOutput);
   }",Buggy,"WI-6566 Empty output in command line toolwindow:
fixed bug with too early processHandler.startNotify() in ScriptRunnerUtil; removed command execution ""cmd \c"" wrapping.
",Buggy,"The commit message indicates a fix for empty output in the command line tool window, attributing it to an early `processHandler.startNotify()` call. The diff moves `processHandler.startNotify()` from after adding the process listener to before it. This change likely ensures that the process handler is fully initialized and ready to capture output before the script execution begins, preventing the loss of early output. The answer is **Buggy**."
intellij-community,18229.json,6ee5fbcfe50c4e017eff321c5ebe208dd89d6945,"@@ -1,9 +1,10 @@
   public ExternalSystemTaskExecutionSettings clone() {
     ExternalSystemTaskExecutionSettings result = new ExternalSystemTaskExecutionSettings();
     result.setExternalSystemIdString(getExternalSystemIdString());
     result.setExternalProjectPath(getExternalProjectPath());
     result.setVmOptions(getVmOptions());
+    result.setScriptParameters(getScriptParameters());
     result.setTaskNames(ContainerUtilRt.newArrayList(getTaskNames()));
     result.setTaskDescriptions(ContainerUtilRt.newArrayList(getTaskDescriptions()));
     return result;
   }",Buggy,"IDEA-117792 Gradle Run/Debug Configuration: support adding Gradle specific parameters. Fix after review.
+ fix bug - http://youtrack.jetbrains.com/issue/IDEA-118470
",Buggy,"The commit message indicates a bug fix related to Gradle Run/Debug Configuration, specifically mentioning issue IDEA-118470. The code change adds `result.setScriptParameters(getScriptParameters());` to the `clone()` method of `ExternalSystemTaskExecutionSettings`. This ensures that script parameters are properly copied when cloning the settings. Without this fix, the script parameters would not be persisted in the cloned object, leading to incorrect behavior. The answer is **Buggy**.
"
hector,2566.json,4e038b0c1c911e81c2ce68123f3ecdb588c0e090,"@@ -1,3 +1,3 @@
   public K getKey() {
-    return keySerializer.fromByteBuffer(entry.getKey()); 
+    return keySerializer.fromByteBuffer(entry.getKey().duplicate());
   }",Buggy,"#345: fix bug with multiple bytebuffer readings
",Buggy,"The commit message indicates a bug fix related to multiple byte buffer readings. The diff shows a change in the `getKey()` method where `entry.getKey().duplicate()` is used instead of `entry.getKey()`. This suggests that the original code might have been modifying the underlying byte buffer's position or limit, leading to incorrect readings when the buffer was accessed multiple times. By using `duplicate()`, a new buffer is created that shares the same underlying data but has its own independent position, limit, and mark. This prevents modifications in one buffer from affecting others, thus fixing the bug related to multiple byte buffer readings. The answer is **Buggy**."
hector,1056.json,77b7e5e1215b64a40f87488849cead5aa67e6ac4,"@@ -1,27 +1,30 @@
   public static void main(String[] args) throws HectorException {
     CassandraClientPool pool = CassandraClientPoolFactory.INSTANCE.get();
     CassandraClient client = pool.borrowClient(""tush"", 9160);
     // A load balanced version would look like this:
     // CassandraClient client = pool.borrowClient(new String[] {""cas1:9160"", ""cas2:9160"", ""cas3:9160""});
 
+    Keyspace keyspace = null;
     try {
-      Keyspace keyspace = client.getKeyspace(""Keyspace1"");
+      keyspace = client.getKeyspace(""Keyspace1"");
       ColumnPath columnPath = new ColumnPath(""Standard1"");
       columnPath.setColumn(bytes(""column-name""));
 
       // insert
       keyspace.insert(""key"", columnPath, bytes(""value""));
 
       // read
       Column col = keyspace.getColumn(""key"", columnPath);
 
       System.out.println(""Read from cassandra: "" + string(col.getValue()));
 
+    } finally {
       // This line makes sure that even if the client had failures and recovered, a correct
       // releaseClient is called, on the up to date client.
-      client = keyspace.getClient();
-    } finally {
-      // return client to pool. do it in a finally block to make sure it's executed
-      pool.releaseClient(client);
+      if (keyspace != null) {
+        client = keyspace.getClient();
+        // return client to pool. do it in a finally block to make sure it's executed
+        pool.releaseClient(client);
+      }
     }
   }",Buggy,"Fix a bug when releasing a client in the ExampleClient
",Buggy,"The commit message indicates a bug fix related to releasing a client in the ExampleClient. The code changes involve ensuring the client is properly released back to the pool, even in cases where failures and recoveries might have occurred. The original code assigned the client from keyspace.getClient() in the try block, but the finally block always released the original client, potentially leading to issues if the client had been replaced due to recovery. The corrected code now checks if the keyspace is not null before attempting to release the client, and only then retrieves the updated client from the keyspace and releases it. This prevents releasing a null client or an outdated client, which could cause connection leaks or other problems. The answer is **Buggy**."
hector,1056.json,7e7fd435503f29b0bfbc6ec8a997e28ecf323a22,"@@ -1,25 +1,28 @@
   public static void main(String[] args) throws IllegalStateException, PoolExhaustedException,
       Exception {
     CassandraClientPool pool = CassandraClientPoolFactory.INSTANCE.get();
     CassandraClient client = pool.borrowClient(""tush"", 9160);
     // A load balanced version would look like this:
     // CassandraClient client = pool.borrowClient(new String[] {""cas1:9160"", ""cas2:9160"", ""cas3:9160""});
 
     try {
       Keyspace keyspace = client.getKeyspace(""Keyspace1"");
       ColumnPath columnPath = new ColumnPath(""Standard1"");
       columnPath.setColumn(bytes(""column-name""));
 
       // insert
       keyspace.insert(""key"", columnPath, bytes(""value""));
 
       // read
       Column col = keyspace.getColumn(""key"", columnPath);
 
       System.out.println(""Read from cassandra: "" + string(col.getValue()));
 
+      // This line makes sure that even if the client had failures and recovered, a correct
+      // releaseClient is called, on the up to date client.
+      client = keyspace.getClient();
     } finally {
       // return client to pool. do it in a finally block to make sure it's executed
       pool.releaseClient(client);
     }
   }",Buggy,"Fix ExampleClient to release the correct client even when there's possible errors and failover
(cherry picked from commit 184b6460812c21e7fde77821245848f9ed2f3e6e)
",Buggy,"The commit message indicates a fix in ExampleClient to ensure the correct client is released back to the pool, even in error or failover scenarios. The code adds `client = keyspace.getClient();` inside the `try` block. This line updates the `client` reference to the most current client instance managed by the `keyspace`, especially after potential failover or recovery events. This ensures that the `releaseClient` call in the `finally` block releases the correct, up-to-date client instance, preventing resource leaks or incorrect client management. The answer is **Buggy**."
hector,1991.json,4be28e7d287b02b6f1036b415c5e732ec804065f,"@@ -1,14 +1,17 @@
   private String getContextPath() {
-    URL url = getClass().getClassLoader().getResource(""/"");
+    ClassLoader loader = getClass().getClassLoader();
+    if(loader == null)
+     return null;
+    URL url = loader.getResource(""/"");
     if (url != null) {
       String[] elements = url.toString().split(""/"");
       for (int i = elements.length - 1; i > 0; --i) {
         // URLs look like this: file:/.../ImageServer/WEB-INF/classes/
         // And we want that part that's just before WEB-INF
         if (""WEB-INF"".equals(elements[i])) {
           return elements[i - 1];
         }
       }
     }
     return null;
   }",Buggy,"fixed a bug which causes NullPointerException in JmxMonitor on some platforms.
",Buggy,"The commit message indicates a fix for a NullPointerException in JmxMonitor. The code change adds a null check for the ClassLoader before attempting to get a resource from it. If the ClassLoader is null, the method returns null, preventing a potential NullPointerException when calling getResource(""/"") on a null ClassLoader. This aligns with the commit message indicating a bug fix. The answer is **Buggy**."
hector,115.json,872da019f266973c0bf570d927371e33d838b4e8,"@@ -1,28 +1,28 @@
   public boolean addColumnToCollection(CFMappingDef<?> cfMapDef, Object obj, String colName,
       byte[] colValue) {
     // if can parse, then at least adheres to formatting
     CollectionItemColName collColumnName;
     try {
       collColumnName = parseCollectionItemColName(colName);
     } catch (HectorObjectMapperException e) {
       return false;
     }
 
     // get property from mapping def - if not there, then isn't a collection
     // (but probably a problem elsewhere)
-    PropertyMappingDefinition md = cfMapDef.getPropMapByPropName(collColumnName.getPropertyName());
+    PropertyMappingDefinition md = cfMapDef.getPropMapByColumnName(collColumnName.getPropertyName());
     if (null == md) {
       return false;
     }
 
     Collection<Object> coll;
     try {
       coll = (Collection<Object>) reflectionHelper.invokeGetter(obj, md);
     } catch (HectorObjectMapperException e) {
       return false;
     }
 
     Object value = deserializeCollectionValue(colValue);
     coll.add(value);
     return true;
   }",Buggy,"fix bug: when mapping java collections, if property name different from column name, would not map correctly
",Buggy,"The commit message indicates a bug fix related to mapping Java collections when the property name differs from the column name. The code diff shows a change from `cfMapDef.getPropMapByPropName()` to `cfMapDef.getPropMapByColumnName()`. This change directly addresses the issue described in the commit message, suggesting that the original code was incorrectly using the property name instead of the column name to retrieve the property mapping definition. This caused mapping failures when the names differed. Therefore, the change fixes a bug. The answer is **Buggy**."
hector,2740.json,bf6efaeda600325fdeed53aad52e1700a0556cfd,"@@ -1,28 +1,28 @@
   public long createClock() {
     switch (this) {
 	    case MICROSECONDS:
 	    	return System.currentTimeMillis() * ONE_THOUSAND;
 	    case MICROSECONDS_SYNC:
 	      // The following simulates a microseconds resolution by advancing a static counter
 	      // every time a client calls the createClock method, simulating a tick.
 	      long us = System.currentTimeMillis() * ONE_THOUSAND;
 	      // Synchronized to guarantee unique time within and across threads.
 	      synchronized (ClockResolution.class) {
 	         if (us > lastTime) {
 	        	 lastTime = us;
 	         } else {
 	        	 // the time i got from the system is equals or less
 	        	 // (hope not - clock going backwards)
 	        	 // One more ""microsecond""
-	        	 us = lastTime++;
+	        	 us = ++lastTime;
 	         }
 	      }
 	      return us;
 	    case MILLISECONDS:
 	      return System.currentTimeMillis();
 	    case SECONDS:
 	      return System.currentTimeMillis() / 1000;
 	}
 
     return System.currentTimeMillis();
   }",Buggy,"Fix bug. Increment lastTime first and later assign

The lastTime was getting incremented right after assigning it to us.
",Buggy,"The commit message indicates a bug fix related to the `lastTime` variable in the `MICROSECONDS_SYNC` case. The diff shows that the increment of `lastTime` was happening after assigning it to `us`, which could lead to incorrect time values being returned. The change modifies the code to increment `lastTime` before assigning it to `us` using the pre-increment operator `++lastTime`. This ensures that the correct incremented value is returned, addressing the bug described in the commit message. The answer is **Buggy**."
hector,52.json,d0401ba77a823d118c98a74db02678c948cf33bc,"@@ -1,16 +1,26 @@
   private byte[] generateColumnFamilyKeyFromPkObj(CFMappingDef<?> cfMapDef, Object pkObj) {
     List<byte[]> segmentList = new ArrayList<byte[]>(cfMapDef.getKeyDef().getIdPropertyMap().size());
-
+    
+    List<String> rm1 = new ArrayList<String>();
+    List<String> rm2 = new ArrayList<String>();
+    
     if (cfMapDef.getKeyDef().isComplexKey()) {
-      for (PropertyDescriptor pd : cfMapDef.getKeyDef().getPropertyDescriptorMap().values()) {
-        segmentList.add(callMethodAndConvertToCassandraType(pkObj, pd.getReadMethod(),
-            new DefaultConverter()));
+    	
+      Map<String, PropertyDescriptor> propertyDescriptorMap = cfMapDef.getKeyDef().getPropertyDescriptorMap(); 	
+      Map<String, PropertyMappingDefinition> idPropertyMap =  cfMapDef.getKeyDef().getIdPropertyMap();
+    	
+      for (String key : cfMapDef.getKeyDef().getIdPropertyMap().keySet()) {
+    	  PropertyDescriptor pd = propertyDescriptorMap.get(key);
+    	  segmentList.add(callMethodAndConvertToCassandraType(pkObj, pd.getReadMethod(),
+    	            new DefaultConverter()));
       }
+     
+    	 
     } else {
       PropertyMappingDefinition md = cfMapDef.getKeyDef().getIdPropertyMap().values().iterator()
                                              .next();
       segmentList.add(md.getConverter().convertObjTypeToCassType(pkObj));
     }
 
     return keyConcatStrategy.concat(segmentList);
   }",Buggy,"Fix for unordered Composite Key when finding values by key

A problem occurred when you attempted to find an object by composite
key.  The composite would be persisted in order by the order of the
composite fields in the class.  When attempting to find the object the
objectMapper would use the natural ordering of the field names instead.
",Buggy,"The commit message indicates a bug related to the order of composite keys when finding values. The code diff shows a change in how the `segmentList` is populated for complex keys. Instead of iterating through the `PropertyDescriptorMap` (which might have a different order), the code now iterates through the `IdPropertyMap`'s key set to retrieve the `PropertyDescriptor` in the correct order. This ensures that the segments are added to the `segmentList` based on the order defined in the `IdPropertyMap`, which reflects the intended order of the composite key. This directly addresses the bug described in the commit message. The answer is **Buggy**.
"
hector,2393.json,befd460c79a1d16c64a1a9857678621106e4a111,"@@ -1,13 +1,16 @@
-        public boolean hasNext()
-        {
-            boolean retval = false;
-            if (isStart)
-            {
-                retval = res.hasResults();
-            }
-            else
-            {
-                retval = res.hasNext();
-            }
-            return retval;
-        }

+		public boolean hasNext() 
+		{
+			boolean retval = false;
+			if (isStart) 
+			{
+				if(res.hasResults() || res.hasNext()) 
+				{
+					retval = true; 
+				}
+			} 
+			else 
+			{
+				retval = res.hasNext();
+			}
+			return retval;
+		}",Buggy,"Fix bug in hasNext when no result for a key is returned
",Buggy,"The commit message indicates a bug fix in the `hasNext()` method when no result is returned for a key. The diff shows a modification to the `hasNext()` method. Specifically, when `isStart` is true, the original code only checked `res.hasResults()`. The corrected code now checks `res.hasResults() || res.hasNext()`. This change suggests that the original code was not correctly handling the case where there were no initial results but subsequent results might exist. This is a bug fix. The answer is **Buggy**.
"
cucumber-jvm,1443.json,5f8f31b3d2b34050a93e27990ed25ce9274f49ac,"@@ -1,22 +1,23 @@
     public void newWorld() {
+        stepDefinitions.clear();
         MutablePicoContainer pico = new DefaultPicoContainer();
         for(Class stepsClass : stepsClasses) {
             pico.addComponent(stepsClass);
         }
 
         for(Object stepObject : pico.getComponents()) {
             for (Method method : stepObject.getClass().getMethods()) {
                 String regexpString = null;
                 if (method.isAnnotationPresent(Given.class)) {
                     regexpString = method.getAnnotation(Given.class).value();
                 } else if (method.isAnnotationPresent(When.class)) {
                     regexpString = method.getAnnotation(When.class).value();
                 } else if (method.isAnnotationPresent(Then.class)) {
                     regexpString = method.getAnnotation(Then.class).value();
                 }
                 if(regexpString != null) {
                     stepDefinitions.add(new StepDefinition(regexpString, stepObject, method));
                 }
             }
         }
     }",Buggy,"Fix Ambiguous problem for Cucumber Java
",Buggy,"The commit message indicates a fix for an ""Ambiguous problem"" in Cucumber Java. The diff shows the addition of `stepDefinitions.clear();` at the beginning of the `newWorld()` method. This suggests that the previous implementation of `newWorld()` was accumulating step definitions from previous scenarios or test runs, leading to ambiguity when Cucumber tried to match steps to multiple definitions. Clearing the `stepDefinitions` list ensures that only the step definitions relevant to the current scenario are considered, resolving the ambiguity. The answer is **Buggy**."
cucumber-jvm,1018.json,d8406dd1cbd14b82a79ccf5205f4367c9883a768,"@@ -1,10 +1,10 @@
         private String calculateElementName(cucumber.api.TestCase testCase) {
             String testCaseName = testCase.getName();
             if (testCaseName.equals(previousTestCaseName)) {
-                return testCaseName + (includesBlank(testCaseName) ? "" "" : ""_"") + ++exampleNumber;
+                return Utils.getUniqueTestNameForScenarioExample(testCaseName, ++exampleNumber);
             } else {
                 previousTestCaseName = testCase.getName();
                 exampleNumber = 1;
                 return testCaseName;
             }
         }",Buggy,"Android: Fix Cucumber execution on Gradle (#1094)

* Fix AndroidInstrumentationReporter for Gradle builds

* the connected check tasks of the Android/Gradle build system do not like non-unique test names
* we add a unique index to the test names if they are non-unique (e.g. on scenario outlines with multiple examples)
* for this bug fix, we provide a unit test

* Fix AndroidInstrumentationReporter for Gradle builds

* the connected check tasks of the Android/Gradle build system do not like non-unique test names
* we add a unique index to the test names if they are non-unique (e.g. on scenario outlines with multiple examples)
* for this bug fix, we provide a unit test

This is a re-integration of PR-1094. The original pull request code was performed on 1.2.6-SNAPSHOT base. This code bases on version 2.3.2-SNAPSHOT.

* Update Cukeulator example project to work with newest Android build tools

- Update to Android Studio 3.0.1, SDK 26+ and Gradle 4.1
- Replace Instrumentation class by CucumberRunner
- CucumberRunner uses AndroidJUnitRunner (JUnit 4+)
- CalculationSteps class uses ActivityTestRule instead of deprecated ActivityInstrumentationTestCase2
- Fix permissions to write reports on internal storage

* Improve Cukeulator example project

* Update README.md
* Enable local Maven dependencies for better development experience
* Describe, how to use Cukeulator example project with locally built Cucumber-JVM

* Rename duplicated test case names in AndroidInstrumentationReporter like JUnitFormatter in cucumber-core

* Fix typo

* Improve readability of AndroidInstrumentationReporterTest

* Share common logic for test case name between JUnitFormatter and AndroidInstrumentationReporter

* Improve code quality

- Create merged method calculateUniqueTestName from getUniqueTestName and ensureUniqueTestName and make it private
- Use better test method name: test_case_names_are_unique_on_equal_scenario_names (instead of scenario_outline_all_test_names_unique)
- Refactor test code: now it should be readable

* Change misleading variable names

* [Android] Split up test of making test names unique.

To provide better documentation of the functionality, that is:
* test names within feature are made unique by appending blank and
  number
* test names within are made unique by appending underscore and number
  when no blank in name
* test names in different features can be the same
* test names are made unique also when not consecutive
",Buggy,"The commit message indicates a bug fix related to non-unique test names in Android/Gradle builds when using Cucumber. The diff shows a change in `calculateElementName` method, where the logic for generating unique test names has been moved to `Utils.getUniqueTestNameForScenarioExample`. This change aims to resolve the issue of non-unique test names, which is a bug. The answer is **Buggy**.
"
hazelcast,11513.json,7a17eba9e37e1ce2953f5320c56c13c92620953d,"@@ -1,12 +1,11 @@
     public boolean shouldWait() {
         WriteBehindQueue<DelayedEntry> writeBehindQueue = store.getWriteBehindQueue();
-        int size = writeBehindQueue.size();
-        if (size == 0) {
+        DelayedEntry entry = writeBehindQueue.peek();
+        if (entry == null) {
             return false;
         }
 
-        DelayedEntry entry = writeBehindQueue.peek();
         long currentSequence = entry.getSequence();
-        return entry != null && currentSequence <= this.sequence
-                && size + currentSequence - 1 >= this.sequence;
+        return currentSequence <= this.sequence
+                && writeBehindQueue.size() + currentSequence - 1 >= this.sequence;
     }",Buggy,"Fixes findbugs problem
",Buggy,"The commit message indicates a fix for a FindBugs problem. The code changes the logic in `shouldWait()` method. The original code first gets the size of the `writeBehindQueue` and checks if the size is 0. If the size is 0, it returns false. Then it peeks the queue. The modified code first peeks the queue and checks if the entry is null. If the entry is null, it returns false. This change avoids getting the size of the queue when the queue is empty and also avoids a potential NullPointerException if the queue is empty and `peek()` returns null in the original code. The change addresses a potential bug or vulnerability identified by FindBugs. The answer is **Buggy**."
hazelcast,27666.json,515671c5314975716054e45c0866ffd8be64f4c2,"@@ -1,17 +1,17 @@
     private void updateConfig(AliasedDiscoveryConfig config, Node node) {
         NamedNodeMap attributes = node.getAttributes();
         for (int a = 0; a < attributes.getLength(); a++) {
             Node att = attributes.item(a);
             String value = getTextContent(att).trim();
             if (""enabled"".equals(lowerCaseInternal(att.getNodeName()))) {
                 config.setEnabled(getBooleanValue(value));
             } else if (att.getNodeName().equals(""connection-timeout-seconds"")) {
                 config.setProperty(""connection-timeout-seconds"", value);
             }
         }
         for (Node n : childElements(node)) {            
-            String key = cleanNodeName(n, ""eureka"".equals(n.getParentNode().getLocalName()));
+            String key = cleanNodeName(n, !""eureka"".equals(n.getParentNode().getLocalName()));
             String value = getTextContent(n).trim();
             config.setProperty(key, value);
         }
     }",Buggy,eureka lowercase bug fix,Buggy,"The commit message indicates a bug fix related to lowercasing in Eureka. The diff modifies the condition in `cleanNodeName`'s key assignment. The original code used `eureka.equals(n.getParentNode().getLocalName())`, while the corrected code uses `!""eureka"".equals(n.getParentNode().getLocalName())`. This suggests the logic was inverted, causing incorrect key generation under certain conditions, which constitutes a bug. The answer is **Buggy**."
hazelcast,16541.json,6ce43d4090fe4924dfc7dc45583a328e25bb1e9f,"@@ -1,14 +1,15 @@
     public void run() throws Exception {
         CollectionWrapper wrapper = getCollectionWrapper();
-        if (wrapper == null || wrapper.getVersion() == version){
+        if (wrapper == null || wrapper.getVersion() != version){
             notify = false;
             return;
         }
+        wrapper.incrementAndGetVersion();
         for (Operation op: opList){
             op.setNodeEngine(getNodeEngine()).setServiceName(getServiceName()).setPartitionId(getPartitionId());
             op.beforeRun();
             op.run();
             op.afterRun();
         }
         getOrCreateContainer().unlock(dataKey, getCallerUuid(), threadId);
     }",Buggy,"Multimap commit txn version bug fixed
",Buggy,"The commit message indicates a bug fix related to the transaction version in a Multimap. The code diff shows a change in the condition `wrapper.getVersion() == version` to `wrapper.getVersion() != version`. Additionally, `wrapper.incrementAndGetVersion()` is called. This suggests that the original code was not correctly handling version comparisons, potentially leading to incorrect execution of operations. The change ensures that operations are executed only when the version has changed, which aligns with the bug fix mentioned in the commit message. The answer is **Buggy**."
hazelcast,28391.json,8a5e458780154dfd44ef41dd02770456c19c7dbf,"@@ -1,4 +1,13 @@
     public final void sendResponse(Object value) {
         OperationResponseHandler responseHandler = getOperationResponseHandler();
-        responseHandler.sendResponse(this, value);
+        if (responseHandler == null) {
+            if (value instanceof Throwable) {
+                // in case of a throwable, we want the stacktrace.
+                getLogger().warning(""Missing responseHandler for "" + toString(), (Throwable) value);
+            } else {
+                getLogger().warning(""Missing responseHandler for "" + toString() + "" value["" + value + ""]"");
+            }
+        } else {
+            responseHandler.sendResponse(this, value);
+        }
     }",Buggy,"NPE on operation sendResponse if no response handler is set

The problem is fixed by adding a check if the response handler is set.

If it isn't set a warning is printed so we don't loose the exception. It is better
that a response handler is always set to prevent loosing any track of problematic flows.
",Buggy,"The commit message indicates a fix for a NullPointerException (NPE) that occurs when sending a response without a response handler being set. The diff adds a check for a null `responseHandler` before attempting to use it. If the handler is null, a warning message is logged, including the stacktrace if the response value is a Throwable. This prevents the NPE and provides some visibility into the issue. This clearly indicates a bug fix. The answer is **Buggy**.
"
hazelcast,21845.json,7ab1db9edc8409f599bf01054636a1643e3cf2d3,"@@ -1,3 +1,11 @@
     public double getRatio() {
-        return ((double) hits / misses) * 100.0;
+        if (misses == 0) {
+            if (hits == 0) {
+                return Double.NaN;
+            } else {
+                return Double.POSITIVE_INFINITY;
+            }
+        } else {
+            return ((double) hits / misses) * PERCENTAGE;
+        }
     }",Buggy,"Fixed checkstyle issue and divide by zero problem for ratio in ""NearCacheStatsImpl""",Buggy,"The commit message indicates a fix for a divide-by-zero problem. The code diff introduces a check for `misses == 0`. If `misses` is zero, it further checks if `hits` is also zero, returning `Double.NaN` in that case, and `Double.POSITIVE_INFINITY` otherwise. If `misses` is not zero, it proceeds with the original calculation. This clearly addresses the divide-by-zero issue. The answer is **Buggy**."
hazelcast,22872.json,0fd3a71c944b64d8b3a3ad75361015fb289be227,"@@ -1,4 +1,4 @@
     protected DiagnosticsLogWriter write(String s) {
-        printWriter.write(s);
+        printWriter.write(s == null ? ""null"" : s);
         return this;
     }",Buggy,"Fixed NPE in Diagnostics

Fix #9085

The problem was that the printWriter was called with a print(null). So the fix is to do a simple null check and write
""null"" in case of null.
",Buggy,"The commit message indicates a fix for a NullPointerException (NPE) in the Diagnostics class, specifically when the `printWriter` is called with a `null` argument. The provided diff shows a modification to the `write(String s)` method where a null check is added. If the input string `s` is `null`, the method now writes the string ""null"" instead of passing the null value to `printWriter.write()`, which would cause an NPE. This aligns perfectly with the commit message, indicating a bug fix. The answer is **Buggy**."
hazelcast,10446.json,cd1d3e928d20e4ec4cc22d0cc91d93c2b6600c9e,"@@ -1,3 +1,3 @@
     public Object getKey() {
-        return key;
+        return new MapKey(name, key);
     }",Buggy,"map txn bug fix: same key overrides txn op with same key but different map name
",Buggy,"The commit message indicates a bug fix related to map transactions where the same key could override transaction operations with the same key but different map names. The diff modifies the `getKey()` method to return a `MapKey` object that encapsulates both the map name and the key. This change ensures that keys are unique across different maps within the same transaction, thus addressing the bug described in the commit message. The answer is **Buggy**."
hazelcast,6968.json,0ae63554413e19520e136b5a35fec3c05d801866,"@@ -1,4 +1,6 @@
     private void createCachesOnCluster() {
         ClientCacheProxyFactory proxyFactory = (ClientCacheProxyFactory) getClientProxyFactory(ICacheService.SERVICE_NAME);
-        proxyFactory.recreateCachesOnCluster();
+        if (proxyFactory != null) {
+            proxyFactory.recreateCachesOnCluster();
+        }
     }",Buggy,"Fix NPE when jcache not in classpath

fixing a bug recently introduced in #13810

fixes #13851
",Buggy,"The commit message indicates a fix for a NullPointerException (NPE) that occurs when the jcache is not in the classpath. The diff adds a null check for `proxyFactory` before calling `recreateCachesOnCluster()`. This prevents the NPE if `getClientProxyFactory` returns null, which would happen if jcache is not available. This is a clear bug fix. The answer is **Buggy**."
hazelcast,29434.json,f701a369208b2acf3066ba1b306d68688a765a56,"@@ -1,7 +1,9 @@
     public void populate(LiveOperations liveOperations) {
         for (Queue<ParkedOperation> parkQueue : parkQueueMap.values()) {
-            for (ParkedOperation op : parkQueue) {
-                liveOperations.add(op.getCallerAddress(), op.getCallId());
+            for (ParkedOperation parkedOperation : parkQueue) {
+                // we need to read out the data from the BlockedOperation; not from the ParkerOperation-container.
+                Operation operation = parkedOperation.getOperation();
+                liveOperations.add(operation.getCallerAddress(), operation.getCallId());
             }
         }
     }",Buggy,"Fix heartbeat problem for BlockingOperations

The problem is that the ParkedOperation; the container around the BlockingOperation; is
asked for callid/calleraddress. But this container object doesn't have any sensible
information, so the heartbeat is not constructed correctly for a blocked Operation.
",Buggy,"The commit message indicates a problem with heartbeat construction for `BlockingOperations` due to incorrect information being retrieved from the `ParkedOperation` container. The diff modifies the `populate` method to extract the `CallerAddress` and `CallId` from the underlying `Operation` object within the `ParkedOperation` instead of directly from the `ParkedOperation` itself. This aligns with the commit message's description of the bug and its resolution. The change ensures that the correct information is used for heartbeat construction, fixing the identified problem. The answer is **Buggy**.
"
hazelcast,17124.json,fc8ec0a85ea7e388794baf5c7710ec824b6cb10c,"@@ -1,20 +1,27 @@
     public void close() throws IOException {
         if (!CLOSED.compareAndSet(this, FALSE, TRUE)) {
             return;
         }
 
+        // we execute this in its own try/catch block because we don't want to skip closing the socketChannel in case
+        // of problems.
+        try {
+            onClose();
+        } catch (Exception e) {
+            getLogger().severe(format(""Failed to call 'onClose' on channel [%s]"", this), e);
+        }
+
         try {
             socketChannel.close();
         } finally {
             for (ChannelCloseListener closeListener : closeListeners) {
                 // it is important we catch exceptions so that other listeners aren't obstructed when
                 // one of the listeners is throwing an exception.
                 try {
                     closeListener.onClose(this);
                 } catch (Exception e) {
-                    ILogger logger = Logger.getLogger(getClass());
-                    logger.severe(format(""Failed to process closeListener [%s] on channel [%s]"", closeListener, this), e);
+                    getLogger().severe(format(""Failed to process closeListener [%s] on channel [%s]"", closeListener, this), e);
                 }
             }
         }
     }",Buggy,"Fix AbstractChannel.close forwarding to reader/writer

A bug was introduced by me in the 3.9 io changes where the forwarding of
the close to the underlying channel reader/writer is skipped.

This pr restores these closes by adding a template method to the AbstractChannel
which is implemented by the NioChannel.
",Buggy,"The commit message indicates a bug fix related to closing the underlying channel reader/writer in `AbstractChannel`. The diff shows the addition of a `try-catch` block around the `onClose()` call, which likely handles the closing of the reader/writer. The `onClose()` method is intended to be implemented by subclasses like `NioChannel` to perform specific closing operations. The added `try-catch` ensures that exceptions during the closing of the reader/writer do not prevent the socket channel from closing and the execution of close listeners. This aligns with the commit message's description of restoring the closing functionality. Therefore, the changes indicate a bug fix. The answer is **Buggy**."
hazelcast,12126.json,8bbe3fbfaa5e85b1e504100aa7d4c425970dee3d,"@@ -1,3 +1,5 @@
     public boolean eval(Object arg) {
-        return (key == null || key.equals(arg)) && predicate.apply((Map.Entry)arg);
+        final QueryEntry entry = (QueryEntry) arg;
+        final Data keyData = entry.getKeyData();
+        return (key == null || key.equals(keyData)) && predicate.apply((Map.Entry)arg);
     }",Buggy,"MapAddListenerRequest serialization bug fix and related to that  QueryEventFilter bug fix
",Buggy,"The commit message indicates a bug fix related to serialization and `QueryEventFilter`. The diff modifies the `eval` method within the `QueryEventFilter`. It appears the original code directly compared `key` with `arg`, which is now cast to `QueryEntry`. The fix extracts the key data (`keyData`) from the `QueryEntry` and compares `key` with `keyData`. This suggests that the original code was incorrectly comparing the key, potentially due to type mismatch or incorrect data extraction from the `QueryEntry`. This change addresses a bug where the key comparison was failing, leading to incorrect filtering. The answer is **Buggy**.
"
hazelcast,12000.json,777bd5c28ac36f1b948f94a68b21328e3743e877,"@@ -1,11 +1,10 @@
     public static int getMaxSizePerNode(MaxSizeConfig maxSizeConfig) {
-
-        int maxSizePerNode = getApproximateMaxSize(maxSizeConfig, MaxSizePolicy.PER_NODE);
+        double maxSizePerNode = getApproximateMaxSize(maxSizeConfig, MaxSizePolicy.PER_NODE);
 
         if (maxSizePerNode == MaxSizeConfig.DEFAULT_MAX_SIZE) {
             // unlimited
             return -1;
         }
 
-        return maxSizePerNode;
+        return (int) maxSizePerNode;
     }",Buggy,"Fixed getApproximateMaxSize calculation bug
",Buggy,"The commit message indicates a bug fix in the `getApproximateMaxSize` calculation. The diff shows a change in the `getMaxSizePerNode` method where the return type of `getApproximateMaxSize` is changed from `int` to `double`, and the result is then cast back to `int` before being returned. This suggests that the original calculation might have been losing precision due to integer division or other integer-related issues, and the fix involves using floating-point arithmetic to get a more accurate result before converting it back to an integer. This is a clear indication of a bug fix. The answer is **Buggy**."
jetty,11444.json,93b6877d724cbec52cc427e168144d539ea8e38b,"@@ -1,9 +1,9 @@
     protected void doStart() throws Exception
     {
         Objects.requireNonNull(httpClient, ""Provided HttpClient is null"");
 
+        super.doStart();
+
         if (!httpClient.isRunning())
             throw new IllegalStateException(""HttpClient is not running (did you forget to start it?): "" + httpClient);
-
-        super.doStart();
     }",Buggy,"Issue #2210 - fixing flaw in safety checks with JSR356 ClientContainer
",Buggy,"The commit message indicates a fix for a flaw in safety checks related to a JSR356 ClientContainer, suggesting a bug fix. The code moves `super.doStart()` after the null check and the `httpClient.isRunning()` check. This ensures that the checks are performed before the parent class's start logic is executed. If `httpClient` is null or not running, an exception is thrown, preventing further execution. This change fixes a potential issue where the parent class's start logic might be executed with an invalid `httpClient`, which could lead to unexpected behavior or errors. The answer is **Buggy**.
"
jetty,3568.json,6c81941142b9efe2b5b80198268ae75687dc6374,"@@ -1,61 +1,66 @@
     public void validate(Certificate[] certChain) throws CertificateException
     {
         try
         {
             ArrayList<X509Certificate> certList = new ArrayList<X509Certificate>();
             for (Certificate item : certChain)
             {
                 if (item == null)
                     continue;
                 
                 if (!(item instanceof X509Certificate))
                 {
                     throw new IllegalStateException(""Invalid certificate type in chain"");
                 }
                 
                 certList.add((X509Certificate)item);
             }
     
             if (certList.isEmpty())
             {
                 throw new IllegalStateException(""Invalid certificate chain"");
                 
             }
     
             X509CertSelector certSelect = new X509CertSelector();
             certSelect.setCertificate(certList.get(0));
             
             // Configure certification path builder parameters
             PKIXBuilderParameters pbParams = new PKIXBuilderParameters(_trustStore, certSelect);
             pbParams.addCertStore(CertStore.getInstance(""Collection"", new CollectionCertStoreParameters(certList)));
     
             // Set maximum certification path length
             pbParams.setMaxPathLength(_maxCertPathLength);
     
             // Enable revocation checking
             pbParams.setRevocationEnabled(true);
     
             // Set static Certificate Revocation List
             if (_crls != null && !_crls.isEmpty())
             {
                 pbParams.addCertStore(CertStore.getInstance(""Collection"", new CollectionCertStoreParameters(_crls)));
             }
     
             // Enable On-Line Certificate Status Protocol (OCSP) support
-            Security.setProperty(""ocsp.enable"",""true"");
-    
+            if (_enableOCSP)
+            {
+                Security.setProperty(""ocsp.enable"",""true"");
+            }
             // Enable Certificate Revocation List Distribution Points (CRLDP) support
-            System.setProperty(""com.sun.security.enableCRLDP"",""true"");
+            if (_enableCRLDP)
+            {
+                System.setProperty(""com.sun.security.enableCRLDP"",""true"");
+            }
     
             // Build certification path
             CertPathBuilderResult buildResult = CertPathBuilder.getInstance(""PKIX"").build(pbParams);               
             
             // Validate certification path
             CertPathValidator.getInstance(""PKIX"").validate(buildResult.getCertPath(),pbParams);
         }
         catch (GeneralSecurityException gse)
         {
             LOG.debug(gse);
             throw new CertificateException(""Unable to validate certificate: "" + gse.getMessage(), gse);
         }
     }",Buggy,"[Bug 373567] cert validation issue with ocsp and crldp always being enabled when validating turned on fixed
",Buggy,"The commit message indicates a bug fix related to certificate validation where OCSP and CRLDP were always enabled regardless of the validation settings. The diff shows that the code was modified to conditionally enable OCSP and CRLDP based on the `_enableOCSP` and `_enableCRLDP` flags, respectively. This aligns with the commit message and confirms that a bug was fixed. The answer is **Buggy**."
jetty,12794.json,c6d86122dbd737f2247b01653da0af938075370d,"@@ -1,45 +1,46 @@
     public Authentication validateRequest(ServletRequest request, ServletResponse response, boolean mandatory) throws ServerAuthException
     {
         HttpServletRequest req = (HttpServletRequest)request;
         HttpServletResponse res = (HttpServletResponse)response;
 
         String header = req.getHeader(HttpHeader.AUTHORIZATION.asString());
+        String authScheme = getAuthSchemeFromHeader(header);
 
         if (!mandatory)
         {
             return new DeferredAuthentication(this);
         }
 
         // The client has responded to the challenge we sent previously
-        if (header != null && header.startsWith(HttpHeader.NEGOTIATE.asString().toLowerCase()))
+        if (header != null && isAuthSchemeNegotiate(authScheme))
         {
             String spnegoToken = header.substring(10);
 
             UserIdentity user = login(null,spnegoToken, request);
 
             if ( user != null )
             {
                 return new UserAuthentication(getAuthMethod(),user);
             }
         }
 
         // A challenge should be sent if any of the following cases are true:
         //   1. There was no Authorization header provided
         //   2. There was an Authorization header for a type other than Negotiate
         try
         {
              if (DeferredAuthentication.isDeferred(res))
              {
                  return Authentication.UNAUTHENTICATED;
              }
 
             LOG.debug(""SpengoAuthenticator: sending challenge"");
             res.setHeader(HttpHeader.WWW_AUTHENTICATE.asString(), HttpHeader.NEGOTIATE.asString());
             res.sendError(HttpServletResponse.SC_UNAUTHORIZED);
             return Authentication.SEND_CONTINUE;
         }
         catch (IOException ioe)
         {
             throw new ServerAuthException(ioe);
         }
     }",Buggy,"Fix a bug around handling ""Negotiate"" case-insensitively in SpnegoAut (#1710)

* Fix a bug around handling ""Negotiate"" case-insensitively in SpnegoAuthenticator

Closes #1709

Signed-off-by: Josh Elser <elserj@apache.org>

* Clean up isAuthSchemeNegotiate(String) since we don't need to use startsWith()

Signed-off-by: Josh Elser <elserj@apache.org>
",Buggy,"The commit message indicates a bug fix related to handling ""Negotiate"" case-insensitively in SpnegoAuthenticator. The diff shows that the code was changed to handle the ""Negotiate"" authentication scheme in a case-insensitive manner. The original code used `header.startsWith(HttpHeader.NEGOTIATE.asString().toLowerCase())`, which only matched if the header started with ""negotiate"" (lowercase). The updated code likely uses a case-insensitive comparison, or extracts the authentication scheme and compares it case-insensitively to ""Negotiate"". This fixes the bug where authentication would fail if the ""Negotiate"" scheme was not in lowercase. The answer is **Buggy**.
"
jetty,12794.json,1fd3e4ad1b8903bc9033dd24c86a0207996097b1,"@@ -1,46 +1,45 @@
     public Authentication validateRequest(ServletRequest request, ServletResponse response, boolean mandatory) throws ServerAuthException
     {
         HttpServletRequest req = (HttpServletRequest)request;
         HttpServletResponse res = (HttpServletResponse)response;
 
         String header = req.getHeader(HttpHeader.AUTHORIZATION.asString());
 
         if (!mandatory)
         {
             return new DeferredAuthentication(this);
         }
 
-        // check to see if we have authorization headers required to continue
-        if ( header == null )
-        {
-            try
-            {
-                 if (DeferredAuthentication.isDeferred(res))
-                 {
-                     return Authentication.UNAUTHENTICATED;
-                 }
-
-                LOG.debug(""SpengoAuthenticator: sending challenge"");
-                res.setHeader(HttpHeader.WWW_AUTHENTICATE.asString(), HttpHeader.NEGOTIATE.asString());
-                res.sendError(HttpServletResponse.SC_UNAUTHORIZED);
-                return Authentication.SEND_CONTINUE;
-            }
-            catch (IOException ioe)
-            {
-                throw new ServerAuthException(ioe);
-            }
-        }
-        else if (header != null && header.startsWith(HttpHeader.NEGOTIATE.asString()))
+        // The client has responded to the challenge we sent previously
+        if (header != null && header.startsWith(HttpHeader.NEGOTIATE.asString().toLowerCase()))
         {
             String spnegoToken = header.substring(10);
 
             UserIdentity user = login(null,spnegoToken, request);
 
             if ( user != null )
             {
                 return new UserAuthentication(getAuthMethod(),user);
             }
         }
 
-        return Authentication.UNAUTHENTICATED;
+        // A challenge should be sent if any of the following cases are true:
+        //   1. There was no Authorization header provided
+        //   2. There was an Authorization header for a type other than Negotiate
+        try
+        {
+             if (DeferredAuthentication.isDeferred(res))
+             {
+                 return Authentication.UNAUTHENTICATED;
+             }
+
+            LOG.debug(""SpengoAuthenticator: sending challenge"");
+            res.setHeader(HttpHeader.WWW_AUTHENTICATE.asString(), HttpHeader.NEGOTIATE.asString());
+            res.sendError(HttpServletResponse.SC_UNAUTHORIZED);
+            return Authentication.SEND_CONTINUE;
+        }
+        catch (IOException ioe)
+        {
+            throw new ServerAuthException(ioe);
+        }
     }",Buggy,"Sends the WWW-Authenticate header if a non-Negotiate authorization he (#1700)

* Sends the WWW-Authenticate header if a non-Negotiate authorization header was given

Fixes #1698

Signed-off-by: Josh Elser <elserj@apache.org>

* Dumb compilation error

Signed-off-by: Josh Elser <elserj@apache.org>

* Adds a test to show the challenge is sent.

Signed-off-by: Josh Elser <elserj@apache.org>

* Refactor the conditionals per Greg's suggestion

Signed-off-by: Josh Elser <elserj@apache.org>

* Add the expected license header

Signed-off-by: Josh Elser <elserj@apache.org>
",Buggy,"The commit message indicates a fix related to sending the `WWW-Authenticate` header when a non-Negotiate authorization header is provided. The diff modifies the logic for sending the `WWW-Authenticate` header. The original code only sent the header if the `Authorization` header was null. The modified code sends the header if the `Authorization` header is null or if it doesn't start with `Negotiate`. This aligns with the commit message, indicating a bug fix. The answer is **Buggy**."
jetty,6485.json,c170801600f37d831007e6f496e7fc7d86e71dd1,"@@ -1,27 +1,27 @@
     protected void doStop() throws Exception
     {
         super.doStop();
 
         try
         {
             for (int i=_configurations.length;i-->0;)
                 _configurations[i].deconfigure(this);
             
             _configurations=null;
             
             // restore security handler
-            if (_securityHandler.getHandler()==null)
+            if (_securityHandler != null && _securityHandler.getHandler()==null)
             {
                 _sessionHandler.setHandler(_securityHandler);
                 _securityHandler.setHandler(_servletHandler);
             }
         }
         finally
         {
             if (_ownClassLoader)
                 setClassLoader(null);
 
             setAvailable(true);
             _unavailableException=null;
         }
     }",Buggy,"Bug 277027 Fix NPE if no security handler installed

git-svn-id: svn+ssh://dev.eclipse.org/svnroot/rt/org.eclipse.jetty/jetty/trunk@239 7e9141cc-0065-0410-87d8-b60c137991c4
",Buggy,The commit message indicates a fix for a NullPointerException (NPE) that occurs when no security handler is installed. The code diff adds a null check for `_securityHandler` before accessing its `getHandler()` method. This prevents the NPE from occurring when `_securityHandler` is null. The answer is **Buggy**.
jetty,1562.json,4236f14955e828946c33f447fed3e65dfa8bfc1e,"@@ -1,15 +1,15 @@
         public void run()
         {
             try
             {
                 if (handle)
                     handleWithContext();
-                else
+                else if (getHttpConfiguration().isNotifyRemoteAsyncErrors())
                     getState().asyncError(failure);
                 callback.succeeded();
             }
             catch (Throwable x)
             {
                 callback.failed(x);
             }
         }",Buggy,"Fixes #1891 - Make HTTP/2 async error notifications configurable.

Introduced HttpConfiguration.notifyRemoteAsyncErrors, true by default.
",Buggy,"The commit message indicates a fix for issue #1891, which involves making HTTP/2 async error notifications configurable. The diff introduces a check `getHttpConfiguration().isNotifyRemoteAsyncErrors()` before calling `getState().asyncError(failure)`. This suggests that previously, async errors were always notified, but now it's configurable. This change addresses a bug or undesired behavior where async errors were always propagated, and the fix allows users to disable this behavior. The answer is **Buggy**."
jetty,6487.json,0b13e6cddd8b8056311176389024fdb187305749,"@@ -1,12 +1,12 @@
     private void dumpUrl()
     {
         Connector[] connectors = getServer().getConnectors();
         for (int i=0;i<connectors.length;i++)
         {
             String displayName = getDisplayName();
             if (displayName == null)
-                displayName = ""WebApp@""+connectors.hashCode();
+                displayName = ""WebApp@""+Arrays.hashCode(connectors);
 
             LOG.info(displayName + "" at http://"" + connectors[i].toString() + getContextPath());
         }
     }",Buggy,"fix error prone error

Signed-off-by: olivier lamy <olamy@webtide.com>
",Buggy,"The commit message indicates a fix for an error-prone error. The code change replaces `connectors.hashCode()` with `Arrays.hashCode(connectors)`. The original code was using the hashCode of the array object itself, which doesn't reflect the contents of the array. The corrected code uses `Arrays.hashCode()` to generate a hash code based on the contents of the `connectors` array. This change likely fixes an issue where the displayed name was not unique or consistent across different deployments if the array object's identity changed, even if the connectors within remained the same. This is a bug fix. The answer is **Buggy**.
"
jetty,5128.json,8ec4e56681231a2370715cde72d46262aba65294,"@@ -1,17 +1,17 @@
     private static MultiMap<String> parseQueryString(String url)
     {
         MultiMap<String> res = new MultiMap<String>();
         int questionMarkIndex = url.indexOf('?');
         if (questionMarkIndex == -1)
         {
             return res;
         }
         int poundIndex = url.indexOf('#');
         if (poundIndex == -1)
         {
             poundIndex = url.length();
         }
-        UrlEncoded.decodeUtf8To(url.getBytes(), questionMarkIndex+1,
+        UrlEncoded.decodeUtf8To(url, questionMarkIndex+1,
                     poundIndex - questionMarkIndex - 1, res);
         return res;
     }",Buggy,"Fixing compile error
",Buggy,"The commit message indicates a fix for a compile error. The diff shows a change in the `parseQueryString` method where `url.getBytes()` is replaced with `url`. This suggests that the `UrlEncoded.decodeUtf8To` method was expecting a String as input, but the original code was passing a byte array. This would indeed cause a compile error if the method signature didn't match. The answer is **Buggy**."
jclouds,26844.json,c0e6a2c51cd131bcef2ee051cfe200b41afda528,"@@ -1,57 +1,57 @@
       public T read(JsonReader in) throws IOException {
          if (in.peek() == JsonToken.NULL) {
             in.nextNull();
             return null;
          }
 
          List<Parameter> params = parameterizedCtor.getParameters();
          Object[] values = new Object[params.size()];
          boolean empty = true;
 
          // Set all primitive constructor params to defaults
          for (Parameter param : params) {
             if (param.getType().getRawType() == boolean.class) {
                values[param.hashCode()] = Boolean.FALSE;
             } else if (param.getType().getRawType().isPrimitive()) {
                values[param.hashCode()] = 0;
             }
          }
 
          try {
             in.beginObject();
             while (in.hasNext()) {
                empty = false;
                String name = in.nextName();
                ParameterReader<?> parameter = parameterReaders.get(name);
-               if (parameter == null) {
+               if (parameter == null || in.peek() == JsonToken.NULL) {
                   in.skipValue();
                } else {
                   Object value = parameter.read(in);
                   if (value != null)
                      values[parameter.position] = value;
                }
             }
          } catch (IllegalStateException e) {
             throw new JsonSyntaxException(e);
          }
 
          for (Parameter param : params) {
             if (param.getType().getRawType().isPrimitive()) {
                checkArgument(values[param.hashCode()] != null,
                   ""Primitive param[%s] in constructor %s cannot be absent!"", param.hashCode(), parameterizedCtor);
             } else if (param.getType().getRawType() == Optional.class && values[param.hashCode()] == null) {
                values[param.hashCode()] = Optional.absent();
             }
          }
          in.endObject();
 
          try {
             return newInstance(values);
          } catch (NullPointerException ex) {
             // If {} was found and constructor threw NPE, we treat the field as null
             if (empty && values.length > 0) {
                return null;
             }
             throw ex;
          }
       }",Buggy,"Fixed bug found in docker provider where deserialization constructors don't handle json null.
",Buggy,"The commit message indicates a fix for a bug where deserialization constructors don't handle JSON null values correctly in the Docker provider. The diff shows a change within the `read` method of a `JsonReader`. Specifically, the code now checks for `JsonToken.NULL` when reading a parameter. If a null token is encountered, the code skips the value. This aligns with the commit message, as it addresses the issue of handling null values during deserialization. The code also contains a try-catch block to handle `NullPointerException` which can occur when the constructor throws an NPE. The answer is **Buggy**."
jclouds,27517.json,b59457a405f1741cebfa14775b397ae225dd714b,"@@ -1,4 +1,4 @@
    public <C extends Context> C unwrap(TypeToken<C> type) {
-      checkArgument(checkNotNull(type, ""type"").isAssignableFrom(backendType), ""backend type: %s not assignable from %s"", backendType, type);
+      checkArgument(checkNotNull(type, ""type"").isAssignableFrom(backendType), ""backend type: %s not assignable to %s"", backendType, type);
       return (C) backend;
    }",Buggy,"Fixing an error message about an unassignable backend
",Buggy,"The commit message indicates a fix to an error message. The diff changes ""from"" to ""to"" in the error message string. This is a bug fix because the original message was misleading or incorrect. The answer is **Buggy**."
jclouds,25412.json,d41101df5932043c3a8614552f25ab91194595ba,"@@ -1,19 +1,19 @@
    private ImmutableMap<String, String> getContentMetadataForManifest(ContentMetadata contentMetadata) {
       Builder<String, String> mapBuilder = ImmutableMap.builder();
       if (contentMetadata.getContentType() != null) {
          mapBuilder.put(""content-type"", contentMetadata.getContentType());
       }
       /**
-       * Do not set content-length. Set automatically to manifest json string length by BindManifestToJsonPayload
+       * Do not set content-length. Set automatically to manifest json string length by BindToJsonPayload
        */
       if (contentMetadata.getContentDisposition() != null) {
          mapBuilder.put(""content-disposition"", contentMetadata.getContentDisposition());
       }
       if (contentMetadata.getContentEncoding() != null) {
          mapBuilder.put(""content-encoding"", contentMetadata.getContentEncoding());
       }
       if (contentMetadata.getContentLanguage() != null) {
          mapBuilder.put(""content-language"", contentMetadata.getContentLanguage());
       }
       return mapBuilder.build();
    }",Buggy,"JCLOUDS-1264: Swift Unicode multipart manifests

This fixes a bug where previously BindManifestToJsonPayload used the
character length as the ContentLength, instead of the byte length,
which caused issues if the JSON contained multi-byte Unicode
characters.
",Buggy,"The commit message indicates a bug fix related to Unicode handling in Swift multipart manifests. Specifically, it mentions that `BindManifestToJsonPayload` was incorrectly using character length instead of byte length for `ContentLength`, leading to issues with multi-byte Unicode characters. The code diff doesn't directly show the fix, but it includes a comment that clarifies that the content-length is automatically set by `BindToJsonPayload`. Given the commit message and the context provided by the code, it's highly likely that the actual fix involves ensuring that `BindToJsonPayload` correctly calculates the byte length of the JSON string, thus resolving the Unicode-related bug. The answer is **Buggy**."
jclouds,18000.json,9965fbcadb0913889f19d4f29e9f66f20e3f4f08,"@@ -1,24 +1,24 @@
    public Instance getWorkingInstance(String zone, String name, String flavorId, int size) {
       InstanceApi instanceApi = api.getInstanceApiForZone(zone);
       for (int retries = 0; retries < 10; retries++) {
          Instance instance = null;
          try {
             instance = instanceApi.create(flavorId, size, name);
          } catch (Exception e) {
 
             Uninterruptibles.sleepUninterruptibly(15, TimeUnit.SECONDS);
 
-            logger.error(e.getStackTrace().toString());
+            logger.error(Arrays.toString(e.getStackTrace()));
             continue;
          }
 
          Instance updatedInstance = awaitAvailable(instance, instanceApi);
          if (updatedInstance != null) {
             return updatedInstance;
          }
          instanceApi.delete(instance.getId());
          InstancePredicates.awaitDeleted(instanceApi).apply(instance);
          
       }
       return null;
    }",Buggy,"Address error-prone warning
",Buggy,"The commit message indicates a fix for an error-prone warning. The diff changes `e.getStackTrace().toString()` to `Arrays.toString(e.getStackTrace())` in the catch block. The original code printed the default `toString()` representation of the stack trace, which isn't very readable. The updated code uses `Arrays.toString()` to provide a more informative string representation of the stack trace for logging purposes. This change improves error reporting and debugging, indicating a bug fix or at least an improvement in error handling. The answer is **Buggy**."
jclouds,1113.json,347926543ee72a14f8587bc83f72e1064cef8784,"@@ -1,4 +1,8 @@
    @Override protected void configure() {
       super.configure();
       bindHttpApi(binder(), UseApiToResolveProjectName.GetProject.class);
+      bind(OAuthScopes.class).toInstance(ReadOrWriteScopes.create( //
+            ""https://www.googleapis.com/auth/compute.readonly"", //
+            ""https://www.googleapis.com/auth/compute"" //
+      ));
    }",Buggy," * Change OAuthScopes into an interface as opposed to boilerplating annotations.
 * Fixed errors because of boilerplating annotations.
",Buggy,"The commit message indicates a change from annotations to an interface for OAuthScopes and mentions fixing errors related to the previous annotation-based approach. The diff shows the binding of OAuthScopes to an instance of ReadOrWriteScopes with specific scopes. This change likely addresses issues or limitations with the previous annotation-based configuration, suggesting a bug fix or improvement related to how OAuth scopes were handled. The answer is **Buggy**."
jclouds,25772.json,f851271ae3d56d1549e62cd3723ad6207ced97f1,"@@ -1,3 +1,3 @@
-      public static NodeAttributes condition(String condition) {
-         return new NodeAttributes().condition(condition);
+      public static NodeAttributes condition(Condition condition) {
+         return new NodeAttributes().condition(condition.name());
       }",Buggy,"minor bug fixes plus wiring up
",Buggy,"The commit message indicates minor bug fixes. The code change modifies the `condition` method in `NodeAttributes`. It changes the parameter type from `String` to `Condition` and then calls `condition.name()` to get the string representation of the condition. This suggests that the previous implementation might have been accepting arbitrary strings as conditions, which could lead to errors if the string was not a valid condition. By changing the parameter type to `Condition`, the code ensures that only valid conditions are accepted, thus fixing a potential bug. The answer is **Buggy**.
"
jclouds,14356.json,6d657b1c90d590d38a6110f18f4683592932336d,"@@ -1,3 +1,3 @@
    public float getCpuUsed() {
-      return cpuUsed != null ? Float.parseFloat(cpuUsed.substring(9, cpuUsed.length() - 1)) : 0.0f;
+      return cpuUsed != null ? Float.parseFloat(cpuUsed.substring(0, cpuUsed.length() - 1)) : 0.0f;
    }",Buggy,"Fix bug in parsing VirtualMachine.cpuUsed
",Buggy,"The commit message indicates a bug fix in parsing `VirtualMachine.cpuUsed`. The diff modifies the substring extraction logic used to parse the `cpuUsed` string. Specifically, the starting index of the substring is changed from 9 to 0. This suggests that the original code was incorrectly extracting the CPU usage value from the string, likely skipping the beginning of the value. The corrected code now starts from the beginning of the string, which should resolve the parsing error. The answer is **Buggy**.
"
okhttp,1134.json,aed222454743ebe5724d6ad438fafed37956521e,"@@ -1,15 +1,25 @@
   @Override public void connectSocket(Socket socket, InetSocketAddress address,
       int connectTimeout) throws IOException {
     try {
       socket.connect(address, connectTimeout);
     } catch (AssertionError e) {
       if (Util.isAndroidGetsocknameError(e)) throw new IOException(e);
       throw e;
     } catch (SecurityException e) {
       // Before android 4.3, socket.connect could throw a SecurityException
       // if opening a socket resulted in an EACCES error.
       IOException ioException = new IOException(""Exception in connect"");
       ioException.initCause(e);
       throw ioException;
+    } catch (ClassCastException e) {
+      // On android 8.0, socket.connect throws a ClassCastException due to a bug
+      // see https://issuetracker.google.com/issues/63649622
+      if (Build.VERSION.SDK_INT == 26) {
+        IOException ioException = new IOException(""Exception in connect"");
+        ioException.initCause(e);
+        throw ioException;
+      } else {
+        throw e;
+      }
     }
   }",Buggy,"Add workaround for https://issuetracker.google.com/issues/63649622 (#3624)

* Add workaround for https://issuetracker.google.com/issues/63649622

This fixes #3438

* Rethrowing the Android O bug CCE as IOException
",Buggy,"The commit message indicates a workaround for a specific issue on Android 8.0 (API level 26), identified by the issue tracker link. The code adds a `catch` block for `ClassCastException`. Inside this block, it checks if the Android SDK version is 26. If it is, it wraps the `ClassCastException` in an `IOException` and rethrows it. Otherwise, it rethrows the original `ClassCastException`. This is clearly a workaround for a bug in Android 8.0 where `socket.connect` throws a `ClassCastException`. The answer is **Buggy**.
"
okhttp,55.json,8bce6897f9f7e0b16508c462665e4f9d26d92e39,"@@ -1,34 +1,34 @@
   private static Request transformRequest(HttpRequest request) {
     Request.Builder builder = new Request.Builder();
 
     RequestLine requestLine = request.getRequestLine();
     String method = requestLine.getMethod();
     builder.url(requestLine.getUri());
 
     String contentType = null;
     for (Header header : request.getAllHeaders()) {
       String name = header.getName();
-      if (""Content-Type"".equals(name)) {
+      if (""Content-Type"".equalsIgnoreCase(name)) {
         contentType = header.getValue();
       } else {
         builder.header(name, header.getValue());
       }
     }
 
     RequestBody body = null;
     if (request instanceof HttpEntityEnclosingRequest) {
       HttpEntity entity = ((HttpEntityEnclosingRequest) request).getEntity();
       if (entity != null) {
         // Wrap the entity in a custom Body which takes care of the content, length, and type.
         body = new HttpEntityBody(entity, contentType);
 
         Header encoding = entity.getContentEncoding();
         if (encoding != null) {
           builder.header(encoding.getName(), encoding.getValue());
         }
       }
     }
     builder.method(method, body);
 
     return builder.build();
   }",Buggy,"fixed a bug that content type is compared by case sensitive
",Buggy,"The commit message indicates a bug fix related to case-sensitive content type comparison. The code diff changes the comparison of the header name ""Content-Type"" from case-sensitive `equals` to case-insensitive `equalsIgnoreCase`. This confirms that the bug fix addresses the issue described in the commit message. The answer is **Buggy**."
okhttp,1015.json,8b17ab4e65b21c6480a38c383a16899f9ffabbcc,"@@ -1,39 +1,42 @@
   public void streamFailed(IOException e) {
     Socket socket;
     Connection releasedConnection;
     boolean noNewStreams = false;
 
     synchronized (connectionPool) {
       if (e instanceof StreamResetException) {
-        StreamResetException streamResetException = (StreamResetException) e;
-        if (streamResetException.errorCode == ErrorCode.REFUSED_STREAM) {
+        ErrorCode errorCode = ((StreamResetException) e).errorCode;
+        if (errorCode == ErrorCode.REFUSED_STREAM) {
+          // Retry REFUSED_STREAM errors once on the same connection.
           refusedStreamCount++;
-        }
-        // On HTTP/2 stream errors, retry REFUSED_STREAM errors once on the same connection. All
-        // other errors must be retried on a new connection.
-        if (streamResetException.errorCode != ErrorCode.REFUSED_STREAM || refusedStreamCount > 1) {
+          if (refusedStreamCount > 1) {
+            noNewStreams = true;
+            route = null;
+          }
+        } else if (errorCode != ErrorCode.CANCEL) {
+          // Keep the connection for CANCEL errors. Everything else wants a fresh connection.
           noNewStreams = true;
           route = null;
         }
       } else if (connection != null
           && (!connection.isMultiplexed() || e instanceof ConnectionShutdownException)) {
         noNewStreams = true;
 
         // If this route hasn't completed a call, avoid it for new connections.
         if (connection.successCount == 0) {
           if (route != null && e != null) {
             routeSelector.connectFailed(route, e);
           }
           route = null;
         }
       }
       releasedConnection = connection;
       socket = deallocate(noNewStreams, false, true);
       if (connection != null || !reportedAcquired) releasedConnection = null;
     }
 
     closeQuietly(socket);
     if (releasedConnection != null) {
       eventListener.connectionReleased(call, releasedConnection);
     }
   }",Buggy,"Fix stream cancel error stopping reused connection
",Buggy,"The commit message indicates a fix for a stream cancel error that was preventing connection reuse. The code changes within the `streamFailed` method specifically address the `ErrorCode.CANCEL`. The original code appears to have been treating `CANCEL` errors the same as other stream errors, leading to the connection being marked as unusable and not reused. The updated code now explicitly keeps the connection alive for `CANCEL` errors, allowing for connection reuse. This is a bug fix because the previous behavior was not the intended behavior and was causing unnecessary connection churn. The answer is **Buggy**."
okhttp,2062.json,a87147e65424df05b7d7829555eef1220dbf4633,"@@ -1,18 +1,18 @@
     public Response response(Request request, DiskLruCache.Snapshot snapshot) {
       String contentType = responseHeaders.get(""Content-Type"");
       String contentLength = responseHeaders.get(""Content-Length"");
       Request cacheRequest = new Request.Builder()
           .url(url)
-          .method(message, null)
+          .method(requestMethod, null)
           .headers(varyHeaders)
           .build();
       return new Response.Builder()
           .request(cacheRequest)
           .protocol(protocol)
           .code(code)
           .message(message)
           .headers(responseHeaders)
           .body(new CacheResponseBody(snapshot, contentType, contentLength))
           .handshake(handshake)
           .build();
     }",Buggy,"Fix a bug where the cacheResponse's request method was wrong.

We were returning the message (like 'OK') rather than the method
(like 'GET'). Ugh.
",Buggy,"The commit message indicates a bug fix related to the `cacheResponse`'s request method. The diff shows that the `method` in the `Request.Builder` was incorrectly using the `message` field (likely the response message like ""OK"") instead of the `requestMethod` field (like ""GET""). The change replaces `message` with `requestMethod`, which aligns with the commit message and fixes the bug. The answer is **Buggy**."
openmrs-core,30.json,35fbc63bd8566c1b7800fcfdd1e34a6640ef0bd4,"@@ -1,30 +1,36 @@
 	protected File getFile(HttpServletRequest request) {
 		
 		String path = request.getPathInfo();
 		
 		Module module = ModuleUtil.getModuleForPath(path);
 		if (module == null) {
 			log.warn(""No module handles the path: "" + path);
 			return null;
 		}
 		
 		String relativePath = ModuleUtil.getPathForResource(module, path);
 		String realPath = getServletContext().getRealPath("""") + MODULE_PATH + module.getModuleIdAsPath() + ""/resources""
 		        + relativePath;
 		
 		//if in dev mode, load resources from the development directory
 		File devDir = ModuleUtil.getDevelopmentDirectory(module.getModuleId());
 		if (devDir != null) {
 			realPath = devDir.getAbsolutePath() + ""/omod/target/classes/web/module/resources"" + relativePath;
 		}
 		
 		realPath = realPath.replace(""/"", File.separator);
 		
 		File f = new File(realPath);
 		if (!f.exists()) {
+			if (isJstlFile(path)) {
+				f =  new File(realPath + "".withjstl"");
+				if (f.exists()) {
+					return f;
+				}
+			}
 			log.warn(""No file with path '"" + realPath + ""' exists for module '"" + module.getModuleId() + ""'"");
 			return null;
 		}
 		
 		return f;
 	}",Buggy,Fixing jslt file load error,Buggy,"The commit message indicates a fix for a JSLT file load error. The code diff shows that when a file is not found, the code now checks if it's a JSTL file and tries to load a file with the "".withjstl"" extension. This suggests that the original code was failing to load JSTL files correctly, and this change addresses that issue. The answer is **Buggy**."
openmrs-core,6620.json,6c5b1205526a53e2327f802adf31e1f4e340d93f,"@@ -1,20 +1,20 @@
 	public PatientIdentifier savePatientIdentifier(PatientIdentifier patientIdentifier) throws APIException {
 		//if the argument or the following required fields are not specified
 		PatientIdentifierType.LocationBehavior locationBehavior = null;
 		if (patientIdentifier != null) {
-			patientIdentifier.getIdentifierType().getLocationBehavior();
+			locationBehavior = patientIdentifier.getIdentifierType().getLocationBehavior();
 		}
 		
 		if (patientIdentifier == null
 		        || patientIdentifier.getPatient() == null
 		        || patientIdentifier.getIdentifierType() == null
 		        || StringUtils.isBlank(patientIdentifier.getIdentifier())
 		        || (locationBehavior == PatientIdentifierType.LocationBehavior.REQUIRED && patientIdentifier.getLocation() == null))
 			throw new APIException(""PatientIdentifier argument or one of its required fields is null or invalid"");
 		if (patientIdentifier.getPatientIdentifierId() == null) {
 			Context.requirePrivilege(PrivilegeConstants.ADD_PATIENT_IDENTIFIERS);
 		} else
 			Context.requirePrivilege(PrivilegeConstants.EDIT_PATIENT_IDENTIFIERS);
 		
 		return dao.savePatientIdentifier(patientIdentifier);
 	}",Buggy,"Fixing forgotten assignment bug for:
PatientService.savePatientIdentifier still requires location -
TRUNK-4056",Buggy,"The commit message indicates a fix for a ""forgotten assignment bug"" related to `PatientService.savePatientIdentifier` and location requirements. The diff shows that the code now assigns the result of `patientIdentifier.getIdentifierType().getLocationBehavior()` to the `locationBehavior` variable. Previously, the result of this method call was discarded, leading to the bug. The subsequent checks now correctly use the `locationBehavior` variable to determine if a location is required. This confirms that the change addresses the bug described in the commit message. The answer is **Buggy**."
openmrs-core,4909.json,837c70069a4bfca920ed92a7e73da0dbead6aff0,"@@ -1,6 +1,3 @@
 	public void purgeConceptClass(ConceptClass cc) throws DAOException  {
-		sessionFactory.getCurrentSession().createQuery(""delete from ConceptClass where concept_class_id = :c"")
-					.setInteger(""c"", cc.getConceptClassId())
-					.executeUpdate();			
 		sessionFactory.getCurrentSession().delete(cc);
 	}",Buggy,"Fixing conceptClass deletion error and fieldAnswer errors found by cmack

git-svn-id: http://svn.openmrs.org/openmrs/trunk@4553 5bac5841-c719-aa4e-b3fe-cce5062f897a
",Buggy,"The commit message indicates fixes for ""conceptClass deletion error"" and ""fieldAnswer errors"". The diff shows a change in the `purgeConceptClass` method. The original code used a direct HQL query to delete a `ConceptClass` based on its ID. The modified code uses `sessionFactory.getCurrentSession().delete(cc)` which is a more object-oriented way to delete the `ConceptClass` object. The original code might have failed to properly handle relationships or cascading deletes, leading to errors. The change to using `sessionFactory.getCurrentSession().delete(cc)` suggests a fix for such issues. The answer is **Buggy**."
openmrs-core,7646.json,f6e02ac804e140765a89e0f983c794be1b3ffa14,"@@ -1,127 +1,127 @@
 	public void execute(Database database) throws CustomChangeException {
 		JdbcConnection connection = (JdbcConnection) database.getConnection();
 		Map<String, HashSet<Integer>> duplicates = new HashMap<String, HashSet<Integer>>();
 		Statement stmt = null;
 		PreparedStatement pStmt = null;
 		ResultSet rs = null;
 		boolean autoCommit = true;
 		try {
 			// set auto commit mode to false for UPDATE action
 			autoCommit = connection.getAutoCommit();
 			connection.setAutoCommit(false);
 			stmt = connection.createStatement();
-			rs = stmt.executeQuery(""SELECT * FROM location_attribute_type""
+			rs = stmt.executeQuery(""SELECT * FROM location_attribute_type ""
 			        + ""INNER JOIN (SELECT name FROM location_attribute_type GROUP BY name HAVING count(name) > 1) ""
 			        + ""dup ON location_attribute_type.name = dup.name"");
 			Integer id = null;
 			String name = null;
 			
 			while (rs.next()) {
 				id = rs.getInt(""location_attribute_type_id"");
 				name = rs.getString(""name"");
 				if (duplicates.get(name) == null) {
 					HashSet<Integer> results = new HashSet<Integer>();
 					results.add(id);
 					duplicates.put(name, results);
 				} else {
 					HashSet<Integer> results = duplicates.get(name);
 					results.add(id);
 				}
 			}
 			
 			Iterator it2 = duplicates.entrySet().iterator();
 			while (it2.hasNext()) {
 				Map.Entry pairs = (Map.Entry) it2.next();
 				HashSet values = (HashSet) pairs.getValue();
 				List<Integer> duplicateNames = new ArrayList<Integer>(values);
 				int duplicateNameId = 1;
 				for (int i = 1; i < duplicateNames.size(); i++) {
 					String newName = pairs.getKey() + ""_"" + duplicateNameId;
 					List<List<Object>> duplicateResult = null;
 					boolean duplicateName = false;
 					Connection con = DatabaseUpdater.getConnection();
 					do {
 						String sqlValidatorString = ""select * from location_attribute_type where name = '"" + newName + ""'"";
 						duplicateResult = DatabaseUtil.executeSQL(con, sqlValidatorString, true);
 						if (!duplicateResult.isEmpty()) {
 							duplicateNameId += 1;
 							newName = pairs.getKey() + ""_"" + duplicateNameId;
 							duplicateName = true;
 						} else {
 							duplicateName = false;
 						}
 					} while (duplicateName);
 					pStmt = connection
 					        .prepareStatement(""update location_attribute_type set name = ?, changed_by = ?, date_changed = ? where location_attribute_type_id = ?"");
 					if (!duplicateResult.isEmpty()) {
 						pStmt.setString(1, newName);
 					}
 					pStmt.setString(1, newName);
 					pStmt.setInt(2, DatabaseUpdater.getAuthenticatedUserId());
 					
 					Calendar cal = Calendar.getInstance();
 					Date date = new Date(cal.getTimeInMillis());
 					
 					pStmt.setDate(3, date);
 					pStmt.setInt(4, duplicateNames.get(i));
 					duplicateNameId += 1;
 					
 					pStmt.executeUpdate();
 				}
 			}
 		}
 		catch (BatchUpdateException e) {
 			log.warn(""Error generated while processsing batch insert"", e);
 			try {
 				log.debug(""Rolling back batch"", e);
 				connection.rollback();
 			}
 			catch (Exception rbe) {
 				log.warn(""Error generated while rolling back batch insert"", e);
 			}
 			// marks the changeset as a failed one
 			throw new CustomChangeException(""Failed to update one or more duplicate LocationAttributeType names"", e);
 		}
 		catch (Exception e) {
 			throw new CustomChangeException(""Error while updating duplicate LocationAttributeType object names"", e);
 		}
 		finally {
 			// reset to auto commit mode
 			try {
 				connection.commit();
 				connection.setAutoCommit(autoCommit);
 			}
 			catch (DatabaseException e) {
 				log.warn(""Failed to reset auto commit back to true"", e);
 			}
 			
 			if (rs != null) {
 				try {
 					rs.close();
 				}
 				catch (SQLException e) {
 					log.warn(""Failed to close the resultset object"");
 				}
 			}
 			
 			if (stmt != null) {
 				try {
 					stmt.close();
 				}
 				catch (SQLException e) {
 					log
 					        .warn(""Failed to close the select statement used to identify duplicate LocationAttributeType object names"");
 				}
 			}
 			
 			if (pStmt != null) {
 				try {
 					pStmt.close();
 				}
 				catch (SQLException e) {
 					log
 					        .warn(""Failed to close the prepared statement used to update duplicate LocationAttributeType object names"");
 				}
 			}
 		}
 	}",Buggy,Fixing liquibase upgrade error for: TRUNK-4334,Buggy,"The commit message indicates a fix for a Liquibase upgrade error, specifically related to TRUNK-4334. The code changes involve identifying and renaming duplicate entries in the `location_attribute_type` table. The code includes error handling with `try-catch` blocks to manage potential exceptions during database operations, including `BatchUpdateException` and general `Exception`. There's also a rollback mechanism in case of batch update failures. The renaming logic includes checking for existing names to avoid conflicts. The presence of error handling, rollback, and specific logic to address duplicate names strongly suggests a bug fix. The answer is **Buggy**."
openmrs-core,6353.json,95b3ab3de2d8ec488d0ca0fcbcb1e8969d491313,"@@ -1,21 +1,28 @@
 	private Class datatypeClassHandled(Type t) {
 		if (t instanceof ParameterizedType) {
 			ParameterizedType pt = (ParameterizedType) t;
 			Type first = pt.getActualTypeArguments()[0];
 			if (first instanceof Class && CustomDatatype.class.isAssignableFrom((Class) first)) {
 				return (Class) first;
 			} else {
 				return datatypeClassHandled(pt.getRawType());
 			}
 			
 		} else if (t instanceof Class) {
+			Type genericSuperclass = ((Class) t).getGenericSuperclass();
+			if (genericSuperclass != null) {
+				Class ret = datatypeClassHandled(genericSuperclass);
+				if (ret != null) {
+					return ret;
+				}
+			}
 			for (Type candidate : ((Class) t).getGenericInterfaces()) {
 				Class ret = datatypeClassHandled(candidate);
 				if (ret != null) {
 					return ret;
 				}
 			}
 		}
 		
 		return null;
 	}",Buggy,"Fixed bug where a custom datatype handler that extends a generic abstract class can't be found  -TRUNK-3499
",Buggy,The commit message indicates a bug fix related to finding custom datatype handlers that extend generic abstract classes. The code diff shows that the `datatypeClassHandled` method has been modified to handle the case where a class `t` has a generic superclass. The code now recursively calls `datatypeClassHandled` on the generic superclass to find the custom datatype handler. This change addresses the bug described in the commit message. The answer is **Buggy**.
openmrs-core,7456.json,4fff45254e5332a1b608a88d6daa83e7a330daf7,"@@ -1,9 +1,9 @@
 	public static byte[] getSavedSecretKey() {
-		String keyText = Context.getRuntimeProperties().getProperty(OpenmrsConstants.ENCRYPTION_VECTOR_RUNTIME_PROPERTY,
-		    OpenmrsConstants.ENCRYPTION_VECTOR_DEFAULT);
+		String keyText = Context.getRuntimeProperties().getProperty(OpenmrsConstants.ENCRYPTION_KEY_RUNTIME_PROPERTY,
+		    OpenmrsConstants.ENCRYPTION_KEY_DEFAULT);
 		
 		if (StringUtils.hasText(keyText))
 			return Base64.decode(keyText);
 		
 		throw new APIException(""no encryption secret key found"");
 	}",Buggy,"TRUNK-1919 - fixed glaring flaw where I pulled in the encryption.vector runtime property instead of encryption.key

git-svn-id: http://svn.openmrs.org/openmrs/trunk@18684 5bac5841-c719-aa4e-b3fe-cce5062f897a
",Buggy,"The commit message indicates a fix for a ""glaring flaw"" where the wrong runtime property was being used for encryption. The diff shows that the code was indeed using `OpenmrsConstants.ENCRYPTION_VECTOR_RUNTIME_PROPERTY` but has been corrected to use `OpenmrsConstants.ENCRYPTION_KEY_RUNTIME_PROPERTY`. This directly addresses the issue described in the commit message. The answer is **Buggy**."
openmrs-core,6446.json,119e3fa6338f5401f0a75873e5a6526b8b23cd80,"@@ -1,10 +1,10 @@
 	private void checkPrivileges(Role role) {
-		Collection<Privilege> privileges = role.getPrivileges();
-		
 		Optional.ofNullable(role.getPrivileges())
 		.map(p -> p.stream().filter(pr -> !Context.hasPrivilege(pr.getPrivilege())).map(Privilege::getPrivilege)
 			.distinct().collect(Collectors.joining("", "")))
 		.ifPresent(missing -> {
-			throw new APIException(""Role.you.must.have.privileges: "", new Object[] { missing });
+			if (StringUtils.isNotBlank(missing)) {
+				throw new APIException(""Role.you.must.have.privileges"", new Object[] { missing });
+			}
 		});
     }",Buggy,"Fix for TRUNK-5726

* Only throws exception if actually missing permissions are found
* Correct error string
* Adds unit tests
",Buggy,"The commit message indicates a fix for TRUNK-5726, which involves throwing an exception only if missing permissions are found, correcting the error string, and adding unit tests. The code diff shows that the exception is now thrown only if the `missing` string (containing the list of missing privileges) is not blank. Additionally, the error string has been corrected from ""Role.you.must.have.privileges: "" to ""Role.you.must.have.privileges"". These changes directly address the issues described in the commit message, indicating a bug fix. The answer is **Buggy**."
openmrs-core,2762.json,29eae1d038bc6036af4903c9b43f42a157b10a1a,"@@ -1,49 +1,49 @@
 	private Provider getProvider(PV1 pv1) throws HL7Exception {
 		XCN hl7Provider = pv1.getAttendingDoctor(0);
 		Provider provider = null;
 		String id = hl7Provider.getIDNumber().getValue();
 		String assignAuth = hl7Provider.getAssigningAuthority().getUniversalID().getValue();
 		String type = hl7Provider.getAssigningAuthority().getUniversalIDType().getValue();
 		String errorMessage = """";
 		if (StringUtils.hasText(id)) {
 			String specificErrorMsg = """";
 			if (OpenmrsUtil.nullSafeEquals(""L"", type)) {
 				if (HL7Constants.PROVIDER_ASSIGNING_AUTH_PROV_ID.equalsIgnoreCase(assignAuth)) {
 					try {
 						provider = Context.getProviderService().getProvider(Integer.valueOf(id));
 					}
 					catch (NumberFormatException e) {
 						// ignore
 					}
 					specificErrorMsg = ""with provider Id"";
 				} else if (HL7Constants.PROVIDER_ASSIGNING_AUTH_IDENTIFIER.equalsIgnoreCase(assignAuth)) {
 					provider = Context.getProviderService().getProviderByIdentifier(id);
-					specificErrorMsg = ""with provider identifier:"" + id;
+					specificErrorMsg = ""with provider identifier"";
 				} else if (HL7Constants.PROVIDER_ASSIGNING_AUTH_PROV_UUID.equalsIgnoreCase(assignAuth)) {
 					provider = Context.getProviderService().getProviderByUuid(id);
 					specificErrorMsg = ""with provider uuid"";
 				}
 			} else {
 				try {
 					Person person = Context.getPersonService().getPerson(Integer.valueOf(id));
 					Collection<Provider> providers = Context.getProviderService().getProvidersByPerson(person);
 					if (!providers.isEmpty())
 						provider = providers.iterator().next();
 				}
 				catch (NumberFormatException e) {
 					// ignore
 				}
 				specificErrorMsg = ""associated to a person with person id"";
 			}
 			
 			errorMessage = ""Could not resolve provider "" + specificErrorMsg + "":"" + id;
 		} else {
 			errorMessage = ""No unique identifier was found for the provider"";
 		}
 		
 		if (provider == null) {
 			throw new HL7Exception(errorMessage);
 		}
 		
 		return provider;
 	}",Buggy,"Follow up to ,Fixed the error messages to be more helpful when the provider can't be resolved - TRUNK-3108

git-svn-id: http://svn.openmrs.org/openmrs/trunk@26248 5bac5841-c719-aa4e-b3fe-cce5062f897a
",Buggy,"The commit message indicates a fix related to improving error messages when a provider cannot be resolved. The code changes modify the error messages to be more descriptive, including the type of identifier used (provider Id, provider identifier, provider uuid, or person id) and the actual identifier value. This makes debugging easier when provider resolution fails. The answer is **Buggy**."
openmrs-core,2762.json,dc4c547246dcf46aa2ff0dcb5e43caee7cd5f1d7,"@@ -1,38 +1,49 @@
 	private Provider getProvider(PV1 pv1) throws HL7Exception {
 		XCN hl7Provider = pv1.getAttendingDoctor(0);
 		Provider provider = null;
 		String id = hl7Provider.getIDNumber().getValue();
 		String assignAuth = hl7Provider.getAssigningAuthority().getUniversalID().getValue();
 		String type = hl7Provider.getAssigningAuthority().getUniversalIDType().getValue();
+		String errorMessage = """";
 		if (StringUtils.hasText(id)) {
+			String specificErrorMsg = """";
 			if (OpenmrsUtil.nullSafeEquals(""L"", type)) {
 				if (HL7Constants.PROVIDER_ASSIGNING_AUTH_PROV_ID.equalsIgnoreCase(assignAuth)) {
 					try {
 						provider = Context.getProviderService().getProvider(Integer.valueOf(id));
 					}
 					catch (NumberFormatException e) {
 						// ignore
 					}
+					specificErrorMsg = ""with provider Id"";
 				} else if (HL7Constants.PROVIDER_ASSIGNING_AUTH_IDENTIFIER.equalsIgnoreCase(assignAuth)) {
 					provider = Context.getProviderService().getProviderByIdentifier(id);
+					specificErrorMsg = ""with provider identifier:"" + id;
 				} else if (HL7Constants.PROVIDER_ASSIGNING_AUTH_PROV_UUID.equalsIgnoreCase(assignAuth)) {
 					provider = Context.getProviderService().getProviderByUuid(id);
+					specificErrorMsg = ""with provider uuid"";
 				}
 			} else {
 				try {
 					Person person = Context.getPersonService().getPerson(Integer.valueOf(id));
 					Collection<Provider> providers = Context.getProviderService().getProvidersByPerson(person);
 					if (!providers.isEmpty())
 						provider = providers.iterator().next();
 				}
 				catch (NumberFormatException e) {
 					// ignore
 				}
+				specificErrorMsg = ""associated to a person with person id"";
 			}
+			
+			errorMessage = ""Could not resolve provider "" + specificErrorMsg + "":"" + id;
+		} else {
+			errorMessage = ""No unique identifier was found for the provider"";
 		}
 		
-		if (provider == null)
-			throw new HL7Exception(""Could not resolve provider"");
+		if (provider == null) {
+			throw new HL7Exception(errorMessage);
+		}
 		
 		return provider;
 	}",Buggy,"Improved the error messages to be more helpful when the provider can't be resolved - TRUNK-3108

git-svn-id: http://svn.openmrs.org/openmrs/trunk@26246 5bac5841-c719-aa4e-b3fe-cce5062f897a
",Buggy,"The commit message indicates an improvement in error messages when a provider cannot be resolved. The diff shows that the code now constructs more specific error messages based on the different scenarios where the provider lookup might fail, such as invalid provider ID, identifier, UUID, or person ID. The code now throws an `HL7Exception` with a more informative error message, which helps in debugging and troubleshooting. This change addresses a lack of clarity in the original error message, which can be considered a bug or at least a significant deficiency. The answer is **Buggy**."
openmrs-core,2762.json,7932358da8194ef18be1ce23e0b72b0c4f51a63a,"@@ -1,39 +1,39 @@
 	private Provider getProvider(PV1 pv1) throws HL7Exception {
 		XCN hl7Provider = pv1.getAttendingDoctor(0);
 		Provider provider = null;
 		String id = hl7Provider.getIDNumber().getValue();
 		String assignAuth = ((HD) hl7Provider.getComponent(8)).getNamespaceID().getValue();
 		String nameTypeCode = ((ID) hl7Provider.getComponent(9)).getValue();
 		
 		if (StringUtils.hasText(id)) {
 			if (OpenmrsUtil.nullSafeEquals(""L"", nameTypeCode)) {
 				if (HL7Constants.PROVIDER_ASSIGNING_AUTH_PROV_ID.equalsIgnoreCase(assignAuth)) {
 					try {
 						provider = Context.getProviderService().getProvider(Integer.valueOf(id));
 					}
 					catch (NumberFormatException e) {
 						// ignore
 					}
 				} else if (HL7Constants.PROVIDER_ASSIGNING_AUTH_IDENTIFIER.equalsIgnoreCase(assignAuth)) {
 					provider = Context.getProviderService().getProviderByIdentifier(id);
 				} else if (HL7Constants.PROVIDER_ASSIGNING_AUTH_PROV_UUID.equalsIgnoreCase(assignAuth)) {
 					provider = Context.getProviderService().getProviderByUuid(id);
 				}
 			} else {
 				try {
 					Person person = Context.getPersonService().getPerson(Integer.valueOf(id));
 					Collection<Provider> providers = Context.getProviderService().getProvidersByPerson(person);
 					if (!providers.isEmpty())
 						provider = providers.iterator().next();
 				}
 				catch (NumberFormatException e) {
 					// ignore
 				}
 			}
 		}
 		
 		if (provider == null)
-			throw new HL7Exception(""Could not resolve provider with personId or identifier as '"" + id + ""'"");
+			throw new HL7Exception(""Could not resolve provider"");
 		
 		return provider;
 	}",Buggy,"Rephrased the error message when the provider can't be resolved when processing an HL7 message

git-svn-id: http://svn.openmrs.org/openmrs/trunk@26240 5bac5841-c719-aa4e-b3fe-cce5062f897a
",Buggy,"The commit message indicates a change in the error message when a provider cannot be resolved. The diff shows that the original error message included the provider ID, while the modified message simply states ""Could not resolve provider"". This change likely aims to avoid exposing potentially sensitive information (the provider ID) in the error message. While not directly fixing a bug in the traditional sense, it addresses a potential security or privacy concern related to information exposure. The change is related to error handling and information security. The answer is **Buggy**."
openmrs-core,6970.json,6f7de13f5e4d0b9e4d8ea553bfd92842cd0c161a,"@@ -1,19 +1,24 @@
 	public FormResource saveFormResource(FormResource formResource) throws APIException {
-		if (formResource == null) {
+	    	if (formResource == null) {
 			return null;
 		}
-		
 		// If a form resource with same name exists, replace it with current value
 		FormResource toPersist = formResource;
 		FormResource original = Context.getFormService().getFormResource(formResource.getForm(), formResource.getName());
 		if (original != null) {
 			original.setName(formResource.getName());
 			original.setValue(formResource.getValue());
 			original.setDatatypeClassname(formResource.getDatatypeClassname());
 			original.setDatatypeConfig(formResource.getDatatypeConfig());
 			original.setPreferredHandlerClassname(formResource.getPreferredHandlerClassname());
 			toPersist = original;
 		}
-		CustomDatatypeUtil.saveIfDirty(toPersist);
+		try {
+		    CustomDatatypeUtil.saveIfDirty(toPersist);
+		}
+		catch (ConstraintViolationException ex) {
+		    throw new InvalidFileTypeException(ex.getMessage(), ex);
+		}
+		
 		return dao.saveFormResource(toPersist);
 	}",Buggy,"TRUNK-4473: Fixed error occuring when user tries to upload binary files with form resource
",Buggy,"The commit message indicates a fix for an error occurring when users upload binary files with a form resource. The code diff shows that a `try-catch` block has been added around the `CustomDatatypeUtil.saveIfDirty(toPersist)` call. This block catches `ConstraintViolationException` and throws an `InvalidFileTypeException` with the original exception's message and exception itself. This change suggests that the `saveIfDirty` method was throwing a `ConstraintViolationException` when users tried to upload binary files, and this exception was not being properly handled, leading to an error. The added `try-catch` block now handles this exception and throws a more specific `InvalidFileTypeException`, which likely provides a better error message to the user. This indicates a bug fix. The answer is **Buggy**."
openmrs-core,105.json,f2d3fdd76e95f7783e530e4c3e7b4f0b7d48a852,"@@ -1,9 +1,10 @@
 	public static String escapeQuotesAndNewlines(String s) {
 		if (s == null)
 			return """";
 		
 		s = s.replace(""\"""", ""\\\"""");
+		s = s.replace(""\r\n"", ""\\r\\n"");
 		s = s.replace(""\n"", ""\\n"");
 		
 		return s;
 	}",Buggy,"Fixed javascript error on form schema designer when field descriptions had multiple lines - #594

git-svn-id: http://svn.openmrs.org/openmrs/trunk@11807 5bac5841-c719-aa4e-b3fe-cce5062f897a
",Buggy,"The commit message indicates a fix for a JavaScript error in the form schema designer related to multi-line field descriptions. The diff shows that the `escapeQuotesAndNewlines` method now replaces `\r\n` with `\\r\\n` in addition to replacing `\n` with `\\n` and escaping quotes. This change likely addresses an issue where multi-line descriptions were not being properly escaped for use in JavaScript, leading to the reported error. Therefore, the changes indicate a bug fix. The answer is **Buggy**."
hadoop,10762.json,d3949058b84c393413ffea11de5c81ab8ad2ae3c,"@@ -1,40 +1,40 @@
   void doRollback(StorageDirectory bpSd, NamespaceInfo nsInfo)
       throws IOException {
     File prevDir = bpSd.getPreviousDir();
     // regular startup if previous dir does not exist
     if (!prevDir.exists())
       return;
     // read attributes out of the VERSION file of previous directory
-    DataStorage prevInfo = new DataStorage();
+    BlockPoolSliceStorage prevInfo = new BlockPoolSliceStorage();
     prevInfo.readPreviousVersionProperties(bpSd);
 
     // We allow rollback to a state, which is either consistent with
     // the namespace state or can be further upgraded to it.
     // In another word, we can only roll back when ( storedLV >= software LV)
     // && ( DN.previousCTime <= NN.ctime)
     if (!(prevInfo.getLayoutVersion() >= HdfsConstants.LAYOUT_VERSION && 
         prevInfo.getCTime() <= nsInfo.getCTime())) { // cannot rollback
       throw new InconsistentFSStateException(bpSd.getRoot(),
           ""Cannot rollback to a newer state.\nDatanode previous state: LV = ""
               + prevInfo.getLayoutVersion() + "" CTime = "" + prevInfo.getCTime()
               + "" is newer than the namespace state: LV = ""
               + nsInfo.getLayoutVersion() + "" CTime = "" + nsInfo.getCTime());
     }
     
     LOG.info(""Rolling back storage directory "" + bpSd.getRoot()
         + "".\n   target LV = "" + nsInfo.getLayoutVersion()
         + ""; target CTime = "" + nsInfo.getCTime());
     File tmpDir = bpSd.getRemovedTmp();
     assert !tmpDir.exists() : ""removed.tmp directory must not exist."";
     // 1. rename current to tmp
     File curDir = bpSd.getCurrentDir();
     assert curDir.exists() : ""Current directory must exist."";
     rename(curDir, tmpDir);
     
     // 2. rename previous to current
     rename(prevDir, curDir);
     
     // 3. delete removed.tmp dir
     deleteDir(tmpDir);
     LOG.info(""Rollback of "" + bpSd.getRoot() + "" is complete"");
   }",Buggy,"HDFS-3970. Fix bug causing rollback of HDFS upgrade to result in bad VERSION file. Contributed by Vinay and Andrew Wang.


git-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1430037 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix related to HDFS upgrade rollback, specifically concerning a bad VERSION file. The code diff modifies the `doRollback` method in `BlockPoolSliceStorage`. It changes `DataStorage prevInfo = new DataStorage();` to `BlockPoolSliceStorage prevInfo = new BlockPoolSliceStorage();`. This change suggests that the original code was using the wrong class to read the previous version properties, potentially leading to incorrect or missing information in the VERSION file after a rollback. The check `prevInfo.getLayoutVersion() >= HdfsConstants.LAYOUT_VERSION && prevInfo.getCTime() <= nsInfo.getCTime()` ensures that the rollback is only performed when the previous state is compatible with the namespace state. Therefore, the changes in the code are directly related to fixing the bug described in the commit message. The answer is **Buggy**."
hadoop,78812.json,64f537da0a216055d5b6eae49a2a9129189cacbf,"@@ -1,18 +1,26 @@
-    public FileStatus[] globStatus(Path pathPattern, PathFilter filter)
+    public FileStatus[] globStatus(final Path pathPattern, final PathFilter filter)
       throws IOException {
+      
       String filename = pathPattern.toUri().getPath();
+      
       List<String> filePatterns = GlobExpander.expand(filename);
       if (filePatterns.size() == 1) {
-        return globStatusInternal(pathPattern, filter);
+        Path p = fixRelativePart(pathPattern);
+        FileSystem fs = getFSofPath(p);
+        URI uri = fs.getUri();
+        return globStatusInternal(uri, p, filter);
       } else {
         List<FileStatus> results = new ArrayList<FileStatus>();
         for (String filePattern : filePatterns) {
-          FileStatus[] files = 
-                      globStatusInternal(new Path(filePattern), filter);
+          Path p = new Path(filePattern);
+          p = fixRelativePart(p);
+          FileSystem fs = getFSofPath(p);
+          URI uri = fs.getUri();
+          FileStatus[] files = globStatusInternal(uri, p, filter);
           for (FileStatus file : files) {
             results.add(file);
           }
         }
         return results.toArray(new FileStatus[results.size()]);
       }
     }",Buggy,"HADOOP-6286. Fix bugs in related to URI handling in glob methods in FileContext. Contributed by Boris Shkolnik.


git-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@822805 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for bugs related to URI handling in glob methods within FileContext. The diff modifies the `globStatus` method to handle multiple file patterns. It introduces `fixRelativePart` and `getFSofPath` methods, suggesting adjustments to path resolution and file system access. The code now iterates through expanded file patterns, obtaining the FileSystem URI for each, and then calls `globStatusInternal`. This suggests a bug fix related to how paths and URIs were previously handled in the globbing process, especially when dealing with multiple patterns or relative paths. The answer is **Buggy**."
hadoop,37278.json,3a154f75ed85d864b3ffd35818992418f2b6aa59,"@@ -1,21 +1,25 @@
   public RLESparseResourceAllocation getRangeOverlapping(long start, long end) {
     readLock.lock();
     try {
       NavigableMap<Long, Resource> a = this.getCumulative();
 
       if (a != null && !a.isEmpty()) {
         // include the portion of previous entry that overlaps start
         if (start > a.firstKey()) {
           long previous = a.floorKey(start);
           a = a.tailMap(previous, true);
         }
-        a = a.headMap(end, true);
+
+        if (end < a.lastKey()) {
+          a = a.headMap(end, true);
+        }
+
       }
       RLESparseResourceAllocation ret =
           new RLESparseResourceAllocation(a, resourceCalculator);
       return ret;
     } finally {
       readLock.unlock();
     }
 
   }",Buggy,"YARN-4525. Fix bug in RLESparseResourceAllocation.getRangeOverlapping(). (Ishai Menache and Carlo Curino via asuresh)
",Buggy,"The commit message indicates a bug fix in `RLESparseResourceAllocation.getRangeOverlapping()`. The diff modifies the logic for creating the submap `a`. Specifically, it adds a condition to check if `end` is less than the last key in the map `a`. If it is, it truncates the map `a` using `headMap`. This suggests that the original code might not have correctly handled cases where the requested range (`start` to `end`) was within the existing allocation map, potentially leading to incorrect results or out-of-bounds exceptions. The added condition ensures that the submap `a` only contains the relevant entries within the specified range. The answer is **Buggy**."
hadoop,3337.json,46cbce9af1272ce0eb6e300f96a1a8d4b08e23e3,"@@ -1,18 +1,18 @@
   private static LocatedBlock toLocatedBlock(final Map<?, ?> m) throws IOException {
     if (m == null) {
       return null;
     }
 
     final ExtendedBlock b = toExtendedBlock((Map<?, ?>)m.get(""block""));
     final DatanodeInfo[] locations = toDatanodeInfoArray(
         (Object[])m.get(""locations""));
     final long startOffset = (Long)m.get(""startOffset"");
     final boolean isCorrupt = (Boolean)m.get(""isCorrupt"");
     final DatanodeInfo[] cachedLocations = toDatanodeInfoArray(
         (Object[])m.get(""cachedLocations""));
 
     final LocatedBlock locatedblock = new LocatedBlock(b, locations,
-        startOffset, isCorrupt, cachedLocations);
+        null, null, startOffset, isCorrupt, cachedLocations);
     locatedblock.setBlockToken(toBlockToken((Map<?, ?>)m.get(""blockToken"")));
     return locatedblock;
   }",Buggy,"HDFS-5508. Fix compilation error after merge. (Contributed by szetszwo)

git-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1541352 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for a compilation error after a merge. The diff shows a change in the `LocatedBlock` constructor call within the `toLocatedBlock` method. It appears that two `null` arguments have been added to the `LocatedBlock` constructor. This suggests that the constructor signature of `LocatedBlock` was changed in a different branch (likely HDFS-2832), and this commit fixes the compilation error caused by the mismatch in the number of arguments after merging the branches. The addition of `null` values is likely a temporary fix to address the immediate compilation issue. The answer is **Buggy**."
hadoop,33132.json,c6ea28c480940d1f95cabe3af452dce197c6669d,"@@ -1,23 +1,24 @@
   public List<PrivilegedOperation> bootstrap(Configuration configuration)
       throws ResourceHandlerException {
     Set<Device> availableDevices = null;
     try {
       availableDevices = devicePlugin.getDevices();
     } catch (Exception e) {
       throw new ResourceHandlerException(""Exception thrown from""
           + "" plugin's \""getDevices\"""" + e.getMessage());
     }
     /**
      * We won't fail the NM if plugin returns invalid value here.
      * */
     if (availableDevices == null) {
       LOG.error(""Bootstrap "" + resourceName
           + "" failed. Null value got from plugin's getDevices method"");
       return null;
     }
     // Add device set. Here we trust the plugin's return value
     deviceMappingManager.addDeviceSet(resourceName, availableDevices);
-    // TODO: Init cgroups
-
+    // Init cgroups
+    this.cGroupsHandler.initializeCGroupController(
+        CGroupsHandler.CGroupController.DEVICES);
     return null;
   }",Buggy,"YARN-9331. [YARN-8851] Fix a bug that lacking cgroup initialization when bootstrap DeviceResourceHandlerImpl. Contributed by Zhankun Tang.
",Buggy,"The commit message indicates a bug fix related to missing cgroup initialization when bootstrapping DeviceResourceHandlerImpl. The diff shows that the code now initializes cgroups within the bootstrap method, specifically calling `this.cGroupsHandler.initializeCGroupController(CGroupsHandler.CGroupController.DEVICES);`. This confirms that the bug fix described in the commit message is implemented in the code. The answer is **Buggy**."
hadoop,22809.json,b524501d4f4b48edeb02901114087f3b5f57691f,"@@ -1,3 +1,3 @@
     public URI[] getCacheFiles() throws IOException {
-      return reduceContext.getCacheArchives();
+      return reduceContext.getCacheFiles();
     }",Buggy,"MAPREDUCE-5685. Fixed a bug with JobContext getCacheFiles API inside the WrappedReducer class. Contributed by Yi Song.


git-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1554320 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix in the `JobContext`'s `getCacheFiles` API within the `WrappedReducer` class. The diff shows that the code was changed from calling `reduceContext.getCacheArchives()` to `reduceContext.getCacheFiles()`. This suggests that the original code was incorrectly retrieving cache archives instead of cache files, which constitutes a bug. The answer is **Buggy**.
"
hadoop,12133.json,68d5dfdc78d121e89eeae4e577d670028a14a955,"@@ -1,90 +1,90 @@
   void startDataNode(List<StorageLocation> dataDirectories,
                      SecureResources resources
                      ) throws IOException {
 
     // settings global for all BPs in the Data Node
     this.secureResources = resources;
     synchronized (this) {
       this.dataDirs = dataDirectories;
     }
     this.dnConf = new DNConf(this);
     checkSecureConfig(dnConf, getConf(), resources);
 
     if (dnConf.maxLockedMemory > 0) {
       if (!NativeIO.POSIX.getCacheManipulator().verifyCanMlock()) {
         throw new RuntimeException(String.format(
             ""Cannot start datanode because the configured max locked memory"" +
             "" size (%s) is greater than zero and native code is not available."",
             DFS_DATANODE_MAX_LOCKED_MEMORY_KEY));
       }
       if (Path.WINDOWS) {
         NativeIO.Windows.extendWorkingSetSize(dnConf.maxLockedMemory);
       } else {
         long ulimit = NativeIO.POSIX.getCacheManipulator().getMemlockLimit();
         if (dnConf.maxLockedMemory > ulimit) {
           throw new RuntimeException(String.format(
             ""Cannot start datanode because the configured max locked memory"" +
             "" size (%s) of %d bytes is more than the datanode's available"" +
             "" RLIMIT_MEMLOCK ulimit of %d bytes."",
             DFS_DATANODE_MAX_LOCKED_MEMORY_KEY,
             dnConf.maxLockedMemory,
             ulimit));
         }
       }
     }
     LOG.info(""Starting DataNode with maxLockedMemory = {}"",
         dnConf.maxLockedMemory);
 
     int volFailuresTolerated = dnConf.getVolFailuresTolerated();
     int volsConfigured = dnConf.getVolsConfigured();
     if (volFailuresTolerated < MAX_VOLUME_FAILURE_TOLERATED_LIMIT
         || volFailuresTolerated >= volsConfigured) {
       throw new DiskErrorException(""Invalid value configured for ""
           + ""dfs.datanode.failed.volumes.tolerated - "" + volFailuresTolerated
-          + "". Value configured is either greater than -1 or >= ""
+          + "". Value configured is either less than -1 or >= ""
           + ""to the number of configured volumes ("" + volsConfigured + "")."");
     }
 
     storage = new DataStorage();
     
     // global DN settings
     registerMXBean();
     initDataXceiver();
     startInfoServer();
     pauseMonitor = new JvmPauseMonitor();
     pauseMonitor.init(getConf());
     pauseMonitor.start();
   
     // BlockPoolTokenSecretManager is required to create ipc server.
     this.blockPoolTokenSecretManager = new BlockPoolTokenSecretManager();
 
     // Login is done by now. Set the DN user name.
     dnUserName = UserGroupInformation.getCurrentUser().getUserName();
     LOG.info(""dnUserName = {}"", dnUserName);
     LOG.info(""supergroup = {}"", supergroup);
     initIpcServer();
 
     metrics = DataNodeMetrics.create(getConf(), getDisplayName());
     peerMetrics = dnConf.peerStatsEnabled ?
         DataNodePeerMetrics.create(getDisplayName()) : null;
     metrics.getJvmMetrics().setPauseMonitor(pauseMonitor);
 
     ecWorker = new ErasureCodingWorker(getConf(), this);
     blockRecoveryWorker = new BlockRecoveryWorker(this);
 
     blockPoolManager = new BlockPoolManager(this);
     blockPoolManager.refreshNamenodes(getConf());
 
     // Create the ReadaheadPool from the DataNode context so we can
     // exit without having to explicitly shutdown its thread pool.
     readaheadPool = ReadaheadPool.getInstance();
     saslClient = new SaslDataTransferClient(dnConf.getConf(),
         dnConf.saslPropsResolver, dnConf.trustedChannelResolver);
     saslServer = new SaslDataTransferServer(dnConf, blockPoolTokenSecretManager);
     startMetricsLogger();
 
     if (dnConf.diskStatsEnabled) {
       diskMetrics = new DataNodeDiskMetrics(this,
           dnConf.outliersReportIntervalMs);
     }
   }",Buggy,"HDFS-14056. Fix error messages in HDFS-12716. Contributed by Ayush Saxena.
",Buggy,"The commit message indicates a fix to error messages. The diff modifies the error message in the DiskErrorException. The original message said ""greater than -1"", which is incorrect, as the value should be ""less than -1"". The change corrects this error, indicating a bug fix. The answer is **Buggy**."
hadoop,33173.json,a457a8951a1b35f06811c40443ca44bb9c698c30,"@@ -1,46 +1,48 @@
   public boolean initPlugin(Configuration conf) {
     this.aliasMap = new HashMap<>();
     if (this.initialized) {
       return true;
     }
     // Find the proper toolchain, mainly aocl
     String pluginDefaultBinaryName = getDefaultBinaryName();
     String pathToExecutable = conf.get(YarnConfiguration.NM_FPGA_PATH_TO_EXEC,
         """");
     if (pathToExecutable.isEmpty()) {
       pathToExecutable = pluginDefaultBinaryName;
     }
     // Validate file existence
     File binaryPath = new File(pathToExecutable);
     if (!binaryPath.exists()) {
       // When binary not exist, fail
       LOG.warn(""Failed to find FPGA discoverer executable configured in "" +
           YarnConfiguration.NM_FPGA_PATH_TO_EXEC +
           "", please check! Try default path"");
       pathToExecutable = pluginDefaultBinaryName;
       // Try to find in plugin's preferred path
       String pluginDefaultPreferredPath = getDefaultPathToExecutable();
       if (null == pluginDefaultPreferredPath) {
         LOG.warn(""Failed to find FPGA discoverer executable from system environment "" +
             getDefaultPathEnvName()+
             "", please check your environment!"");
       } else {
         binaryPath = new File(pluginDefaultPreferredPath + ""/bin"", pluginDefaultBinaryName);
         if (binaryPath.exists()) {
-          pathToExecutable = pluginDefaultPreferredPath;
+          pathToExecutable = binaryPath.getAbsolutePath();
+          LOG.info(""Succeed in finding FPGA discoverer executable: "" +
+              pathToExecutable);
         } else {
           pathToExecutable = pluginDefaultBinaryName;
           LOG.warn(""Failed to find FPGA discoverer executable in "" +
               pluginDefaultPreferredPath + "", file doesn't exists! Use default binary"" + pathToExecutable);
         }
       }
     }
     setPathToExecutable(pathToExecutable);
     if (!diagnose(10*1000)) {
       LOG.warn(""Intel FPGA for OpenCL diagnose failed!"");
       this.initialized = false;
     } else {
       this.initialized = true;
     }
     return this.initialized;
   }",Buggy,"YARN-8456. Fix a configuration handling bug when user leave FPGA discover executable path configuration default but set OpenCL SDK path environment variable. (Zhankun Tang via wangda)

Change-Id: Iff150ea98ba0c60d448474fd940eb121afce6965
",Buggy,"The commit message indicates a bug fix related to configuration handling when the user leaves the FPGA discover executable path configuration default but sets the OpenCL SDK path environment variable.

The code changes address the scenario where the configured path to the FPGA discover executable is empty. The original code would then default to `pluginDefaultBinaryName`. However, if the binary doesn't exist at that default location, the code would attempt to find it in the plugin's preferred path (derived from the environment variable). The bug seems to be that when the binary *is* found in the preferred path, the code was assigning `pluginDefaultPreferredPath` to `pathToExecutable` instead of the full path to the executable. This has been corrected by assigning `binaryPath.getAbsolutePath()` to `pathToExecutable`.

The changes ensure that the correct path to the executable is used, resolving the configuration handling bug described in the commit message.

The answer is **Buggy**."
hadoop,16528.json,c3ca348b81bdf6aa0857a1d4d140c6c9d64be490,"@@ -1,29 +1,29 @@
   public void checkSuperuserPrivilege() throws  AccessControlException {
 
     // Try to get the ugi in the RPC call.
     UserGroupInformation ugi = null;
     try {
       ugi = NameNode.getRemoteUser();
     } catch (IOException e) {
       // Ignore as we catch it afterwards
     }
     if (ugi == null) {
       LOG.error(""Cannot get the remote user name"");
       throw new AccessControlException(""Cannot get the remote user name"");
     }
 
     // Is this by the Router user itself?
-    if (ugi.getUserName().equals(superUser)) {
+    if (ugi.getShortUserName().equals(superUser)) {
       return;
     }
 
     // Is the user a member of the super group?
     List<String> groups = Arrays.asList(ugi.getGroupNames());
     if (groups.contains(superGroup)) {
       return;
     }
 
     // Not a superuser
     throw new AccessControlException(
         ugi.getUserName() + "" is not a super user"");
   }",Buggy,"HDFS-14620. RBF: Fix 'not a super user' error when disabling a namespace in kerberos with superuser principal. Contributed by luhuachao.
",Buggy,"The commit message indicates a fix for a ""not a super user"" error in a Kerberos environment when disabling a namespace with a superuser principal. The code diff modifies the `checkSuperuserPrivilege` method. Specifically, it changes `ugi.getUserName()` to `ugi.getShortUserName()` when comparing the user name with the `superUser`. This suggests that the original code was using the full principal name (e.g., user/host@REALM) instead of the short user name (e.g., user) for the superuser check. This could lead to a superuser being incorrectly denied access if the full principal name didn't exactly match the configured `superUser`. The change to `getShortUserName()` corrects this, aligning with the commit message's description of fixing a superuser privilege error. The answer is **Buggy**.
"
hadoop,75002.json,c8abf5f20a7ca802e3e7c93c8c5d260902cb4052,"@@ -1,37 +1,44 @@
-  private int init(String[] args) throws IOException {
+  protected int init(String[] args) throws IOException {
+    // no args should print the help message
+    if (0 == args.length) {
+      printCredShellUsage();
+      ToolRunner.printGenericCommandUsage(System.err);
+      return 1;
+    }
+
     for (int i = 0; i < args.length; i++) { // parse command line
       if (args[i].equals(""create"")) {
         String alias = args[++i];
         command = new CreateCommand(alias);
         if (alias.equals(""-help"")) {
           printCredShellUsage();
-          return -1;
+          return 0;
         }
       } else if (args[i].equals(""delete"")) {
         String alias = args[++i];
         command = new DeleteCommand(alias);
         if (alias.equals(""-help"")) {
           printCredShellUsage();
-          return -1;
+          return 0;
         }
       } else if (args[i].equals(""list"")) {
         command = new ListCommand();
       } else if (args[i].equals(""-provider"")) {
         userSuppliedProvider = true;
         getConf().set(CredentialProviderFactory.CREDENTIAL_PROVIDER_PATH, 
             args[++i]);
       } else if (args[i].equals(""-i"") || (args[i].equals(""-interactive""))) {
         interactive = true;
       } else if (args[i].equals(""-v"") || (args[i].equals(""-value""))) {
         value = args[++i];
       } else if (args[i].equals(""-help"")) {
         printCredShellUsage();
-        return -1;
+        return 0;
       } else {
         printCredShellUsage();
         ToolRunner.printGenericCommandUsage(System.err);
-        return -1;
+        return 1;
       }
     }
     return 0;
   }",Buggy,"HADOOP-10927. Fix CredentialShell help behavior and error codes. Contributed by Josh Elser.

git-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1615827 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for the `CredentialShell`'s help behavior and error codes. The diff shows several changes related to argument parsing and help message display. Specifically, it addresses the case where no arguments are provided, printing the usage and returning an error code. It also modifies the return codes when the `-help` option is used or when an invalid argument is encountered. The return codes are changed from -1 to 0 or 1, depending on the situation. These changes directly relate to correcting the help behavior and error codes as stated in the commit message. The answer is **Buggy**."
hadoop,22351.json,1d915238a6a06d09e1789532994f00f496bd969c,"@@ -1,3 +1,3 @@
     public URI[] getCacheFiles() throws IOException {
-      return mapContext.getCacheArchives();
+      return mapContext.getCacheFiles();
     }",Buggy,"MAPREDUCE-5385. Fixed a bug with JobContext getCacheFiles API. Contributed by Omkar Vinit Joshi.


git-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1508595 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix in the `JobContext`'s `getCacheFiles` API. The diff shows that the method was incorrectly calling `mapContext.getCacheArchives()` instead of `mapContext.getCacheFiles()`. This correction aligns perfectly with the commit message, indicating a bug where the wrong set of cached resources was being retrieved. The answer is **Buggy**.
"
hadoop,8777.json,3b5ea8750202ad9ed0e297d92a90d6dc772ce12a,"@@ -1,45 +1,45 @@
   FSImageStorageInspector readAndInspectDirs()
       throws IOException {
     Integer layoutVersion = null;
     boolean multipleLV = false;
     StringBuilder layoutVersions = new StringBuilder();
 
     // First determine what range of layout versions we're going to inspect
     for (Iterator<StorageDirectory> it = dirIterator();
          it.hasNext();) {
       StorageDirectory sd = it.next();
       if (!sd.getVersionFile().exists()) {
         FSImage.LOG.warn(""Storage directory "" + sd + "" contains no VERSION file. Skipping..."");
         continue;
       }
       readProperties(sd); // sets layoutVersion
       int lv = getLayoutVersion();
       if (layoutVersion == null) {
         layoutVersion = Integer.valueOf(lv);
       } else if (!layoutVersion.equals(lv)) {
         multipleLV = true;
       }
       layoutVersions.append(""("").append(sd.getRoot()).append("", "").append(lv).append("") "");
     }
     
     if (layoutVersion == null) {
       throw new IOException(""No storage directories contained VERSION information"");
     }
     if (multipleLV) {            
       throw new IOException(
-          ""Storage directories containe multiple layout versions: ""
+          ""Storage directories contain multiple layout versions: ""
               + layoutVersions);
     }
     // If the storage directories are with the new layout version
     // (ie edits_<txnid>) then use the new inspector, which will ignore
     // the old format dirs.
     FSImageStorageInspector inspector;
     if (LayoutVersion.supports(Feature.TXID_BASED_LAYOUT, getLayoutVersion())) {
       inspector = new FSImageTransactionalStorageInspector();
     } else {
       inspector = new FSImagePreTransactionalStorageInspector();
     }
     
     inspectStorageDirs(inspector);
     return inspector;
   }",Buggy,"HDFS-3629. Fix the typo in the error message about inconsistent storage layout version. Contributed by Brandon Li. (harsh)


git-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1359905 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for a typo in an error message related to inconsistent storage layout versions. The diff shows a change from ""containe"" to ""contain"" in the error message string. This is a direct correction of a typographical error, which constitutes a bug fix. The answer is **Buggy**."
hadoop,35650.json,c4d7bbda5cb3ceaf54a022f3cf97109e8e190d71,"@@ -1,8 +1,8 @@
-  protected void serviceStart() throws Exception {
-    super.serviceStart();
+  protected void serviceInit(Configuration conf) throws Exception {
+    super.serviceInit(conf);
+    getDispatcher().register(SystemMetricsEventType.class,
+        new TimelineV2EventHandler());
     publishContainerMetrics = getConfig().getBoolean(
         YarnConfiguration.RM_PUBLISH_CONTAINER_METRICS_ENABLED,
         YarnConfiguration.DEFAULT_RM_PUBLISH_CONTAINER_METRICS_ENABLED);
-    getDispatcher().register(SystemMetricsEventType.class,
-        new TimelineV2EventHandler());
   }",Buggy,"YARN-4460. [Bug fix] RM fails to start when SMP is enabled. (Li Lu via Varun Saxena)
",Buggy,"The commit message indicates a bug fix where the Resource Manager (RM) fails to start when Secure Mode Proxy (SMP) is enabled. The code diff moves the registration of the `TimelineV2EventHandler` from `serviceStart()` to `serviceInit()`. This change likely addresses a timing issue or dependency problem that prevented the RM from starting correctly in certain configurations, such as when SMP is enabled. Moving the registration to `serviceInit()` ensures that the event handler is registered earlier in the startup process, potentially resolving the bug. The answer is **Buggy**."
hadoop,34713.json,3d00c8f3942da931150de79f42cd4913bf751123,"@@ -1,3 +1,3 @@
   public Service[] getServices() {
-    return nodeManagerServices;
+    return NODE_MANAGER_SERVICES;
   }",Buggy,"Made fixes for whitespace errors and checstyle warnings before merge.
",NotBuggy,"The commit message indicates fixes for whitespace errors and checkstyle warnings. The diff replaces `nodeManagerServices` with `NODE_MANAGER_SERVICES`. This change suggests a correction to adhere to coding standards, likely related to variable naming conventions (e.g., using a constant instead of a mutable variable). This is not a bug fix. The answer is **NotBuggy**.
"
hadoop,71639.json,9591765040b85927ac69179ab46383eef9560a28,"@@ -1,25 +1,44 @@
   private byte remoteLookup(Message response, Name name, int type,
       int iterations) {
+    // If retrieving the root zone, query for NS record type
+    if (name.toString().equals(""."")) {
+      type = Type.NS;
+    }
+
+    // Always add any CNAMEs to the response first
+    if (type != Type.CNAME) {
+      Record[] cnameAnswers = getRecords(name, Type.CNAME);
+      if (cnameAnswers != null) {
+        for (Record cnameR : cnameAnswers) {
+          if (!response.findRecord(cnameR)) {
+            response.addRecord(cnameR, Section.ANSWER);
+          }
+        }
+      }
+    }
+
     // Forward lookup to primary DNS servers
     Record[] answers = getRecords(name, type);
     try {
       for (Record r : answers) {
-        if (r.getType() == Type.SOA) {
-          response.addRecord(r, Section.AUTHORITY);
-        } else {
-          response.addRecord(r, Section.ANSWER);
+        if (!response.findRecord(r)) {
+          if (r.getType() == Type.SOA) {
+            response.addRecord(r, Section.AUTHORITY);
+          } else {
+            response.addRecord(r, Section.ANSWER);
+          }
         }
         if (r.getType() == Type.CNAME) {
           Name cname = ((CNAMERecord) r).getAlias();
           if (iterations < 6) {
-            remoteLookup(response, cname, Type.CNAME, iterations + 1);
+            remoteLookup(response, cname, type, iterations + 1);
           }
         }
       }
     } catch (NullPointerException e) {
       return Rcode.NXDOMAIN;
     } catch (Throwable e) {
       return Rcode.SERVFAIL;
     }
     return Rcode.NOERROR;
   }",Buggy,"YARN-8410.  Fixed a bug in A record lookup by CNAME record.
            Contributed by Shane Kumpf
",Buggy,"The commit message indicates a bug fix related to A record lookup by CNAME record. The code changes include:
1. Adding a condition to query for NS record type if retrieving the root zone.
2. Adding CNAME records to the response before forwarding the lookup.
3. Ensuring records are not added to the response if they already exist.
4. Modifying the recursive call to `remoteLookup` to pass the original `type` instead of `Type.CNAME`.

The most significant change is the modification of the recursive call `remoteLookup(response, cname, type, iterations + 1)`. Previously, it was calling `remoteLookup` with `Type.CNAME`, which would only look for CNAME records. Now, it passes the original `type`, which could be `Type.A`, `Type.AAAA`, etc. This fixes the bug where the A record lookup by CNAME record was not working correctly.

The other changes improve the logic and prevent duplicate records from being added to the response.

Therefore, the changes indicate a bug fix. The answer is **Buggy**."
hadoop,4984.json,076ecf79ca38cbb908f54ea58d985d24486ceefc,"@@ -1,4 +1,4 @@
   public String toString() {
-    return getClass().getSimpleName() + "": "" + snapshotId + "" (post=""
-        + (posteriorDiff == null? null: posteriorDiff.snapshotId) + "")"";
+    return getClass().getSimpleName() + "": "" + this.getSnapshotId() + "" (post=""
+        + (posteriorDiff == null? null: posteriorDiff.getSnapshotId()) + "")"";
   }",Buggy,"HDFS-5726. Fix compilation error in AbstractINodeDiff for JDK7. Contributed by Jing Zhao.

git-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1556433 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for a compilation error related to JDK7 in `AbstractINodeDiff`. The diff replaces direct field access `snapshotId` with the getter method `getSnapshotId()`. This change likely addresses an issue where the field `snapshotId` might not be directly accessible or visible in certain contexts under JDK7, potentially due to changes in access modifiers or inheritance. Using the getter ensures proper access and resolves the compilation error. The answer is **Buggy**."
hadoop,43166.json,23f394240e1568a38025e63e9dc0842e8c5235f7,"@@ -1,61 +1,61 @@
   public int initiateUpgrade(Service service) throws YarnException,
       IOException {
     boolean upgradeEnabled = getConfig().getBoolean(
         YARN_SERVICE_UPGRADE_ENABLED,
         YARN_SERVICE_UPGRADE_ENABLED_DEFAULT);
     if (!upgradeEnabled) {
       throw new YarnException(ErrorStrings.SERVICE_UPGRADE_DISABLED);
     }
     Service persistedService =
         ServiceApiUtil.loadService(fs, service.getName());
     if (!StringUtils.isEmpty(persistedService.getId())) {
       cachedAppInfo.put(persistedService.getName(), new AppInfo(
           ApplicationId.fromString(persistedService.getId()),
           persistedService.getKerberosPrincipal().getPrincipalName()));
     }
 
     if (persistedService.getVersion().equals(service.getVersion())) {
       String message =
           service.getName() + "" is already at version "" + service.getVersion()
               + "". There is nothing to upgrade."";
       LOG.error(message);
       throw new YarnException(message);
     }
 
     Service liveService = getStatus(service.getName());
     if (!liveService.getState().equals(ServiceState.STABLE)) {
       String message = service.getName() + "" is at "" +
           liveService.getState()
-          + "" state, upgrade can not be invoked when service is STABLE."";
+          + "" state and upgrade can only be initiated when service is STABLE."";
       LOG.error(message);
       throw new YarnException(message);
     }
 
     Path serviceUpgradeDir = checkAppNotExistOnHdfs(service, true);
     ServiceApiUtil.validateAndResolveService(service, fs, getConfig());
     ServiceApiUtil.createDirAndPersistApp(fs, serviceUpgradeDir, service);
 
     ApplicationReport appReport =
         yarnClient.getApplicationReport(getAppId(service.getName()));
     if (StringUtils.isEmpty(appReport.getHost())) {
       throw new YarnException(service.getName() + "" AM hostname is empty"");
     }
     ClientAMProtocol proxy = createAMProxy(service.getName(), appReport);
 
     UpgradeServiceRequestProto.Builder requestBuilder =
         UpgradeServiceRequestProto.newBuilder();
     requestBuilder.setVersion(service.getVersion());
     if (service.getState().equals(ServiceState.UPGRADING_AUTO_FINALIZE)) {
       requestBuilder.setAutoFinalize(true);
     }
     UpgradeServiceResponseProto responseProto = proxy.upgrade(
         requestBuilder.build());
     if (responseProto.hasError()) {
       LOG.error(""Service {} upgrade to version {} failed because {}"",
           service.getName(), service.getVersion(), responseProto.getError());
       throw new YarnException(""Failed to upgrade service "" + service.getName()
           + "" to version "" + service.getVersion() + "" because "" +
           responseProto.getError());
     }
     return EXIT_SUCCESS;
   }",Buggy,"YARN-8610.  Fixed initiate upgrade error message.
            Contributed by Chandni Singh
",Buggy,"The commit message indicates a fix to an error message in the `initiateUpgrade` method. The diff shows a change in the error message when the service is not in a STABLE state. Specifically, the message ""upgrade can not be invoked when service is STABLE"" is changed to ""upgrade can only be initiated when service is STABLE."" This correction clarifies the condition under which an upgrade can be initiated. The change addresses a bug in the error message, making it more accurate and informative. The answer is **Buggy**."
hadoop,81000.json,1ddb48872f6a4985f4d0baadbb183899226cff68,"@@ -1,9 +1,9 @@
   public long getTimeDuration(String name, long defaultValue,
       TimeUnit defaultUnit, TimeUnit returnUnit) {
     String vStr = get(name);
     if (null == vStr) {
-      return defaultValue;
+      return returnUnit.convert(defaultValue, defaultUnit);
     } else {
       return getTimeDurationHelper(name, vStr, defaultUnit, returnUnit);
     }
   }",Buggy,"HADOOP-16265. Fix bug causing Configuration#getTimeDuration to use incorrect units when the default value is used. Contributed by starphin.
",Buggy,"The commit message indicates a bug fix in `Configuration#getTimeDuration` where incorrect units were being used when the default value was returned. The diff shows that when the configuration value is null (i.e., the property is not set), the original code simply returned the `defaultValue` without converting it to the `returnUnit`. The corrected code now converts the `defaultValue` from `defaultUnit` to `returnUnit` before returning it. This aligns with the commit message and fixes the bug. The answer is **Buggy**.
"
hadoop,10220.json,3335e502446b1542fc99c0c831e0542e53eac6f1,"@@ -1,11 +1,11 @@
-  void shutDownAll() throws InterruptedException {
-    BPOfferService[] bposArray = this.getAllNamenodeThreads();
-    
-    for (BPOfferService bpos : bposArray) {
-      bpos.stop(); //interrupts the threads
-    }
-    //now join
-    for (BPOfferService bpos : bposArray) {
-      bpos.join();
+  void shutDownAll(BPOfferService[] bposArray) throws InterruptedException {
+    if (bposArray != null) {
+      for (BPOfferService bpos : bposArray) {
+        bpos.stop(); //interrupts the threads
+      }
+      //now join
+      for (BPOfferService bpos : bposArray) {
+        bpos.join();
+      }
     }
   }",Buggy,"HDFS-3616. Fix a ConcurrentModificationException bug that BP actor threads may not be shutdown properly in DataNode.  Contributed by Jing Zhao 


git-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1402608 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for a ConcurrentModificationException related to DataNode's BP actor threads not being shut down properly. The diff introduces a null check for the `bposArray` before iterating and attempting to stop/join the threads. This prevents a potential NullPointerException if `getAllNamenodeThreads()` returns null, which could lead to the ConcurrentModificationException described in the commit message. The change directly addresses the bug mentioned in the commit message. The answer is **Buggy**."
hadoop,26655.json,977d7cc5b947682478ad7b38bd442f0efa1cd204,"@@ -1,45 +1,38 @@
     protected void setup(JobImpl job) throws IOException {
 
       String oldJobIDString = job.oldJobId.toString();
       String user = 
         UserGroupInformation.getCurrentUser().getShortUserName();
       Path path = MRApps.getStagingAreaDir(job.conf, user);
       if(LOG.isDebugEnabled()) {
         LOG.debug(""startJobs: parent="" + path + "" child="" + oldJobIDString);
       }
 
       job.remoteJobSubmitDir =
           FileSystem.get(job.conf).makeQualified(
               new Path(path, oldJobIDString));
       job.remoteJobConfFile =
           new Path(job.remoteJobSubmitDir, MRJobConfig.JOB_CONF_FILE);
 
       // Prepare the TaskAttemptListener server for authentication of Containers
       // TaskAttemptListener gets the information via jobTokenSecretManager.
       JobTokenIdentifier identifier =
           new JobTokenIdentifier(new Text(oldJobIDString));
       job.jobToken =
           new Token<JobTokenIdentifier>(identifier, job.jobTokenSecretManager);
       job.jobToken.setService(identifier.getJobId());
       // Add it to the jobTokenSecretManager so that TaskAttemptListener server
       // can authenticate containers(tasks)
       job.jobTokenSecretManager.addTokenForJob(oldJobIDString, job.jobToken);
       LOG.info(""Adding job token for "" + oldJobIDString
           + "" to jobTokenSecretManager"");
 
       // Upload the jobTokens onto the remote FS so that ContainerManager can
       // localize it to be used by the Containers(tasks)
       Credentials tokenStorage = new Credentials();
       TokenCache.setJobToken(job.jobToken, tokenStorage);
 
       if (UserGroupInformation.isSecurityEnabled()) {
         tokenStorage.addAll(job.fsTokens);
       }
-
-      Path remoteJobTokenFile =
-          new Path(job.remoteJobSubmitDir,
-              MRJobConfig.APPLICATION_TOKENS_FILE);
-      tokenStorage.writeTokenStorageFile(remoteJobTokenFile, job.conf);
-      LOG.info(""Writing back the job-token file on the remote file system:""
-          + remoteJobTokenFile.toString());
     }",Buggy,"MAPREDUCE-3233. Fixed a bug in MR Job so as to be able to restart the application on AM crash. Contributed by Mahadev Konar.


git-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1187669 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for a bug related to restarting a MapReduce job after the Application Master (AM) crashes. The diff removes the writing of the job-token file to the remote file system. This change suggests that the previous implementation of writing the job token file was causing issues during AM restart, potentially leading to the bug described in the commit message. Therefore, the removal of this functionality is likely a bug fix. The answer is **Buggy**."
hadoop,10300.json,eb7fe1d588de903be2ff6e20384c25c184881532,"@@ -1,24 +1,26 @@
   private void breakHardlinks(File file, Block b) throws IOException {
     final FileIoProvider fileIoProvider = getFileIoProvider();
     final File tmpFile = DatanodeUtil.createFileWithExistsCheck(
         getVolume(), b, DatanodeUtil.getUnlinkTmpFile(file), fileIoProvider);
-    try (FileInputStream in = fileIoProvider.getFileInputStream(
-        getVolume(), file)) {
-      try (FileOutputStream out = fileIoProvider.getFileOutputStream(
-          getVolume(), tmpFile)) {
-        IOUtils.copyBytes(in, out, 16 * 1024);
+    try {
+      try (FileInputStream in = fileIoProvider.getFileInputStream(
+          getVolume(), file)) {
+        try (FileOutputStream out = fileIoProvider.getFileOutputStream(
+            getVolume(), tmpFile)) {
+          IOUtils.copyBytes(in, out, 16 * 1024);
+        }
       }
       if (file.length() != tmpFile.length()) {
-        throw new IOException(""Copy of file "" + file + "" size "" + file.length()+
-                              "" into file "" + tmpFile +
-                              "" resulted in a size of "" + tmpFile.length());
+        throw new IOException(""Copy of file "" + file + "" size "" + file.length()
+            + "" into file "" + tmpFile + "" resulted in a size of ""
+            + tmpFile.length());
       }
       fileIoProvider.replaceFile(getVolume(), tmpFile, file);
     } catch (IOException e) {
       if (!fileIoProvider.delete(getVolume(), tmpFile)) {
         DataNode.LOG.info(""detachFile failed to delete temporary file "" +
                           tmpFile);
       }
       throw e;
     }
   }",Buggy,"HDFS-13509. Bug fix for breakHardlinks() of ReplicaInfo/LocalReplica, and fix TestFileAppend failures on Windows. Contributed by Xiao Liang.
",Buggy,"The commit message indicates a bug fix for `breakHardlinks()` and failures on Windows. The code diff modifies the `breakHardlinks` method by adding a try-catch block around the FileInputStream and FileOutputStream resources. This change ensures that even if an exception occurs during the file copy process, the temporary file `tmpFile` is deleted. This is a bug fix because without this try-catch, the temporary file might be left behind, potentially causing issues with subsequent operations or disk space. The change addresses potential resource leaks and improves error handling. The answer is **Buggy**.
"
hadoop,62895.json,2e61ed306f1d525096a800f28546601ef585a832,"@@ -1,14 +1,14 @@
   public boolean equals(Object obj) {
     if (this == obj)
       return true;
-    if (!super.equals(obj))
+    if (obj == null)
       return false;
     if (getClass() != obj.getClass())
       return false;
     NodeId other = (NodeId) obj;
     if (!this.getHost().equals(other.getHost()))
       return false;
     if (this.getPort() != other.getPort())
       return false;
     return true;
   }",Buggy,"MAPREDUCE-3030. Fixed a bug in NodeId.equals() that was causing RM to reject all NMs. Contributed by Devaraj K.


git-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1172638 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix in the `NodeId.equals()` method that was causing the Resource Manager (RM) to reject all Node Managers (NMs). The diff shows a modification to the `equals()` method. Specifically, the `super.equals(obj)` check has been replaced with a `obj == null` check. This change likely addresses a scenario where the superclass's `equals()` implementation was causing incorrect comparisons, leading to the rejection of NMs. The new check ensures that the comparison proceeds only if the object being compared is not null. This is a bug fix. The answer is **Buggy**."
facebook-android-sdk,2616.json,e3a111ea10f0646633777dd8cf5ed06838bf2a62,"@@ -1,13 +1,14 @@
     static float[] maxPool1D(float[] x, int rows, int cols, int pool_size) {
         int len = rows - pool_size + 1;
         float[] res = new float[len * cols];
 
         for (int c = 0; c < cols; c++) {
             for (int i = 0; i < len; i++) {
-                for (int r = i; r < i + pool_size; r++) {
+                res[i * cols + c] = x[i * cols + c];
+                for (int r = i + 1; r < i + pool_size; r++) {
                     res[i * cols + c] = Math.max(res[i * cols + c], x[r * cols + c]);
                 }
             }
         }
         return res;
     }",Buggy,"Fix the bug of MaxPool1D

Summary:
Previously, for code:
```
for (int r = i; r < i + pool_size; r++) {
    res[i * cols + c] = Math.max(res[i * cols + c], x[r * cols + c]);
```
will get 0 if all the number in x is smaller than 0, which would make the prediction not accurate if there is no relu() after maxpooling.

Reviewed By: ninanina0815

Differential Revision: D20881716

fbshipit-source-id: a8a50f1fb584a35780683377a10548f19ab4fede
",Buggy,"The commit message clearly states a bug fix in the `maxPool1D` function. The original code would return 0 if all input values were negative, leading to inaccurate predictions. The diff modifies the code to initialize `res[i * cols + c]` with `x[i * cols + c]` before the inner loop, ensuring that the maximum value is correctly calculated even when all input values are negative. This resolves the identified bug. The answer is **Buggy**.
"
facebook-android-sdk,2859.json,6b275f68c69db30100c542919db3325837938c25,"@@ -1,5 +1,6 @@
     public void onReceive(Context context, Intent intent) {
-        if (AccessTokenManager.ACTION_CURRENT_ACCESS_TOKEN_CHANGED.equals(intent.getAction())) {
+        if (AccessTokenManager.ACTION_CURRENT_ACCESS_TOKEN_CHANGED.equals(intent.getAction())
+                && FacebookSdk.isInitialized()) {
             AccessTokenManager.getInstance().currentAccessTokenChanged();
         }
     }",Buggy,"Fixes bug with SDK Initialize on broadcast

Summary: [android sdk] Fixes bug with SDK Initialize on broadcast

Reviewed By: ct2mak

Differential Revision: D15450566

fbshipit-source-id: 7ec5a7f8f55a658959a5ac7143bd8577c57a2a74
",Buggy,"The commit message indicates a bug fix related to SDK initialization on broadcast. The code change adds a check `FacebookSdk.isInitialized()` before proceeding with `AccessTokenManager.getInstance().currentAccessTokenChanged()`. This prevents the `currentAccessTokenChanged()` method from being called before the SDK is properly initialized, which could lead to errors or unexpected behavior. Therefore, the change addresses a bug. The answer is **Buggy**.
"
facebook-android-sdk,1309.json,39a0d134e7f137fc0b7d727eb0d0f229d43f1db0,"@@ -1,20 +1,20 @@
     private String getChromePackage() {
         if (currentPackage != null) {
             return currentPackage;
         }
         Context context = loginClient.getActivity();
-        Intent serviceIntent = new Intent(CUSTOM_TABS_SERVICE_ACTION);
+        Intent serviceIntent = new Intent(CustomTabsService.ACTION_CUSTOM_TABS_CONNECTION);
         List<ResolveInfo> resolveInfos =
                 context.getPackageManager().queryIntentServices(serviceIntent, 0);
         if (resolveInfos != null) {
             Set<String> chromePackages = new HashSet<>(Arrays.asList(CHROME_PACKAGES));
             for (ResolveInfo resolveInfo : resolveInfos) {
                 ServiceInfo serviceInfo = resolveInfo.serviceInfo;
                 if (serviceInfo != null && chromePackages.contains(serviceInfo.packageName)) {
                     currentPackage = serviceInfo.packageName;
                     return currentPackage;
                 }
             }
         }
         return null;
     }",Buggy,"AndroidX Custom Tab Issue Fix (#670)

Summary:
Thanks for proposing a pull request!

To help us review the request, please complete the following:

- [ ] sign [contributor license agreement](https://developers.facebook.com/opensource/cla)
- [ ] I've ensured that all existing tests pass and added tests (when/where necessary)
- [ ] I've updated the documentation (when/where necessary) and [Changelog](CHANGELOG.md) (when/where necessary)
- [ ] I've added the proper label to this pull request (e.g. `bug` for bug fixes)

## Pull Request Details
Custom Tab has to be working for AndroidX enabled projects

## Expected Results
What do you expect to happen?
If the Facebook app is not installed, then on click of the Facebook login button in App, Custom Chrome Tab has to be open.

## Actual Results
What actually happened? Can you provide a stack trace?
If the Facebook app is not installed, then on click of Facebook login button in App, by default WebView is opening instead of Custom Chrome Tab even I have enabled the Custom Tab for my project.

## Steps to Reproduce
What are the steps necessary to reproduce this issue?
Integrate Facebook Login Button in App
Uninstall Facebook App from the Device
Make sure your project is migrated to AndroidX.
Click on the ""Login With Facebook"" button, WebView is opening instead of Custom Chrome Tab

Please merge the Pull Requests, so that we can use the Custom Chrome Tab in Facebook SDK for AndroidX Projects
Pull Request resolved: https://github.com/facebook/facebook-android-sdk/pull/670

Test Plan:
Built Hackbook with Gradle and buck and chrome custom tabs works for both
Buck: buck install hb4a. This uses Android X and is not working without this fix
Gradle: On Android studio

Went to Login > Login with custom tab to test the feature

Reviewed By: Mxiim

Differential Revision: D19603133

Pulled By: ct2mak

fbshipit-source-id: 44d2f463d2fbd3a50646dc8caf3e4f7cb02c026b
",Buggy,"The commit message clearly states a bug fix related to AndroidX Custom Tabs. The diff modifies the intent used to query for Custom Tabs services, changing it from a custom action string to `CustomTabsService.ACTION_CUSTOM_TABS_CONNECTION`. This indicates a correction in how the app identifies available Custom Tabs services, which aligns with the bug description of WebView opening instead of Custom Chrome Tab. The answer is **Buggy**."
facebook-android-sdk,2911.json,4c16e0ff5234b07ad26111a74cade26f1fecfcb0,"@@ -1,78 +1,80 @@
     public static void setAppEventExtendedDeviceInfoParameters(
             JSONObject params,
             Context appContext
     ) throws JSONException {
         JSONArray extraInfoArray = new JSONArray();
         extraInfoArray.put(EXTRA_APP_EVENTS_INFO_FORMAT_VERSION);
 
         Utility.refreshPeriodicExtendedDeviceInfo(appContext);
 
         // Application Manifest info:
         String pkgName = appContext.getPackageName();
         int versionCode = -1;
         String versionName = """";
 
         try {
             PackageInfo pi = appContext.getPackageManager().getPackageInfo(pkgName, 0);
             versionCode = pi.versionCode;
             versionName = pi.versionName;
         } catch (PackageManager.NameNotFoundException e) {
             // Swallow
         }
 
         // Application Manifest info:
         extraInfoArray.put(pkgName);
         extraInfoArray.put(versionCode);
         extraInfoArray.put(versionName);
 
         // OS/Device info
         extraInfoArray.put(Build.VERSION.RELEASE);
         extraInfoArray.put(Build.MODEL);
 
         // Locale
         Locale locale;
         try {
             locale = appContext.getResources().getConfiguration().locale;
         } catch (Exception e) {
             locale = Locale.getDefault();
         }
         extraInfoArray.put(locale.getLanguage() + ""_"" + locale.getCountry());
 
         // Time zone
         extraInfoArray.put(deviceTimezoneAbbreviation);
 
         // Carrier
         extraInfoArray.put(carrierName);
 
         // Screen dimensions
         int width = 0;
         int height = 0;
         double density = 0;
         try {
             WindowManager wm = (WindowManager) appContext.getSystemService(Context.WINDOW_SERVICE);
             if (wm != null) {
                 Display display = wm.getDefaultDisplay();
                 DisplayMetrics displayMetrics = new DisplayMetrics();
                 display.getMetrics(displayMetrics);
                 width = displayMetrics.widthPixels;
                 height = displayMetrics.heightPixels;
                 density = displayMetrics.density;
             }
         } catch (Exception e) {
             // Swallow
         }
         extraInfoArray.put(width);
         extraInfoArray.put(height);
-        extraInfoArray.put(String.format(""%.2f"", density));
+
+        final DecimalFormat df = new DecimalFormat(""#.##"");
+        extraInfoArray.put(df.format(density));
 
         // CPU Cores
         extraInfoArray.put(refreshBestGuessNumberOfCPUCores());
 
         // External Storage
         extraInfoArray.put(totalExternalStorageGB);
         extraInfoArray.put(availableExternalStorageGB);
 
         extraInfoArray.put(deviceTimeZoneName);
 
         params.put(""extinfo"", extraInfoArray.toString());
     }",Buggy,"Resolve Issue with String Format

Summary:
Should fix a problem where `String.format` crashes with:

```
Fatal Exception: java.lang.NullPointerException
Attempt to get length of null array
```

For more context, view the [bug report](https://developers.internmc.facebook.com/bugs/406495719925086/)

Reviewed By: KylinChang

Differential Revision: D14219986

fbshipit-source-id: 7d2a4ec0720deb48661fabcbcb39d861ff5b70a7
",Buggy,"The commit message indicates a fix for a `NullPointerException` in `String.format`. The diff replaces `String.format(""%.2f"", density)` with a `DecimalFormat` object. This change likely addresses a locale-specific issue where the format string `%.2f` might not be correctly interpreted, potentially leading to a `NullPointerException` if the locale's decimal separator is not handled properly by `String.format`. The use of `DecimalFormat` provides more control over the formatting process and avoids potential locale-related issues. The answer is **Buggy**."
facebook-android-sdk,3863.json,1583b645976fdcdbe41d34020b13fb6939c5bf8a,"@@ -1,5 +1,5 @@
     public final String getAccessToken() {
         synchronized (this.lock) {
-            return this.tokenInfo.getToken();
+            return (this.tokenInfo == null) ? null : this.tokenInfo.getToken();
         }
     }",Buggy,"[android-sdk] Don't add access_token parameters from a closed Session.

Summary:
Request would try to append an access_token if it could get one from a supplied Session, even if the Session
was closed. This is incorrect; it should act as if no Session was provided and let the server return an error if
appropriate.

Test Plan:
- Modified HelloFacebook to always enable the ""Pick Some Friends"" button
- Logged in
- Picked friends
- Logged out
- Picked friends, observed an error message indicating no access token was provided, as expected
- Added unit tests (and fixed one broken one), verified they ran

Revert Plan:

Reviewers: mmarucheck, mingfli, karthiks, gregschechte

Reviewed By: mmarucheck

Differential Revision: https://phabricator.fb.com/D616959

Task ID: 1852996
",Buggy,"The commit message indicates a bug where the `getAccessToken` method would attempt to retrieve a token even from a closed session, which is incorrect. The fix ensures that if the `tokenInfo` is null (likely indicating a closed session), the method returns null, preventing the incorrect access token from being appended to the request. This aligns with the provided diff, which adds a null check for `tokenInfo` before attempting to access the token. The test plan also confirms that the fix behaves as expected. The answer is **Buggy**.
"
titan,920.json,74166f9174459bcbd8b7b8d7812d6b14c90faba5,"@@ -1,15 +1,34 @@
-    public void unlock(KeyColumn kc, ConsistentKeyLockTransaction requestor) {
+    public boolean unlock(KeyColumn kc, ConsistentKeyLockTransaction requestor) {
 
-        assert locks.containsKey(kc);
-
-        AuditRecord audit = new AuditRecord(requestor, 0);
-
-        assert locks.get(kc).equals(audit);
-
-        locks.remove(kc, audit);
-
-        if (log.isTraceEnabled()) {
-            log.trace(""Local unlock succeeded: {} namespace={} txn={}"",
-                    new Object[]{kc, name, requestor});
+        if (!locks.containsKey(kc)) {
+            log.error(""Local unlock failed: no locks found for {}"", kc);
+            return false;
         }
+
+        AuditRecord unlocker = new AuditRecord(requestor, 0);
+
+        AuditRecord holder = locks.get(kc);
+
+        if (!holder.equals(unlocker)) {
+            log.error(""Local unlock of {} by {} failed: it is held by {}"",
+                    new Object[] { kc, unlocker, holder });
+            return false;
+        }
+
+        boolean removed = locks.remove(kc, unlocker);
+
+        if (removed) {
+            if (log.isTraceEnabled()) {
+                log.trace(""Local unlock succeeded: {} namespace={} txn={}"",
+                        new Object[] { kc, name, requestor });
+            }
+        } else {
+            log.warn(""Local unlock warning: lock record for {} disappeared ""
+                    + ""during removal; this suggests the lock either expired ""
+                    + ""while we were removing it, or that it was erroneously ""
+                    + ""unlocked multiple times."", kc);
+        }
+
+        // Even if !removed, we're finished unlocking, so return true
+        return true;
     }",Buggy,"Locking logging tweaks and renewal fix

* Added new logging statements and reworded existing statements in
  ConsistentKeyLockTransaction and LocalLockMediator

* LocalLockMediator#unlock() now logs errors and returns false instead
  of generating assertion failures when called on an expired or
  nonexistent lock; call sites updated accordingly

* Replaced broken unit test in LockKeyColumnValueStoreTest named
  relockExtendsLocalExpiration with a new test method called
  repeatLockingDoesNotExtendExpiration (closes #264)
",Buggy,"The commit message indicates a fix in `LocalLockMediator#unlock()` where assertion failures were replaced with error logging and a boolean return value. The diff confirms this change: assertions are removed, error logging is added for cases where the lock doesn't exist or is held by another transaction, and the method now returns a boolean indicating success or failure. This addresses a bug where improper unlocking led to assertion failures, which is a clear indication of a bug fix. The answer is **Buggy**."
titan,316.json,9b77325b51d00dd5c81ddce7b8cf312b04d7057c,"@@ -1,4 +1,9 @@
     public <O> void set(String key, O value) {
-        if (value==null) config.clearProperty(key);
-        else config.setProperty(key,value);
+        if (value==null) {
+            config.clearProperty(key);
+        } else if (Duration.class.isAssignableFrom(value.getClass())) {
+            config.setProperty(key,((Duration)value).getLength(TimeUnit.MILLISECONDS));
+        } else {
+            config.setProperty(key,value);
+        }
     }",Buggy,"Make CommonsConfiguration write Durations as ms

CommonsConfiguration read Durations as unitless positive milliseconds
expressed as a bare integer, but would serialize through the toString
behavior used for the general case.  The toString was something like
""Duration [60 ms]"", so it was asymmetrical.  Special-casing Duration
on the write path to make it symmetrical.  This fixes an error when
copying the graph's local configuration into a Hadoop configuration
object prior to launching a MapReduce-based index management job.
",Buggy,"The commit message indicates a fix for asymmetrical behavior in how Durations are read and written by CommonsConfiguration. Durations were read as unitless milliseconds but serialized with a ""Duration [60 ms]"" format. The diff modifies the `set` method to special-case Duration objects, converting them to milliseconds before setting the property. This ensures symmetrical behavior and fixes an error encountered when copying configurations. The answer is **Buggy**."
titan,3867.json,a457c6bc3a440d72e045436f8be7a93948568f2e,"@@ -1,21 +1,21 @@
     public List<SliceQuery> getQueries() {
         if (isGlobalGraphIndex()) {
             //Everything
-            return ImmutableList.of(new SliceQuery(BufferUtil.zeroBuffer(128), BufferUtil.oneBuffer(128)));
+            return ImmutableList.of(new SliceQuery(BufferUtil.zeroBuffer(1), BufferUtil.oneBuffer(128)));
         } else {
             RelationTypeIndexWrapper wrapper = (RelationTypeIndexWrapper)index;
             InternalRelationType wrappedType = wrapper.getWrappedType();
             Direction direction=null;
             for (Direction dir : Direction.values()) if (wrappedType.isUnidirected(dir)) direction=dir;
             assert direction!=null;
 
             StandardTitanTx tx = (StandardTitanTx)graph.get().buildTransaction().readOnly().start();
             try {
                 QueryContainer qc = new QueryContainer(tx);
                 qc.addQuery().type(wrappedType).direction(direction).relations();
                 return qc.getSliceQueries();
             } finally {
                 tx.rollback();
             }
         }
     }",Buggy,"Fixed bug in IndexRemoveJob. The issue was that a slicequery starting with 128 0s was used as the start slice in an attempt to get all columns. However, if the contents of the buffer are identical it compares the lengths. Hence, a staticbuffer with just a single 0 in it will be considered smaller and hence such index records werent' returned.
",Buggy,"The commit message indicates a bug in `IndexRemoveJob` related to slice queries. The bug occurred when a slice query with 128 zeros was used as the start slice, but a smaller buffer with a single zero was considered smaller, causing index records to be missed. The diff changes `BufferUtil.zeroBuffer(128)` to `BufferUtil.zeroBuffer(1)`. This change directly addresses the described bug by using a smaller zero buffer as the start slice, ensuring that the comparison logic works as intended and all relevant index records are returned. The answer is **Buggy**."
spring-boot,2459.json,ccb1eaf8eda94ae41db2a432b1781efb421c86c8,"@@ -1,3 +1,3 @@
-		public RedisFlushMode getFlushMode() {
-			return this.flushMode;
-		}

+	public HazelcastFlushMode getFlushMode() {
+		return this.flushMode;
+	}",NotBuggy,"Ensure compatibility with Spring Session module split

This commit updates Spring Session auto-configuration to ensure
compatibility with extraction of `SessionRepository` implementations into
separate Spring Session modules.

See gh-9554
",Buggy,"The commit message indicates a compatibility update related to Spring Session module splits, specifically addressing issue gh-9554. The code diff changes `RedisFlushMode` to `HazelcastFlushMode`. This suggests that the original code was incorrectly referencing `RedisFlushMode` when it should have been `HazelcastFlushMode`, likely due to a configuration issue or incorrect dependency. This correction ensures the application functions correctly with Hazelcast, indicating a bug fix. The answer is **Buggy**.
"
spring-boot,686.json,d6977ea89f526f45319fb1f57f8eb679e0f5132a,"@@ -1,13 +1,13 @@
 	public void validate() {
-		if (this.getOpaqueToken().getIntrospectionUri() != null) {
+		if (this.getOpaquetoken().getIntrospectionUri() != null) {
 			if (this.getJwt().getJwkSetUri() != null) {
 				handleError(""jwt.jwk-set-uri"");
 			}
 			if (this.getJwt().getIssuerUri() != null) {
 				handleError(""jwt.issuer-uri"");
 			}
 			if (this.getJwt().getPublicKeyLocation() != null) {
 				handleError(""jwt.public-key-location"");
 			}
 		}
 	}",NotBuggy,"Remove hyphenation of opaquetoken in OAuth2 property names

See gh-17290
",Buggy,"The commit message indicates the removal of hyphenation in ""opaquetoken"" within OAuth2 property names. The diff shows a change from `this.getOpaqueToken()` to `this.getOpaquetoken()`. This change directly reflects the commit message's intention to remove the hyphen. This is likely a bug fix because the original property name was incorrect, leading to potential configuration or access issues. The answer is **Buggy**."
spring-boot,8615.json,a6a19292745d2e70aee12feed6660ef0a14840d2,"@@ -1,12 +1,12 @@
-	private void addAstTransformations(final CompilationUnit compilationUnit) {
+	private LinkedList[] getPhaseOperations(final CompilationUnit compilationUnit) {
 		try {
 			Field field = CompilationUnit.class.getDeclaredField(""phaseOperations"");
 			field.setAccessible(true);
 			LinkedList[] phaseOperations = (LinkedList[]) field.get(compilationUnit);
-			processConversionOperations(phaseOperations[Phases.CONVERSION]);
+			return phaseOperations;
 		}
-		catch (Exception npe) {
+		catch (Exception ex) {
 			throw new IllegalStateException(
 					""Phase operations not available from compilation unit"");
 		}
 	}",NotBuggy,"Extract inner-classes from GroovyCompiler

Extract and refactor inner-classes from the GroovyCompiler to improve
code readability.
",NotBuggy,"The commit message indicates a refactoring to improve code readability by extracting inner classes from GroovyCompiler. The diff shows a change in the method `addAstTransformations` to `getPhaseOperations`. The code now returns the `phaseOperations` array instead of processing conversion operations. The exception handling is also updated. This change does not appear to be directly related to fixing a bug, but rather to reorganize the code. The answer is **NotBuggy**."
spring-boot,4024.json,2dfd916c96b8fa16c21a9dfcd1a4b9a06893fff4,"@@ -1,12 +1,10 @@
 	public CloudFoundryInfoEndpointWebExtension cloudFoundryInfoEndpointWebExtension(
 			GitProperties properties, ObjectProvider<InfoContributor> infoContributors) {
 		List<InfoContributor> contributors = infoContributors.orderedStream()
-				.map((infoContributor) -> {
-					if (infoContributor instanceof GitInfoContributor) {
-						return new GitInfoContributor(properties,
-								InfoPropertiesInfoContributor.Mode.FULL);
-					}
-					return infoContributor;
-				}).collect(Collectors.toList());
+				.map((infoContributor) -> (infoContributor instanceof GitInfoContributor)
+						? new GitInfoContributor(properties,
+								InfoPropertiesInfoContributor.Mode.FULL)
+						: infoContributor)
+				.collect(Collectors.toList());
 		return new CloudFoundryInfoEndpointWebExtension(new InfoEndpoint(contributors));
 	}",NotBuggy,"Polish
",NotBuggy,"The code changes a stream operation from a verbose lambda expression to a more concise ternary operator. This is purely a stylistic change, improving readability without altering functionality. There's no indication of a bug fix. The answer is **NotBuggy**."
spring-boot,13196.json,8a6e254465a7a11001ba0b3a6234b1a2cf9ee501,"@@ -1,10 +1,10 @@
 	private void writeDefaultValue(AsciidocBuilder builder) {
 		String defaultValue = (this.defaultValue != null) ? this.defaultValue : """";
-		defaultValue = defaultValue.replace(""\\"", ""\\\\"").replace(""|"", ""{vbar}"" + System.lineSeparator());
+		defaultValue = defaultValue.replace(""\\"", ""\\\\"");
 		if (defaultValue.isEmpty()) {
-			builder.appendln(""|"");
+			builder.appendln("""");
 		}
 		else {
-			builder.appendln(""|`+"", defaultValue, ""+`"");
+			builder.appendln(""`+"", defaultValue, ""+`"");
 		}
 	}",NotBuggy,"Avoid need to escape pipe character in reference tables
",Buggy,"The commit message indicates an effort to avoid escaping the pipe character in reference tables. The diff replaces the pipe character replacement with ""{vbar}"" and a newline with a unicode character """". This suggests a change in how the pipe character is handled to avoid the need for escaping it, likely to prevent issues with its interpretation in the context of reference tables. This could be in response to a bug where the pipe character was being incorrectly interpreted or causing formatting problems. The answer is **Buggy**."
spring-boot,11072.json,9ebe15232e20e61edc91089dd0a88d3bcf5f7524,"@@ -1,4 +1,4 @@
-	public ConfigurationMetadataRepositoryJsonBuilder withJsonResource(InputStream in)
-			throws IOException {
-		return withJsonResource(in, defaultCharset);
+	public ConfigurationMetadataRepositoryJsonBuilder withJsonResource(
+			InputStream inputStream) throws IOException {
+		return withJsonResource(inputStream, this.defaultCharset);
 	}",NotBuggy,"Polish
",NotBuggy,"The commit message ""Polish"" suggests minor improvements or refinements rather than a bug fix. The code change involves renaming the `in` parameter to `inputStream` for better readability and explicitly using `this.defaultCharset` instead of just `defaultCharset`. These changes enhance code clarity but don't address any specific bug. The answer is **NotBuggy**.
"
spring-boot,2926.json,f7032bd8d3c8bb6a721bd5860bba9a43e611561e,"@@ -1,16 +1,16 @@
 		public void process(AnnotationMetadata annotationMetadata,
 				DeferredImportSelector deferredImportSelector) {
 			Assert.state(
 					deferredImportSelector instanceof AutoConfigurationImportSelector,
 					() -> String.format(
-							""AutoConfigurationImportSelector only supports %s implementations, got %s"",
+							""Only %s implementations are supported, got %s"",
 							AutoConfigurationImportSelector.class.getSimpleName(),
 							deferredImportSelector.getClass().getName()));
 			AutoConfigurationEntry autoConfigurationEntry = ((AutoConfigurationImportSelector) deferredImportSelector)
 					.getAutoConfigurationEntry(getAutoConfigurationMetadata(),
 							annotationMetadata);
 			this.autoConfigurationEntries.add(autoConfigurationEntry);
 			for (String importClassName : autoConfigurationEntry.getConfigurations()) {
 				this.entries.putIfAbsent(importClassName, annotationMetadata);
 			}
 		}",NotBuggy,"Polish contribution

Closes gh-13963
",NotBuggy,"The commit message ""Polish contribution"" suggests improvements rather than bug fixes. The code change modifies an error message within the `process` method of an `AutoConfigurationImportSelector`. Specifically, it removes ""AutoConfigurationImportSelector only supports"" from the error message, making it more concise. This change doesn't address any logical errors or incorrect behavior; it's purely a textual refinement. Therefore, it's not a bug fix. The answer is **NotBuggy**."
spring-boot,5928.json,9bdad06c10ad40c7ef02971cb923d52fd84c8f53,"@@ -1,3 +1,3 @@
 		public String chooseServerAlias(String keyType, Principal[] issuers, Socket socket) {
-			return this.keyManager.chooseServerAlias(keyType, issuers, socket);
+			return this.delegate.chooseServerAlias(keyType, issuers, socket);
 		}",NotBuggy,"Polish Netty SSL key alias support

See gh-19197
",NotBuggy,"The commit message indicates a polish of Netty SSL key alias support, and the diff shows a change in `chooseServerAlias` method where `this.keyManager` is replaced with `this.delegate`. This suggests a change in how the server alias is chosen, possibly delegating the responsibility to another component. While it's a change, it doesn't explicitly indicate a bug fix. It could be a refactoring or improvement of the existing functionality.
The answer is **NotBuggy**."
spring-boot,4528.json,90949669be923b3b9b7692b663b49e9dee6702ff,"@@ -1,7 +1,4 @@
-	Map<String, Object> parseMap(String json,
+	protected final Map<String, Object> parseMap(String json,
 			Function<String, Map<String, Object>> parser) {
-		assert parser != null;
-
-		return trimIfStartsWith(json, START_MAP).map(parser::apply)
-				.orElseThrow(AbstractJsonParser::cannotParseJson);
+		return trimParse(json, ""{"", parser);
 	}",NotBuggy,"Polish ""refactor spring-boot JSON parser""

Polish JSON parser refactoring and remove some more duplication.

Closes gh-12428
",NotBuggy,"The commit message indicates a polish and refactoring of the JSON parser, aiming to remove duplication. The diff shows a change in the `parseMap` method, making it `protected final` and replacing the original logic with a call to `trimParse`. This suggests a consolidation of parsing logic and removal of duplicated code, aligning with the commit message. There's no explicit bug fix mentioned or implied in the commit message or the code changes. The answer is **NotBuggy**."
spring-boot,8354.json,76ad1975844550609ff58caa0ce6255be15a9cb7,"@@ -1,11 +1,12 @@
 		private void initializeServlet(Context context, String name) {
 			Container child = context.findChild(name);
 			if (child instanceof StandardWrapper) {
 				try {
-					((StandardWrapper) child).allocate();
+					StandardWrapper wrapper = (StandardWrapper) child;
+					wrapper.deallocate(wrapper.allocate());
 				}
 				catch (ServletException ex) {
 					// Continue
 				}
 			}
 		}",NotBuggy,"Deallocate servlet after forcing initialization in mappings endpoint

Previously, when using Tomcat, a call to mappings endpoint would force
the initialization of any DispatcherServlets in the context. This was
done by calling allocate on Tomcat's StandardWrapper. This left the
wrapper in a state that would cause it to block for two seconds during
shutdown as the wrapper has an outstanding allocation.

This commit immediately deallocates the servlet after it has been
allocated. This ensures that the DispatcherServlet has been initialized
while also leaving the wrapper in a state that it can shut down
immediately when asked to do so.

Closes gh-14898
",Buggy,The commit message indicates a fix for a shutdown blocking issue in Tomcat caused by the mappings endpoint forcing servlet initialization without proper deallocation. The code diff shows that the `initializeServlet` method now allocates and immediately deallocates the servlet wrapper. This ensures the servlet is initialized while preventing the wrapper from remaining in a state that blocks shutdown. The change directly addresses the problem described in the commit message. The answer is **Buggy**.
spring-boot,1398.json,685babc8295d48710b0c93861ca6b1c3e1a90d8d,"@@ -1,19 +1,20 @@
 	public ConditionEvaluationReport getDelta(ConditionEvaluationReport previousReport) {
 		ConditionEvaluationReport delta = new ConditionEvaluationReport();
-		this.outcomes.forEach((key, value) -> {
-			ConditionAndOutcomes previous = previousReport.outcomes.get(key);
+		this.outcomes.forEach((source, sourceOutcomes) -> {
+			ConditionAndOutcomes previous = previousReport.outcomes.get(source);
 			if (previous == null
-					|| previous.isFullMatch() != value.isFullMatch()) {
-				value.forEach((conditionAndOutcome) -> delta.recordConditionEvaluation(
-								key, conditionAndOutcome.getCondition(),
+					|| previous.isFullMatch() != sourceOutcomes.isFullMatch()) {
+				sourceOutcomes.forEach(
+						(conditionAndOutcome) -> delta.recordConditionEvaluation(source,
+								conditionAndOutcome.getCondition(),
 								conditionAndOutcome.getOutcome()));
 			}
 		});
 		List<String> newExclusions = new ArrayList<>(this.exclusions);
 		newExclusions.removeAll(previousReport.getExclusions());
 		delta.recordExclusions(newExclusions);
 		List<String> newUnconditionalClasses = new ArrayList<>(this.unconditionalClasses);
 		newUnconditionalClasses.removeAll(previousReport.unconditionalClasses);
 		delta.unconditionalClasses.addAll(newUnconditionalClasses);
 		return delta;
 	}",NotBuggy,"Polish ""Use lambdas for map entry iteration where possible""

Closes gh-12626
",NotBuggy,The commit message indicates a polish related to lambda usage for map entry iteration. The diff replaces `key` with `source` in the lambda expression within the `forEach` loop. This change seems to improve the readability and clarity of the code by using a more descriptive variable name. There is no indication of a bug fix in the code. The answer is **NotBuggy**.
spring-boot,1012.json,99e72664d9eda43c170d339ca7b5c5986f6b8a33,"@@ -1,11 +1,5 @@
 	protected PlatformTransactionManager createTransactionManager() {
-		PlatformTransactionManager txManager;
-		if (this.entityManagerFactory != null) {
-			txManager = new JpaTransactionManager(this.entityManagerFactory);
-		}
-		else {
-			txManager = new DataSourceTransactionManager(this.dataSource);
-		}
-		this.transactionProperties.applyTo(txManager);
-		return txManager;
+		AbstractPlatformTransactionManager transactionManager = createAppropriateTransactionManager();
+		this.properties.getTransaction().applyTo(transactionManager);
+		return transactionManager;
 	}",NotBuggy,"Polish spring transaction manager properties

Polish and update contribution so that TransactionManager properties can
be defined per technology, rather than globally.

Closes gh-7561
",NotBuggy,"The commit message indicates a polishing and update of transaction manager properties, allowing them to be defined per technology instead of globally. The code diff shows that the original logic for creating a transaction manager based on either `entityManagerFactory` or `dataSource` is replaced by a call to `createAppropriateTransactionManager()`. The transaction properties are then applied to the created transaction manager. This change suggests a refactoring and enhancement of the transaction management configuration, but it doesn't directly indicate a bug fix. The change aims to provide more granular control over transaction manager properties. The answer is **NotBuggy**.
"
spring-boot,4009.json,3f00ba3cad8c708eaa06ef599be104e077736132,"@@ -1,9 +1,10 @@
-	private Map<String, Link> getAccessibleLinks(AccessLevel accessLevel, Map<String, Link> links) {
+	private Map<String, Link> getAccessibleLinks(AccessLevel accessLevel,
+			Map<String, Link> links) {
 		if (accessLevel == null) {
 			return new LinkedHashMap<>();
 		}
 		return links.entrySet().stream()
-				.filter((e) -> e.getKey().equals(""self"")
-						|| accessLevel.isAccessAllowed(e.getKey()))
+				.filter((entry) -> entry.getKey().equals(""self"")
+						|| accessLevel.isAccessAllowed(entry.getKey()))
 				.collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));
 	}",NotBuggy,"Polish
",NotBuggy,"The commit message ""Polish"" suggests improvements to the code without fixing any specific bug. The diff involves a minor change in the lambda expression used in the `getAccessibleLinks` method. Specifically, `e` is changed to `entry`. This change improves readability and clarity, but does not fix any bug. The answer is **NotBuggy**."
spring-boot,3679.json,598e9bb842d02aba092e290045bd53d280bddbe0,"@@ -1,4 +1,4 @@
-	public WavefrontMeterRegistry wavefrontMeterRegistry(WavefrontConfig config,
+	public WavefrontMeterRegistry wavefrontMeterRegistry(WavefrontConfig wavefrontConfig,
 			Clock clock) {
-		return new WavefrontMeterRegistry(config, clock);
+		return new WavefrontMeterRegistry(wavefrontConfig, clock);
 	}",NotBuggy,"Polish
",NotBuggy,"The commit message ""Polish"" suggests general improvements or refinements rather than a specific bug fix. The diff shows a renaming of a parameter from `config` to `wavefrontConfig`. This change improves code readability and consistency by aligning the parameter name with the class name `WavefrontConfig`. It doesn't address any functional errors or logical flaws. Therefore, it's a refactoring change, not a bug fix. The answer is **NotBuggy**.
"
guava,14599.json,0c81386a239370c073572212079efc727c911d0b,"@@ -1,11 +1,11 @@
     public ImmutableTable<R, C, V> build() {
       int size = cells.size();
       switch (size) {
         case 0:
           return of();
         case 1:
-          return new SingletonImmutableTable<R, C, V>(Iterables.getOnlyElement(cells));
+          return new SingletonImmutableTable<>(Iterables.getOnlyElement(cells));
         default:
           return RegularImmutableTable.forCells(cells, rowComparator, columnComparator);
       }
     }",NotBuggy,"Use diamond operator in collect
-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=165450412
",NotBuggy,"The commit message indicates the use of the diamond operator in the `collect` method. The diff shows that the diamond operator `<>` is used when creating a `SingletonImmutableTable`. This change simplifies the code by removing redundant type information, but it doesn't directly fix a bug. It's a code style improvement. The answer is **NotBuggy**."
guava,22386.json,8894dba44923a6e06842b6696c6eb2bc08c3c7e0,"@@ -1,13 +1,13 @@
     ImmutableList<E> createAsList() {
       return new ImmutableAsList<E>() {
         @Override
         public E get(int index) {
-          return entries[index].getElement();
+          return Indexed.this.get(index);
         }
 
         @Override
-        ImmutableCollection<E> delegateCollection() {
-          return ElementSet.this;
+        Indexed<E> delegateCollection() {
+          return Indexed.this;
         }
       };
     }",NotBuggy,"Pull out a subclass of ImmutableSet for indexed implementations, which are common enough that it saves code overall.
-------------
Created by MOE: http://code.google.com/p/moe-java
MOE_MIGRATED_REVID=90549526
",NotBuggy,"The commit message indicates a refactoring to reduce code duplication by introducing a subclass for indexed implementations of `ImmutableSet`. The diff shows that the `createAsList` method now directly calls `Indexed.this.get(index)` instead of `entries[index].getElement()`. Also, the `delegateCollection` now returns `Indexed.this` instead of `ElementSet.this`. These changes suggest a structural reorganization of the code to better handle indexed sets, aligning with the commit message's intention of code savings. There's no indication of a bug fix in this change. The answer is **NotBuggy**."
guava,16888.json,afe6ff88023693d9c825b8d619dab56276fa8885,"@@ -1,3 +1,3 @@
-    Entry<E> get(int index) {
-      return getEntry(index);
+    E get(int index) {
+      return entries.get(index).getElement();
     }",NotBuggy,"Add hash flooding protection to ImmutableMultiset, falling back to a JDK HashMap backed implementation when probable flooding is detected.

While we're at it, rearchitect ImmutableMultiset on GWT to use that implementation rather than the handrolled hash table in RegularImmutableMultiset.

RELNOTES=Add hash flooding protection to ImmutableMultiset, and significantly improve its GWT impl while we're at it.

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=185173745
",Buggy,"The commit message indicates the addition of hash flooding protection to `ImmutableMultiset` and improvements to its GWT implementation. The diff shows a change in the `get` method of an unspecified class. The original code returns an `Entry<E>`, while the modified code returns `E`. This suggests a change in how elements are accessed or represented within the `ImmutableMultiset`, potentially related to the new hash flooding protection or the GWT implementation improvements. The change directly affects the return type of a method, which suggests a functional change related to the bug fix. The answer is **Buggy**."
guava,9796.json,4a6ce819542d5fe440bd5d13956bb4e46084325c,"@@ -1,3 +1,3 @@
     public Set<N> predecessors(N node) {
-      return network.successors(node); // transpose
+      return delegate().successors(node); // transpose
     }",NotBuggy,"common.graph: create (package-protected) ForwardingValueGraph and ForwardingNetwork classes, and use those (and ForwardingGraph) to simplify the implementation of the Transposed* inner classes in Graphs.

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=161856111
",NotBuggy,"The commit message indicates a refactoring to simplify the implementation of transposed graph classes using forwarding classes. The diff shows a change from `network.successors(node)` to `delegate().successors(node)`. This change suggests that the `network` field is being replaced by a `delegate()` method call, which likely returns the underlying graph being forwarded to. This is consistent with the creation of `ForwardingValueGraph` and `ForwardingNetwork` classes mentioned in the commit message. The change itself doesn't appear to be fixing a bug, but rather refactoring the code to be more maintainable and readable by using the new forwarding classes. The answer is **NotBuggy**.
"
guava,12300.json,78b54f8250067837828e9b7235d7db6f8d728a24,"@@ -1,6 +1,11 @@
   public void forEach(BiConsumer<? super K, ? super V> action) {
     checkNotNull(action);
-    for (int i = firstEntryIndex(); i >= 0; i = getSuccessor(i)) {
-      action.accept((K) keys[i], (V) values[i]);
+    @Nullable Map<K, V> delegate = delegateOrNull();
+    if (delegate != null) {
+      delegate.forEach(action);
+    } else {
+      for (int i = firstEntryIndex(); i >= 0; i = getSuccessor(i)) {
+        action.accept((K) keys[i], (V) values[i]);
+      }
     }
   }",NotBuggy,"Make Compact(Linked)HashMap fall back to a java.util.LinkedHashMap if hash flooding is detected, just as the immutable collections do.

RELNOTES=n/a

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=286604248
",Buggy,"The commit message indicates a fallback mechanism is being added to `Compact(Linked)HashMap` to handle hash flooding, similar to how immutable collections do. The diff shows that the `forEach` method now checks for a `delegateOrNull()`. If it returns a non-null delegate, the `forEach` method of the delegate is called. Otherwise, the original logic is executed. This delegate is likely a `java.util.LinkedHashMap` that is used as a fallback when hash flooding is detected. This change is a bug fix because it addresses a potential vulnerability or performance issue related to hash flooding. The answer is **Buggy**."
guava,17771.json,0c81386a239370c073572212079efc727c911d0b,"@@ -1,3 +1,3 @@
   ImmutableCollection<V> createValues() {
-    return new Values<K, V>(this);
+    return new Values<>(this);
   }",NotBuggy,"Use diamond operator in collect
-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=165450412
",NotBuggy,"The commit message indicates the use of the diamond operator in the `collect` method. The diff shows that the diamond operator `<>` is used when creating a new `Values` object, which simplifies the code by removing redundant type declarations. This change doesn't seem to be related to any bug fix. The answer is **NotBuggy**.
"
guava,3767.json,6f22af40e1526b8c194e9e36d457bcd37680c6a3,"@@ -1,3 +1,3 @@
-  protected boolean standardEquals(@Nullable Object object) {
+  protected boolean standardEquals(@NullableDecl Object object) {
     return Multisets.equalsImpl(this, object);
   }",NotBuggy,"Migrate from jsr305 @Nullable to Checker Framework @NullableDecl.
[]

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=178274875
",NotBuggy,"The commit message indicates a migration from jsr305's `@Nullable` annotation to Checker Framework's `@NullableDecl` annotation. The diff shows a simple replacement of the annotation in the `standardEquals` method. This change doesn't fix a bug but rather updates the annotation used for nullability, likely for better static analysis or compatibility with other tools. The answer is **NotBuggy**."
guava,15700.json,6f22af40e1526b8c194e9e36d457bcd37680c6a3,"@@ -1,4 +1,4 @@
-  public V remove(@Nullable Object rowKey, @Nullable Object columnKey) {
+  public V remove(@NullableDecl Object rowKey, @NullableDecl Object columnKey) {
     Map<C, V> row = Maps.safeGet(rowMap(), rowKey);
     return (row == null) ? null : Maps.safeRemove(row, columnKey);
   }",NotBuggy,"Migrate from jsr305 @Nullable to Checker Framework @NullableDecl.
[]

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=178274875
",NotBuggy,"The commit message indicates a migration from jsr305's `@Nullable` annotation to Checker Framework's `@NullableDecl` annotation. The diff shows the replacement of `@Nullable` with `@NullableDecl` in the `remove` method's parameters. This change doesn't inherently indicate a bug fix; it's more about code modernization or standardization of nullability annotations. The functionality of the code remains the same. The answer is **NotBuggy**.
"
guava,12211.json,8894dba44923a6e06842b6696c6eb2bc08c3c7e0,"@@ -1,13 +1,13 @@
     ImmutableList<E> createAsList() {
       return new ImmutableAsList<E>() {
         @Override
         public E get(int index) {
-          return entries[index].getElement();
+          return Indexed.this.get(index);
         }
 
         @Override
-        ImmutableCollection<E> delegateCollection() {
-          return ElementSet.this;
+        Indexed<E> delegateCollection() {
+          return Indexed.this;
         }
       };
     }",NotBuggy,"Pull out a subclass of ImmutableSet for indexed implementations, which are common enough that it saves code overall.
-------------
Created by MOE: http://code.google.com/p/moe-java
MOE_MIGRATED_REVID=90549526
",NotBuggy,"The commit message indicates a refactoring to reduce code duplication by extracting a subclass. The diff shows that `ElementSet.this` is replaced with `Indexed.this` and `entries[index].getElement()` is replaced with `Indexed.this.get(index)`. These changes suggest a reorganization of the code structure, likely to facilitate the use of indexed implementations. There is no indication of a bug fix. The answer is **NotBuggy**."
guava,22187.json,109421efed0515d0921da5895cd67e8c102bed0e,"@@ -1,4 +1,5 @@
   public static PairwiseEquivalence<?> instantiate(SerializationStreamReader reader)
       throws SerializationException {
+    checkGwtRpcEnabled();
     return create((Equivalence<?>) reader.readObject());
   }",NotBuggy,"Prepare for removing GWT-RPC support for Guava types by adding a system property that disables it.

When the time comes, we'll flip the default value, and users who didn't see our earlier warnings can use a system property to restore support until they can stop serializing our types.

[]

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=265490471
",NotBuggy,"The commit message indicates a preparation for removing GWT-RPC support, and the diff adds a `checkGwtRpcEnabled()` call. While the commit message doesn't explicitly mention a bug fix, the addition of `checkGwtRpcEnabled()` suggests a potential issue related to GWT-RPC support that needs to be addressed before its removal. This could be related to preventing unexpected behavior or errors when GWT-RPC is disabled. However, without more context, it's difficult to definitively say if it's a bug fix. It seems more like a preventative measure or a feature flag implementation.

The change itself doesn't seem to directly fix a bug but rather adds a check to potentially disable functionality. Therefore, it's not a bug fix in the traditional sense.

**NotBuggy**
"
guava,271.json,6f22af40e1526b8c194e9e36d457bcd37680c6a3,"@@ -1,7 +1,7 @@
-    public boolean equals(@Nullable Object obj) {
+    public boolean equals(@NullableDecl Object obj) {
       if (obj instanceof SupplierFunction) {
         SupplierFunction<?> that = (SupplierFunction<?>) obj;
         return this.supplier.equals(that.supplier);
       }
       return false;
     }",NotBuggy,"Migrate from jsr305 @Nullable to Checker Framework @NullableDecl.
[]

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=178274875
",NotBuggy,"The commit message indicates a migration from jsr305's `@Nullable` annotation to Checker Framework's `@NullableDecl` annotation. The diff shows a change in the `equals` method's parameter type from `@Nullable Object` to `@NullableDecl Object`. This change doesn't inherently indicate a bug fix. It's a migration of annotations, which is a refactoring or code maintenance task. The functionality of the code remains the same. The answer is **NotBuggy**.
"
guava,20365.json,de8eeba94bd6a12a83a05786cf2ddd00d8547b48,"@@ -1,9 +1,9 @@
     HashCode makeHash(Hasher[] hashers) {
       // TODO(user): Get rid of the ByteBuffer here?
       byte[] bytes = new byte[bits / 8];
       ByteBuffer buffer = ByteBuffer.wrap(bytes);
       for (Hasher hasher : hashers) {
         buffer.put(hasher.hash().asBytes());
       }
-      return HashCodes.fromBytes(bytes);
+      return HashCodes.fromBytesNoCopy(bytes);
     }",NotBuggy,"Expose c.g.common.hash.HashCodes and its static factory methods.
-------------
Created by MOE: http://code.google.com/p/moe-java
MOE_MIGRATED_REVID=27671928
",NotBuggy,"The commit message indicates a change in the visibility of `HashCodes` and its factory methods, suggesting an API change rather than a bug fix. The code diff replaces `HashCodes.fromBytes(bytes)` with `HashCodes.fromBytesNoCopy(bytes)`. This change suggests an optimization or a modification in how the `HashCode` is created from the byte array, potentially avoiding a copy of the byte array. Without more context, it's difficult to determine if this change is related to a bug fix. However, based on the commit message and the code change, it seems more like an API exposure and potential optimization rather than a bug fix. The answer is **NotBuggy**."
guava,17383.json,6f22af40e1526b8c194e9e36d457bcd37680c6a3,"@@ -1,3 +1,3 @@
-      public V2 apply(@Nullable V1 v1) {
+      public V2 apply(@NullableDecl V1 v1) {
         return transformer.transformEntry(key, v1);
       }",NotBuggy,"Migrate from jsr305 @Nullable to Checker Framework @NullableDecl.
[]

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=178274875
",NotBuggy,The commit message indicates a migration from jsr305's `@Nullable` annotation to Checker Framework's `@NullableDecl` annotation. The diff shows a simple replacement of `@Nullable` with `@NullableDecl`. This change doesn't fix any bug; it's a migration/refactoring task. The answer is **NotBuggy**.
guava,20839.json,8cdd8c718cceb67bd94c83fe60544ff46f9f4460,"@@ -1,3 +1,3 @@
   public static NetworkBuilder<Object, Object> undirected() {
-    return new NetworkBuilder<Object, Object>(false);
+    return new NetworkBuilder<>(false);
   }",NotBuggy,"Use diamond operator for rest.
-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=165581940
",NotBuggy,"The commit message indicates the use of the diamond operator. The diff shows the replacement of `new NetworkBuilder<Object, Object>(false)` with `new NetworkBuilder<>(false)`. This change utilizes the diamond operator to infer the generic type arguments from the context, which simplifies the code and reduces redundancy. This is a code improvement, not a bug fix. The answer is **NotBuggy**.
"
guava,1897.json,6f22af40e1526b8c194e9e36d457bcd37680c6a3,"@@ -1,24 +1,24 @@
-  public List<V> replaceValues(@Nullable K key, Iterable<? extends V> values) {
+  public List<V> replaceValues(@NullableDecl K key, Iterable<? extends V> values) {
     List<V> oldValues = getCopy(key);
     ListIterator<V> keyValues = new ValueForKeyIterator(key);
     Iterator<? extends V> newValues = values.iterator();
 
     // Replace existing values, if any.
     while (keyValues.hasNext() && newValues.hasNext()) {
       keyValues.next();
       keyValues.set(newValues.next());
     }
 
     // Remove remaining old values, if any.
     while (keyValues.hasNext()) {
       keyValues.next();
       keyValues.remove();
     }
 
     // Add remaining new values, if any.
     while (newValues.hasNext()) {
       keyValues.add(newValues.next());
     }
 
     return oldValues;
   }",NotBuggy,"Migrate from jsr305 @Nullable to Checker Framework @NullableDecl.
[]

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=178274875
",NotBuggy,The commit message indicates a migration from jsr305's `@Nullable` annotation to Checker Framework's `@NullableDecl` annotation. The diff shows that the `@Nullable` annotation has been replaced with `@NullableDecl` in the `replaceValues` method. This change is purely related to annotation migration and does not indicate a bug fix. The functionality of the code remains the same. The answer is **NotBuggy**.
pmd,1864.json,e1b12da1747de470397d4e02e01caaf4e2c81eed,"@@ -1,4 +1,12 @@
-    protected static String[] packageNamesIn(Map<String, String> params) {
-        // TODO
-        return null;
+    protected static String[] packageNamesIn(Map<PropertyDescriptorField, String> params) {
+        String[] packageNames = StringUtil.substringsOf(params.get(LEGAL_PACKAGES),
+                                                        PACKAGE_NAME_DELIMITER);
+
+        for (String name : packageNames) {
+            if (!packageNamePattern.matcher(name).matches()) {
+                throw new IllegalArgumentException(""One name is not a package: '"" + name + ""'"");
+            }
+        }
+
+        return packageNames;
     }",NotBuggy,"Some documentation
",Buggy,"The commit message ""Some documentation"" is misleading, as the diff shows a significant change in the `packageNamesIn` method. The method's signature is updated to use `PropertyDescriptorField` instead of `String` as the key in the input map. The method now extracts package names from the map, validates them against a regular expression, and throws an `IllegalArgumentException` if any name is not a valid package name. This indicates a bug fix or feature implementation related to package name validation. The original method simply returned null. The answer is **Buggy**."
pmd,4829.json,d9ad819dabcbf8eb6923cc334af48182ad27852e,"@@ -1,13 +1,13 @@
     private boolean insideLoop(AbstractNode node) {
-        Node n = node.jjtGetParent();
+        Node n = node.getParent();
 
         while (n != null) {
             if (n instanceof ASTDoLoopStatement || n instanceof ASTWhileLoopStatement
                     || n instanceof ASTForLoopStatement || n instanceof ASTForEachStatement) {
                 return true;
             }
-            n = n.jjtGetParent();
+            n = n.getParent();
         }
 
         return false;
     }",NotBuggy,"Replace usages of node methods everywhere
",NotBuggy,"The commit message indicates a replacement of node methods. The diff shows the replacement of `jjtGetParent()` with `getParent()`. This change doesn't seem to be related to any bug fix, but rather a refactoring or standardization of method calls. The answer is **NotBuggy**."
pmd,3801.json,320339240fc1d34844b1f6eb123265898a0fed4f,"@@ -1,3 +1,3 @@
     public Object visit(ASTColonSubsript node, Object data) {
-        return visit((AbstractModelicaNode) node, data);
+        return visit((ModelicaNode) node, data);
     }",NotBuggy,"[modelica] Small cleanups

* Move AbstractModelicaRule into the `rule` package
* Make the visitor interface mention only published types (ie not
AbstractModelicaNode)
* Other cleanups(eg make interfaces extend ModelicaNode)
",NotBuggy,"The commit message indicates general cleanups and refactoring. The diff shows a change in the type of the argument passed to the `visit` method, from `AbstractModelicaNode` to `ModelicaNode`. This suggests a change in the interface or class hierarchy, likely as part of the cleanup process. There is no indication of a bug fix in either the commit message or the code diff. The change seems to be related to making the visitor interface mention only published types. The answer is **NotBuggy**."
pmd,8296.json,4175460d3559266c2046a632e40e6c878ade355f,"@@ -1,38 +1,35 @@
     private boolean isMemberVisibleFromClass(Class<?> classWithMember, int modifiers, Class<?> accessingClass) {
         if (accessingClass == null) {
             return false;
         }
 
         // public members
         if (Modifier.isPublic(modifiers)) {
             return true;
         }
 
-        Package accessingPackage = accessingClass.getPackage();
         boolean areInTheSamePackage;
-        if (accessingPackage != null) {
-            areInTheSamePackage = accessingPackage.getName().startsWith(
+        if (accessingClass.getPackage() != null) {
+            areInTheSamePackage = accessingClass.getPackage().getName().startsWith(
                     classWithMember.getPackage().getName());
         } else {
-            return false;
+            return false; // if the package information is null, we can't do nothin'
         }
 
         // protected members
-        if (Modifier.isProtected(modifiers)
-                && (areInTheSamePackage || classWithMember.isAssignableFrom(accessingClass))) {
-            return true;
-        }
-
-        // package private
-        if (!(Modifier.isPrivate(modifiers) || Modifier.isPublic(modifiers) || Modifier.isProtected(modifiers))
-                && areInTheSamePackage) {
-            return true;
-        }
-
-        // private members
-        if (Modifier.isPrivate(modifiers) && classWithMember.equals(accessingClass)) {
+        if (Modifier.isProtected(modifiers)) {
+            if (areInTheSamePackage || classWithMember.isAssignableFrom(accessingClass)) {
+                return true;
+            }
+            // private members
+        } else if (Modifier.isPrivate(modifiers)) {
+            if (classWithMember.equals(accessingClass)) {
+                return true;
+            }
+            // package private members
+        } else if (areInTheSamePackage) {
             return true;
         }
 
         return false;
     }",NotBuggy,"Java, typeres: clean up code, add comments
",Buggy,"The commit message indicates code cleanup and the addition of comments. The diff refactors the logic for determining member visibility, specifically reordering the conditional checks for `protected`, `private`, and package-private members. The change in the order of checks and the simplification of conditions might address a subtle bug related to incorrect visibility determination in certain scenarios. The added comment ""if the package information is null, we can't do nothin'"" suggests a defensive check that might have been missing before.

The reordering of the conditional checks and the added comment suggest that the original logic might have had a flaw in how it handled different visibility modifiers.

Therefore, the answer is **Buggy**."
pmd,2554.json,9ade75a621f58c22e6aa2eeb71419c7901d1076d,"@@ -1,13 +1,11 @@
      private Language getLanguageByExtension(String extension) {
          Language result = null;
-         File dir = new File(""."");
-         String filename = ""file."" + extension;
 
          for (Language language : languages.values()) {
-             if (language.getFileFilter().accept(dir, filename)) {
+             if (language.getExtensions().contains(extension)) {
                  result = language;
                  break;
              }
          }
          return result;
      }",NotBuggy,"Refactor CPDGui to detect available languages dynamically
",NotBuggy,"The commit message indicates a refactoring to dynamically detect available languages in CPDGui. The code change replaces the file-based language detection with a direct check against language extensions. This eliminates the need to create a dummy file and directory, improving efficiency and simplifying the logic. The change does not appear to be related to fixing a bug. The answer is **NotBuggy**.
"
pmd,6893.json,40ce9313aef9adcbefb4769d9639751286c14a92,"@@ -1,8 +1,3 @@
     public String getOperator() {
-        if (getImage() != null) {
-            return getImage(); // <<
-        }
-
-        ASTRSIGNEDSHIFT rsshift = (ASTRSIGNEDSHIFT) jjtGetChild(1);
-        return rsshift != null ? "">>"" : "">>>"";
+        return getImage();
     }",NotBuggy,"Deprecate R[UN]SIGNEDSHIFT

No usages to report
",Buggy,The commit message indicates the deprecation of `R[UN]SIGNEDSHIFT`. The diff removes the logic that determines the operator based on the child node `ASTRSIGNEDSHIFT` and directly returns the image. This change simplifies the code and removes the dependency on the child node. The original code likely had a bug or was unnecessarily complex in determining the operator. The answer is **Buggy**.
pmd,1967.json,6b50104cb57d40e5057c169fd817b7468f187c48,"@@ -1,3 +1,3 @@
     public static <E> EnumPBuilder<E> builder(String name, Class<E> type) {
-        return new EnumPBuilder<>(type).name(name);
+        return new EnumPBuilder<>(name, type);
     }",NotBuggy,"Corrections for PR #669
",Buggy,"The commit message indicates corrections for a previous pull request. The code change modifies the `EnumPBuilder` instantiation within the `builder` method. Instead of calling the constructor with just the `type` and then setting the `name` using a setter, it now directly calls the constructor with both `name` and `type`. This suggests a potential issue with the original builder pattern implementation or constructor usage, which is now being corrected. The change directly addresses how the `EnumPBuilder` is initialized, implying a fix to its construction logic. The answer is **Buggy**.
"
pmd,8796.json,88ff3f0628f4ff6c1228771c4d9297422d7bb28a,"@@ -1,4 +1,4 @@
     public Object visit(ASTClassOrInterfaceDeclaration decl, Object data) {
-        checkCommentMeetsRequirement(data, decl, HEADER_CMT_REQUIREMENT_DESCRIPTOR);
+        checkCommentMeetsRequirement(data, decl, CLASS_CMT_REQUIREMENT_DESCRIPTOR);
         return super.visit(decl, data);
     }",NotBuggy,"[java] CommentRequired: Separate header comment from class comment and header top-level comment

Refs #1683
",NotBuggy,"The commit message indicates a change in how comments are handled, specifically separating header comments from class comments. The diff shows that the `checkCommentMeetsRequirement` method is now called with `CLASS_CMT_REQUIREMENT_DESCRIPTOR` instead of `HEADER_CMT_REQUIREMENT_DESCRIPTOR`. This suggests that the code is now checking for class-level comments instead of header comments in this specific part of the code. This change is related to the feature request of separating header comments from class comments, and it doesn't seem to be fixing a bug. The answer is **NotBuggy**."
pmd,1304.json,32b7cba79a9a8350fb3b13cbedce804ec3d24157,"@@ -1,4 +1,4 @@
         public void syntaxError(Recognizer<?, ?> recognizer, Object offendingSymbol, int line, int charPositionInLine,
-                String msg, RecognitionException ex) {
+                                String msg, RecognitionException ex) {
             throw new ANTLRSyntaxError(msg, line, charPositionInLine, ex);
         }",NotBuggy,"abstraction for cpd with antlr
",NotBuggy,"The commit message indicates an abstraction related to CPD (Copy/Paste Detector) using ANTLR. The diff shows a change in the `syntaxError` method, specifically adjusting the formatting of the method signature. While the commit message mentions ""abstraction,"" the diff itself doesn't directly reflect an abstraction being created or modified. The change is purely cosmetic, adjusting the spacing in the method declaration. There's no indication of a bug fix in the provided diff. The answer is **NotBuggy**.
"
pmd,1359.json,af5130ab0037fc9b0c6a13305d0037bf7afd88d3,"@@ -1,3 +1,3 @@
     public boolean isDeprecated() {
-        return deprecated != null && deprecated.booleanValue();
+        return deprecated != null && deprecated;
     }",NotBuggy,"Improvements and planned updates around PropertySource
",NotBuggy,"The commit message indicates general improvements and planned updates. The code change simplifies a boolean expression by directly returning the `deprecated` field, which is already a boolean. This eliminates the need for `.booleanValue()`. While it's an improvement, it doesn't directly address a bug. The answer is **NotBuggy**.
"
pmd,2224.json,785d663553072e5aeca8d8448347087e9d6c8aa3,"@@ -1,3 +1,3 @@
-    protected Object createFrom(String value) {
-        return Integer.valueOf(value);
+    protected Integer createFrom(String toParse) {
+        return Integer.valueOf(toParse);
     }",NotBuggy,"Finished refactoring properties
",NotBuggy,"The commit message indicates a refactoring of properties. The diff shows a change in the return type of the `createFrom` method from `Object` to `Integer` and renames the `value` parameter to `toParse`. This is a type refinement and parameter renaming, which are typical refactoring activities. There's no indication of a bug fix in this change. The answer is **NotBuggy**."
pmd,7422.json,d9ad819dabcbf8eb6923cc334af48182ad27852e,"@@ -1,3 +1,3 @@
     public ASTStatement getBody() {
-        return (ASTStatement) jjtGetChild(jjtGetNumChildren() - 1);
+        return (ASTStatement) getChild(getNumChildren() - 1);
     }",NotBuggy,"Replace usages of node methods everywhere
",NotBuggy,"The commit message indicates a replacement of node methods. The diff shows a change from `jjtGetChild` to `getChild`. This suggests a refactoring or code cleanup to use a different method for accessing child nodes. There is no indication of a bug fix in the traditional sense, such as correcting a logical error or handling an exception. The change appears to be a straightforward method replacement. The answer is **NotBuggy**.
"
pmd,4286.json,89fcd45f75f79f5f223e4f572ef332f75869dcef,"@@ -1,8 +1,8 @@
-    public boolean visit(AstNode node) {
-	if (parents.peek() == node) {
-	    return true;
-	} else {
-	    buildInternal(node);
-	    return false;
-	}
-    }

+	public boolean visit(AstNode node) {
+		if (parents.peek() == node) {
+			return true;
+		} else {
+			build(node);
+			return false;
+		}
+	}",NotBuggy,Next step towards compile ;-),NotBuggy,"The commit message ""Next step towards compile ;-)"" is vague. The diff replaces `buildInternal(node)` with `build(node)`. Without knowing the context of `AstNode`, `parents`, `buildInternal`, and `build`, it's impossible to determine if this change fixes a bug. It could be a refactoring, renaming, or a step towards a larger goal. Therefore, based on the information provided, it is not possible to determine if this is a bug fix. The answer is **NotBuggy**.
"
pmd,1620.json,0eaac2fbdb4fe4207b3f329fec2fe0b0e19bbc32,"@@ -1,8 +1,8 @@
         public Status step(int type) {
             for (int i = 0; i < this.nextSteps.size(); i++) {
-                if (type == ((Status) nextSteps.get(i)).type) {
-                    return (Status) nextSteps.get(i);
+                if (type == nextSteps.get(i).type) {
+                    return nextSteps.get(i);
                 }
             }
             return null;
         }",NotBuggy,"Checking in some Java 5 changes


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@4993 51baf565-9d33-0410-a72c-fc3788e3496d
",NotBuggy,"The change removes explicit casting `(Status)` when retrieving elements from `nextSteps`. This suggests the code was updated to leverage generics, which were introduced in Java 5. While this improves code clarity and type safety, it doesn't directly address a bug. The functionality remains the same, but the code is cleaner. The answer is **NotBuggy**."
pmd,3945.json,eb7738c3416f467477c81b805c5f28313527b610,"@@ -1,3 +1,3 @@
     public Object visit(ASTExpression node, Object data) {
-        return visit((AbstractApexNodeBase) node, data);
+        return visit((ApexNode<?>) node, data);
     }",NotBuggy,"Make FormalComment extend ApexNode
",NotBuggy,"The commit message indicates a change in the inheritance structure of `FormalComment` to extend `ApexNode`. The diff shows a change in the `visit` method where the cast from `ASTExpression` is changed from `AbstractApexNodeBase` to `ApexNode<?>`. This suggests a modification to accommodate the new inheritance structure, ensuring that the `visit` method can correctly handle nodes of type `ApexNode`. This change is likely related to adapting the code to the new class hierarchy and doesn't directly indicate a bug fix. The answer is **NotBuggy**."
hbase,11147.json,d822ee3a7ccc4959ed5a4b85bb54ff6142aa7d6e,"@@ -1,5 +1,5 @@
   protected void resetActive() {
     // Reset heap to not include any keys
-    this.active = SegmentFactory.instance().createMutableSegment(conf, comparator);
-    this.timeOfOldestEdit = Long.MAX_VALUE;
+    active = SegmentFactory.instance().createMutableSegment(conf, comparator);
+    timeOfOldestEdit = Long.MAX_VALUE;
   }",NotBuggy,"HBASE-20542: Better heap utilization for IMC with MSLABs
",NotBuggy,The commit message indicates an improvement in heap utilization. The diff shows a minor change in the `resetActive` method where `this.active` and `this.timeOfOldestEdit` are directly assigned instead of using `this`. This change doesn't seem to be directly related to heap utilization or bug fixing. It's more likely a code style improvement or a minor refactoring. The answer is **NotBuggy**.
hbase,10378.json,d1147eeb7e1d5f41161c7cf5bc5ddb4744ca5b57,"@@ -1,8 +1,7 @@
   public double getFlushPressure() {
     if (getRegionServerAccounting() == null || cacheFlusher == null) {
       // return 0 during RS initialization
       return 0.0;
     }
-    return getRegionServerAccounting().getGlobalMemstoreSize() * 1.0
-        / cacheFlusher.globalMemStoreLimitLowMark;
+    return getRegionServerAccounting().getFlushPressure();
   }",NotBuggy,"HBASE-15787 Change the flush related heuristics to work with offheap size
configured (Ram)
",Buggy,"The commit message indicates a change in flush-related heuristics to work with off-heap size configurations. The code diff shows that the `getFlushPressure` method now directly returns the value from `getRegionServerAccounting().getFlushPressure()` instead of calculating it based on `getGlobalMemstoreSize()` and `cacheFlusher.globalMemStoreLimitLowMark`. This suggests that the flush pressure calculation logic has been moved to the `RegionServerAccounting` class, likely to incorporate off-heap memory considerations. This change is related to how memory usage is calculated and used to trigger flushes, which is a core part of HBase's operation. The change is likely to fix a bug or improve the accuracy of flush decisions when off-heap memory is used. The answer is **Buggy**."
hbase,5011.json,4c6a98bd767f3bec4d2e6107016267a09d58dd12,"@@ -1,6 +1,6 @@
-  public void toStringClassDetails(StringBuilder sb) {
+  public void toStringClassDetails(final StringBuilder sb) {
     sb.append(getClass().getSimpleName());
     sb.append("" (table="");
     sb.append(getTableName());
     sb.append("")"");
   }",NotBuggy,"HBASE-16618 Procedure v2 - Add base class for table and ns procedures
",NotBuggy,"The commit message indicates the addition of a base class for table and namespace procedures, which suggests a new feature or enhancement rather than a bug fix. The diff shows a minor change in the `toStringClassDetails` method signature, adding the `final` keyword to the `StringBuilder` parameter. This change doesn't seem related to correcting any erroneous behavior or addressing a bug. The answer is **NotBuggy**."
hbase,12378.json,30424ec73f7f100b233e27196c0a6a90abd62ad2,"@@ -1,3 +1,3 @@
-  public long getModificationTime() {
-    return this.fileStatus.getModificationTime();
+  public long getModificationTime() throws IOException {
+    return getFileStatus().getModificationTime();
   }",NotBuggy,"HBASE-12749 Tighten HFileLink api to enable non-snapshot uses
",Buggy,"The commit message indicates a change to the HFileLink API. The diff shows that the `getModificationTime()` method now throws an `IOException`. This suggests that the method might have previously failed silently or returned an incorrect value in certain error scenarios. The addition of the `IOException` indicates that the method now properly propagates potential errors, which is a bug fix. The answer is **Buggy**."
hbase,35546.json,f3f17fa111f37233ddc42ddb9c38594e35d8d501,"@@ -1,8 +1,8 @@
-  private void releaseLock(final Procedure proc, final boolean force) {
-    final TEnvironment env = getEnvironment();
+  private void releaseLock(Procedure<TEnvironment> proc, boolean force) {
+    TEnvironment env = getEnvironment();
     // For how the framework works, we know that we will always have the lock
     // when we call releaseLock(), so we can avoid calling proc.hasLock()
-    if (force || !proc.holdLock(env)) {
-      proc.doReleaseLock(env);
+    if (force || !proc.holdLock(env) || proc.isFinished()) {
+      proc.doReleaseLock(env, store);
     }
   }",NotBuggy,"HBASE-20846 Restore procedure locks when master restarts
",Buggy,"The commit message indicates a fix related to procedure locks during master restarts in HBase. The diff modifies the `releaseLock` method, adding a check `proc.isFinished()` to the condition for releasing the lock. This suggests that the original code might have prematurely released locks for procedures that were not yet finished, potentially leading to inconsistencies or errors during master restarts. The addition of this check ensures that locks are held until the procedure is truly finished, addressing a potential bug. The answer is **Buggy**."
hbase,6414.json,6c22333599b9910314f57d0b6a580fb69eb7aa2b,"@@ -1,14 +1,14 @@
   public void whoAmI(RpcController controller, AuthenticationProtos.WhoAmIRequest request,
                      RpcCallback<AuthenticationProtos.WhoAmIResponse> done) {
-    User requestUser = RequestContext.getRequestUser();
+    User requestUser = RpcServer.getRequestUser();
     AuthenticationProtos.WhoAmIResponse.Builder response =
         AuthenticationProtos.WhoAmIResponse.newBuilder();
     if (requestUser != null) {
       response.setUsername(requestUser.getShortName());
       AuthenticationMethod method = requestUser.getUGI().getAuthenticationMethod();
       if (method != null) {
         response.setAuthMethod(method.name());
       }
     }
     done.run(response.build());
   }",NotBuggy,"HBASE-13397 Purge duplicate rpc request thread local
",Buggy,"The commit message indicates a fix for duplicate RPC request thread local data. The code diff replaces `RequestContext.getRequestUser()` with `RpcServer.getRequestUser()`. This suggests that `RequestContext` might have been incorrectly storing or retrieving user information, potentially leading to issues with authentication or authorization in RPC requests. By switching to `RpcServer.getRequestUser()`, the code now retrieves the user information directly from the RPC server's context, which is likely the correct source of truth. This change addresses a bug related to how user information is handled in RPC requests. The answer is **Buggy**."
hbase,35116.json,17703f03614e0803f46eadb70a2242060d04125c,"@@ -1,23 +1,23 @@
   public String toString() {
     StringBuilder sb = new StringBuilder();
     toStringClassDetails(sb);
 
     if (procId != null) {
       sb.append("" id="");
       sb.append(getProcId());
     }
 
     if (hasParent()) {
       sb.append("" parent="");
       sb.append(getParentProcId());
     }
 
     if (hasOwner()) {
       sb.append("" owner="");
       sb.append(getOwner());
     }
 
     sb.append("" state="");
-    sb.append(getState());
+    toStringState(sb);
     return sb.toString();
   }",NotBuggy,"HBASE-14015 Allow setting a richer state value when toString a pv2
",NotBuggy,"The commit message indicates an enhancement to the `toString` method of a process. The diff shows that the `getState()` call has been replaced with `toStringState(sb)`. This suggests that the state representation is being enriched, as the commit message describes. This is an enhancement, not a bug fix. The answer is **NotBuggy**."
hbase,7109.json,a9b671b31f07ade8968b42956aa60c722032dcc8,"@@ -1,48 +1,48 @@
   private byte[] createModifiedVisExpression(final List<Tag> tags)
       throws IOException {
     StringBuilder visibilityString = new StringBuilder();
     for (Tag tag : tags) {
       if (tag.getType() == TagType.VISIBILITY_TAG_TYPE) {
         if (visibilityString.length() != 0) {
           visibilityString.append(VisibilityConstants.CLOSED_PARAN).append(
               VisibilityConstants.OR_OPERATOR);
         }
-        int offset = tag.getTagOffset();
-        int endOffset = offset + tag.getTagLength();
+        int offset = tag.getValueOffset();
+        int endOffset = offset + tag.getValueLength();
         boolean expressionStart = true;
         while (offset < endOffset) {
-          Pair<Integer, Integer> result = StreamUtils.readRawVarint32(tag.getBuffer(), offset);
+          Pair<Integer, Integer> result = TagUtil.readVIntValuePart(tag, offset);
           int currLabelOrdinal = result.getFirst();
           if (currLabelOrdinal < 0) {
             int temp = -currLabelOrdinal;
             String label = this.labelsCache.getLabel(temp);
             if (expressionStart) {
               // Quote every label in case of unicode characters if present
               visibilityString.append(VisibilityConstants.OPEN_PARAN)
                   .append(VisibilityConstants.NOT_OPERATOR).append(CellVisibility.quote(label));
             } else {
               visibilityString.append(VisibilityConstants.AND_OPERATOR)
                   .append(VisibilityConstants.NOT_OPERATOR).append(CellVisibility.quote(label));
             }
           } else {
             String label = this.labelsCache.getLabel(currLabelOrdinal);
             if (expressionStart) {
               visibilityString.append(VisibilityConstants.OPEN_PARAN).append(
                   CellVisibility.quote(label));
             } else {
               visibilityString.append(VisibilityConstants.AND_OPERATOR).append(
                   CellVisibility.quote(label));
             }
           }
           expressionStart = false;
           offset += result.getSecond();
         }
       }
     }
     if (visibilityString.length() != 0) {
       visibilityString.append(VisibilityConstants.CLOSED_PARAN);
       // Return the string formed as byte[]
       return Bytes.toBytes(visibilityString.toString());
     }
     return null;
   }",NotBuggy,"HBASE-12593 Tags to work with ByteBuffer.
",NotBuggy,"The commit message ""HBASE-12593 Tags to work with ByteBuffer"" suggests an enhancement to handle tags with ByteBuffer. The code changes involve replacing `tag.getTagOffset()` with `tag.getValueOffset()` and `tag.getTagLength()` with `tag.getValueLength()`. Also, `StreamUtils.readRawVarint32(tag.getBuffer(), offset)` is replaced with `TagUtil.readVIntValuePart(tag, offset)`. These changes indicate that the code is being adapted to work with a new representation of tags, likely involving ByteBuffers. The change from `getTagOffset` and `getTagLength` to `getValueOffset` and `getValueLength` suggests that the offset and length now refer to the value part of the tag, which is consistent with working with ByteBuffers. The change of `StreamUtils.readRawVarint32` to `TagUtil.readVIntValuePart` suggests a change in how the variable integer value part is read from the tag. These changes are related to adapting the code to work with `ByteBuffer`, and do not indicate a bug fix. The answer is **NotBuggy**."
hbase,20259.json,d82c1a6c2b534c63bb347cf1d21c36af400b0f19,"@@ -1,9 +1,9 @@
       public void read(org.apache.thrift.protocol.TProtocol prot, addColumnFamily_result struct) throws org.apache.thrift.TException {
-        TTupleProtocol iprot = (TTupleProtocol) prot;
-        BitSet incoming = iprot.readBitSet(1);
+        org.apache.thrift.protocol.TTupleProtocol iprot = (org.apache.thrift.protocol.TTupleProtocol) prot;
+        java.util.BitSet incoming = iprot.readBitSet(1);
         if (incoming.get(0)) {
           struct.io = new TIOError();
           struct.io.read(iprot);
           struct.setIoIsSet(true);
         }
       }",NotBuggy,"HBASE-21791 Upgrade thrift dependency to 0.12.0
",NotBuggy,"The commit message indicates an upgrade of the Thrift dependency. The code diff shows a change in the type of `TTupleProtocol` and `BitSet` to include the package name. This change is likely due to API changes in the new Thrift version. This doesn't seem like a bug fix, but rather an adaptation to a new version of a dependency. The answer is **NotBuggy**."
hbase,5138.json,826b9436fbba39ce78650680c71425bc8547cd39,"@@ -1,9 +1,9 @@
   private TableQueue getTableQueue(TableName tableName) {
     TableQueue node = AvlTree.get(tableMap, tableName, TABLE_QUEUE_KEY_COMPARATOR);
     if (node != null) return node;
 
-    NamespaceQueue nsQueue = getNamespaceQueue(tableName.getNamespaceAsString());
-    node = new TableQueue(tableName, nsQueue, getTablePriority(tableName));
+    node = new TableQueue(tableName, tablePriorities.getPriority(tableName),
+        locking.getTableLock(tableName), locking.getNamespaceLock(tableName.getNamespaceAsString()));
     tableMap = AvlTree.insert(tableMap, node);
     return node;
   }",NotBuggy,"HBASE-17605 Changes
- Moved locks out of MasterProcedureScheduler#Queue. One Queue object is used for each namespace/table, which aren't more than 100. So we don't need complexity arising from all functionalities being in one place. SchemaLocking now owns locks and locking implementaion has been moved to procedure2 package.
- Removed NamespaceQueue because it wasn't being used as Queue (add,peek,poll,etc functions threw UnsupportedOperationException). It's was only used for locks on namespaces. Now that locks have been moved out of Queue class, it's not needed anymore.
- Remoed RegionEvent which was there only for locking on regions. Tables/namespaces used locking from Queue class and regions couldn't (there are no separate proc queue at region level), hence the redundance. Now that locking is separate, we can use the same for regions too.
- Removed QueueInterface class. No declarations, except one implementaion, which makes the point of having an interface moot.
- Removed QueueImpl, which was the only concrete implementation of abstract Queue class. Moved functions to Queue class itself to avoid unnecessary level in inheritance hierarchy.
- Removed ProcedureEventQueue class which was just a wrapper around ArrayDeque class. But we now have ProcedureWaitQueue as 'Type class'.
- Encapsulated table priority related stuff in a single class.
- Removed some unused functions.
Change-Id: I6a60424cb41e280bc111703053aa179d9071ba17
",NotBuggy,"The commit message describes a large refactoring effort focused on simplifying the locking mechanism and removing unnecessary abstractions in the HBase MasterProcedureScheduler. The diff shows that the `TableQueue` constructor now receives a table priority directly from `tablePriorities.getPriority(tableName)` and locks from `locking.getTableLock` and `locking.getNamespaceLock`, instead of a `NamespaceQueue`. This change aligns with the commit message's description of moving locks out of the `Queue` class and removing the `NamespaceQueue`. The changes are refactoring and simplification, not a bug fix. The answer is **NotBuggy**."
hbase,28109.json,dec0ec229070465ce5a1f7381366a82278149f06,"@@ -1,7 +1,4 @@
   public HBaseProtos.TableState convert() {
     return HBaseProtos.TableState.newBuilder()
-        .setState(this.state.convert())
-        .setTable(ProtobufUtil.toProtoTableName(this.tableName)) // set for backward compatibility
-        .setTimestamp(this.timestamp)
-            .build();
+        .setState(this.state.convert()).build();
   }",NotBuggy,"HBASE-13016 Clean up remnants of table states stored in table descriptors (Andrey Stepachev)
",Buggy,"The commit message indicates a cleanup of table states stored in table descriptors. The diff removes setting the table name and timestamp in the `convert` method, suggesting these were remnants of a previous design. This removal implies a correction or simplification of the table state representation, which can be considered a bug fix if the previous state caused issues or inconsistencies. The answer is **Buggy**."
hbase,3595.json,eddc746457d0d25db1b51bef953db684131d4dd0,"@@ -1,3 +1,3 @@
-      protected void chore() {
-        master.balance();
-      }

+  protected void chore() {
+    master.balance();
+  }",NotBuggy,"HBASE-6730 Enable rolling averages in StochasticLoadBalancer

git-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1387865 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates an enhancement related to the StochasticLoadBalancer, specifically enabling rolling averages. The provided diff shows a minor change in the chore method, seemingly adjusting the indentation. While the commit message suggests a feature addition, the diff itself doesn't reveal any bug fix. The change in indentation doesn't imply any error correction or logical improvement. The answer is **NotBuggy**."
hbase,14572.json,33bedf8d4d7ec320c5cc01c1c031035a1523f973,"@@ -1,14 +1,14 @@
   public void put(E e) throws InterruptedException {
-    if (e == null) throw new NullPointerException();
+    Objects.requireNonNull(e);
 
     lock.lock();
     try {
       while (queue.remainingCapacity() == 0) {
         notFull.await();
       }
       this.queue.add(e);
       notEmpty.signal();
     } finally {
       lock.unlock();
     }
   }",NotBuggy,"HBASE-23308: Review of NullPointerExceptions (#836)

Signed-off-by: stack <stack@apache.org>",Buggy,"The commit message indicates a fix for NullPointerExceptions. The code change replaces a `NullPointerException` check with `Objects.requireNonNull(e)`. While both achieve the same goal of throwing a `NullPointerException` if `e` is null, `Objects.requireNonNull` is generally preferred for its conciseness and readability. This change directly addresses the potential for a null pointer exception, indicating a bug fix. The answer is **Buggy**."
hbase,883.json,4b91a6425eadc3578ec291496bbf7fde9742f96a,"@@ -1,31 +1,32 @@
-  public static void copySnapshotForScanner(Configuration conf, FileSystem fs, Path rootDir,
-      Path restoreDir, String snapshotName) throws IOException {
+  public static RestoreMetaChanges copySnapshotForScanner(Configuration conf, FileSystem fs,
+      Path rootDir, Path restoreDir, String snapshotName) throws IOException {
     // ensure that restore dir is not under root dir
     if (!restoreDir.getFileSystem(conf).getUri().equals(rootDir.getFileSystem(conf).getUri())) {
       throw new IllegalArgumentException(""Filesystems for restore directory and HBase root "" +
           ""directory should be the same"");
     }
     if (restoreDir.toUri().getPath().startsWith(rootDir.toUri().getPath())) {
       throw new IllegalArgumentException(""Restore directory cannot be a sub directory of HBase "" +
           ""root directory. RootDir: "" + rootDir + "", restoreDir: "" + restoreDir);
     }
 
     Path snapshotDir = SnapshotDescriptionUtils.getCompletedSnapshotDir(snapshotName, rootDir);
     SnapshotDescription snapshotDesc = SnapshotDescriptionUtils.readSnapshotInfo(fs, snapshotDir);
     SnapshotManifest manifest = SnapshotManifest.open(conf, fs, snapshotDir, snapshotDesc);
 
     MonitoredTask status = TaskMonitor.get().createStatus(
         ""Restoring  snapshot '"" + snapshotName + ""' to directory "" + restoreDir);
     ForeignExceptionDispatcher monitor = new ForeignExceptionDispatcher();
 
     // we send createBackRefs=false so that restored hfiles do not create back reference links
     // in the base hbase root dir.
     RestoreSnapshotHelper helper = new RestoreSnapshotHelper(conf, fs,
       manifest, manifest.getTableDescriptor(), restoreDir, monitor, status, false);
-    helper.restoreHdfsRegions(); // TODO: parallelize.
+    RestoreMetaChanges metaChanges = helper.restoreHdfsRegions(); // TODO: parallelize.
 
     if (LOG.isDebugEnabled()) {
       LOG.debug(""Restored table dir:"" + restoreDir);
       FSUtils.logFileSystemState(fs, restoreDir, LOG);
     }
+    return metaChanges;
   }",NotBuggy,"HBASE-14714 some cleanup to snapshot code
",NotBuggy,"The commit message indicates ""some cleanup to snapshot code"". The diff shows a change in the return type of the `copySnapshotForScanner` method from `void` to `RestoreMetaChanges`. Also, the return value of `helper.restoreHdfsRegions()` is now returned by `copySnapshotForScanner`. These changes suggest a refactoring or enhancement of the snapshot restoration process, rather than a bug fix.
The answer is **NotBuggy**."
hbase,40253.json,6af4292630daca370de7fce3a2b2b3401cd1abfa,"@@ -1,4 +1,5 @@
-  public String toString() {
-    return ""id: "" + transactionId + "", particpants: ""
-        + participatingRegions.size();
+  public static String toString(final byte [] b1,
+                                String sep,
+                                final byte [] b2) {
+    return toString(b1, 0, b1.length) + sep + toString(b2, 0, b2.length);
   }",NotBuggy,"HBASE-1304 - New client server implementation of how gets and puts are handled. -- Thanks to jgray,holstad,stack,rawson



git-svn-id: https://svn.apache.org/repos/asf/hadoop/hbase/trunk@782178 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a significant change in how gets and puts are handled in the HBase client-server implementation. The diff shows the addition of a `toString` method that concatenates two byte arrays with a separator. While this specific change doesn't scream ""bug fix,"" it's part of a larger implementation change that could address inefficiencies or incorrect behavior in the previous get/put handling. Without more context, it's difficult to definitively say if this specific change is a bug fix. However, given the scope of the commit message, it's likely that the changes address some form of issue. The answer is **Buggy**."
ant,1414.json,020f9a52c4a54560f617aea642a180e96294e9cc,"@@ -1,3 +1,3 @@
-        public String getFile() {
-            return m_filename;
-        }

+    public String getFile() {
+        return m_filename;
+    }",NotBuggy,"Make tagEntry a top level class.


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@272453 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates a structural change, moving `tagEntry` to a top-level class. The provided diff snippet only shows a getter method, which doesn't inherently suggest a bug fix. The change seems to be related to code organization or visibility, rather than correcting a functional error. The answer is **NotBuggy**."
ant,9732.json,1b76f1b6dae88a7a158e50222bb19f6f5ca26d70,"@@ -1,7 +1,7 @@
-    public Object setProperty(String key, String value)
+	public Object setProperty(final String key, final String value)
         throws NullPointerException {
-        Object obj = super.setProperty(key, value);
+        final Object obj = super.setProperty(key, value);
         // the above call will have failed if key or value are null
         innerSetProperty(key, value);
         return obj;
     }",NotBuggy,checkstyle,NotBuggy,"The commit message ""checkstyle"" suggests changes related to code style or formatting, rather than a bug fix. The diff introduces `final` keywords for the `key`, `value`, and `obj` variables. This change doesn't alter the functionality of the code and is likely related to code style or checkstyle rules. Therefore, it's not a bug fix. The answer is **NotBuggy**.
"
ant,632.json,761f82ae3158f6a4b131f0f4f3efff152c72bd95,"@@ -1,3 +1,3 @@
     public void setExecutor(Executor e) {
-        addReference(""ant.executor"", e);
+        addReference(MagicNames.ANT_EXECUTOR_REFERENCE, e);
     }",NotBuggy,"moving some magic names into place, using them where needed.


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@278417 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates a refactoring effort to replace hardcoded strings (magic names) with named constants. The diff shows that the string ""ant.executor"" is replaced with `MagicNames.ANT_EXECUTOR_REFERENCE`. This change improves code maintainability and readability by centralizing the definition of these names. There is no indication of a bug fix. The answer is **NotBuggy**.
"
ant,8540.json,a738000acab66d8d7ce5cf563f6dd96d9c7024fb,"@@ -1,3 +1,7 @@
     public Reference getLoader() {
+        if (isReference()) {
+            return ((AbstractClasspathResource) getCheckedRef()).getLoader();
+        }
+        dieOnCircularReference();
         return loader;
     }",NotBuggy,"refid-audit of indirect subclasses of DataType

git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@722437 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message ""refid-audit of indirect subclasses of DataType"" suggests a review and potential adjustments related to reference IDs within the `DataType` hierarchy. The code diff introduces a conditional check `if (isReference())` before accessing the loader. This check suggests that the code previously might have attempted to access the loader without verifying if the instance was a reference, potentially leading to a `NullPointerException` or other unexpected behavior. The added check prevents this by ensuring that the `getLoader()` method is only called on instances that are references. This indicates a bug fix. The answer is **Buggy**."
ant,7011.json,1b76f1b6dae88a7a158e50222bb19f6f5ca26d70,"@@ -1,3 +1,3 @@
-    protected void addExtdirsToClasspath(Path classpath) {
+	protected void addExtdirsToClasspath(final Path classpath) {
         classpath.addExtdirs(extdirs);
     }",NotBuggy,checkstyle,NotBuggy,"The commit message ""checkstyle"" suggests changes related to code style or formatting, often performed by automated tools like Checkstyle. The diff shows a minor change: adding the `final` keyword to the `classpath` parameter in the `addExtdirsToClasspath` method. This change doesn't alter the functionality of the code but rather improves code clarity and adheres to style guidelines by indicating that the `classpath` variable should not be reassigned within the method. Therefore, this change is not related to fixing a bug. The answer is **NotBuggy**.
"
ant,9308.json,048caece4061746150290b0264128118a4be3870,"@@ -1,30 +1,4 @@
     protected String getElementName(Project project, Object element)
     {
-        Hashtable elements = project.getTaskDefinitions();
-        String typeName = ""task"";
-        if (!elements.contains( element.getClass() ))
-        {
-            elements = project.getDataTypeDefinitions();
-            typeName = ""data type"";
-            if (!elements.contains( element.getClass() ))
-            {
-                elements = null;
-            }
-        }
-
-        if (elements != null)
-        {
-            Enumeration e = elements.keys();
-            while (e.hasMoreElements())
-            {
-                String elementName = (String) e.nextElement();
-                Class elementClass = (Class) elements.get( elementName );
-                if ( element.getClass().equals( elementClass ) )
-                {
-                    return ""The <"" + elementName + ""> "" + typeName;
-                }
-            }
-        }
-
-        return ""Class "" + element.getClass().getName();
+        return project.getElementName(element);
     }",NotBuggy,"move method refactoring - applied in a way that keeps
IntrospectionHelper's API backwards compatible.

Inspired by:	Jon Skeet <jon.skeet@peramon.com>


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@271451 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,The commit message indicates a refactoring to maintain backward compatibility. The diff shows that the method `getElementName` is simplified to delegate to `project.getElementName(element)`. This change suggests that the original implementation was moved to the `Project` class. This is a refactoring and not a bug fix. The answer is **NotBuggy**.
ant,3462.json,789422e131b6c2c003d94f394169a64297e986c6,"@@ -1,3 +1,3 @@
-    public void setTimeout(long timeout) {
+    public void setTimeout(final long timeout) {
         maxwait = timeout;
     }",NotBuggy,oops - make it compile under Java5 again,Buggy,"The commit message indicates a fix to ensure Java 5 compatibility. The diff adds the `final` keyword to the `timeout` parameter of the `setTimeout` method. This change likely addresses a compilation issue under Java 5 related to variable access within inner classes or anonymous classes. While not a bug in the traditional sense of incorrect logic, it's a fix for a compatibility issue, which can be considered a bug in the build/execution environment. The answer is **Buggy**."
ant,7372.json,ff9b546d81da21c253e71319dc8a756380bac15c,"@@ -1,3 +1,3 @@
     public void setString(String str) {
-        str_text = str;
+        strText = str;
     }",NotBuggy,"checkstyle - mostly removal of _


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@278353 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates a change related to code style, specifically the removal of underscores. The diff confirms this by showing the renaming of a variable `str_text` to `strText`. This change does not involve any bug fixes or functional changes. The answer is **NotBuggy**.
"
ant,8584.json,f67efecf7b71abb13cde1c89fcb56e4e8b7c5c9e,"@@ -1,4 +1,4 @@
-    public void setBase(File b) {
+    public void setBaseDir(File b) {
         checkAttributesAllowed();
-        base = b;
+        baseDir = b;
     }",NotBuggy,"Renamed <file> resource's ""base"" attribute to ""basedir"",
for consistency / familiarity.


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@278388 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates a renaming of an attribute from ""base"" to ""basedir"" for consistency. The diff shows the corresponding change in the Java code, where the setter method `setBase` is renamed to `setBaseDir`, and the member variable `base` is renamed to `baseDir`. This change doesn't appear to be related to fixing a bug; rather, it's a refactoring change to improve code clarity and consistency. The answer is **NotBuggy**."
ant,7270.json,9caeed5081071d6a2de28844d0a724918f8c6d86,"@@ -1,18 +1,18 @@
-    protected synchronized void dieOnCircularReference(Stack stk, Project p)
+    protected synchronized void dieOnCircularReference(Stack<Object> stk, Project p)
         throws BuildException {
         if (isChecked()) {
             return;
         }
         if (isReference()) {
             super.dieOnCircularReference(stk, p);
         } else {
             if (mapper != null) {
                 pushAndInvokeCircularReferenceCheck(mapper, stk, p);
             }
-            for (Iterator i = setRefs.iterator(); i.hasNext(); ) {
-                pushAndInvokeCircularReferenceCheck((PropertySet) i.next(), stk,
+            for (PropertySet propertySet : setRefs) {
+                pushAndInvokeCircularReferenceCheck(propertySet, stk,
                                                     p);
             }
             setChecked(true);
         }
     }",NotBuggy,"Java 1.5, continued

git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@1376015 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message ""Java 1.5, continued"" suggests an update related to Java 1.5 compatibility. The code diff changes the type of the `stk` parameter in the `dieOnCircularReference` method from `Stack` to `Stack<Object>`. Additionally, the code iterates through `setRefs` using a for-each loop instead of an iterator. These changes are related to updating the code to be compatible with Java 1.5 features, specifically generics and enhanced for loops. There is no indication of a bug fix in the traditional sense. The changes are more about code modernization and compatibility. The answer is **NotBuggy**."
ant,9623.json,ffea0a9f7459b7c64148ae244b22e847bad9128b,"@@ -1,3 +1,3 @@
     public String getPattern() {
-        return pattern.pattern();
+        return pattern;
     }",NotBuggy,"Rewritten part of the regexp stuff, adding substitution and options.
New optional replaceregexp tasks that takes advantage of it.

Submitted by:	Matthew Inger <matti@sedonacorp.com>

This doesn't really work (but doesn't break anything either), but I
wanted to get the stuff committed now, I will put work into it
tomorrow.  See mail to the list for details.


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@269820 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates a rewrite of regular expression functionality, including the addition of substitution and options. The diff shows a change in the `getPattern()` method, where `pattern.pattern()` is replaced with `pattern`. It is difficult to determine if this change fixes a bug without more context. However, the commit message mentions that the changes ""doesn't really work (but doesn't break anything either),"" suggesting that the changes are experimental and may address a potential issue or improve existing functionality. Given the uncertainty and the experimental nature of the changes, it's difficult to definitively say if it's a bug fix.

The change is from `pattern.pattern()` to `pattern`. This suggests that `pattern` was previously a `java.util.regex.Pattern` object, and `pattern()` was being called to retrieve the pattern string. Now, `pattern` seems to directly hold the pattern string. This could be an attempt to simplify the code or address a potential issue with how the pattern was being stored or accessed.

Given the information, it's not clear if this change is directly related to fixing a bug. It could be a refactoring or an attempt to improve the code, but without more context, it's hard to say for sure.

The answer is **NotBuggy**."
ant,7050.json,1b76f1b6dae88a7a158e50222bb19f6f5ca26d70,"@@ -1,3 +1,3 @@
-    public void setAddproperty (String addproperty) {
+    public void setAddproperty (final String addproperty) {
         this.addproperty = addproperty;
     }",NotBuggy,checkstyle,NotBuggy,"The commit message ""checkstyle"" suggests changes related to code style or formatting, rather than a functional bug fix. The diff shows the addition of the `final` keyword to the `addproperty` parameter in the `setAddproperty` method. This change is likely to enforce immutability and improve code clarity, which aligns with checkstyle rules. It doesn't appear to address any specific bug or incorrect behavior. The answer is **NotBuggy**.
"
ant,1185.json,dccbf1fcec0fc4e0812e8906494b15a1301ac32a,"@@ -1,7 +1,3 @@
     public boolean hasMoreTokens() {
-        if (lookahead != null) {
-            return true;
-        }
-
-        return tokenizer.hasMoreTokens();
+        return lookahead != null || tokenizer.hasMoreTokens();
     }",NotBuggy,"Checkstyle and simplify, next iteration",NotBuggy,"The commit message indicates a simplification and code style change. The diff combines two boolean expressions into one. This change doesn't seem to fix any specific bug, but rather improves the code's readability or conciseness. The answer is **NotBuggy**."
ant,1488.json,70f35e75416468e31f537f33b248caaae29ee587,"@@ -1,19 +1,19 @@
-    private static String removePackageName(String line, List packageNames) {
+    private static String removePackageName(String line,
+                                            String[] packagePrefixes,
+                                            int[] prefixLengths) {
+        if (line.length() < FILE_STRING_LENGTH) {
+            return null;
+        }
         boolean matched = false;
-        for (Iterator iter = packageNames.iterator(); iter.hasNext(); ) {
-            String toBeRemoved = FILE_STRING + iter.next() + ""/"";
-            int len = toBeRemoved.length();
-            if (line.length() > len) {
-                if (line.startsWith(toBeRemoved)) {
-                    matched = true;
-                    line = line.substring(len);
-                    break;
-                }
+        for (int i = 0; i < packagePrefixes.length; i++) {
+            if (line.startsWith(packagePrefixes[i])) {
+                matched = true;
+                line = line.substring(prefixLengths[i]);
+                break;
             }
         }
-        if (!matched && line.length() > FILE_STRING.length()) {
-            line = line.substring(FILE_STRING.length());
-            matched = true;
+        if (!matched) {
+            line = line.substring(FILE_STRING_LENGTH);
         }
-        return !matched ? null : line;
+        return line;
     }",NotBuggy,"optimize handling of package names

git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@705452 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates an optimization in handling package names. The diff replaces the use of a `List` of package names with a `String[]` of package prefixes and an `int[]` of prefix lengths. It also replaces the `Iterator` with a simple `for` loop. The code now iterates through the `packagePrefixes` array and checks if the input `line` starts with any of the prefixes. If a match is found, the corresponding prefix is removed from the `line`. If no match is found, the `FILE_STRING_LENGTH` is removed from the line. This change aims to improve the efficiency of removing package names from a given line. The original code used an iterator and string concatenation within the loop, which can be less efficient than using a simple array and direct string comparison. The added length check `line.length() < FILE_STRING_LENGTH` suggests a potential issue where the code might have been trying to substring a string that was too short, which could lead to an exception or incorrect behavior. This suggests that the change addresses a potential bug or edge case. The answer is **Buggy**."
ant,393.json,8741512d8c2e4cad30f0c6e6134d9a793f00d4b7,"@@ -1,3 +1,3 @@
     public void setName(String name) {
-        this.name = new StringBuffer(normalizeFileName(name));
+        this.name = new StringBuffer(normalizeFileName(name, false));
     }",NotBuggy,"Make preserveLeadingSlashes in tar work again, was broken by revision 755231

git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@807513 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix related to the `preserveLeadingSlashes` functionality in the `tar` task, which was broken by a previous revision. The diff shows a change in the `setName` method where the `normalizeFileName` method is called. The change involves passing `false` as an argument to the `normalizeFileName` method. This suggests that the `normalizeFileName` method now takes a boolean argument that controls whether leading slashes should be preserved or not. The previous revision likely introduced a change that caused leading slashes to be incorrectly removed, and this commit fixes that issue by explicitly passing `false` to preserve them. The answer is **Buggy**.
"
lucene-solr,41126.json,e167e9124757b3f3597db8149c49b7f388c48627,"@@ -1,23 +1,22 @@
   static MatchesIterator fromTermsEnum(LeafReaderContext context, int doc, String field, BytesRefIterator terms) throws IOException {
     Objects.requireNonNull(field);
     List<MatchesIterator> mis = new ArrayList<>();
     Terms t = context.reader().terms(field);
     if (t == null)
       return null;
     TermsEnum te = t.iterator();
     PostingsEnum reuse = null;
     for (BytesRef term = terms.next(); term != null; term = terms.next()) {
       if (te.seekExact(term)) {
         PostingsEnum pe = te.postings(reuse, PostingsEnum.OFFSETS);
         if (pe.advance(doc) == doc) {
-          // TODO do we want to use the copied term here, or instead create a label that associates all of the TMIs with a single term?
-          mis.add(new TermMatchesIterator(BytesRef.deepCopyOf(term), pe));
+          mis.add(new TermMatchesIterator(pe));
           reuse = null;
         }
         else {
           reuse = pe;
         }
       }
     }
     return fromSubIterators(mis);
   }",NotBuggy,"LUCENE-8270: Remove MatchesIterator.term()
",Buggy,"The commit message indicates the removal of `MatchesIterator.term()`. The diff shows a change in `fromTermsEnum` where a `TermMatchesIterator` is created. Previously, it was created with a copy of the term (`BytesRef.deepCopyOf(term)`) and a `PostingsEnum`. Now, it's created only with the `PostingsEnum`. The comment `// TODO do we want to use the copied term here, or instead create a label that associates all of the TMIs with a single term?` suggests that the term was previously used, but now it's not. This removal could be due to a bug or an unnecessary dependency on the term. Therefore, the change indicates a bug fix or code simplification by removing an unused or problematic term. The answer is **Buggy**."
lucene-solr,27518.json,72f99b47b4673d858e3ffd42afdca76e0a5bb860,"@@ -1,3 +1,7 @@
     public int startOffset() {
-      return startOffsets[nextPos-1];
+      if (startOffsets == null) {
+        return -1;
+      } else {
+        return startOffsets[nextPos-1];
+      }
     }",NotBuggy,"LUCENE-4230: allow consumer to specify that payloads are not require when pulling D&PEnum, so codec can optimize if possible

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1367186 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a performance optimization by allowing consumers to specify whether payloads are required. The diff adds a null check for `startOffsets` before accessing it. If `startOffsets` is null, it returns -1. This change addresses a potential `NullPointerException` if `startOffsets` is not initialized or is explicitly set to null, which could occur if payloads are not required. This is a bug fix. The answer is **Buggy**."
lucene-solr,22902.json,aa1d5feba01a50d4bc002e346e54ff26f6abc664,"@@ -1,14 +1,14 @@
-    private void rehash() {
-      long[] oldVals = vals;
-      int newCapacity = vals.length << 1;
-      vals = new long[newCapacity];
-      mask = newCapacity - 1;
-      threshold = (int) (newCapacity * LOAD_FACTOR);
-      cardinality = 0;
+  private void rehash() {
+    long[] oldVals = vals;
+    int newCapacity = vals.length << 1;
+    vals = new long[newCapacity];
+    mask = newCapacity - 1;
+    threshold = (int) (newCapacity * LOAD_FACTOR);
+    cardinality = 0;
 
-      for (long val : oldVals) {
-        if (val != 0) {
-          add(val);
-        }
+    for (long val : oldVals) {
+      if (val != 0) {
+        add(val);
       }
-    }

+    }
+  }",NotBuggy,"SOLR-11093: add Points to GraphQuery
",NotBuggy,"The code change involves rehashing logic, specifically doubling the capacity of an array and re-inserting non-zero values. While the commit message ""SOLR-11093: add Points to GraphQuery"" doesn't directly suggest a bug fix related to rehashing, the need to rehash could arise from adding new points to the graph query, potentially triggering the need for a larger data structure. However, without more context, it is impossible to determine if this change is directly fixing a bug. The change itself doesn't obviously indicate a bug fix. The answer is **NotBuggy**."
lucene-solr,49335.json,831c0648f992faea7024e36167749cd210cddc14,"@@ -1,6 +1,6 @@
   public long get(int index) {
     final int o = blockOffset(index);
     final int b = offsetInBlock(index);
 
-    return (blocks[o] >> shifts[b]) & readMask;
+    return (blocks[o] >>> shifts[b]) & readMask;
   }",NotBuggy,"LUCENE-4062: PackedInts improvements.

 * Direct64 now uses System.arraycopy for its bulk operations,
 * Packed64SingleBlock unnecessary optimizations have been removed.


git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1351682 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates improvements to PackedInts, specifically mentioning changes to Direct64 and Packed64SingleBlock. The diff shows a change in the `get` method of an unspecified class. The change replaces the `>>` operator with `>>>`. The `>>` operator is a signed right shift, while `>>>` is an unsigned right shift. This change ensures that the bits are shifted correctly regardless of the sign of the value, which is crucial for packed integer representations. This indicates a bug fix related to handling signed values. The answer is **Buggy**."
lucene-solr,2051.json,f7aa200d406dbd05a35d6116198302d90b92cb29,"@@ -1,3 +1,3 @@
-      public boolean exists(int doc) {
+      public boolean exists(int doc) throws IOException {
         return vals.exists(doc);
       }",NotBuggy,"LUCENE-7407: switch doc values usage to an iterator API, based on DocIdSetIterator, instead of random acces, freeing codecs for future improvements
",Buggy,"The commit message indicates a change in the API for accessing doc values, switching to an iterator-based approach. The diff shows that the `exists` method now throws an `IOException`. This suggests that the underlying implementation of `vals.exists(doc)` might now involve I/O operations, which could potentially fail. This change is likely related to the new iterator-based API and the need to handle potential I/O errors during iteration. Therefore, this change is related to a bug fix or improvement in error handling. The answer is **Buggy**.
"
lucene-solr,7428.json,ce172acb8fec6c3bbb18837a4d640da6c5aad649,"@@ -1,7 +1,6 @@
   public static void close(HttpClient httpClient) { 
-    if (httpClient instanceof CloseableHttpClient) {
-      org.apache.solr.common.util.IOUtils.closeQuietly((CloseableHttpClient) httpClient);
-    } else {
-      httpClient.getConnectionManager().shutdown();
-    }
+
+    org.apache.solr.common.util.IOUtils.closeQuietly((CloseableHttpClient) httpClient);
+
+    assert ObjectReleaseTracker.release(httpClient);
   }",NotBuggy,"SOLR-4509: Move to non deprecated HttpClient impl classes to remove stale connection check on every request and move connection lifecycle management towards the client.
",Buggy,"The commit message indicates a move to non-deprecated HttpClient implementation classes, aiming to remove stale connection checks and improve connection lifecycle management. The code diff removes the conditional check for `CloseableHttpClient` and directly closes the `httpClient` using `IOUtils.closeQuietly`. The `else` block, which shut down the connection manager for non-`CloseableHttpClient` instances, is removed. The assertion `ObjectReleaseTracker.release(httpClient)` is added. This change suggests a shift in how HttpClient instances are managed and closed, potentially addressing issues related to connection handling or resource leaks associated with the deprecated implementation. The removal of the conditional check and the direct closure using `closeQuietly` suggest a simplification and potentially a fix related to proper resource management. The assertion also hints at tracking object releases, which is related to resource management and bug prevention. The answer is **Buggy**."
lucene-solr,2821.json,f01b3e97d19c4b96ca9eb7d4e2a8647f68fbb969,"@@ -1,13 +1,16 @@
   public void finishStage(ResponseBuilder rb) {
     if (rb.isAnalytics() && rb.stage == ResponseBuilder.STAGE_GET_FIELDS) {
       AnalyticsRequestManager reqManager = getAnalyticsRequestManager(rb);
       // Generate responses from the merged shard data
       if (rb.isOlapAnalytics()) {
         rb.rsp.add(AnalyticsResponseHeadings.COMPLETED_OLD_HEADER, reqManager.createOldResponse());
       } else {
         rb.rsp.add(AnalyticsResponseHeadings.COMPLETED_HEADER, reqManager.createResponse());
       }
+      if (reqManager.isPartialResults()) {
+        rb.rsp.getResponseHeader().asShallowMap().put(SolrQueryResponse.RESPONSE_HEADER_PARTIAL_RESULTS_KEY,true);
+      }
     }
 
     super.finishStage(rb);
   }",NotBuggy,"SOLR-13904: Make Analytics component sensitive to timeAllowed.
",Buggy,"The commit message indicates a change to make the Analytics component sensitive to `timeAllowed`. The diff adds a check for partial results (`reqManager.isPartialResults()`) and, if true, sets the `RESPONSE_HEADER_PARTIAL_RESULTS_KEY` in the response header. This suggests that the component now considers the time allowed and might return partial results if the time limit is reached. This is a bug fix because the component was not previously sensitive to `timeAllowed`. The answer is **Buggy**."
lucene-solr,42269.json,765403b2af47c2aef71dd824f57c1d90d3a21cef,"@@ -1,17 +1,17 @@
-  public Iterable<? extends Accountable> getChildResources() {
+  public Iterable<Accountable> getChildResources() {
     List<Accountable> resources = new ArrayList<>();
     
     long docBaseDeltaBytes = RamUsageEstimator.shallowSizeOf(docBasesDeltas);
     for (PackedInts.Reader r : docBasesDeltas) {
       docBaseDeltaBytes += r.ramBytesUsed();
     }
     resources.add(Accountables.namedAccountable(""doc base deltas"", docBaseDeltaBytes));
     
     long startPointerDeltaBytes = RamUsageEstimator.shallowSizeOf(startPointersDeltas);
     for (PackedInts.Reader r : startPointersDeltas) {
       startPointerDeltaBytes += r.ramBytesUsed();
     }
     resources.add(Accountables.namedAccountable(""start pointer deltas"", startPointerDeltaBytes));
     
-    return resources;
+    return Collections.unmodifiableList(resources);
   }",NotBuggy,"LUCENE-6104: simplify internals of Lucene50NormsProducer

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1645711 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,The commit message indicates simplification of internal components. The code changes involve modifying the return type of `getChildResources` from `Iterable<? extends Accountable>` to `Iterable<Accountable>` and wrapping the returned list with `Collections.unmodifiableList`. This change makes the code more type-safe and prevents external modification of the internal list. The changes do not indicate a bug fix. The answer is **NotBuggy**.
lucene-solr,48313.json,93fa72f77bd024aa09eef043c65c64a6524613dc,"@@ -1,3 +1,13 @@
   final int getCharClass(int c) {
-    return Operations.findIndex(c, points);
+
+    // binary search
+    int a = 0;
+    int b = points.length;
+    while (b - a > 1) {
+      int d = (a + b) >>> 1;
+      if (points[d] > c) b = d;
+      else if (points[d] < c) a = d;
+      else return d;
+    }
+    return a;
   }",NotBuggy,"LUCENE-7465: add SimplePatternTokenizer and SimpleSplitPatternTokenizer, for tokenization using Lucene's regexp/automaton implementation
",NotBuggy,"The commit message indicates the addition of new tokenizers using regular expressions. The code diff shows a change in the `getCharClass` method, which now uses a binary search algorithm to find the character class. This change improves the efficiency of the character class lookup, which is essential for regular expression processing. While this change is related to the new tokenizers, it doesn't directly fix a bug. It's more of an optimization or enhancement. The answer is **NotBuggy**.
"
lucene-solr,32009.json,af680af77f3f80c779e038a0ad8a136c9dcb9f5d,"@@ -1,3 +1,3 @@
-  public ImpactsEnum impacts(FieldInfo fieldInfo, BlockTermState state, SimScorer scorer, int flags) throws IOException {
+  public ImpactsEnum impacts(FieldInfo fieldInfo, BlockTermState state, int flags) throws IOException {
     throw new UnsupportedOperationException(""Should never be called, IDVersionSegmentTermsEnum implements impacts directly"");
   }",NotBuggy,"LUCENE-8142: Make postings APIs expose raw impacts rather than scores.
",NotBuggy,"The commit message indicates a change in how postings APIs expose data, moving from scores to raw impacts. The code diff confirms this by removing the `SimScorer scorer` parameter from the `impacts` method signature. This suggests a modification to the API to provide raw data instead of pre-computed scores. This is not a bug fix. The answer is **NotBuggy**."
lucene-solr,22923.json,8007ac0cb0c88838ba6e58e56e2bc23374c15dc4,"@@ -1,19 +1,19 @@
   private void markAndSweepByRamSize() {
     assert markAndSweepLock.isHeldByCurrentThread() : ""markAndSweepLock held by another thread"";
     List<CacheEntry<K, V>> entriesInAccessOrder = new ArrayList<>(map.size());
     map.forEach((o, kvCacheEntry) -> {
       kvCacheEntry.lastAccessedCopy = kvCacheEntry.lastAccessed; // important because we want to avoid volatile read during comparisons
       entriesInAccessOrder.add(kvCacheEntry);
     });
 
     Collections.sort(entriesInAccessOrder); // newer access is smaller, older access is bigger
 
     // iterate in oldest to newest order
     for (int i = entriesInAccessOrder.size() - 1; i >= 0; i--) {
       CacheEntry<K, V> kvCacheEntry = entriesInAccessOrder.get(i);
       evictEntry(kvCacheEntry.key);
-      if (ramBytes.get() <= ramLowerWatermark)  {
+      if (ramBytes.sum() <= ramLowerWatermark)  {
         break; // we are done!
       }
     }
   }",NotBuggy,"SOLR-8241: Add CaffeineCache, an efficient implementation of SolrCache.
",Buggy,"The commit message introduces a new cache implementation. The code diff modifies the `markAndSweepByRamSize` method, specifically changing the condition `ramBytes.get() <= ramLowerWatermark` to `ramBytes.sum() <= ramLowerWatermark`. This suggests a correction in how the RAM usage is checked during the cache eviction process. The original code likely used an incorrect method to retrieve the RAM usage, leading to premature or delayed eviction. The change to `ramBytes.sum()` indicates that the correct method for summing RAM usage across the cache entries was not used before. This correction addresses a potential bug in the cache eviction logic. The answer is **Buggy**."
lucene-solr,16275.json,f177a660f5745350207dc61b46396b49404fd383,"@@ -1,29 +1,30 @@
   public ZkWriteCommand updateShardState(ClusterState clusterState, ZkNodeProps message) {
     String collectionName = message.getStr(ZkStateReader.COLLECTION_PROP);
     if (!checkCollectionKeyExistence(message)) return ZkStateWriter.NO_OP;
     log.info(""Update shard state invoked for collection: "" + collectionName + "" with message: "" + message);
 
     DocCollection collection = clusterState.getCollection(collectionName);
     Map<String, Slice> slicesCopy = new LinkedHashMap<>(collection.getSlicesMap());
     for (String key : message.keySet()) {
       if (ZkStateReader.COLLECTION_PROP.equals(key)) continue;
       if (Overseer.QUEUE_OPERATION.equals(key)) continue;
 
       Slice slice = collection.getSlice(key);
       if (slice == null) {
         throw new RuntimeException(""Overseer.updateShardState unknown collection: "" + collectionName + "" slice: "" + key);
       }
       log.info(""Update shard state "" + key + "" to "" + message.getStr(key));
       Map<String, Object> props = slice.shallowCopy();
       
-      if (Slice.State.getState((String) props.get(ZkStateReader.STATE_PROP)) == Slice.State.RECOVERY
-          && Slice.State.getState(message.getStr(key)) == Slice.State.ACTIVE) {
+      if (Slice.State.getState(message.getStr(key)) == Slice.State.ACTIVE) {
         props.remove(Slice.PARENT);
+        props.remove(""shard_parent_node"");
+        props.remove(""shard_parent_zk_session"");
       }
       props.put(ZkStateReader.STATE_PROP, message.getStr(key));
       Slice newSlice = new Slice(slice.getName(), slice.getReplicasCopy(), props);
       slicesCopy.put(slice.getName(), newSlice);
     }
 
     return new ZkWriteCommand(collectionName, collection.copyWithSlices(slicesCopy));
   }",NotBuggy,"SOLR-9438: Shard split can be marked successful and sub-shard states switched to 'active' even when one or more sub-shards replicas do not recover due to the leader crashing or restarting between the time the replicas are created and before they can recover
",Buggy,"The commit message indicates a fix for a shard split issue where sub-shards could be incorrectly marked as 'active' even if their replicas failed to recover due to leader crashes. The code diff modifies the `updateShardState` method. Specifically, when a slice's state is being updated to `ACTIVE`, the code now removes the `Slice.PARENT`, `""shard_parent_node""`, and `""shard_parent_zk_session""` properties. This suggests that the presence of these properties was preventing the replicas from properly recovering or causing the shard to be incorrectly marked as active. Therefore, the change addresses a bug related to shard split and replica recovery. The answer is **Buggy**."
lucene-solr,27550.json,54f256503867c4265a994b94af2917e1375c2181,"@@ -1,14 +1,14 @@
   protected int split(byte[] minPackedValue, byte[] maxPackedValue) {
     // Find which dim has the largest span so we can split on it:
     int splitDim = -1;
     for(int dim=0;dim<numDims;dim++) {
       NumericUtils.subtract(bytesPerDim, dim, maxPackedValue, minPackedValue, scratchDiff);
-      if (splitDim == -1 || StringHelper.compare(bytesPerDim, scratchDiff, 0, scratch1, 0) > 0) {
+      if (splitDim == -1 || FutureArrays.compareUnsigned(scratchDiff, 0, bytesPerDim, scratch1, 0, bytesPerDim) > 0) {
         System.arraycopy(scratchDiff, 0, scratch1, 0, bytesPerDim);
         splitDim = dim;
       }
     }
 
     //System.out.println(""SPLIT: "" + splitDim);
     return splitDim;
   }",NotBuggy,"LUCENE-8469: Inline calls to the deprecated StringHelper.compare, removed StringHelper.compare from master.
",NotBuggy,"The commit message indicates the removal of `StringHelper.compare` and inlining its calls. The diff shows that `StringHelper.compare` is replaced with `FutureArrays.compareUnsigned`. This change involves replacing a deprecated method with its functional equivalent, which is likely part of a larger effort to remove the deprecated method. This is not a bug fix, but rather a refactoring or code cleanup task. The answer is **NotBuggy**."
lucene-solr,47988.json,f1fda6880b784c8554439efcce4e6d464d97763b,"@@ -1,56 +1,54 @@
-  public static void decompress(DataInput compressed, int decompressedLen, BytesRef destBytes) throws IOException {
-    final byte[] dest = destBytes.bytes;
+  public static int decompress(DataInput compressed, int decompressedLen, byte[] dest, int dOff) throws IOException {
     final int destEnd = dest.length;
-    int dOff = 0;
 
-    while (dOff < decompressedLen) {
+    do {
       // literals
       final int token = compressed.readByte() & 0xFF;
       int literalLen = token >>> 4;
 
       if (literalLen != 0) {
         if (literalLen == 0x0F) {
           byte len;
           while ((len = compressed.readByte()) == (byte) 0xFF) {
             literalLen += 0xFF;
           }
           literalLen += len & 0xFF;
         }
         compressed.readBytes(dest, dOff, literalLen);
         dOff += literalLen;
       }
 
       if (dOff >= decompressedLen) {
         break;
       }
 
       // matchs
       final int matchDec = (compressed.readByte() & 0xFF) | ((compressed.readByte() & 0xFF) << 8);
       assert matchDec > 0;
 
       int matchLen = token & 0x0F;
       if (matchLen == 0x0F) {
         int len;
         while ((len = compressed.readByte()) == (byte) 0xFF) {
           matchLen += 0xFF;
         }
         matchLen += len & 0xFF;
       }
       matchLen += MIN_MATCH;
 
       // copying a multiple of 8 bytes can make decompression from 5% to 10% faster
       final int fastLen = ((matchLen - 1) & 0xFFFFFFF8) + 8;
       if (matchDec < matchLen || dOff + fastLen > destEnd) {
         // overlap -> naive incremental copy
         for (int ref = dOff - matchDec, end = dOff + matchLen; dOff < end; ++ref, ++dOff) {
           dest[dOff] = dest[ref];
         }
       } else {
         // no overlap -> arraycopy
         System.arraycopy(dest, dOff - matchDec, dest, dOff, fastLen);
         dOff += matchLen;
       }
-    }
-    destBytes.offset = 0;
-    destBytes.length = dOff;
+    } while (dOff < decompressedLen);
+
+    return dOff;
   }",NotBuggy,"LUCENE-4554: CompressingStoredFieldsFormat: don't write the original length at the beginning of the chunk with LZ4 compressors


git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1408476 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a change related to how the original length is written at the beginning of a chunk when using LZ4 compression in Lucene. The diff modifies the `decompress` method, changing its return type from `void` to `int`, and modifying its parameters. The core decompression logic appears to be retained, but the way the decompressed data is handled and the method's signature are altered. The change in return type and parameters suggests a modification in how the decompressed data length is managed, which aligns with the commit message. This change likely addresses an issue related to how the original length was handled, indicating a bug fix. The answer is **Buggy**.
"
lucene-solr,4912.json,23c73d8ff85f9b6eea30e2c03930d9e8ba116da0,"@@ -1,13 +1,10 @@
-  private void loadParams(String params, Properties props) throws SQLException {
-    try {
-      String[] pairs = params.split(""&"");
-      for (String pair : pairs) {
-        String[] keyValue = pair.split(""="");
-        String key = URLDecoder.decode(keyValue[0], ""UTF-8"");
-        String value = URLDecoder.decode(keyValue[1], ""UTF-8"");
-        props.put(key, value);
+  private void loadParams(URI uri, Properties props) throws SQLException {
+    List<NameValuePair> parsedParams = URLEncodedUtils.parse(uri, ""UTF-8"");
+    for (NameValuePair pair : parsedParams) {
+      if (pair.getValue() != null) {
+        props.put(pair.getName(), pair.getValue());
+      } else {
+        props.put(pair.getName(), """");
       }
-    } catch(Exception e) {
-      throw new SQLException(e);
     }
   }",NotBuggy,"SOLR-8179: SQL JDBC - DriverImpl loadParams doesn't support keys with no values in the connection string

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1716198 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for SOLR-8179, stating that the `loadParams` method in `DriverImpl` doesn't support keys without values in the connection string. The diff replaces the manual parsing of the connection string with `URLEncodedUtils.parse` and adds a check for null values, assigning an empty string if a value is not present. This addresses the bug described in the commit message. The answer is **Buggy**."
jna,3166.json,81908ed5b40ce3cdab32aabd29db56e2d0611e1f,"@@ -1,18 +1,18 @@
         public void fromXModifierKeymap(X11.XModifierKeymapRef xModifierKeymapRef) {
             int count = xModifierKeymapRef.max_keypermod;
             byte[] keys = xModifierKeymapRef.modifiermap.getByteArray(0, 8*count);
 
             ArrayList<Byte>[] allModifiers = getAllModifiers();
 
             for (int modNr = 0; modNr < 8; modNr++) {
                 ArrayList<Byte> modifier = allModifiers[modNr];
                 modifier.clear();
 
                 for (int keyNr = 0; keyNr < count; keyNr++) {
                     byte key = keys[modNr*count + keyNr];
                     if (key != 0) {
-                        modifier.add(new Byte(key));
+                        modifier.add(Byte.valueOf(key));
                     }
                 }
             }
         }",NotBuggy,"Apply generic definitions wherever applicable
",NotBuggy,"The commit message indicates a change to apply generic definitions. The code diff replaces `new Byte(key)` with `Byte.valueOf(key)`. This change is related to using the `valueOf` method for creating `Byte` objects, which can improve performance by using a cached instance for common byte values. This is a code improvement rather than a bug fix. The answer is **NotBuggy**.
"
jna,2910.json,bd184d5b389666d7eeeda102c46914196e31da8d,"@@ -1,9 +1,7 @@
     public static void lSetXAttr(String path, String name, byte[] value) throws IOException {
-        Memory valueMem = bytesToMemory(value);
-        final int retval = XAttr.INSTANCE.lsetxattr(path, name, valueMem,
-            new size_t(valueMem.size()), 0);
+        final int retval = XAttr.INSTANCE.lsetxattr(path, name, value, new size_t(value.length), 0);
         if (retval != 0) {
             final int eno = Native.getLastError();
             throw new IOException(""errno: "" + eno);
         }
     }",NotBuggy,"Add byte[] based bindings to c.s.j.p.linux.XAttr
",Buggy,"The commit message indicates the addition of byte[] based bindings to XAttr. The diff shows that the code was modified to directly use the byte[] value instead of converting it to Memory first. The original code converted the byte array to Memory, which is not necessary. The updated code directly passes the byte array to the native function, which is more efficient and correct. This change eliminates the unnecessary conversion to Memory, which could be considered a bug fix. The answer is **Buggy**."
jna,1326.json,3bba92c542dbb2ccd4daf6120ae914a43c297294,"@@ -1,3 +1,4 @@
     public static boolean IsWindowsXPSP3OrGreater() {
-        return IsWindowsVersionOrGreater(HIBYTE(WinNT.WIN32_WINNT_WINXP), LOBYTE(WinNT.WIN32_WINNT_WINXP), 3);
+        return IsWindowsVersionOrGreater((byte) (Kernel32.WIN32_WINNT_WINXP >>> 8), (byte) Kernel32.WIN32_WINNT_WINXP,
+                3);
     }",NotBuggy,"Javadocs, better typecasting",Buggy,"The commit message indicates improvements to Javadocs and typecasting. The diff involves a change in the `IsWindowsXPSP3OrGreater` method, specifically in how `WIN32_WINNT_WINXP` is used within `IsWindowsVersionOrGreater`. The original code used `HIBYTE` and `LOBYTE` macros, while the modified code directly casts `Kernel32.WIN32_WINNT_WINXP` to bytes using bitwise right shift (>>> 8) and a simple cast. This change suggests an attempt to handle the `WIN32_WINNT_WINXP` value more explicitly and correctly as bytes, potentially addressing an issue with how the version number was being interpreted. The explicit casting to byte suggests that the original code might have had issues with the size or representation of the `WIN32_WINNT_WINXP` constant, which could lead to incorrect version checks. This indicates a bug fix. The answer is **Buggy**."
jna,3850.json,d395318a8ba8744fb61805cde5644863daaf5f94,"@@ -1,3 +1,3 @@
     public String[] getWideStringArray(long offset, int length) {
-        return getStringArray(offset, -1, NativeString.WIDE_STRING);
+        return getStringArray(offset, length, NativeString.WIDE_STRING);
     }",NotBuggy,"Pointer.getWideStringArray respects the length parameter
",Buggy,"The commit message indicates that the `Pointer.getWideStringArray` method was not correctly using the provided `length` parameter. The diff shows that the method `getStringArray` is now being called with the `length` parameter, instead of a hardcoded `-1`. This suggests that the previous implementation was ignoring the length parameter, potentially leading to incorrect or unexpected behavior when retrieving wide string arrays. This change fixes that bug. The answer is **Buggy**.
"
jna,324.json,e4a6950d98b96f258fa31576ae9fc606091e66b6,"@@ -1,38 +1,41 @@
-	public static PRINTER_INFO_2 getPrinterInfo2(String printerName) {
-		IntByReference pcbNeeded = new IntByReference();
-		IntByReference pcReturned = new IntByReference();
-		HANDLEByReference pHandle = new HANDLEByReference();
+    public static PRINTER_INFO_2 getPrinterInfo2(String printerName) {
+        IntByReference pcbNeeded = new IntByReference();
+        IntByReference pcReturned = new IntByReference();
+        HANDLEByReference pHandle = new HANDLEByReference();
 
-		if (!Winspool.INSTANCE.OpenPrinter(printerName, pHandle, null))
-			throw new Win32Exception(Kernel32.INSTANCE.GetLastError());
+        if (!Winspool.INSTANCE.OpenPrinter(printerName, pHandle, null)) {
+            throw new Win32Exception(Kernel32.INSTANCE.GetLastError());
+        }
 
-		Win32Exception we = null;
-		PRINTER_INFO_2 pinfo2 = null;
+        Win32Exception we = null;
+        PRINTER_INFO_2 pinfo2 = null;
 
-		try {
-			Winspool.INSTANCE.GetPrinter(pHandle.getValue(), 2, null, 0, pcbNeeded);
-			if (pcbNeeded.getValue() <= 0)
-				return new PRINTER_INFO_2();
+        try {
+            Winspool.INSTANCE.GetPrinter(pHandle.getValue(), 2, null, 0, pcbNeeded);
+            if (pcbNeeded.getValue() <= 0) {
+                return new PRINTER_INFO_2();
+            }
 
-			pinfo2 = new PRINTER_INFO_2(pcbNeeded.getValue());
-			if (!Winspool.INSTANCE.GetPrinter(pHandle.getValue(), 2, pinfo2.getPointer(), pcbNeeded.getValue(), pcReturned))
-				throw new Win32Exception(Kernel32.INSTANCE.GetLastError());
+            pinfo2 = new PRINTER_INFO_2(pcbNeeded.getValue());
+            if (!Winspool.INSTANCE.GetPrinter(pHandle.getValue(), 2, pinfo2.getPointer(), pcbNeeded.getValue(), pcReturned)) {
+                throw new Win32Exception(Kernel32.INSTANCE.GetLastError());
+            }
 
-			pinfo2.read();
-		} catch (Win32Exception e) {
-			we = e;
-		} finally {
-			if (!Winspool.INSTANCE.ClosePrinter(pHandle.getValue())) {
-				Win32Exception ex = new Win32Exception(Kernel32.INSTANCE.GetLastError());
-				if (we != null) {
-					ex.addSuppressedReflected(we);
-				}
-			}
-		}
+            pinfo2.read();
+        } catch (Win32Exception e) {
+            we = e;
+        } finally {
+            if (!Winspool.INSTANCE.ClosePrinter(pHandle.getValue())) {
+                Win32Exception ex = new Win32Exception(Kernel32.INSTANCE.GetLastError());
+                if (we != null) {
+                    ex.addSuppressedReflected(we);
+                }
+            }
+        }
 
-		if (we != null) {
-			throw we;
-		}
+        if (we != null) {
+            throw we;
+        }
 
-		return pinfo2;
-	}

+        return pinfo2;
+    }",NotBuggy,"Checkstyle FileTabCharacter, Trailing Whitespace, line endings to \n, indention
",NotBuggy,"The commit message indicates changes related to code style and formatting, specifically Checkstyle rules, whitespace, line endings, and indentation. The diff shows changes in indentation and spacing, which aligns with the commit message. There are no changes related to error handling, logic, or exception handling. Therefore, it's a code style improvement rather than a bug fix. The answer is **NotBuggy**."
jna,3493.json,6705513b86030fd7d609d8e03539527d2a553651,"@@ -1,6 +1,9 @@
         protected void finalize() {
-            for (Iterator i=registeredClasses.entrySet().iterator();i.hasNext();) {
-                Map.Entry e = (Map.Entry)i.next();
-                unregister((Class)e.getKey(), (long[])e.getValue());
+            synchronized(registeredClasses) {
+                for (Iterator i=registeredClasses.entrySet().iterator();i.hasNext();) {
+                    Map.Entry e = (Map.Entry)i.next();
+                    unregister((Class)e.getKey(), (long[])e.getValue());
+                    i.remove();
+                }
             }
         }",NotBuggy,"add structure, string, NIO Buffer and primitive array support to direct type mapping

git-svn-id: https://svn.java.net/svn/jna~svn/trunk@841 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80
",Buggy,"The diff adds a `synchronized` block around the loop in the `finalize` method and calls `i.remove()` within the loop. This addresses a potential concurrency issue where the `registeredClasses` map could be modified by another thread while the iterator is in use, leading to a `ConcurrentModificationException`. The addition of `i.remove()` ensures that the entries are removed from the map as they are unregistered, preventing memory leaks and other potential issues. The synchronization and removal of elements from the iterator indicates a bug fix related to concurrent access and resource management. The answer is **Buggy**."
jna,3649.json,8e6cbda11092f95b4e00a0f650dfd3de52ac5bb8,"@@ -1,3 +1,3 @@
-        static boolean isBuffer(Class cls) {
+        static boolean isBuffer(Class<?> cls) {
             return Buffer.class.isAssignableFrom(cls);
         }",NotBuggy,"Use generic definition of Native#loadLibrary
",NotBuggy,"The commit message indicates a change to use a generic definition. The diff shows a change from `Class cls` to `Class<?> cls`. This change introduces a generic type to the `cls` parameter, which improves type safety and avoids potential unchecked cast warnings. This is a refactoring change and not a bug fix. The answer is **NotBuggy**.
"
jna,2373.json,596022a4f955a5decfd41e6c709a90a4680184f9,"@@ -1,43 +1,57 @@
 	public Object invoke(final Object proxy, final java.lang.reflect.Method method, final Object[] args)
 			throws Throwable {
                 boolean declaredAsInterface = 
                         (method.getAnnotation(ComMethod.class) != null)
                         ||(method.getAnnotation(ComProperty.class) != null);
             
 		if ((! declaredAsInterface) && (method.getDeclaringClass().equals(Object.class)
                         || method.getDeclaringClass().equals(IRawDispatchHandle.class)
                         || method.getDeclaringClass().equals(com.sun.jna.platform.win32.COM.util.IUnknown.class)
                         || method.getDeclaringClass().equals(com.sun.jna.platform.win32.COM.util.IDispatch.class)
                         || method.getDeclaringClass().equals(IConnectionPoint.class)
                         )) {
                         try {
                             return method.invoke(this, args);
                         } catch (InvocationTargetException ex) {
                             throw ex.getCause();
                         }
 		}
 
 		Class<?> returnType = method.getReturnType();
 		boolean isVoid = Void.TYPE.equals(returnType);
 
 		ComProperty prop = method.getAnnotation(ComProperty.class);
 		if (null != prop) {
+                        int dispId = prop.dispId();
 			if (isVoid) {
-				String propName = this.getMutatorName(method, prop);
-				this.setProperty(propName, args[0]);
-				return null;
+                                if(dispId != -1) {
+                                    this.setProperty(new DISPID(dispId), args[0]);
+                                    return null;
+                                } else {
+                                    String propName = this.getMutatorName(method, prop);
+                                    this.setProperty(propName, args[0]);
+                                    return null;
+                                }
 			} else {
-				String propName = this.getAccessorName(method, prop);
-				return this.getProperty(returnType, propName, args);
+                                if(dispId != -1) {
+                                    return this.getProperty(returnType, new DISPID(dispId), args);
+                                } else {
+                                    String propName = this.getAccessorName(method, prop);
+                                    return this.getProperty(returnType, propName, args);
+                                }
 			}
 		}
 
 		ComMethod meth = method.getAnnotation(ComMethod.class);
 		if (null != meth) {
-			String methName = this.getMethodName(method, meth);
-			Object res = this.invokeMethod(returnType, methName, args);
-			return res;
+                        int dispId = meth.dispId();
+                        if(dispId != -1) {
+                            return this.invokeMethod(returnType, new DISPID(dispId), args);
+                        } else {
+                            String methName = this.getMethodName(method, meth);
+                            return this.invokeMethod(returnType, methName, args);
+                        }
 		}
 
 		return null;
 	}",NotBuggy,"Add dispId to ComProperty and use value to simplify COM calls (skip GetIDsOfNames)

dispIDs can be queried via GetIDsOfNames at runtime or retrieved from the
typelibrary when bindings are generated.

Using the dispID eliminates one additional call into native environment.

It was also observed, that runtime reflection sometimes retrieves wrong
values (toString on IHTMLDocument2), so using a previously retrieved value
seems to be saver.
",Buggy,"The commit message indicates a performance improvement by using `dispId` to simplify COM calls, avoiding `GetIDsOfNames`. It also mentions a potential bug where runtime reflection retrieves wrong values. The code changes introduce the use of `dispId` from the `ComProperty` annotation. If `dispId` is present (not -1), it's used directly in `setProperty` and `getProperty`, otherwise, the old method using property names is used. This suggests that the `dispId` is used as an optimization and a workaround for potentially incorrect values retrieved by reflection, which can be considered a bug fix. The answer is **Buggy**."
jna,1645.json,429bed5aba63585863f9d734b7a1b0e8364610ff,"@@ -1,3 +1,3 @@
-		public SCODE getValue() {
-			return new SCODE(getPointer().getLong(0));
+		public CHAR getValue() {
+			return new CHAR(getPointer().getChar(0));
 		}",NotBuggy,"datatypes overworked 2
",Buggy,"The commit message ""datatypes overworked 2"" is vague. The diff shows a change in the return type of the `getValue()` method from `SCODE` to `CHAR`, and the method now retrieves a `char` instead of a `long` from the pointer. This suggests a change in how the value is interpreted or stored, which could be due to a bug fix or a change in the data model. Without more context, it's difficult to definitively say if this is a bug fix, but the change in data type and retrieval method suggests a correction or adjustment. The answer is **Buggy**."
jna,602.json,2cea7182931b6f5c314d9d26afa8d1f73a0fd400,"@@ -1,3 +1,3 @@
         public String getPrintName() {
-            return String.copyValueOf(PathBuffer, this.PrintNameOffset.intValue() / 2, this.PrintNameLength.intValue() / 2);
+            return String.copyValueOf(PathBuffer, PrintNameOffset / 2, PrintNameLength / 2);
         }",NotBuggy,"Changes based on feedback
",NotBuggy,"The commit message ""Changes based on feedback"" is vague. The diff simplifies the code by directly accessing `PrintNameOffset` and `PrintNameLength` instead of using `intValue()` on `Integer` objects. This change suggests a simplification or refactoring based on feedback, rather than a bug fix. The code becomes more readable and potentially slightly more efficient by avoiding the unnecessary method call.

The answer is **NotBuggy**."
jna,3448.json,fcf1f5c283ee83da50cfbf2b6d2900cc944eee18,"@@ -1,8 +1,3 @@
 	public ActiveWorkbook getActiveWorkbook() {
-		VARIANT.ByReference result = new VARIANT.ByReference();
-		HRESULT hr = oleMethod(OleAuto.DISPATCH_PROPERTYGET, result,
-				this.iDispatch, ""ActiveWorkbook"");
-
-		COMUtils.SUCCEEDED(hr);
-		return new ActiveWorkbook((IDispatch) result.getValue());
+		return new ActiveWorkbook(this.getAutomationProperty(""ActiveWorkbook""));
 	}",NotBuggy,"restored because of lost source code
",NotBuggy,"The commit message ""restored because of lost source code"" suggests a recovery of previously existing code. The diff simplifies the `getActiveWorkbook` method by directly using `getAutomationProperty` instead of manually handling `VARIANT` and `HRESULT`. While the change simplifies the code, the commit message indicates a restoration rather than a bug fix. Therefore, it's likely the original code was accidentally removed and this commit restores it. The answer is **NotBuggy**."
jna,657.json,e69d7fa485d90d0ae2541f2adbb0f194035188f7,"@@ -1,3 +1,3 @@
 		public void setValue(String value) {
-			getPointer().setString(0, value, true);
+			getPointer().setString(0, value);
 		}",NotBuggy,"some changes
",Buggy,"The diff modifies the `setValue` method to set a string value using a pointer. The original code used `setString(0, value, true)`, while the modified code uses `setString(0, value)`. The boolean parameter in the original code likely controlled whether a copy of the string was made or not. Removing this parameter suggests a change in how the string is handled, potentially affecting memory management or string immutability. Without more context, it's difficult to determine if this change is a bug fix. However, the removal of a parameter that likely controlled copying behavior suggests a potential correction or simplification of the string handling logic. The answer is **Buggy**.
"
jna,3553.json,ca95bfb424336336be4b3e2ef8d670281a554690,"@@ -1,18 +1,19 @@
     static void removeTemporaryFiles() throws IOException {
         File dir = getTempDir();
         FilenameFilter filter = new FilenameFilter() {
+            @Override
             public boolean accept(File dir, String name) {
                 return name.endsWith("".x"") && name.startsWith(JNA_TMPLIB_PREFIX);
             }
         };
         File[] files = dir.listFiles(filter);
         for (int i=0;files != null && i < files.length;i++) {
             File marker = files[i];
             String name = marker.getName();
             name = name.substring(0, name.length()-2);
             File target = new File(marker.getParentFile(), name);
             if (!target.exists() || target.delete()) {
                 marker.delete();
             }
         }
     }",NotBuggy,"Added volume management functions
",NotBuggy,"The commit message ""Added volume management functions"" doesn't directly suggest a bug fix. The code diff shows a function `removeTemporaryFiles` that removes temporary files ending with "".x"" and starting with ""jna"". It checks if a corresponding file (without the "".x"") exists or can be deleted before deleting the marker file. This looks like a cleanup routine. There's no clear indication of a bug being fixed here, so it's likely a new feature or enhancement. The answer is **NotBuggy**.
"
jna,3910.json,5dc6bfb195a7e309feb17ec9a69b4540b03fc7da,"@@ -1,3 +1,3 @@
-        public void setChar(int bOff, char value) {
+        public String getString(long bOff, boolean wide) {
             throw new UnsupportedOperationException(MSG); 
         }",NotBuggy,"use long offsets and sizes for pointer arithmetic

git-svn-id: https://svn.java.net/svn/jna~svn/trunk@381 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80
",Buggy,"The commit message indicates a change to use long offsets and sizes for pointer arithmetic. The diff shows the removal of a `setChar` method and the addition of a `getString` method with a `long` offset (`bOff`). This change suggests a modification to handle larger memory offsets, which aligns with the commit message's intention to use `long` for pointer arithmetic. While the diff doesn't explicitly show a bug fix, the change to use `long` offsets suggests a potential fix for issues related to addressable memory limitations. The answer is **Buggy**.
"
jna,2915.json,bd184d5b389666d7eeeda102c46914196e31da8d,"@@ -1,6 +1,4 @@
     public static String getXAttr(String path, String name, String encoding) throws IOException {
-        Memory valueMem = getXAttrAsMemory(path, name);
-        return Charset.forName(encoding)
-            .decode(valueMem.getByteBuffer(0, valueMem.size()))
-            .toString();
+        byte[] valueMem = getXAttrBytes(path, name);
+        return new String(valueMem, Charset.forName(encoding));
     }",NotBuggy,"Add byte[] based bindings to c.s.j.p.linux.XAttr
",NotBuggy,"The commit message indicates the addition of byte array-based bindings to the `XAttr` class. The diff shows that the `getXAttr` method is modified to directly return a byte array (`getXAttrBytes`) and then convert it to a String using the specified encoding. This change avoids the intermediate `Memory` object and `ByteBuffer` operations, which could be related to performance or memory management improvements. While it doesn't explicitly scream ""bug fix,"" the change in implementation suggests a potential improvement in how attributes are handled, possibly addressing a previous inefficiency or limitation. It's possible that the previous implementation had issues with certain character encodings or memory handling, which the new byte array-based approach resolves.

The answer is **NotBuggy**."
spring-framework,19645.json,fe7e2a7f54c49efd99de2236dfdddf2639c2b354,"@@ -1,3 +1,3 @@
-	public int compareTo(MediaTypeExpression other) {
+	public int compareTo(AbstractMediaTypeExpression other) {
 		return MediaType.SPECIFICITY_COMPARATOR.compare(this.getMediaType(), other.getMediaType());
 	}",NotBuggy,"Improvements to the way the content of RequestConditions is exposed.

RequestCondition types keep individual expression types (e.g. the
discrete header or param expressions) package private. Although the 
implementation of these types should remain private, there is no 
reason not to provide access to the underlying expression data --
e.g. for creating a REST endpoint documentation tool, or if you 
want to know which of the ""consumes""/""produces"" media types
are negated.

This change ensures that all RequestCondition types have a public
getter that makes available the basic expression data. 



",NotBuggy,"The commit message describes improvements to the way the content of RequestConditions is exposed, specifically focusing on providing access to underlying expression data for various RequestCondition types. The diff shows a change in the `compareTo` method signature of `MediaTypeExpression`, renaming the parameter type from `MediaTypeExpression` to `AbstractMediaTypeExpression`. This change suggests a widening of the type accepted by the method, potentially to allow comparison with a broader range of media type expressions. This aligns with the commit message's intent to expose more data and functionality related to RequestConditions. The change itself doesn't inherently indicate a bug fix, but rather an enhancement or refactoring to improve accessibility and flexibility. The answer is **NotBuggy**.
"
spring-framework,1371.json,00ed8da5c5b5d82cf60c90de1fe41c687c822b2f,"@@ -1,3 +1,3 @@
-	public Session getSession(Class<? extends Session> sessionType) {
+	public <S extends Session> S getSession(Class<S> sessionType) {
 		return getSession(sessionType, null);
 	}",NotBuggy,"JmsTransactionManager supports lazy resource retrieval

Closes gh-22468
",NotBuggy,"The commit message indicates a new feature: ""JmsTransactionManager supports lazy resource retrieval."" The code change involves modifying the `getSession` method to use a generic type `S` that extends `Session`. This change allows the method to return a more specific type of `Session` if needed, providing more flexibility in resource retrieval. This is not a bug fix, but rather an enhancement to the existing functionality. The answer is **NotBuggy**.
"
spring-framework,5771.json,098ac0bbb88cd178e85b7dc31642bed091560316,"@@ -1,4 +1,4 @@
-	public boolean equals(Object other) {
+	public boolean equals(@Nullable Object other) {
 		return (this == other || (other instanceof ApplicationListenerDetector &&
 				this.applicationContext == ((ApplicationListenerDetector) other).applicationContext));
 	}",NotBuggy,"Annotate Object#equals parameter with @Nullable

Closes gh-23093
",NotBuggy,"The commit message indicates the addition of a `@Nullable` annotation to the `equals` method's parameter. This change clarifies that the `other` parameter can be null, which is standard practice for `equals` methods. While this doesn't directly fix a bug in the sense of incorrect behavior, it improves code clarity and helps prevent potential NullPointerExceptions if the method is used improperly. It's more of a preventative measure and improved documentation than a bug fix. The answer is **NotBuggy**.
"
spring-framework,29158.json,347f34c68a7a5bab46cf869e762fb3cee9a1fedc,"@@ -1,3 +1,3 @@
 	public String getParameter(String paramName) {
-		return (String) getExternalContext().getRequestParameterMap().get(paramName);
+		return getExternalContext().getRequestParameterMap().get(paramName);
 	}",NotBuggy,"EL container integration; support for contextual objects; removal of deprecated Spring 2.0 functionality; Java 5 code style
",Buggy,"The code changes the return type of `getRequestParameterMap().get(paramName)` from `Object` to `String`. This suggests a type casting issue was present, which could lead to runtime errors if the returned object was not actually a String. By removing the explicit cast, the code now relies on the type information provided by the `getRequestParameterMap()` method, which is likely more accurate. This resolves a potential bug related to incorrect type casting. The answer is **Buggy**.
"
spring-framework,12706.json,a7c736915a4c23b3e3a5eb17d0c6aea904b53bd1,"@@ -1,16 +1,15 @@
 	public DataBuffer encodeValue(ByteBuf byteBuf, DataBufferFactory bufferFactory,
 			ResolvableType valueType, @Nullable MimeType mimeType, @Nullable Map<String, Object> hints) {
 
-		DataBuffer dataBuffer;
-		if (bufferFactory instanceof NettyDataBufferFactory) {
-			dataBuffer = ((NettyDataBufferFactory) bufferFactory).wrap(byteBuf);
-		} else {
-			dataBuffer = bufferFactory.wrap(byteBuf.nioBuffer());
-		}
-
 		if (logger.isDebugEnabled() && !Hints.isLoggingSuppressed(hints)) {
 			String logPrefix = Hints.getLogPrefix(hints);
-			logger.debug(logPrefix + ""Writing "" + dataBuffer.readableByteCount() + "" bytes"");
+			logger.debug(logPrefix + ""Writing "" + byteBuf.readableBytes() + "" bytes"");
 		}
-		return dataBuffer;
+		if (bufferFactory instanceof NettyDataBufferFactory) {
+			return ((NettyDataBufferFactory) bufferFactory).wrap(byteBuf);
+		}
+		byte[] bytes = new byte[byteBuf.readableBytes()];
+		byteBuf.readBytes(bytes);
+		byteBuf.release();
+		return bufferFactory.wrap(bytes);
 	}",NotBuggy,"Polishing and completing contribution

See gh-24866
",Buggy,"The commit message indicates polishing and completing a contribution. The code changes involve modifications to the `encodeValue` method. The original code created a `DataBuffer` based on whether the `bufferFactory` was a `NettyDataBufferFactory` or not. The modified code moves the debug logging statement and changes the way the `DataBuffer` is created when the `bufferFactory` is not a `NettyDataBufferFactory`. Instead of wrapping the `byteBuf.nioBuffer()`, it now reads the bytes from the `byteBuf` into a byte array, releases the `byteBuf`, and then wraps the byte array. This change likely addresses a potential issue with how `byteBuf` was being handled, possibly related to resource management or data consistency. The `byteBuf.release()` call suggests that the original code might have been leaking resources or not properly managing the `byteBuf`'s lifecycle. Therefore, this change is likely a bug fix. The answer is **Buggy**."
spring-framework,23230.json,9d6c38bd54fe8931ee73fbb7a076625deb934e28,"@@ -1,3 +1,3 @@
 	public boolean isCompilable() {
-		return this.exitTypeDescriptor != null;
+		return (this.exitTypeDescriptor != null);
 	}",NotBuggy,"Consistent bracket alignment
",NotBuggy,"The commit message indicates a change in code style related to bracket alignment. The diff shows that parentheses have been added around the expression `this.exitTypeDescriptor != null` in the `isCompilable` method's return statement. This change does not affect the functionality of the code and is purely for aesthetic or stylistic reasons. Therefore, it is not a bug fix. The answer is **NotBuggy**.
"
spring-framework,25172.json,e881d4b1441b4ade35806b6f06d5b7ebe1828e92,"@@ -1,4 +1,3 @@
-	public Jackson2ObjectMapperBuilder serializationInclusion(JsonInclude.Include serializationInclusion) {
-		this.serializationInclusion = serializationInclusion;
-		return this;
+	public Jackson2ObjectMapperBuilder serializationInclusion(JsonInclude.Include inclusion) {
+		return serializationInclusion(JsonInclude.Value.construct(inclusion, inclusion));
 	}",NotBuggy,"Allow use of JsonInclude.Value

Closes gh-24824
",NotBuggy,"The commit message indicates a feature enhancement to allow the use of `JsonInclude.Value`. The diff shows that the method `serializationInclusion` now accepts `JsonInclude.Include` and constructs a `JsonInclude.Value` object from it. This change allows for more fine-grained control over serialization inclusion, as `JsonInclude.Value` provides more options than `JsonInclude.Include`. This change does not appear to be fixing a bug, but rather adding functionality. The answer is **NotBuggy**.
"
spring-framework,470.json,b5e5e3307838359b42c7f4b1ed592b03b249eb4f,"@@ -1,3 +1,3 @@
 	public TransactionContext getParent() {
-		return parent;
+		return this.parent;
 	}",NotBuggy,"Remove legacy config options from AbstractReactiveTransactionManager

Includes general revision of reactive transaction sources.

See gh-22646
",NotBuggy,"The commit message indicates the removal of legacy config options and a general revision of reactive transaction sources. The provided diff shows a change in the `getParent()` method, where `parent` is changed to `this.parent`. This change doesn't seem to be directly related to removing legacy config options or a bug fix. It's likely a code style change or a minor refactoring. The answer is **NotBuggy**."
spring-framework,18397.json,b0790bf5e7b6091552ccf17a2e707ab438d129d7,"@@ -1,3 +1,5 @@
-	protected final void copyModelParameters(Map model, Transformer transformer) {
-		copyMapEntriesToTransformerParameters(model, transformer);
+	protected final void copyModelParameters(Map<String, Object> model, Transformer transformer) {
+		for (Map.Entry<String, Object> entry : model.entrySet()) {
+			transformer.setParameter(entry.getKey(), entry.getValue());
+		}
 	}",NotBuggy,"Java 5 code style
",NotBuggy,"The commit message ""Java 5 code style"" suggests a change to adhere to Java 5 coding conventions. The diff replaces `copyMapEntriesToTransformerParameters` with an explicit loop that iterates through the map entries and sets the transformer parameters. This change aligns with the commit message, as it replaces a method call with a more verbose, Java 5-compatible approach. It doesn't appear to be fixing a bug, but rather updating the code to a specific style. The answer is **NotBuggy**."
spring-framework,27849.json,d0c8545d9bf4b97c47d257b445cfe0a2ed09db88,"@@ -1,3 +1,3 @@
 	public String getCookieName() {
-		return cookieName;
+		return this.cookieName;
 	}",NotBuggy,"updated setCookieMaxAge to match getCookieMaxAge return type (SPR-6070)
",NotBuggy,"The commit message indicates an update to match the return type of `getCookieMaxAge` with `setCookieMaxAge`, suggesting a potential type mismatch or inconsistency. However, the provided diff only shows a change in `getCookieName` from `cookieName` to `this.cookieName`. This change doesn't seem to be related to the commit message's intent of aligning return types. It's a minor change that doesn't indicate a bug fix. The answer is **NotBuggy**.
"
spring-framework,19387.json,f2b926467491098b09b38296b95d9543c97e22ad,"@@ -1,3 +1,3 @@
-		public int read(@NotNull byte[] b, int off, int len) throws IOException {
+		public int read(byte[] b, int off, int len) throws IOException {
 			return this.delegate.read(b, off, len);
 		}",NotBuggy,"Remove Jetbrains annotations
",NotBuggy,"The commit message indicates the removal of Jetbrains annotations. The diff confirms this by showing the removal of the `@NotNull` annotation. This change doesn't fix a bug; it's more of a code cleanup or dependency management task. The answer is **NotBuggy**.
"
spring-framework,402.json,8dabb3e6269ab18bb2256266a70d4c285bbb4ff5,"@@ -1,8 +1,8 @@
-	private DefaultReactiveTransactionStatus newTransactionStatus(
-			ReactiveTransactionSynchronizationManager synchronizationManager, TransactionDefinition definition,
+	private GenericReactiveTransaction newReactiveTransaction(
+			TransactionSynchronizationManager synchronizationManager, TransactionDefinition definition,
 			@Nullable Object transaction, boolean newTransaction, boolean debug, @Nullable Object suspendedResources) {
 
-		return new DefaultReactiveTransactionStatus(transaction, newTransaction,
+		return new GenericReactiveTransaction(transaction, newTransaction,
 				!synchronizationManager.isSynchronizationActive(),
 				definition.isReadOnly(), debug, suspendedResources);
 	}",NotBuggy,"Shorter class names for common reactive transaction API types

Introduces TransactionExecution base interface for TransactionStatus as well as ReactiveTransaction. Renames getTransaction method to getReactiveTransaction, allowing for combined implementations of PlatformTransactionManager and ReactiveTransactionManager.

See gh-22646
",NotBuggy,The commit message describes refactoring and renaming of classes related to reactive transactions. The diff shows the renaming of `newTransactionStatus` to `newReactiveTransaction` and the replacement of `DefaultReactiveTransactionStatus` with `GenericReactiveTransaction`. These changes are consistent with the commit message and indicate a refactoring rather than a bug fix. The answer is **NotBuggy**.
spring-framework,6430.json,d93303c0089d311f2b014f45f1b345ca7ab9cb1f,"@@ -1,20 +1,21 @@
 		public void processGroupImports() {
 			for (DeferredImportSelectorGrouping grouping : this.groupings.values()) {
+				Predicate<String> candidateFilter = grouping.getCandidateFilter();
 				grouping.getImports().forEach(entry -> {
-					ConfigurationClass configurationClass = this.configurationClasses.get(
-							entry.getMetadata());
+					ConfigurationClass configurationClass = this.configurationClasses.get(entry.getMetadata());
 					try {
-						processImports(configurationClass, asSourceClass(configurationClass),
-								asSourceClasses(entry.getImportClassName()), false);
+						processImports(configurationClass, asSourceClass(configurationClass, candidateFilter),
+								Collections.singleton(asSourceClass(entry.getImportClassName(), candidateFilter)),
+								candidateFilter, false);
 					}
 					catch (BeanDefinitionStoreException ex) {
 						throw ex;
 					}
 					catch (Throwable ex) {
 						throw new BeanDefinitionStoreException(
 								""Failed to process import candidates for configuration class ["" +
 										configurationClass.getMetadata().getClassName() + ""]"", ex);
 					}
 				});
 			}
 		}",NotBuggy,"ImportSelector.getCandidateFilter() for transitive filtering of classes

Closes gh-24175
",NotBuggy,"The commit message indicates a feature addition related to transitive filtering of classes using `ImportSelector.getCandidateFilter()`. The code changes introduce a `candidateFilter` predicate and use it when processing imports. The `asSourceClass` method now takes this filter into account. This change does not appear to be fixing a bug, but rather adding functionality for more refined class filtering during import processing. The answer is **NotBuggy**."
spring-framework,42.json,a25e24f37ee46595ce30045c6fb9efdb692b7386,"@@ -1,3 +1,6 @@
-		public Object getResult() {
+		public T getResult() {
+			if (this.exception != null) {
+				ReflectionUtils.rethrowRuntimeException(this.exception);
+			}
 			return this.result;
 		}",NotBuggy,"generified TransactionCallback; WebSphereUowTransactionManager preserves original exception in case of rollback (SPR-5270)
",Buggy,"The commit message indicates a change related to exception handling during transaction rollback in WebSphereUowTransactionManager. The diff shows that if an exception occurred during the transaction, it is now rethrown using `ReflectionUtils.rethrowRuntimeException()` when `getResult()` is called. This ensures that the original exception is propagated, which is a bug fix. The answer is **Buggy**.
"
spring-framework,2798.json,098ac0bbb88cd178e85b7dc31642bed091560316,"@@ -1,10 +1,10 @@
-	public boolean equals(Object other) {
+	public boolean equals(@Nullable Object other) {
 		if (this == other) {
 			return true;
 		}
 		if (!(other instanceof DefaultIntroductionAdvisor)) {
 			return false;
 		}
 		DefaultIntroductionAdvisor otherAdvisor = (DefaultIntroductionAdvisor) other;
 		return (this.advice.equals(otherAdvisor.advice) && this.interfaces.equals(otherAdvisor.interfaces));
 	}",NotBuggy,"Annotate Object#equals parameter with @Nullable

Closes gh-23093
",NotBuggy,"The commit message indicates that the change is to add a `@Nullable` annotation to the `equals` method's parameter. The diff confirms this change. Adding `@Nullable` doesn't fix a bug but clarifies that the parameter can be null.
The answer is **NotBuggy**."
sonarqube,9746.json,1df148803610cd54f182b8636f01c0e6ece92b19,"@@ -1,3 +1,3 @@
-  public Date getUpdatedAt() {
+  public Long getUpdatedAt() {
     return updatedAt;
   }",NotBuggy,"Extract module sonar-db
",NotBuggy,"The commit message ""Extract module sonar-db"" suggests a structural change, likely moving code into a new module named ""sonar-db."" The diff shows a change in the return type of the `getUpdatedAt()` method from `Date` to `Long`. This could be part of the module extraction process, potentially to align with how timestamps are handled in the new module or to improve data serialization/deserialization. This change doesn't inherently indicate a bug fix, but rather a modification related to the module extraction. The answer is **NotBuggy**.
"
sonarqube,10338.json,85b940dd935f57860f26dff7f1bfaa9de1ba8af2,"@@ -1,4 +1,4 @@
   public List<String> selectQProfileUuidsByOrganizationAndGroups(DbSession dbSession, OrganizationDto organization, Collection<GroupDto> groups) {
-    return DatabaseUtils.executeLargeInputs(groups.stream().map(GroupDto::getId).collect(toList()),
+    return DatabaseUtils.executeLargeInputs(groups.stream().map(GroupDto::getUuid).collect(toList()),
       g -> mapper(dbSession).selectQProfileUuidsByOrganizationAndGroups(organization.getUuid(), g));
   }",NotBuggy,"SONAR-13221 change PK of GROUPS and update FKs
",Buggy,The commit message indicates a change in the primary key (PK) of the GROUPS table and an update to foreign keys (FKs). The code diff shows that the `selectQProfileUuidsByOrganizationAndGroups` method now uses `GroupDto::getUuid` instead of `GroupDto::getId`. This change suggests that the primary key of the `GROUPS` table has been changed from `id` to `uuid`. This change is likely to fix a bug or improve the data model. The answer is **Buggy**.
sonarqube,18294.json,74c4ac9a4a5c9df13116862dd5dca982cc3f84bd,"@@ -1,13 +1,13 @@
   public boolean equals(Object obj) {
     if (!(obj instanceof Rule)) {
       return false;
     }
     if (this == obj) {
       return true;
     }
     Rule other = (Rule) obj;
     return new EqualsBuilder()
-        .append(pluginName, other.getPluginName())
+        .append(pluginName, other.getRepositoryKey())
         .append(key, other.getKey())
         .isEquals();
   }",NotBuggy,"Deprecate Rule.setPluginName() , Rule.getPluginName() and related stuff, because was replaced by repositoryKey
",NotBuggy,"The commit message indicates the deprecation of `Rule.setPluginName()` and `Rule.getPluginName()` and their replacement with `repositoryKey`. The diff shows that the `equals` method now uses `getRepositoryKey()` instead of `getPluginName()` for comparison. This change aligns with the commit message and suggests a direct replacement of deprecated functionality with the new one. This is not a bug fix, but a replacement of deprecated functionality. The answer is **NotBuggy**."
sonarqube,14645.json,f6b2a269c44043f1c230b646dbb8662aed3fa5a4,"@@ -1,15 +1,15 @@
     private int computeValueForMetric(NewCoverageCounter counter, Metric metric) {
-      if (metric.getKey().equals(outputMetricKeys.getNewLinesToCover())) {
+      if (metric.getKey().equals(NEW_LINES_TO_COVER_KEY)) {
         return counter.getNewLines();
       }
-      if (metric.getKey().equals(outputMetricKeys.getNewUncoveredLines())) {
+      if (metric.getKey().equals(NEW_UNCOVERED_LINES_KEY)) {
         return counter.getNewLines() - counter.getNewCoveredLines();
       }
-      if (metric.getKey().equals(outputMetricKeys.getNewConditionsToCover())) {
+      if (metric.getKey().equals(NEW_CONDITIONS_TO_COVER_KEY)) {
         return counter.getNewConditions();
       }
-      if (metric.getKey().equals(outputMetricKeys.getNewUncoveredConditions())) {
+      if (metric.getKey().equals(NEW_UNCOVERED_CONDITIONS_KEY)) {
         return counter.getNewConditions() - counter.getNewCoveredConditions();
       }
       throw new IllegalArgumentException(""Unsupported metric "" + metric.getKey());
     }",NotBuggy,"SONAR-11795 Drop persistit and move coverage measures computation to CE side
",NotBuggy,"The commit message indicates a move of coverage measures computation to the CE (Compute Engine) side and dropping persistit. The diff replaces calls to getters (e.g., `outputMetricKeys.getNewLinesToCover()`) with direct references to constants (e.g., `NEW_LINES_TO_COVER_KEY`). This change suggests a refactoring to remove a dependency or improve performance by avoiding method calls. There's no indication of a bug fix in the traditional sense, such as correcting a calculation or handling an error. The change seems to be more about architectural improvement and code simplification.
The answer is **NotBuggy**."
sonarqube,2497.json,184dbe792ce14f192f75b13507c30a4b697d5852,"@@ -1,3 +1,3 @@
   private static Predicate<QProfileDto> byName(SearchWsRequest request) {
-    return p -> request.getProfileName() == null || Objects.equals(p.getName(), request.getProfileName());
+    return p -> request.getQualityProfile() == null || Objects.equals(p.getName(), request.getQualityProfile());
   }",NotBuggy,"SONAR-9865 Sanitize WS api/qualityprofiles/*

- functional key of a quality profile is: name, language and organization
- quality profile name is named 'name'
- quality profile key is named 'key'
- quality profile key parameter is deprecated when appropriate
",Buggy,"The commit message indicates sanitization of the WS api/qualityprofiles/*, focusing on quality profile identification using name, language, and organization. The code change replaces `request.getProfileName()` with `request.getQualityProfile()` in the `byName` predicate. This suggests a correction in how the quality profile's name is accessed from the request object, aligning with the commit's intention to sanitize the API. It's likely that `getProfileName()` was deprecated or incorrect, and `getQualityProfile()` is the correct method to retrieve the quality profile name. This indicates a bug fix. The answer is **Buggy**."
sonarqube,4754.json,0f0c3b73cc423fe90d6f4b97d6e73fe7bcdc485d,"@@ -1,5 +1,5 @@
   private static <PARAM> PARAM getNonNullValue(LiveMeasureDto measure, Function<LiveMeasureDto, PARAM> function) {
     PARAM value = function.apply(measure);
-    checkState(value != null, ""Measure not found"");
+    checkState(value != null, ""Measure has not been found"");
     return value;
   }",NotBuggy,"SONAR-10713 Badges can only be used on projects, long living branches and applications
",NotBuggy,"The commit message indicates a fix related to the usage of badges, restricting them to projects, long-living branches, and applications. The code change modifies the error message in the `checkState` method from ""Measure not found"" to ""Measure has not been found"". This change doesn't seem directly related to the restriction of badge usage mentioned in the commit message. It's a minor text update, likely for clarity or consistency, but it doesn't address a functional bug. The answer is **NotBuggy**.
"
sonarqube,21891.json,bff9cf373e2ecbe66344d58c3423b873f1fb17c5,"@@ -1,4 +1,4 @@
-  public SelectWsRequest setProjectKey(String projectKey) {
+  public RemoveGroupRequest setProjectKey(String projectKey) {
     this.projectKey = projectKey;
     return this;
   }",NotBuggy,"Auto-generate sonar-ws for integration tests
",Buggy,"The commit message indicates auto-generation of sonar-ws for integration tests, which doesn't directly imply a bug fix. The diff shows a change in the return type of a method from `SelectWsRequest` to `RemoveGroupRequest`. This suggests a potential refactoring or restructuring of the code related to web service requests, but not necessarily a bug fix. It's possible that the original return type was incorrect or caused issues, but without more context, it's difficult to definitively say if this change addresses a bug. However, the change in return type indicates a potential issue in the original code.

**Buggy**
"
sonarqube,17860.json,a4f8bbcf6f4d44cd0b637eff93ee8a3e49906edb,"@@ -1,4 +1,4 @@
-    public NewRule setType(Type t) {
+    public NewRule setType(RuleType t) {
       this.type = t;
       return this;
     }",NotBuggy,"SONAR-7353 Move RuleType to API
",NotBuggy,"The commit message indicates a change related to moving `RuleType` to an API. The code diff shows that the `setType` method's parameter type has been changed from `Type` to `RuleType`. This suggests a refactoring or restructuring of the codebase, specifically related to the `RuleType` class. It doesn't inherently indicate a bug fix. The answer is **NotBuggy**."
sonarqube,16129.json,3821d2d07b3d299f44a643d77aec3d48fadf6c74,"@@ -1,10 +1,10 @@
   public static SecurityStandards fromSecurityStandards(Set<String> securityStandards) {
     Set<String> standards = securityStandards.stream()
       .filter(Objects::nonNull)
       .collect(MoreCollectors.toSet());
     Set<String> owaspTop10 = toOwaspTop10(standards);
     Set<String> cwe = toCwe(standards);
     Set<String> sansTop25 = toSansTop25(cwe);
-    Set<String> sq = toSonarSourceSecurityCategories(cwe);
+    Set<SQCategory> sq = toSQCategories(cwe);
     return new SecurityStandards(standards, cwe, owaspTop10, sansTop25, sq);
   }",NotBuggy,"SONAR-12717 use enum for SQ categories in SecurityStandards
",NotBuggy,"The commit message indicates a change to use an enum for SQ categories in SecurityStandards. The diff shows that the type of the `sq` variable is changed from `Set<String>` to `Set<SQCategory>`. This change suggests a refactoring to use a more structured and type-safe representation for security categories, which can improve code clarity and maintainability. This is not a bug fix. The answer is **NotBuggy**."
sonarqube,22005.json,bff9cf373e2ecbe66344d58c3423b873f1fb17c5,"@@ -1,3 +1,3 @@
-  public String getComponentId() {
-    return componentId;
+  public String getComponent() {
+    return component;
   }",NotBuggy,"Auto-generate sonar-ws for integration tests
",NotBuggy,"The commit message indicates auto-generation of sonar-ws for integration tests, which doesn't directly suggest a bug fix. The diff shows a simple renaming of a method from `getComponentId` to `getComponent`. This change is likely related to aligning the code with the auto-generated sonar-ws or a broader refactoring effort. There's no indication of error handling, logical correction, or exception handling improvements. Therefore, the changes do not indicate a bug fix. The answer is **NotBuggy**.
"
sonarqube,16517.json,34cdbe891e9b45f42d35ee6deb2776cdfe50b431,"@@ -1,3 +1,3 @@
-  public Optional<Long> getDurationInMs() {
+  public Optional<Integer> getDurationInMs() {
     return Optional.ofNullable(durationInMs);
   }",NotBuggy,"SONAR-8353 persist deliveries in Compute Engine

Deliveries older than 1 month are purged.
",NotBuggy,"The commit message indicates a feature implementation related to persisting deliveries in Compute Engine, with a purging mechanism for older deliveries. The code diff changes the return type of `getDurationInMs()` from `Optional<Long>` to `Optional<Integer>`. This suggests a change in the data type used to represent the duration, possibly due to limitations or constraints in the persistence mechanism or to optimize storage. This change does not inherently indicate a bug fix, but rather a modification to the data representation. The answer is **NotBuggy**.
"
sonarqube,19146.json,6adb553ddc2711cb88632be9bd96b4e91c51082e,"@@ -1,3 +1,3 @@
   protected Logger newInstance(String name) {
-    return new ConsoleLogger(name);
+    return new ConsoleLogger();
   }",NotBuggy,"SONAR-5700 ability to change level of loggers of 3rd-party libraries
",NotBuggy,"The commit message indicates a change related to logging levels for third-party libraries. The diff shows a change in the `newInstance` method of a `Logger` class. The original code creates a `ConsoleLogger` with a given name, while the modified code creates a `ConsoleLogger` without a name. This change might be related to allowing the configuration of loggers without specific names, potentially affecting the ability to change the logging level of third-party libraries as described in the commit message. While it's not a direct bug fix, it's enabling a new feature or capability, which could be in response to a previous limitation or issue. Therefore, it's not a bug fix in the traditional sense. The answer is **NotBuggy**.
"
sonarqube,12862.json,1d8455866b14fc692bfebe2de64e4437438a5ce8,"@@ -1,4 +1,4 @@
-  public MetricDto setDomain(String domain) {
+  public MetricDto setDomain(@Nullable String domain) {
     this.domain = domain;
     return this;
   }",NotBuggy,"SONAR-8116 Return rating metrics in api/qualitygates/app
",NotBuggy,"The commit message indicates a change related to returning rating metrics in an API endpoint. The diff shows a change in the `setDomain` method of the `MetricDto` class, specifically adding the `@Nullable` annotation to the `domain` parameter. This change suggests that the `domain` parameter can now accept null values. While this change might be related to handling cases where the domain is not available, it doesn't directly indicate a bug fix in the sense of correcting incorrect behavior. It's more of an enhancement or a change to accommodate potentially missing data. The answer is **NotBuggy**."
sonarqube,3675.json,8de13aed2d978699d7f8d379f6af8610c173c29e,"@@ -1,4 +1,7 @@
   private Map<String, String> getKeysToDisplayMap(Set<String> keys) {
     return keys.stream()
-      .collect(Collectors.toMap(propertyDefinitions::validKey, Function.identity()));
+      .collect(Collectors.toMap(propertyDefinitions::validKey, Function.identity(),
+        (u, v) -> {
+          throw new IllegalArgumentException(format(""'%s' and '%s' cannot be used at the same time as they refer to the same setting"", u, v));
+        }));
   }",NotBuggy,"SONAR-7969 Fail when using key and deprecated in values WS

Display specific message when key and deprecated key are used at the same time
",Buggy,"The code change introduces a merge function to the `Collectors.toMap` operation. This merge function throws an `IllegalArgumentException` when duplicate keys are encountered, indicating that the same setting is being referred to by two different keys. This change prevents the application from proceeding with inconsistent or ambiguous configurations, which suggests a bug fix related to handling duplicate or conflicting keys. The answer is **Buggy**."
sonarqube,12954.json,e0ad2379d3e1f0e4c286570b5b86dd0a351589da,"@@ -1,4 +1,3 @@
   public void deleteGlobalProperty(String key, DbSession session) {
-    PropertiesMapper mapper = session.getMapper(PropertiesMapper.class);
-    mapper.deleteGlobalProperty(key);
+    getMapper(session).deleteGlobalProperty(key);
   }",NotBuggy,"SONAR-7676 some modernization of PropertiesDao and PropertiesDaoTest
",NotBuggy,"The commit message indicates modernization of `PropertiesDao` and `PropertiesDaoTest`. The diff shows a change in the `deleteGlobalProperty` method where `PropertiesMapper mapper = session.getMapper(PropertiesMapper.class); mapper.deleteGlobalProperty(key);` is replaced by `getMapper(session).deleteGlobalProperty(key);`. This change simplifies the code by directly calling `getMapper(session)` instead of assigning it to a local variable. This refactoring doesn't seem to address any specific bug. The answer is **NotBuggy**.
"
voldemort,6120.json,45f14abf745371e4920d9c4d7ea0164ddaa2b9e5,"@@ -1,4 +1,9 @@
     public static voldemort.client.protocol.pb.VProto.VoldemortRequest parseDelimitedFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return newBuilder().mergeDelimitedFrom(input).buildParsed();
+      Builder builder = newBuilder();
+      if (builder.mergeDelimitedFrom(input)) {
+        return builder.buildParsed();
+      } else {
+        return null;
+      }
     }",NotBuggy,"Updated Protobuff to 2.3
",Buggy,"The code adds a null check after attempting to merge from the input stream. If `mergeDelimitedFrom` fails (returns false), the method now returns null instead of proceeding with a potentially incomplete or invalid builder. This indicates a defensive programming approach to handle potential errors during the merge operation, which could be caused by malformed input or other I/O issues. This is likely a bug fix to prevent unexpected behavior or crashes when the input stream is invalid. The answer is **Buggy**.
"
voldemort,9648.json,b80351a00a6a6f3a0938aac86e57245badfe0daa,"@@ -1,8 +1,8 @@
     public void clearCount(int nodeId) {
         AtomicLong counter = values.get(nodeId);
         if(counter == null) {
             counter = new AtomicLong(0L);
-            values.putIfAbsent(nodeId, counter);
-        }
-        counter.set(0L);
+            values.put(nodeId, counter);
+        } else
+            counter.set(0L);
     }",NotBuggy,"Minor code cleanup
",NotBuggy,"The commit message indicates a minor code cleanup. The diff modifies the `clearCount` method. The original code used `putIfAbsent`, which would only put a new `AtomicLong` if one didn't already exist. However, the code then immediately sets the counter to 0. The modified code uses `put` instead of `putIfAbsent`. This ensures that the counter is always updated, even if it already exists. This change doesn't seem to fix a bug, but rather simplifies the code and ensures that the counter is always reset to 0. The answer is **NotBuggy**."
voldemort,938.json,2150275cb31c85a0a8d56744abd030c96f8ba804,"@@ -1,29 +1,28 @@
         public void pruneJob(int nodeId, String store) {
             logger.info(""Kicking off prune job on Node "" + nodeId + "" for store "" + store);
             VAdminProto.PruneJobRequest.Builder jobRequest = VAdminProto.PruneJobRequest.newBuilder()
                                                                                         .setStoreName(store);
 
             VAdminProto.VoldemortAdminRequest adminRequest = VAdminProto.VoldemortAdminRequest.newBuilder()
                                                                                               .setPruneJob(jobRequest)
                                                                                               .setType(VAdminProto.AdminRequestType.PRUNE_JOB)
                                                                                               .build();
-            // FIXME VC probably need a helper to do all this, at some point..
-            // All
+            // TODO probably need a helper to do all this, at some point.. all
             // of this file has repeated code
             Node node = AdminClient.this.getAdminClientCluster().getNodeById(nodeId);
             SocketDestination destination = new SocketDestination(node.getHost(),
                                                                   node.getAdminPort(),
                                                                   RequestFormatType.ADMIN_PROTOCOL_BUFFERS);
             SocketAndStreams sands = socketPool.checkout(destination);
 
             try {
                 DataOutputStream outputStream = sands.getOutputStream();
                 ProtoUtils.writeMessage(outputStream, adminRequest);
                 outputStream.flush();
             } catch(IOException e) {
                 helperOps.close(sands.getSocket());
                 throw new VoldemortException(e);
             } finally {
                 socketPool.checkin(destination, sands);
             }
         }",NotBuggy,"Adding test for Prune job
",NotBuggy,"The commit message indicates the addition of a test for the Prune job. The diff shows modifications to the `pruneJob` method, specifically changing ""FIXME"" to ""TODO"" in a comment. This change doesn't fix a bug but rather indicates a future improvement or task. The core functionality of the `pruneJob` method remains the same. The answer is **NotBuggy**."
voldemort,4195.json,d06fcec37fb3f3d45ffbd3fe15eafce77ba64069,"@@ -1,3 +1,3 @@
-      public int getRoStoreVersionsCount() {
-        return result.getRoStoreVersionsCount();
+      public voldemort.client.protocol.pb.VAdminProto.ROMetadataMap getRoStoreMetadata(int index) {
+        return result.getRoStoreMetadata(index);
       }",NotBuggy,"Another intermediate commit which has the code for changing rebalancing state. Still need to add rollback options
",NotBuggy,"The commit message indicates ongoing work related to rebalancing state changes, with a focus on adding rollback options. The diff shows a change in the return type of a method from `int` to `voldemort.client.protocol.pb.VAdminProto.ROMetadataMap`. This suggests a modification in how read-only store versions are handled or accessed, but without more context, it's difficult to determine if this change is directly related to fixing a bug. The change seems to be more about modifying the API or data structure used for managing read-only store versions. Therefore, it's unlikely to be a bug fix. The answer is **NotBuggy**."
voldemort,7783.json,94be1a5451f7c763e84401ce4eb6a9e58b27b91b,"@@ -1,11 +1,10 @@
-    private void writeResults(DataOutputStream outputStream, List<Versioned<byte[]>> values)
-            throws IOException {
-        outputStream.writeInt(values.size());
-        for(Versioned<byte[]> v: values) {
-            byte[] clock = ((VectorClock) v.getVersion()).toBytes();
-            byte[] value = v.getValue();
-            outputStream.writeInt(clock.length + value.length);
+    public void writeResponse(DataOutputStream outputStream) throws IOException {
+        outputStream.writeShort(0);
+        outputStream.writeInt(results.size());
+        for(Version v: results) {
+            byte[] clock = ((VectorClock) v).toBytes();
+
+            outputStream.writeInt(clock.length);
             outputStream.write(clock);
-            outputStream.write(value);
         }
     }",NotBuggy,"ShareBuffer Refactoring

Refactored the Shared Buffer code to eliminate the separate read and
write buffers. Now a common buffer is used and the code is refactored
into its own classes. running the unit test.
",NotBuggy,"The commit message indicates a refactoring of the Shared Buffer code, focusing on eliminating separate read and write buffers and reorganizing the code into its own classes. The provided diff snippet shows a change in the `writeResults` method, which is now `writeResponse`. The changes involve writing the size of the results and the clock information to the output stream. The value writing part is removed. This aligns with the refactoring goal of streamlining the buffer management. There's no explicit indication of a bug fix in the commit message or the diff. The changes seem to be focused on improving the code structure and potentially performance, rather than correcting a specific error. The answer is **NotBuggy**."
voldemort,3071.json,a4f42bf9d1d1b83d91d8f562019eb1f9e225460d,"@@ -1,4 +1,5 @@
       public Builder clearStoreName() {
-        result.storeName_ = java.util.Collections.emptyList();
+        result.hasStoreName = false;
+        result.storeName_ = getDefaultInstance().getStoreName();
         return this;
       }",NotBuggy,"Removed the new cleaner way to get read-only metadata. Fall back to the old dirty multiple message format to maintain backwards compatibility...Sigh!
",NotBuggy,"The commit message indicates a removal of a new cleaner approach in favor of an older method to maintain backward compatibility. The code change shows the `clearStoreName` method being reverted to its default state. The change suggests a decision to prioritize backward compatibility over a new feature or improvement, and the code reverts to a previous state. This doesn't inherently indicate a bug fix, but rather a deliberate rollback of functionality. The answer is **NotBuggy**."
voldemort,3530.json,a4f42bf9d1d1b83d91d8f562019eb1f9e225460d,"@@ -1,4 +1,5 @@
       public Builder clearStoreName() {
-        result.storeName_ = java.util.Collections.emptyList();
+        result.hasStoreName = false;
+        result.storeName_ = getDefaultInstance().getStoreName();
         return this;
       }",NotBuggy,"Removed the new cleaner way to get read-only metadata. Fall back to the old dirty multiple message format to maintain backwards compatibility...Sigh!
",Buggy,"The commit message indicates a reversion to an older method to maintain backward compatibility. The code change involves resetting the `storeName_` field to the default instance's value instead of simply clearing it. This suggests a potential issue with how the `storeName_` field was being handled in the newer implementation, possibly leading to incorrect or unexpected behavior. The fallback implies that the new cleaner way had issues. The answer is **Buggy**."
voldemort,6071.json,45f14abf745371e4920d9c4d7ea0164ddaa2b9e5,"@@ -1,4 +1,9 @@
     public static voldemort.client.protocol.pb.VProto.VoldemortRequest parseDelimitedFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return newBuilder().mergeDelimitedFrom(input).buildParsed();
+      Builder builder = newBuilder();
+      if (builder.mergeDelimitedFrom(input)) {
+        return builder.buildParsed();
+      } else {
+        return null;
+      }
     }",NotBuggy,"Updated Protobuff to 2.3
",Buggy,"The diff modifies the `parseDelimitedFrom` method to handle a potential failure during the merge operation. Specifically, it checks the return value of `mergeDelimitedFrom(input)`. If the merge operation fails (returns false), the method now returns null, indicating a parsing error. This addition of error handling suggests a bug fix related to how parsing failures were previously handled (or not handled). The answer is **Buggy**."
voldemort,978.json,0cc38b8ce750c9940eefe9b3274c4fb2f9e5437e,"@@ -1,28 +1,28 @@
     public static void validateReadOnlyStores(Cluster cluster,
                                               List<StoreDefinition> storeDefs,
                                               AdminClient adminClient) {
         List<StoreDefinition> readOnlyStores = filterStores(storeDefs, true);
 
         if(readOnlyStores.size() == 0) {
             // No read-only stores
             return;
         }
 
-        List<String> storeNames = RebalanceUtils.getStoreNames(readOnlyStores);
+        List<String> storeNames = getStoreNames(readOnlyStores);
         for(Node node: cluster.getNodes()) {
             if(node.getNumberOfPartitions() != 0) {
                 for(Entry<String, String> storeToStorageFormat: adminClient.getROStorageFormat(node.getId(),
                                                                                                storeNames)
                                                                            .entrySet()) {
                     if(storeToStorageFormat.getValue()
                                            .compareTo(ReadOnlyStorageFormat.READONLY_V2.getCode()) != 0) {
                         throw new VoldemortRebalancingException(""Cannot rebalance since node ""
                                                                 + node.getId() + "" has store ""
                                                                 + storeToStorageFormat.getKey()
                                                                 + "" not using format ""
                                                                 + ReadOnlyStorageFormat.READONLY_V2);
                     }
                 }
             }
         }
     }",NotBuggy,"a) Add metadata file for fetch files in case of new node
b) Moved down ""show-plan"" option to get better idea of what is happening
",NotBuggy,"The commit message mentions adding a metadata file and moving a ""show-plan"" option. The code diff focuses on validating read-only stores during rebalancing. Specifically, it retrieves the storage format of read-only stores on each node and checks if it matches the expected `READONLY_V2` format. If a mismatch is found, a `VoldemortRebalancingException` is thrown. The change itself doesn't seem to directly address a bug, but rather enforces a constraint on the storage format during rebalancing. The function `RebalanceUtils.getStoreNames` is replaced with `getStoreNames`, which could be a refactoring or a move of the function. Without more context, it's difficult to determine if this change is related to a bug fix.

**NotBuggy**
"
voldemort,8068.json,97d3f70279da869411b1d6c6344d5fe8b164c891,"@@ -1,9 +1,9 @@
     public int getAvailableNodeCount() {
         int available = 0;
 
         for(NodeStatus nodeStatus: nodeStatusMap.values())
-            if(nodeStatus.isAvailable())
+            if(isAvailable(nodeStatus))
                 available++;
 
         return available;
     }",NotBuggy,"Work on implementation-specific unit tests for FailureDetectors. Also reintroduced usage of Time to allow for easier unit testing.
",NotBuggy,"The commit message indicates work on unit tests and reintroduction of `Time` for easier testing, suggesting a focus on testability rather than direct bug fixes. The code change replaces `nodeStatus.isAvailable()` with `isAvailable(nodeStatus)`. Without knowing the implementation of `isAvailable(nodeStatus)`, it's hard to tell if this is a bug fix. However, given the context of the commit message, it's likely a refactoring or a change to align with the new testing framework. Therefore, it's unlikely to be a bug fix. The answer is **NotBuggy**."
voldemort,577.json,4c276b83ff229e20956e3f25803a31d633727318,"@@ -1,7 +1,7 @@
     public int taskCount() {
         int numTasks = 0;
         for(RebalanceBatchPlan batchPlan: batchPlans) {
-            numTasks += batchPlan.getBatchPlan().size();
+            numTasks += batchPlan.getTaskCount();
         }
         return numTasks;
     }",NotBuggy,"Added ""progress bar"" for rebalance batch plan

Added RebalanceBatchPlanProgressBar
- progress tracking object for each rebalancing batch
- integrated with RebalanceController, AsyncRebalanceTask, and
  (Donor|Stealer)BasedRebalanceTask

Did other general clean up of logging during rebalance to make it
appropriately verbose (more verbose in some places, less verbose in
others...)
",NotBuggy,"The commit message indicates the addition of a progress bar for rebalancing batches and general cleanup of logging. The diff shows a change in the `taskCount()` method, where `batchPlan.getBatchPlan().size()` is replaced with `batchPlan.getTaskCount()`. This suggests that the way the number of tasks is calculated for each batch plan has been modified, likely to accommodate the new progress tracking object. This change is related to the feature addition described in the commit message and doesn't directly indicate a bug fix. The answer is **NotBuggy**.
"
voldemort,42.json,e011531aa5e8504081b92044bd4b1ecf7baf791d,"@@ -1,8 +1,9 @@
             public boolean accept(Path input) {
-                if(input.getName().startsWith(Integer.toString(partitionId) + ""_""
-                                              + Integer.toString(replicaType))) {
+                if(input.getName().matches(""^"" + Integer.toString(partitionId) + ""_""
+                                           + Integer.toString(replicaType) + ""_""
+                                           + Integer.toString(chunkId) + ""\\.data"")) {
                     return true;
                 } else {
                     return false;
                 }
             }",NotBuggy,"a) Changed back RO to use one reducer per chunk - Why? Finding diff at one chunk level is simpler than at chunk set level
b) Changed collision iterator to return key as well
c) Loads of unit tests to test other wrapper data file chunk set code
",Buggy,"The code change modifies a file path filter. It changes the `startsWith` check to a `matches` check using a regular expression. The original code simply checked if the file name started with a specific string, while the updated code uses a regular expression to enforce a more precise pattern for the file name, including the chunkId and "".data"" extension. This suggests a bug fix where the original filter was too broad and incorrectly included files that didn't match the intended pattern. The commit message doesn't explicitly mention a bug fix related to this file path filtering, but the change from `startsWith` to a regular expression match indicates a refinement of the selection criteria, likely to address an issue where unintended files were being processed. The answer is **Buggy**.
"
voldemort,3034.json,d06fcec37fb3f3d45ffbd3fe15eafce77ba64069,"@@ -1,3 +1,3 @@
-      public java.util.List<voldemort.client.protocol.pb.VAdminProto.ROStoreVersionDirMap> getRoStoreVersionsList() {
-        return java.util.Collections.unmodifiableList(result.roStoreVersions_);
+      public java.util.List<voldemort.client.protocol.pb.VAdminProto.RebalancePartitionInfoMap> getRebalancePartitionInfoListList() {
+        return java.util.Collections.unmodifiableList(result.rebalancePartitionInfoList_);
       }",NotBuggy,"Another intermediate commit which has the code for changing rebalancing state. Still need to add rollback options
",NotBuggy,"The commit message indicates an intermediate step in implementing rebalancing state changes, focusing on adding code for this functionality. The diff shows a change in the return type of a method from `ROStoreVersionDirMap` to `RebalancePartitionInfoMap`. This suggests a modification related to how rebalancing partition information is accessed or represented. Since the commit message mentions ongoing work and the diff reflects a change in data structures related to rebalancing, it's likely part of a larger feature implementation rather than a specific bug fix. The answer is **NotBuggy**."
voldemort,10042.json,6f1cf577b021124cd3f377420c69299819ee3c40,"@@ -1,3 +1,5 @@
-        public List<Version> execute(Store<ByteArray, byte[]> store, ByteArray key) {
+        public List<Version> execute(Store<ByteArray, byte[], byte[]> store,
+                                     ByteArray key,
+                                     byte[] transforms) {
             return store.getVersions(key);
         }",NotBuggy,"Intermediate check-in for server-side transforms
Conflicts:

	contrib/mongodb/example/MongoDBClient.java
	contrib/mongodb/src/java/voldemort/store/mongodb/MongoDBStorageConfiguration.java
	contrib/mongodb/src/java/voldemort/store/mongodb/MongoDBStorageEngine.java
	contrib/mongodb/test/voldemort/store/mongodb/MongoDBStorageEngineTest.java
	src/java/voldemort/client/AbstractStoreClientFactory.java
	src/java/voldemort/client/SocketStoreClientFactory.java
	src/java/voldemort/server/StoreRepository.java
	src/java/voldemort/server/storage/StorageService.java
	src/java/voldemort/store/invalidmetadata/InvalidMetadataCheckingStore.java
	src/java/voldemort/store/readonly/ReadOnlyStorageEngine.java
	src/java/voldemort/store/rebalancing/RebootstrappingStore.java
	src/java/voldemort/store/rebalancing/RedirectingStore.java
	src/java/voldemort/store/routed/RoutedStore.java
	src/java/voldemort/store/socket/SocketStore.java
	test/integration/voldemort/performance/AbstractLoadTestHarness.java
	test/integration/voldemort/performance/LocalDirectLoadTest.java
	test/integration/voldemort/performance/LocalRoutedStoreLoadTest.java
	test/integration/voldemort/performance/ReadOnlyStorePerformanceTest.java
	test/integration/voldemort/performance/RemoteHttpStoreLoadTest.java
	test/integration/voldemort/performance/RemoteStoreComparisonTest.java
	test/integration/voldemort/performance/RemoteTest.java
	test/integration/voldemort/performance/SemiLocalHttpStoreLoadTest.java
	test/unit/voldemort/server/EndToEndTest.java
	test/unit/voldemort/store/routed/ReadRepairerTest.java
	test/unit/voldemort/store/routed/RoutedStoreTest.java
	test/unit/voldemort/store/socket/AbstractSocketStoreTest.java
",NotBuggy,"The commit message ""Intermediate check-in for server-side transforms"" indicates a feature addition or enhancement related to server-side data transformations. The diff modifies the `execute` method signature within an unspecified class, adding a `transforms` parameter of type `byte[]`. This suggests the introduction of a mechanism to pass transformation instructions or data to the store's `getVersions` method. This change doesn't directly indicate a bug fix but rather an evolution of the system's capabilities. The answer is **NotBuggy**."
voldemort,7698.json,7871933f0f0f056e2eeac03a01db1e9cf81f8bda,"@@ -1,3 +1,29 @@
     public boolean isCompleteRequest(ByteBuffer buffer) {
-        throw new VoldemortException(""Non-blocking server not supported for AdminServiceRequestHandler"");
+        DataInputStream inputStream = new DataInputStream(new ByteBufferBackedInputStream(buffer));
+
+        try {
+            int dataSize = inputStream.readInt();
+
+            if(logger.isTraceEnabled())
+                logger.trace(""In isCompleteRequest, dataSize: "" + dataSize + "", buffer position: ""
+                             + buffer.position());
+
+            if(dataSize == -1)
+                return true;
+
+            // Here we skip over the data (without reading it in) and
+            // move our position to just past it.
+            buffer.position(buffer.position() + dataSize);
+
+            return true;
+        } catch(Exception e) {
+            // This could also occur if the various methods we call into
+            // re-throw a corrupted value error as some other type of exception.
+            // For example, updating the position on a buffer past its limit
+            // throws an InvalidArgumentException.
+            if(logger.isTraceEnabled())
+                logger.trace(""In isCompleteRequest, probable partial read occurred: "" + e);
+
+            return false;
+        }
     }",NotBuggy,"Changes to support streaming in NIO-based socket server. No longer using interrupts to stop the NioSocketServer as this can interfere with server-side routing. Also deprecated voldemort.server.socket.AdminService class.
",Buggy,"The commit message indicates changes to support streaming in an NIO-based socket server and deprecates a class. The diff shows a significant change in the `isCompleteRequest` method. Instead of throwing an exception, it now attempts to read the size of the data from the buffer, skips over the data, and returns `true` if successful. It also includes error handling (try-catch block) to catch exceptions during the read or buffer manipulation, returning `false` in case of an error, which suggests a partial read. This addresses a potential issue where the server might not correctly handle incomplete requests, which can be considered a bug fix. The added logging also supports this interpretation. The answer is **Buggy**.
"
cassandra,18667.json,a991b64811f4d6adb6c7b31c0df52288eb06cf19,"@@ -1,5 +1,5 @@
-    public void close() throws IOException
+    public void close()
     {
-        if (subIterator != null)
-            subIterator.close();
+        if (iterator != null)
+            iterator.close();
     }",NotBuggy,"Storage engine refactor, a.k.a CASSANDRA-8099

Initial patch, see ticket for details
",NotBuggy,"The commit message indicates a refactoring of the storage engine. The diff shows a change in the `close()` method where `subIterator` is replaced with `iterator`. While refactoring can sometimes address underlying issues, the provided diff doesn't inherently suggest a bug fix. It seems to be a renaming or restructuring of the iterator being closed. Therefore, it's unlikely this change directly fixes a bug. The answer is **NotBuggy**."
cassandra,17698.json,2457599427d361314dce4833abeb5cd4915d0b06,"@@ -1,4 +1,4 @@
-    public int hashCode()
+    public final int hashCode()
     {
-        return Objects.hash(timestamp(), ttl(), localDeletionTime());
+        return Objects.hashCode(partitionDeletion, ranges);
     }",NotBuggy,"Simplify some 8099's implementations

patch by slebresne; reviewed by iamalesky for CASSANDRA-9705
",Buggy,"The commit message indicates simplification of implementations. The diff shows a change in the `hashCode()` method. The original method calculated the hash based on `timestamp()`, `ttl()`, and `localDeletionTime()`. The modified method calculates the hash based on `partitionDeletion` and `ranges`. This change suggests a simplification by using different fields for hash code calculation. It's difficult to determine if this is a bug fix without more context, but the change in hash code calculation could be due to a previous incorrect implementation or an optimization. Since the commit message suggests simplification, it's likely that the previous implementation had issues. The answer is **Buggy**."
cassandra,2369.json,831bebdba86ac1956852bd216a4cc62d898c87d7,"@@ -1,4 +1,4 @@
     public boolean isEmpty()
     {
-        return getDelegate().isEmpty();
+        return this.restrictions.isEmpty();
     }",NotBuggy,"Refactor Restriction hierarchy

patch by Benjamin Lerer; reviewed by Tyler Hobbs for CASSANDRA-11354
",NotBuggy,"The commit message indicates a refactoring of the Restriction hierarchy. The code change replaces `getDelegate().isEmpty()` with `this.restrictions.isEmpty()`. This change suggests that the `isEmpty()` method was previously relying on a delegate object, but now directly checks the `restrictions` field. This could be part of the refactoring to simplify the hierarchy or improve performance. It's unlikely to be a bug fix, as it seems to be a change in implementation rather than correcting erroneous behavior. The answer is **NotBuggy**.
"
cassandra,5527.json,2d991a7e4cc73b522f2c8adf14b5ff37a7947427,"@@ -1,4 +1,4 @@
     public ByteBuffer serialize(UUID value)
     {
-        return value == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : ByteBuffer.wrap(UUIDGen.decompose(value));
+        return value == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : UUIDGen.toByteBuffer(value);
     }",NotBuggy,"Minor optimizations

patch by slebresne; reviewed by blambov for CASSANDRA-10410

The patch includes the 3 minor optimizations desribed below:
1) Faster copy in CompositeType.build(). We call CompositeType.build()
   relatively often when a table has a composite partition key. When
   copying source buffers into the composite result in that method, we
   can use our existing ByteBuffer.arrayCopy() method which is supposed
   to be faster since it uses FastByteOperations and saves the
   duplication of the buffer to copy
2) Faster UUID->ByteBuffer. We convert UUID to their ByteBuffer
    representation in a few places and do that by converting the UUID to
    a byte[] and then wrapping it. But our convertion of UUID to byte[]
    writes the 2 long the UUID is composed of byte by byte, while for a
    ByteBuffer we can easily just ""put"" both long. So the patch
    introduces a specific method to do that. That's arguably cleaner
    anyway.
3) Memoize hash value for DataResource. Every request does a hash map
   lookup on a DataResource (for validating access rights) which imply
   calling DataResource.hashCode(). Currently that uses
   Objects.hashCode(), which is a varargs methods and allocate an array
   (which, according to MissionControl seems to not be stack allocated).
   The patch does 2 things: it computes the hash only once in the ctor
   and it stores the DataResource for each table in the CFMetaData and
   uses that when checking access permissions for SelectStatement and
   ModificationStatement (the 2 statements where performance matters).
   The 2nd point also saves some unecessary test that the table exists
   when we actually know it does.
",NotBuggy,"The commit message describes minor optimizations, including a faster UUID to ByteBuffer conversion. The diff replaces the existing method of converting a UUID to a byte array and then wrapping it in a ByteBuffer with a more direct `UUIDGen.toByteBuffer(value)` method. This aligns with the commit message's description of a faster and cleaner approach. This is an optimization, not a bug fix. The answer is **NotBuggy**.
"
cassandra,12544.json,490c1c27c9b700f14212d9591a516ddb8d0865c7,"@@ -1,18 +1,18 @@
         public void merge(CombinedValue<Long> other)
         {
             if (!(other instanceof Token))
                 return;
 
             Token o = (Token) other;
             assert o.get().equals(token);
 
             if (o instanceof DKToken)
             {
                 keys.addAll(((DKToken) o).keys);
             }
             else
             {
-                for (RowKey key : o)
+                for (DecoratedKey key : o)
                     keys.add(key);
             }
         }",NotBuggy,"Revert ""Add row offset support to SASI""

This reverts commit 7d857b46fb070548bf5e5f6ff81db588f08ec22a.
",Buggy,"The commit message indicates a revert of a previous commit that added row offset support to SASI. The diff shows a change in the `merge` method of the `Token` class, specifically reverting from `RowKey` to `DecoratedKey`. This suggests that the previous commit introduced a change that was later found to be problematic, necessitating a rollback. Reverting a commit is often done to undo a bug or problematic feature. The answer is **Buggy**.
"
cassandra,11667.json,0626be8667aefdcf50a051471f83da90bbae9bcf,"@@ -1,10 +1,12 @@
-    protected CBuilder buildIndexClusteringPrefix(ByteBuffer rowKey, ClusteringPrefix prefix, CellPath path)
+    public CBuilder buildIndexClusteringPrefix(ByteBuffer partitionKey,
+                                               ClusteringPrefix prefix,
+                                               CellPath path)
     {
         CBuilder builder = CBuilder.create(getIndexComparator());
-        builder.add(rowKey);
-        for (int i = 0; i < Math.min(columnDef.position(), prefix.size()); i++)
+        builder.add(partitionKey);
+        for (int i = 0; i < Math.min(indexedColumn.position(), prefix.size()); i++)
             builder.add(prefix.get(i));
-        for (int i = columnDef.position() + 1; i < prefix.size(); i++)
+        for (int i = indexedColumn.position() + 1; i < prefix.size(); i++)
             builder.add(prefix.get(i));
         return builder;
     }",NotBuggy,"New 2i API and implementations for built in indexes

Patch by Sam Tunnicliffe; reviewed by Sergio Bossa, Sylvain Lebresne and
Aleksey Yeschenko for CASSANDRA-9459

Conflicts:
	src/java/org/apache/cassandra/db/ColumnFamilyStore.java
	src/java/org/apache/cassandra/db/PartitionRangeReadCommand.java
	src/java/org/apache/cassandra/db/compaction/CompactionIterator.java
	src/java/org/apache/cassandra/db/index/AbstractSimplePerColumnSecondaryIndex.java
	src/java/org/apache/cassandra/db/index/PerColumnSecondaryIndex.java
	src/java/org/apache/cassandra/db/index/SecondaryIndexManager.java
	src/java/org/apache/cassandra/db/index/composites/CompositesIndex.java
	src/java/org/apache/cassandra/db/index/composites/CompositesIndexOnClusteringKey.java
	src/java/org/apache/cassandra/db/partitions/AtomicBTreePartition.java
	src/java/org/apache/cassandra/db/partitions/PartitionUpdate.java
	src/java/org/apache/cassandra/index/internal/ColumnIndexSearcher.java
	src/java/org/apache/cassandra/index/internal/composites/CompositesSearcher.java
	src/java/org/apache/cassandra/index/internal/keys/KeysSearcher.java
	src/java/org/apache/cassandra/io/sstable/format/SSTableReader.java
	test/unit/org/apache/cassandra/schema/DefsTest.java
	test/unit/org/apache/cassandra/schema/LegacySchemaMigratorTest.java
",NotBuggy,"The commit message indicates a new API and implementations for built-in indexes, which is a feature addition rather than a bug fix. The code diff modifies `buildIndexClusteringPrefix` to use `indexedColumn` instead of `columnDef`, which seems to be related to the new indexing API. The change also renames `rowKey` to `partitionKey` for clarity. These changes are part of the new feature implementation, not a bug fix. The answer is **NotBuggy**."
cassandra,17797.json,c055ab997ea66faccdb10eddd5241f909ff73408,"@@ -1,5 +1,5 @@
-    public static void writeDelTime(DeletionTime dt, SerializationHeader header, DataOutputPlus out) throws IOException
+    public void writeDeletionTime(DeletionTime dt, DataOutputPlus out) throws IOException
     {
-        out.writeVInt(header.encodeTimestamp(dt.markedForDeleteAt()));
-        out.writeVInt(header.encodeDeletionTime(dt.localDeletionTime()));
+        writeTimestamp(dt.markedForDeleteAt(), out);
+        writeLocalDeletionTime(dt.localDeletionTime(), out);
     }",NotBuggy,"Minor improvements to RowStats

patch by slebresne; reviewed by JoshuaMcKenzie for CASSANDRA-9828
",NotBuggy,"The commit message indicates minor improvements to RowStats. The diff shows a change in the method signature from `writeDelTime` to `writeDeletionTime` and removes the `SerializationHeader` parameter. Instead of using `header.encodeTimestamp` and `header.encodeDeletionTime`, it now calls `writeTimestamp` and `writeLocalDeletionTime` directly. This suggests a refactoring or simplification of the code, potentially to improve readability or maintainability. There's no explicit indication of a bug fix in the commit message or the diff itself. Therefore, the answer is **NotBuggy**."
cassandra,4841.json,17624248efc316de125d1bc3c6be4f0cb4e174a2,"@@ -1,14 +1,10 @@
                 protected Row computeNext()
                 {
-                    try {
-                        while (currentPage == null || !currentPage.hasNext())
-                        {
-                            if (pager.isExhausted())
-                                return endOfData();
-                            currentPage = select.process(pager.fetchPage(pageSize)).rows.iterator();
-                        }
-                        return new Row(metadata, currentPage.next());
-                    } catch (RequestValidationException | RequestExecutionException e) {
-                        throw new RuntimeException(e);
+                    while (currentPage == null || !currentPage.hasNext())
+                    {
+                        if (pager.isExhausted())
+                            return endOfData();
+                        currentPage = select.process(pager.fetchPage(pageSize)).rows.iterator();
                     }
+                    return new Row(metadata, currentPage.next());
                 }",NotBuggy,"Make CassandraException unchecked, extend RuntimeException

Patch by Robert Stupp, reviewed by Sylvain Lebresne for CASSANDRA-8560
",Buggy,"The provided diff removes a try-catch block that was wrapping the main logic of the `computeNext` method. The catch block was catching `RequestValidationException` and `RequestExecutionException` and re-throwing them as a `RuntimeException`. Since the commit message indicates that `CassandraException` is being made unchecked by extending `RuntimeException`, the try-catch block is no longer needed, as the exceptions will now propagate up the call stack as unchecked exceptions. This change is directly related to the commit message and indicates a bug fix or at least a change in how exceptions are handled. The answer is **Buggy**."
cassandra,992.json,7dffa83a0619613a17deb3f0d08858006f320e03,"@@ -1,22 +1,8 @@
-    static boolean serialize(List<GossipDigest> gDigestList, DataOutputStream dos) throws IOException
+    static void serialize(List<GossipDigest> gDigestList, DataOutputStream dos) throws IOException
     {
-        boolean bVal = true;
-        int size = gDigestList.size();                        
-        dos.writeInt(size);
-        
-        int estimate = 0;            
+        dos.writeInt(gDigestList.size());
         for ( GossipDigest gDigest : gDigestList )
         {
-            if ( Gossiper.MAX_GOSSIP_PACKET_SIZE - dos.size() < estimate )
-            {
-                logger_.info(""@@@@ Breaking out to respect the MTU size in GD @@@@"");
-                bVal = false;
-                break;
-            }
-            int pre = dos.size();               
             GossipDigest.serializer().serialize( gDigest, dos );
-            int post = dos.size();
-            estimate = post - pre;
         }
-        return bVal;
     }",NotBuggy,"remove obsolete gossip size limit.  patch by Anthony Molinaro and jbellis for CASSANDRA-1138

git-svn-id: https://svn.apache.org/repos/asf/cassandra/trunk@957593 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates the removal of an obsolete gossip size limit. The diff shows the removal of code related to checking and limiting the size of the gossip digest list being serialized. Specifically, it removes the logic that would break out of the serialization loop if the estimated size of the remaining data exceeded the maximum gossip packet size. This change directly addresses the obsolete size limit mentioned in the commit message. The removal of this limit suggests that it was previously causing issues or unnecessary restrictions, indicating a bug fix or improvement. The answer is **Buggy**."
cassandra,10965.json,59b5b6bef0fa76bf5740b688fcd4d9cf525760d0,"@@ -1,4 +1,4 @@
     public String getDatacenter(String host) throws UnknownHostException
     {
-        return DatabaseDescriptor.getEndpointSnitch().getDatacenter(InetAddress.getByName(host));
+        return DatabaseDescriptor.getEndpointSnitch().getDatacenter(InetAddressAndPort.getByName(host));
     }",NotBuggy,"Allow storage port to be configurable per node

Patch by Ariel Weisberg; Reviewed by Jason Brown for CASSANDRA-7544
",NotBuggy,"The commit message indicates a feature enhancement, allowing the storage port to be configurable per node. The code change replaces `InetAddress.getByName(host)` with `InetAddressAndPort.getByName(host)`. This change suggests that the original code only considered the IP address of the host, while the modified code now also considers the port. This is a feature enhancement rather than a bug fix. The answer is **NotBuggy**.
"
cassandra,11631.json,0626be8667aefdcf50a051471f83da90bbae9bcf,"@@ -1,4 +1,7 @@
-    protected ByteBuffer getIndexedValue(ByteBuffer rowKey, Clustering clustering, ByteBuffer cellValue, CellPath path)
+    public ByteBuffer getIndexedValue(ByteBuffer partitionKey,
+                                      Clustering clustering,
+                                      CellPath path,
+                                      ByteBuffer cellValue)
     {
         return cellValue;
     }",NotBuggy,"New 2i API and implementations for built in indexes

Patch by Sam Tunnicliffe; reviewed by Sergio Bossa, Sylvain Lebresne and
Aleksey Yeschenko for CASSANDRA-9459

Conflicts:
	src/java/org/apache/cassandra/db/ColumnFamilyStore.java
	src/java/org/apache/cassandra/db/PartitionRangeReadCommand.java
	src/java/org/apache/cassandra/db/compaction/CompactionIterator.java
	src/java/org/apache/cassandra/db/index/AbstractSimplePerColumnSecondaryIndex.java
	src/java/org/apache/cassandra/db/index/PerColumnSecondaryIndex.java
	src/java/org/apache/cassandra/db/index/SecondaryIndexManager.java
	src/java/org/apache/cassandra/db/index/composites/CompositesIndex.java
	src/java/org/apache/cassandra/db/index/composites/CompositesIndexOnClusteringKey.java
	src/java/org/apache/cassandra/db/partitions/AtomicBTreePartition.java
	src/java/org/apache/cassandra/db/partitions/PartitionUpdate.java
	src/java/org/apache/cassandra/index/internal/ColumnIndexSearcher.java
	src/java/org/apache/cassandra/index/internal/composites/CompositesSearcher.java
	src/java/org/apache/cassandra/index/internal/keys/KeysSearcher.java
	src/java/org/apache/cassandra/io/sstable/format/SSTableReader.java
	test/unit/org/apache/cassandra/schema/DefsTest.java
	test/unit/org/apache/cassandra/schema/LegacySchemaMigratorTest.java
",NotBuggy,"The commit message indicates a new API and implementations for built-in indexes, which is a feature addition rather than a bug fix. The diff shows a change in the signature of the `getIndexedValue` method, which is part of the new API. This change involves adding `partitionKey` as the first argument and swapping the order of `cellValue` and `CellPath`. This is part of the new feature implementation, not a bug fix. The answer is **NotBuggy**."
cassandra,20574.json,2457599427d361314dce4833abeb5cd4915d0b06,"@@ -1,5 +1,4 @@
     public Mutation makeMutation()
     {
-        assert update != null;
         return new Mutation(update);
     }",NotBuggy,"Simplify some 8099's implementations

patch by slebresne; reviewed by iamalesky for CASSANDRA-9705
",NotBuggy,"The commit message indicates simplification of code. The diff removes an assertion `assert update != null;`. Removing an assertion can be part of a simplification process, especially if the condition is already checked elsewhere or deemed unnecessary. This change doesn't inherently indicate a bug fix, but rather a code cleanup or simplification. The answer is **NotBuggy**.
"
cassandra,4540.json,35668435090eb47cf8c5e704243510b6cee35a7b,"@@ -1,14 +1,14 @@
     public boolean equals(Object o)
     {
         if (!(o instanceof UDAggregate))
             return false;
 
         UDAggregate that = (UDAggregate) o;
         return Objects.equal(name, that.name)
-            && Functions.typeEquals(argTypes, that.argTypes)
-            && Functions.typeEquals(returnType, that.returnType)
+            && Functions.typesMatch(argTypes, that.argTypes)
+            && Functions.typesMatch(returnType, that.returnType)
             && Objects.equal(stateFunction, that.stateFunction)
             && Objects.equal(finalFunction, that.finalFunction)
             && Objects.equal(stateType, that.stateType)
             && Objects.equal(initcond, that.initcond);
     }",NotBuggy,"Improve handling of UDA and UDF metadata

patch by Aleksey Yeschenko; reviewed by Robert Stupp for CASSANDRA-9665
",Buggy,"The commit message indicates an improvement in handling UDA and UDF metadata. The code diff replaces `typeEquals` with `typesMatch`. This suggests a change in the logic for comparing types, likely to handle type compatibility or equivalence more accurately. It is plausible that the original `typeEquals` method had a bug where it was not correctly identifying matching types, leading to incorrect behavior in UDA/UDF processing. The change to `typesMatch` likely fixes this issue. The answer is **Buggy**."
cassandra,20466.json,af3fe39dcabd9ef77a00309ce6741268423206df,"@@ -1,5 +1,9 @@
         public Builder add(KeyspaceMetadata keyspace)
         {
             keyspaces.put(keyspace.name, keyspace);
+
+            keyspace.tables.forEach(t -> tables.put(t.id, t));
+            keyspace.views.forEach(v -> tables.put(v.metadata.id, v.metadata));
+
             return this;
         }",NotBuggy,"Make TableMetadata immutable, optimize Schema

patch by Aleksey Yeschenko; reviewed by Sylvain Lebresne for
CASSANDRA-9425
",NotBuggy,"The commit message indicates changes related to making `TableMetadata` immutable and optimizing `Schema`. The diff shows that when a `KeyspaceMetadata` is added, the tables and views within that keyspace are also added to the `tables` map. This suggests that the change is related to ensuring that all tables and views are properly tracked when the schema is updated, potentially to improve performance or consistency. The change doesn't directly indicate a bug fix, but rather an optimization and structural improvement. The answer is **NotBuggy**."
cassandra,16107.json,a991b64811f4d6adb6c7b31c0df52288eb06cf19,"@@ -1,4 +1,4 @@
-    public int remainingCount()
-    {
-        return ctype.size() - size;
-    }

+        public int remainingCount()
+        {
+            return values.length - size;
+        }",NotBuggy,"Storage engine refactor, a.k.a CASSANDRA-8099

Initial patch, see ticket for details
",NotBuggy,"The commit message indicates a refactoring of the storage engine. The diff shows a change in the `remainingCount()` method, where `ctype.size()` is replaced with `values.length`. This suggests a change in how the remaining count is calculated, likely due to underlying data structure modifications during the refactoring. The change itself doesn't inherently indicate a bug fix, but rather an adaptation to the new storage engine structure. The answer is **NotBuggy**.
"
jgit,6893.json,6e05d98cce318056f95700e562cec6b68fcf7475,"@@ -1,4 +1,5 @@
 	public CloneCommand setDirectory(File directory) {
+		validateDirs(directory, gitDir, bare);
 		this.directory = directory;
 		return this;
 	}",NotBuggy,"Allow explicit configuration of git directory in CloneCommand

This feature is needed to support the new submodule layout where the
.git folder of the submodules is under .git/modules/<submodule>.

Change-Id: If5f13426cfd09b7677e23478e9700c8c25a6dae5
",NotBuggy,"The commit message indicates a new feature to allow explicit configuration of the git directory in CloneCommand, specifically to support submodule layouts. The diff adds a call to `validateDirs(directory, gitDir, bare)` in the `setDirectory` method. This suggests that the new feature involves validating the provided directory against the git directory and whether the repository is bare. This validation likely aims to prevent incorrect configurations or errors during the cloning process, which can be considered a preventative measure against potential bugs arising from misconfiguration. However, the change itself doesn't directly fix a bug. The answer is **NotBuggy**.
"
jgit,4585.json,683bd09092e90aef5b7cf963355995d76aefa439,"@@ -1,3 +1,3 @@
 		public CompressedBitmap andNot(Bitmap other) {
-			return new CompressedBitmap(bitmap.andNot(bitmapOf(other)));
+			return new CompressedBitmap(bitmap.andNot(ewahBitmap(other)), bitmapIndex);
 		}",NotBuggy,"Make BitmapIndexImpl.CompressedBitmap, CompressedBitmapBuilder static

A CompressedBitmap represents a pair (EWAH bit vector, PackIndex
assigning bit positions to git objects).  The bit vector is a member
field and the PackIndex is implicit via the 'this' reference to the
outer class.

Make this clearer by making CompressedBitmap a static class and
replacing the 'this' reference by an explicit field.

Likewise for CompressedBitmapBuilder.

Change-Id: Id85659fc4fc3ad82034db3370cce4cdbe0c5492c
Suggested-by: Terry Parker <tparker@google.com>
",NotBuggy,"The commit message indicates a refactoring to clarify the relationship between `CompressedBitmap`, `CompressedBitmapBuilder`, and their outer class `BitmapIndexImpl`. The change involves making `CompressedBitmap` and `CompressedBitmapBuilder` static and replacing the implicit `this` reference with an explicit field. The provided diff snippet shows that the `andNot` method now explicitly passes `bitmapIndex` to the `CompressedBitmap` constructor. This change improves code clarity and maintainability by making the dependencies explicit. There is no indication of a bug fix. The answer is **NotBuggy**.
"
jgit,319.json,6d370d837c5faa7caff2e6e3e4723b887f2fbdca,"@@ -1,3 +1,3 @@
-	protected boolean containsHelp(final String... args) {
+	protected boolean containsHelp(String... args) {
 		return TextBuiltin.containsHelp(args);
 	}",NotBuggy,"Remove 'final' in parameter lists

Change-Id: Id924f79c8b2c720297ebc49bf9c5d4ddd6d52547
Signed-off-by: Han-Wen Nienhuys <hanwen@google.com>
",NotBuggy,"The commit message indicates the removal of the `final` keyword from parameter lists. The diff confirms this change, removing `final` from the `args` parameter in the `containsHelp` method. This change does not appear to be related to fixing a bug; it's more likely a stylistic or code cleanup change. The answer is **NotBuggy**.
"
jgit,1651.json,1f2022e3a7e9482caa4823e031504b6bbe8246af,"@@ -1,6 +1,6 @@
 		public void onSendPack(UploadPack up,
 				Collection<? extends ObjectId> wants,
 				Collection<? extends ObjectId> haves)
-				throws UploadPackMayNotContinueException {
+				throws ServiceMayNotContinueException {
 			// Do nothing.
 		}",NotBuggy,"Modify refs in UploadPack/ReceivePack using a hook interface

This is intended to replace the RefFilter interface (but does not yet,
for backwards compatibility). That interface required lots of extra
scanning and copying in filter cases such as only advertising a subtree
of the refs directory. Instead, provide a hook that can be executed
right before ref advertisement, using the public methods on
UploadPack/ReceivePack to explicitly set the map of advertised refs.

Change-Id: I0067019a191c8148af2cfb71a675f2258c5af0ca
",NotBuggy,The commit message describes a change to replace the RefFilter interface with a hook interface for modifying refs in UploadPack/ReceivePack. The provided diff shows a change in the exception type thrown by the onSendPack method from UploadPackMayNotContinueException to ServiceMayNotContinueException. This change doesn't directly indicate a bug fix but rather a modification of the API or exception handling. It's likely related to the new hook interface and how it interacts with the existing UploadPack/ReceivePack functionality. The change in exception type doesn't inherently suggest a bug being fixed. The answer is **NotBuggy**.
jgit,3458.json,89b91ad40677c7048d31925986e48150e975bcce,"@@ -1,3 +1,3 @@
-			public InMemoryRepository build() throws IOException {
-				throw new UnsupportedOperationException();
-			}

+		public InMemoryRepository build() throws IOException {
+			return new InMemoryRepository(this);
+		}",NotBuggy,"InMemoryRepository: Use a real Builder class

Change-Id: I161b98a58503415955a21f2720395611f439ce98
",NotBuggy,"The commit message indicates a change to use a real Builder class for InMemoryRepository. The diff shows that the `build()` method, which previously threw an `UnsupportedOperationException`, is now implemented to return a new `InMemoryRepository` instance. This suggests that the previous implementation was incomplete or a placeholder, and the current change completes the Builder pattern implementation. This is not necessarily a bug fix, but rather a feature implementation or enhancement. The answer is **NotBuggy**."
jgit,1828.json,eadfcd3ec166c55c1ff3f3fe0b5e97dd94ff8d83,"@@ -1,8 +1,8 @@
-	private static boolean abort(Iterable<Command> cmdList) {
-		for (Command cmd : cmdList) {
-			if (cmd.getResult() == NOT_ATTEMPTED) {
-				reject(cmd, JGitText.get().transactionAborted);
+	public static void abort(Iterable<ReceiveCommand> commands) {
+		for (ReceiveCommand c : commands) {
+			if (c.getResult() == NOT_ATTEMPTED) {
+				c.setResult(REJECTED_OTHER_REASON,
+						JGitText.get().transactionAborted);
 			}
 		}
-		return false;
 	}",NotBuggy,"ReceiveCommand.abort(): Utility to mark batch of commands as failed

If one or more commands is failing the entire group usually has to
also fail with ""transaction aborted"". Pull this loop into a helper
so the idiom can be easily reused in several places throughout JGit.

Change-Id: I3b9399b7e26ce2b0dc5f7baa85d585a433b4eaed
",Buggy,"The commit message indicates the introduction of a utility method to mark a batch of commands as failed, specifically when a transaction is aborted. The diff shows the creation of a public static method `abort` that iterates through a collection of `ReceiveCommand` objects. If a command's result is `NOT_ATTEMPTED`, it's marked as `REJECTED_OTHER_REASON` with the message ""transactionAborted"". This aligns with the commit message's intention of providing a reusable way to handle transaction aborts. The original code likely had this logic duplicated in several places, and this change centralizes it into a helper method. This change addresses a potential bug where commands might not be properly marked as failed when a transaction is aborted, leading to inconsistent state. The answer is **Buggy**."
jgit,5753.json,f3ec7cf3f0436a79e252251a31dbc62694555897,"@@ -1,7 +1,7 @@
-	static RawCharSequence textFor(final RevCommit cmit) {
+	static RawCharSequence textFor(RevCommit cmit) {
 		final byte[] raw = cmit.getRawBuffer();
 		final int b = RawParseUtils.commitMessage(raw, 0);
 		if (b < 0)
 			return RawCharSequence.EMPTY;
 		return new RawCharSequence(raw, b, raw.length);
 	}",NotBuggy,"Remove further unnecessary 'final' keywords

Remove it from

 * package private functions.

 * try blocks

 * for loops

this was done with the following python script:

$ cat f.py
import sys
import re
import os

def replaceFinal(m):
  return m.group(1) + ""("" +  m.group(2).replace('final ', '') + "")""

methodDecl = re.compile(r""^([\t ]*[a-zA-Z_ ]+)\(([^)]*)\)"")

def subst(fn):
  input = open(fn)
  os.rename(fn, fn + ""~"")

  dest = open(fn, 'w')
  for l in input:
    l = methodDecl.sub(replaceFinal, l)
    dest.write(l)
  dest.close()


for root, dirs, files in os.walk(""."", topdown=False):
  for f in files:
    if not f.endswith('.java'):
      continue

    full = os.path.join(root, f)
    print full
    subst(full)

Change-Id: If533a75a417594fc893e7c669d2c1f0f6caeb7ca
Signed-off-by: Han-Wen Nienhuys <hanwen@google.com>
",NotBuggy,"The commit message indicates the removal of unnecessary `final` keywords from package-private functions, try blocks, and for loops. The provided diff shows the removal of the `final` keyword from the `textFor` method's parameter. This aligns with the commit message's intention to remove unnecessary `final` keywords. This change doesn't seem to be related to fixing any bug. The answer is **NotBuggy**.
"
jgit,5925.json,6d370d837c5faa7caff2e6e3e4723b887f2fbdca,"@@ -1,20 +1,20 @@
-	private void insertChild(final int stIdx, final DirCacheTree st) {
+	private void insertChild(int stIdx, DirCacheTree st) {
 		final DirCacheTree[] c = children;
 		if (childCnt + 1 <= c.length) {
 			if (stIdx < childCnt)
 				System.arraycopy(c, stIdx, c, stIdx + 1, childCnt - stIdx);
 			c[stIdx] = st;
 			childCnt++;
 			return;
 		}
 
 		final int n = c.length;
 		final DirCacheTree[] a = new DirCacheTree[n + 1];
 		if (stIdx > 0)
 			System.arraycopy(c, 0, a, 0, stIdx);
 		a[stIdx] = st;
 		if (stIdx < n)
 			System.arraycopy(c, stIdx, a, stIdx + 1, n - stIdx);
 		children = a;
 		childCnt++;
 	}",NotBuggy,"Remove 'final' in parameter lists

Change-Id: Id924f79c8b2c720297ebc49bf9c5d4ddd6d52547
Signed-off-by: Han-Wen Nienhuys <hanwen@google.com>
",NotBuggy,The commit message indicates the removal of the `final` keyword from parameter lists. The diff confirms this change in the `insertChild` method. Removing `final` from parameters doesn't inherently fix a bug; it's more of a stylistic or refactoring change. The functionality of the code remains the same. The answer is **NotBuggy**.
jgit,1557.json,d4e7b7060610678e863fbf091746ae1fcc240b0d,"@@ -1,18 +1,18 @@
 		byte[] toArray() throws IOException {
 			try {
 				if (length >= 0) {
 					final byte[] r = new byte[(int) length];
-					NB.readFully(in, r, 0, r.length);
+					IO.readFully(in, r, 0, r.length);
 					return r;
 				}
 
 				final ByteArrayOutputStream r = new ByteArrayOutputStream();
 				final byte[] buf = new byte[2048];
 				int n;
 				while ((n = in.read(buf)) >= 0)
 					r.write(buf, 0, n);
 				return r.toByteArray();
 			} finally {
 				in.close();
 			}
 		}",NotBuggy,"Move pure IO utility functions to a utility class of its own.

According the javadoc, and implied by the name of the class, NB
is about network byte order. The purpose of moving the IO only,
and non-byte order related functions to another class is to
make it easier for new contributors to understand that they
can use these functions in general and it's also makes it easier
to understand where to put new IO related utility functions

Change-Id: I4a9f6b39d5564bc8a694b366e7ff3cc758c5181b
Signed-off-by: Robin Rosenberg <robin.rosenberg@dewire.com>
Signed-off-by: Shawn O. Pearce <spearce@spearce.org>
",NotBuggy,"The commit message indicates a refactoring effort to improve code organization and clarity by moving IO utility functions to a dedicated utility class. The diff shows that `NB.readFully` is replaced with `IO.readFully`. This change aligns with the commit message's intention of separating IO-related functions from network byte order functions. There is no indication of a bug fix. The answer is **NotBuggy**.
"
jgit,8136.json,6d370d837c5faa7caff2e6e3e4723b887f2fbdca,"@@ -1,17 +1,17 @@
-	public static Boolean toBooleanOrNull(final String stringValue) {
+	public static Boolean toBooleanOrNull(String stringValue) {
 		if (stringValue == null)
 			return null;
 
 		if (equalsIgnoreCase(""yes"", stringValue) //$NON-NLS-1$
 				|| equalsIgnoreCase(""true"", stringValue) //$NON-NLS-1$
 				|| equalsIgnoreCase(""1"", stringValue) //$NON-NLS-1$
 				|| equalsIgnoreCase(""on"", stringValue)) //$NON-NLS-1$
 			return Boolean.TRUE;
 		else if (equalsIgnoreCase(""no"", stringValue) //$NON-NLS-1$
 				|| equalsIgnoreCase(""false"", stringValue) //$NON-NLS-1$
 				|| equalsIgnoreCase(""0"", stringValue) //$NON-NLS-1$
 				|| equalsIgnoreCase(""off"", stringValue)) //$NON-NLS-1$
 			return Boolean.FALSE;
 		else
 			return null;
 	}",NotBuggy,"Remove 'final' in parameter lists

Change-Id: Id924f79c8b2c720297ebc49bf9c5d4ddd6d52547
Signed-off-by: Han-Wen Nienhuys <hanwen@google.com>
",NotBuggy,"The commit message indicates the removal of the `final` keyword from parameter lists. The diff confirms this change in the `toBooleanOrNull` method. This change does not indicate a bug fix, but rather a stylistic or code convention update. The functionality of the code remains the same. The answer is **NotBuggy**."
jgit,1282.json,6d370d837c5faa7caff2e6e3e4723b887f2fbdca,"@@ -1,4 +1,4 @@
-	public void setPushRefSpecs(final List<RefSpec> specs) {
+	public void setPushRefSpecs(List<RefSpec> specs) {
 		push.clear();
 		push.addAll(specs);
 	}",NotBuggy,"Remove 'final' in parameter lists

Change-Id: Id924f79c8b2c720297ebc49bf9c5d4ddd6d52547
Signed-off-by: Han-Wen Nienhuys <hanwen@google.com>
",NotBuggy,"The commit message indicates the removal of the `final` keyword from parameter lists. The diff confirms this by showing the removal of `final` from the `specs` parameter in the `setPushRefSpecs` method. This change doesn't seem to be related to fixing a bug; it's more likely a style or convention change. The answer is **NotBuggy**.
"
jgit,348.json,f3ec7cf3f0436a79e252251a31dbc62694555897,"@@ -1,3 +1,3 @@
-	void enableBoundary(final boolean on) {
+	void enableBoundary(boolean on) {
 		enableRevSort(RevSort.BOUNDARY, on);
 	}",NotBuggy,"Remove further unnecessary 'final' keywords

Remove it from

 * package private functions.

 * try blocks

 * for loops

this was done with the following python script:

$ cat f.py
import sys
import re
import os

def replaceFinal(m):
  return m.group(1) + ""("" +  m.group(2).replace('final ', '') + "")""

methodDecl = re.compile(r""^([\t ]*[a-zA-Z_ ]+)\(([^)]*)\)"")

def subst(fn):
  input = open(fn)
  os.rename(fn, fn + ""~"")

  dest = open(fn, 'w')
  for l in input:
    l = methodDecl.sub(replaceFinal, l)
    dest.write(l)
  dest.close()


for root, dirs, files in os.walk(""."", topdown=False):
  for f in files:
    if not f.endswith('.java'):
      continue

    full = os.path.join(root, f)
    print full
    subst(full)

Change-Id: If533a75a417594fc893e7c669d2c1f0f6caeb7ca
Signed-off-by: Han-Wen Nienhuys <hanwen@google.com>
",NotBuggy,"The commit message indicates the removal of unnecessary 'final' keywords from package-private functions, try blocks, and for loops. The provided diff shows the removal of the `final` keyword from the `enableBoundary` method's parameter. This aligns with the commit message's intention to remove unnecessary `final` keywords. This change does not indicate a bug fix, but rather a code cleanup or style change. The answer is **NotBuggy**."
jgit,1850.json,6d370d837c5faa7caff2e6e3e4723b887f2fbdca,"@@ -1,3 +1,3 @@
-	public void setResult(final Result s) {
+	public void setResult(Result s) {
 		setResult(s, null);
 	}",NotBuggy,"Remove 'final' in parameter lists

Change-Id: Id924f79c8b2c720297ebc49bf9c5d4ddd6d52547
Signed-off-by: Han-Wen Nienhuys <hanwen@google.com>
",NotBuggy,"The commit message indicates the removal of the `final` keyword from parameter lists. The diff confirms this change in the `setResult` method. Removing `final` doesn't inherently fix a bug, but rather relaxes a restriction on the parameter. This change does not indicate a bug fix. The answer is **NotBuggy**.
"
jgit,7804.json,effc2f34b8fe282ac26f7eb3be028bdfab651ee3,"@@ -1,8 +1,8 @@
-	private boolean isBinary(Entry entry) throws IOException {
+	private static boolean isBinary(Entry entry) throws IOException {
 		InputStream in = entry.openInputStream();
 		try {
 			return RawText.isBinary(in);
 		} finally {
 			safeClose(in);
 		}
 	}",NotBuggy,"[findBugs] Declare some private methods of WorkingTreeIterator static

Change-Id: I09cd39c367f408b5a963ff004f235f558990f338
Signed-off-by: Matthias Sohn <matthias.sohn@sap.com>",NotBuggy,"The commit message indicates a change related to FindBugs, suggesting that some private methods in `WorkingTreeIterator` are being declared as static. The diff confirms this by adding the `static` keyword to the `isBinary` method. This change likely aims to improve code analysis and potentially performance by allowing the method to be called without an instance of the class, as it doesn't rely on instance-specific state. This change doesn't directly fix a bug but rather improves the code structure and maintainability. The answer is **NotBuggy**."
jgit,1077.json,17fb542e9eaca9dffc4c84a6d23608dadb4b1ea1,"@@ -1,4 +1,4 @@
 		public String getMessage() {
 			return MessageFormat.format(JGitText.get().largeObjectExceedsLimit,
-					getObjectName(), limit, size);
+					getObjectName(), Long.valueOf(limit), Long.valueOf(size));
 		}",NotBuggy,"Remove 86 boxing warnings

Use Integer, Character, and Long valueOf methods when
passing parameters to MessageFormat and other places
that expect objects instead of primitives

Change-Id: I5942fbdbca6a378136c00d951ce61167f2366ca4
",NotBuggy,"The commit message indicates the removal of boxing warnings by using `Integer`, `Character`, and `Long` `valueOf` methods. The diff shows the usage of `Long.valueOf` to convert primitive `long` values `limit` and `size` to `Long` objects before passing them to `MessageFormat.format`. This change addresses the boxing warning by explicitly creating `Long` objects instead of relying on implicit boxing. This is not a bug fix, but rather a code improvement to avoid boxing/unboxing overhead and warnings. The answer is **NotBuggy**.
"
weka,8307.json,9aaad03db32231667841ba32d5c424452de6029f,"@@ -1,18 +1,20 @@
   public void updateClassifier(Instance instance) throws Exception {
-    if (m_Filter.numPendingOutput() > 0)
+    if (m_Filter.numPendingOutput() > 0) {
       throw new Exception(""Filter output queue not empty!"");
+    }
     if (!m_Filter.input(instance)) {
-      if (m_Filter.numPendingOutput() > 0)
+      if (m_Filter.numPendingOutput() > 0) {
         throw new Exception(""Filter output queue not empty!"");
+      }
 
       // nothing to train on if the filter does not make an instance available
       return;
       // throw new
       // Exception(""Filter didn't make the train instance immediately available!"");
     }
 
     m_Filter.batchFinished();
     Instance newInstance = m_Filter.output();
 
     ((UpdateableClassifier) m_Classifier).updateClassifier(newInstance);
   }",NotBuggy,"Now implements UpdateableBatchProcessor

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@11099 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates the implementation of the `UpdateableBatchProcessor` interface. The code diff shows the implementation of the `updateClassifier` method. The method checks if the filter's output queue is empty before and after processing an instance. If the queue is not empty, an exception is thrown. This suggests that the code is designed to handle cases where the filter might not immediately produce an output instance, which could be a potential source of errors or unexpected behavior. The addition of exception handling and checks for filter output indicates a bug fix or prevention of a potential issue. The answer is **Buggy**."
weka,31489.json,6567cae2752e3d3cbfb7eefd7f1f5230c3f800fa,"@@ -1,3 +1,3 @@
   public String getRevision() {
-    return RevisionUtils.extract(""$Revision: 1.7 $"");
+    return RevisionUtils.extract(""$Revision: 1.8 $"");
   }",NotBuggy,"Now uses the current class loader rather than the system class loader to load icons


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@4400 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",NotBuggy,"The commit message indicates a change in how icons are loaded, specifically using the current class loader instead of the system class loader. The provided diff, however, only shows a change in the revision number. This discrepancy suggests that the actual code changes related to the class loader are not present in this diff. Therefore, based on the available information, it's impossible to determine if the change constitutes a bug fix. The answer is **NotBuggy**."
weka,25251.json,19e3b5a26bbba1eeced04be767b1839216872d5f,"@@ -1,7 +1,8 @@
   public int graphType() {
-    
-    if (m_Classifier instanceof Drawable)
-      return ((Drawable)m_Classifier).graphType();
-    else 
+
+    if (m_Classifier instanceof Drawable) {
+      return ((Drawable) m_Classifier).graphType();
+    } else {
       return Drawable.NOT_DRAWABLE;
+    }
   }",NotBuggy,"Further cleanup of option handling and general code cleanup, removing warnings.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@10153 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",NotBuggy,"The commit message indicates general code cleanup and removal of warnings. The diff introduces an `else` block to explicitly return `Drawable.NOT_DRAWABLE` when the classifier is not an instance of `Drawable`. This change enhances code clarity and ensures a defined return value in all cases. It does not appear to be fixing a bug, but rather improving the structure and readability of the code. The answer is **NotBuggy**."
weka,4982.json,1d7e46e21d7fb5c04f903c0b15f5348eaf41faaa,"@@ -1,19 +1,13 @@
   protected Vector computeMean(Instances data, double[] totalWeight, int aI) {
 
     Vector meanVector = new DenseVector(data.numAttributes() - 1);
     totalWeight[aI] = 0;
     for (Instance inst : data) {
       if (!inst.classIsMissing()) {
-        int index = 0;
-        for (int j = 0; j < inst.numAttributes(); j++) {
-          if (j != inst.classIndex()) {
-            meanVector.add(index, inst.weight() * inst.value(index));
-            index++;
-          }
-        }
+        meanVector.add(inst.weight(), instanceToVector(inst));
         totalWeight[aI] += inst.weight();
       }
     }
     meanVector.scale(1.0 / totalWeight[aI]);
     return meanVector;
   }",NotBuggy,"Further progress but probably not correct yet.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@13005 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message ""Further progress but probably not correct yet"" suggests ongoing development and potential bug fixes. The code diff simplifies the `computeMean` method by replacing the inner loop with a call to `instanceToVector`. The original code had an indexing error where `index` was used to access both the `meanVector` and `inst` values, but `index` was only incremented when `j != inst.classIndex()`. This could lead to incorrect calculations of the mean vector. The change replaces this logic with a call to `instanceToVector`, which presumably handles the attribute indexing correctly. This indicates a correction of a logical error. The answer is **Buggy**."
weka,4307.json,99f772d675df4a96af5dd8b8e60e50c3fc90ce78,"@@ -1,26 +1,26 @@
-  public void setMatrix ( double[] v, boolean columnFirst ) {
+  public void setMatrix(double[] v, boolean columnFirst) {
     try {
-      if( v.length != m * n ) 
-	throw new IllegalArgumentException(""sizes not match."");
-      int i, j, count = 0;
-      if( columnFirst ) {
-	for( i = 0; i < m; i++ ) {
-	  for( j = 0; j < n; j++ ) {
-	    A[i][j] = v[count];
-	    count ++;
-	  }
-	}
+      if (v.length != m * n) {
+        throw new IllegalArgumentException(""sizes not match."");
       }
-      else {
-	for( j = 0; j < n; j++ ) {
-	  for( i = 0; i < m; i++ ){
-	    A[i][j] = v[count];
-	    count ++;
-	  }
-	}
+      int i, j, count = 0;
+      if (columnFirst) {
+        for (i = 0; i < m; i++) {
+          for (j = 0; j < n; j++) {
+            A[i][j] = v[count];
+            count++;
+          }
+        }
+      } else {
+        for (j = 0; j < n; j++) {
+          for (i = 0; i < m; i++) {
+            A[i][j] = v[count];
+            count++;
+          }
+        }
       }
 
-    } catch( ArrayIndexOutOfBoundsException e ) {
-      throw new ArrayIndexOutOfBoundsException( ""Submatrix indices"" );
+    } catch (ArrayIndexOutOfBoundsException e) {
+      throw new ArrayIndexOutOfBoundsException(""Submatrix indices"");
     }
   }",NotBuggy,"Code clean-up: generic type arguments, elimination of FastVector, clean-up of option handling.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@10374 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",NotBuggy,"The commit message indicates code cleanup, specifically mentioning generic type arguments, elimination of FastVector, and cleanup of option handling. The diff shows changes in the `setMatrix` method, primarily related to spacing and formatting, and a minor change in the exception message. The core logic of the method remains the same. The `ArrayIndexOutOfBoundsException` is still caught and re-thrown with a slightly different message. There is no indication of a bug fix in this diff. The changes seem to be purely for code style and readability. The answer is **NotBuggy**."
weka,27566.json,7d9903c92f07f756915f15332672dfdcaf118c5d,"@@ -1,32 +1,39 @@
   public void buildClassifier(Instances data) throws Exception {
   
-    if (!data.classAttribute().isNumeric()) {
-      throw new Exception(""Class attribute has to be numeric for regression!"");
-    }
-    if (data.numInstances() == 0) {
-      throw new Exception(""No instances in training file!"");
-    }
-    if (data.checkForStringAttributes()) {
-      throw new Exception(""Can't handle string attributes!"");
+    if (!m_checksTurnedOff) {
+      if (!data.classAttribute().isNumeric()) {
+	throw new Exception(""Class attribute has to be numeric for regression!"");
+      }
+      if (data.numInstances() == 0) {
+	throw new Exception(""No instances in training file!"");
+      }
+      if (data.checkForStringAttributes()) {
+	throw new Exception(""Can't handle string attributes!"");
+      }
     }
 
     // Preprocess instances
     m_TransformedData = data;
-    m_TransformFilter = new NominalToBinaryFilter();
-    m_TransformFilter.setInputFormat(m_TransformedData);
-    m_TransformedData = Filter.useFilter(m_TransformedData, m_TransformFilter);
-    m_MissingFilter = new ReplaceMissingValuesFilter();
-    m_MissingFilter.setInputFormat(m_TransformedData);
-    m_TransformedData = Filter.useFilter(m_TransformedData, m_MissingFilter);
-    m_TransformedData.deleteWithMissingClass();
+    if (!m_checksTurnedOff) {
+      m_TransformFilter = new NominalToBinaryFilter();
+      m_TransformFilter.setInputFormat(m_TransformedData);
+      m_TransformedData = Filter.useFilter(m_TransformedData, m_TransformFilter);
+      m_MissingFilter = new ReplaceMissingValuesFilter();
+      m_MissingFilter.setInputFormat(m_TransformedData);
+      m_TransformedData = Filter.useFilter(m_TransformedData, m_MissingFilter);
+      m_TransformedData.deleteWithMissingClass();
+    } else {
+      m_TransformFilter = null;
+      m_MissingFilter = null;
+    }
     m_ClassIndex = m_TransformedData.classIndex();
 
     // Calculate attribute standard deviations
     calculateAttributeDeviations();
 
     // Perform the regression
     findBestModel();
 
     // Save memory
     m_TransformedData = new Instances(m_TransformedData, 0);
   }",NotBuggy,"Added more options


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@1166 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message ""Added more options"" is vague. The diff introduces a boolean flag `m_checksTurnedOff`. When this flag is true, the code skips data validation checks (numeric class attribute, no instances, no string attributes) and preprocessing steps (NominalToBinaryFilter, ReplaceMissingValuesFilter, deleteWithMissingClass). This suggests that the original code might have been too strict or inflexible, and the new option allows users to bypass these checks and preprocessing steps. This could be in response to situations where these checks were causing problems or were unnecessary. Therefore, the changes indicate a bug fix or workaround for a limitation in the original code. The answer is **Buggy**."
weka,32015.json,c94302fbe3f0a3b1c29340ff055b6f4ef5a329b7,"@@ -1,44 +1,44 @@
    public Object readCollection(Element node) throws Exception {
-      Collection           coll;
-      Vector               v;
-      Vector               children;
+      Collection<Object>           coll;
+      Vector<Object>               v;
+      Vector<Element>               children;
       Element              child;
       int                  i;
       int                  index;
       int                  currIndex;
 
       // for debugging only
       if (DEBUG)
          trace(new Throwable(), node.getAttribute(ATT_NAME));
 
       m_CurrentNode = node;
       
       children = XMLDocument.getChildTags(node); 
-      v        = new Vector();
+      v        = new Vector<Object>();
 
       // determine highest index for size
       index    = children.size() - 1;
       for (i = 0; i < children.size(); i++) {
         child     = (Element) children.get(i);
         currIndex = Integer.parseInt(child.getAttribute(ATT_NAME));
         if (currIndex > index)
           index = currIndex;
       }
       v.setSize(index + 1);
 
 
       // put the children in the vector to sort them according their index
       for (i = 0; i < children.size(); i++) {
          child = (Element) children.get(i);
          v.set(
                Integer.parseInt(child.getAttribute(ATT_NAME)), 
                invokeReadFromXML(child));
       }
       
       // populate collection
-      coll = (Collection) Class.forName(
-                  node.getAttribute(ATT_CLASS)).newInstance();
+      coll = Utils.cast(Class.forName(node.getAttribute(ATT_CLASS)).
+                        newInstance());
       coll.addAll(v);
       
       return coll;
    }",NotBuggy,"New version of core package that  that does not depend on FastVector anymore. However, FastVector still exists, extending ArrayList, for backwards compatibility: obviously lots of code outside the core package uses FastVector. Also eliminated all unchecked compile time warnings from the core package.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@5953 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",NotBuggy,"The commit message indicates a major refactoring, including the removal of a dependency on `FastVector` and the elimination of unchecked compile-time warnings. The diff shows changes in the `readCollection` method, specifically in the types of collections and vectors used. The code now uses generic types like `Collection<Object>` and `Vector<Object>`, and `Vector<Element>`. It also uses `Utils.cast` for type conversion, which suggests an effort to avoid unchecked casts and related warnings. These changes are related to the removal of dependencies and elimination of warnings, not directly related to fixing a specific bug. The answer is **NotBuggy**."
weka,36275.json,ae9b86a4d68c6d5a823bd20243149bb187a082e9,"@@ -1,39 +1,40 @@
   protected static weka.core.converters.Loader readStepPropertyLoader(
     JSONNode loaderNode) throws WekaException {
 
     String clazz = loaderNode.getChild(CLASS).getValue().toString();
     try {
       weka.core.converters.Loader loader =
-        (weka.core.converters.Loader) Beans.instantiate(
-          JSONFlowUtils.class.getClassLoader(), clazz);
+        (weka.core.converters.Loader) WekaPackageClassLoaderManager.objectForName(clazz);
+          /* Beans.instantiate(
+          JSONFlowUtils.class.getClassLoader(), clazz); */
 
       if (loader instanceof OptionHandler) {
         String optionString =
           loaderNode.getChild(OPTIONS).getValue().toString();
         if (optionString != null && optionString.length() > 0) {
           ((OptionHandler) loader).setOptions(Utils.splitOptions(optionString));
         }
       }
 
       if (loader instanceof weka.core.converters.AbstractFileLoader) {
         String filePath = loaderNode.getChild(""filePath"").getValue().toString();
         if (filePath.length() > 0) {
 
           ((weka.core.converters.AbstractFileLoader) loader)
             .setSource(new File(filePath));
         }
       }
 
       if (loader instanceof weka.core.converters.FileSourcedConverter) {
         Boolean relativePath =
           (Boolean) loaderNode.getChild(""useRelativePath"").getValue();
         ((weka.core.converters.FileSourcedConverter) loader)
           .setUseRelativePath(relativePath);
 
       }
 
       return loader;
     } catch (Exception ex) {
       throw new WekaException(ex);
     }
   }",NotBuggy,"Changes necessary to support using child classloaders for packages

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@13476 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",NotBuggy,"The commit message indicates changes to support child classloaders for packages. The code diff replaces `Beans.instantiate(JSONFlowUtils.class.getClassLoader(), clazz)` with `WekaPackageClassLoaderManager.objectForName(clazz)`. This change suggests a modification in how classloaders are managed, likely to allow for more flexible or isolated classloading scenarios, which is related to supporting child classloaders. The original code used the classloader of `JSONFlowUtils`, while the new code uses `WekaPackageClassLoaderManager` to obtain the object. This could be to address issues with class visibility or classloader isolation when using child classloaders. The change directly addresses the commit message's intention. It's not immediately apparent if this is a bug fix, but it is enabling a new feature or capability. The answer is **NotBuggy**."
weka,23944.json,08514d45c889fe77d6b7780452f396dd83daf165,"@@ -1,4 +1,5 @@
   public void setRowHidden(int index, boolean hidden) {
-    if ( (index >= 0) && (index < getRowCount()) )
+    if ((index >= 0) && (index < getRowCount())) {
       m_RowHidden[index] = hidden;
+    }
   }",NotBuggy,"DefaultListModels is not generic in Java 1.6. Cleaned up a further few classes.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@10204 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",NotBuggy,"The commit message indicates a cleanup and adaptation for Java 1.6, specifically mentioning generics. The diff shows a minor change in the `setRowHidden` method, adding braces to an `if` statement. This change doesn't seem to be related to generics or any specific bug fix. It's more likely a code style improvement for better readability or maintainability. The answer is **NotBuggy**."
weka,19765.json,1e7162781f076ec9a5024dfa5a04bec8d4e4e0f5,"@@ -1,10 +1,12 @@
   public void processHeadlessEvents(List<EventObject> headless) {
     // only process if we're not headless
     if (!java.awt.GraphicsEnvironment.isHeadless()) {
+      m_processingHeadlessEvents = true;
       for (EventObject e : headless) {
         if (e instanceof DataSetEvent) {
           acceptDataSet((DataSetEvent)e);
         }
       }
     }
+    m_processingHeadlessEvents = false;
   }",NotBuggy,"No longer generates image events when processing previously collected events received during headless execution (prevents charts from being generated twice - once when executed headless on the server and then a second time when results are retrieved).

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@7680 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix to prevent duplicate chart generation when processing events received during headless execution. The code changes introduce a check `java.awt.GraphicsEnvironment.isHeadless()` to ensure that events are only processed if the application is not running in headless mode. The added lines `m_processingHeadlessEvents = true;` and `m_processingHeadlessEvents = false;` seem misplaced and unnecessary, as the loop is already guarded by the `isHeadless()` check. However, the core logic of preventing duplicate chart generation based on the headless state suggests a bug fix. The answer is **Buggy**."
weka,35996.json,c94302fbe3f0a3b1c29340ff055b6f4ef5a329b7,"@@ -1,41 +1,41 @@
   protected static Instances getMiningSchemaAsInstances(Element model,
                                                         Instances dataDictionary) 
     throws Exception {
-    FastVector attInfo = new FastVector();
+    ArrayList<Attribute> attInfo = new ArrayList<Attribute>();
     NodeList fieldList = model.getElementsByTagName(""MiningField"");
     int classIndex = -1;
     int addedCount = 0;
     for (int i = 0; i < fieldList.getLength(); i++) {
       Node miningField = fieldList.item(i);
       if (miningField.getNodeType() == Node.ELEMENT_NODE) {
         Element miningFieldEl = (Element)miningField;
         String name = miningFieldEl.getAttribute(""name"");
         String usage = miningFieldEl.getAttribute(""usageType"");
         // TO-DO: also missing value replacement etc.
 
         // find this attribute in the dataDictionary
         Attribute miningAtt = dataDictionary.attribute(name);
         if (miningAtt != null) {
           if (usage.length() == 0 || usage.equals(""active"") || usage.equals(""predicted"")) {
-            attInfo.addElement(miningAtt);
+            attInfo.add(miningAtt);
             addedCount++;
           }
           if (usage.equals(""predicted"")) {
             classIndex = addedCount - 1;
           }
         } else {
           throw new Exception(""Can't find mining field: "" + name 
                               + "" in the data dictionary."");
         }
       }
     }
     
     Instances insts = new Instances(""miningSchema"", attInfo, 0);
     //    System.out.println(insts);
     if (classIndex != -1) {
       insts.setClassIndex(classIndex);
     }
 
 
     return insts;
   }",NotBuggy,"New version of core package that  that does not depend on FastVector anymore. However, FastVector still exists, extending ArrayList, for backwards compatibility: obviously lots of code outside the core package uses FastVector. Also eliminated all unchecked compile time warnings from the core package.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@5953 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",NotBuggy,"The commit message indicates a transition from using `FastVector` to `ArrayList` for core package classes, aiming to remove dependencies and eliminate unchecked warnings. The code diff shows a change from `FastVector` to `ArrayList<Attribute>` in the `getMiningSchemaAsInstances` method. This change aligns with the commit message's goal of removing `FastVector` dependencies. Since the change is a refactoring to remove a dependency, it is not a bug fix. The answer is **NotBuggy**."
weka,20858.json,60be28974823b8c8f15a54035f2cdee1449d7617,"@@ -1,6 +1,7 @@
             public void actionPerformed(ActionEvent e) {
-              bc.remove();
+              bc.remove(m_mainKFPerspective.getCurrentTabIndex());
               m_beanLayout.revalidate();
               m_beanLayout.repaint();
+              m_mainKFPerspective.setEditedStatus(true);
               notifyIsDirty();
             }",NotBuggy,"Fisrt stage of modernizing the KnowledgeFlow user interface and functionality.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@7124 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",NotBuggy,"The commit message indicates a modernization of the KnowledgeFlow user interface. The code diff shows a modification to the `actionPerformed` method, specifically when a component is removed. The change involves removing a component at a specific tab index (`m_mainKFPerspective.getCurrentTabIndex()`) and setting the edited status to true. This suggests a functional update rather than a bug fix. The original code likely had an issue with removing components from the correct tab or not updating the edited status. However, without more context, it's difficult to definitively say it's a bug fix. The change seems to be adding functionality and ensuring the UI reflects the changes correctly. The answer is **NotBuggy**."
weka,15488.json,55b9fb2a4bd0d399e6823c494c1c78ae0af0979c,"@@ -1,45 +1,27 @@
   public void setOptions(String[] options) throws Exception {
     
-    String attributeIndex = Utils.getOption('C', options);
-    if (attributeIndex.length() != 0) {
-      if (attributeIndex.toLowerCase().equals(""last"")) {
-	setAttributeIndex(-1);
-      } else if (attributeIndex.toLowerCase().equals(""first"")) {
-	setAttributeIndex(0);
-      } else {
-	setAttributeIndex(Integer.parseInt(attributeIndex) - 1);
-      }
+    String attIndex = Utils.getOption('C', options);
+    if (attIndex.length() != 0) {
+      setAttributeIndex(attIndex);
     } else {
-      setAttributeIndex(-1);
+      setAttributeIndex(""last"");
     }
-    
-    String firstIndex = Utils.getOption('F', options);
-    if (firstIndex.length() != 0) { 
-      if (firstIndex.toLowerCase().equals(""last"")) {
-	setFirstValueIndex(-1);
-      } else if (firstIndex.toLowerCase().equals(""first"")) {
-	setFirstValueIndex(0);
-      } else {
-	setFirstValueIndex(Integer.parseInt(firstIndex) - 1);
-      }
+
+    String firstValIndex = Utils.getOption('F', options);
+    if (firstValIndex.length() != 0) {
+      setFirstValueIndex(firstValIndex);
     } else {
-      setFirstValueIndex(-1);
+      setFirstValueIndex(""first"");
     }
-     
-    String secondIndex = Utils.getOption('S', options);
-    if (secondIndex.length() != 0) {
-      if (secondIndex.toLowerCase().equals(""last"")) {
-	setSecondValueIndex(-1);
-      } else if (secondIndex.toLowerCase().equals(""first"")) {
-	setSecondValueIndex(0);
-      } else {
-	setSecondValueIndex(Integer.parseInt(secondIndex) - 1); 
-      }
+
+    String secondValIndex = Utils.getOption('S', options);
+    if (secondValIndex.length() != 0) {
+      setSecondValueIndex(secondValIndex);
     } else {
-      setSecondValueIndex(-1);
+      setSecondValueIndex(""last"");
     }
    
     if (getInputFormat() != null) {
       setInputFormat(getInputFormat());
     }
   }",NotBuggy,"Indices now start from 1 for (hopefully) all filters. Changed default behaviour of Remove...java filters in unsupervised.instances. Changed some meta classifiers accordingly.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@1795 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a change in indexing, starting from 1, and modifications to the default behavior of ""Remove..."" filters. The code diff shows changes in how attribute indices are handled in the `setOptions` method. Instead of directly parsing the index and subtracting 1, the code now directly sets the attribute index using the provided string. The same logic applies to `firstValueIndex` and `secondValueIndex`. This change aligns with the commit message's intention to start indices from 1, as the subtraction is removed. This is likely to fix a bug where indices were off by one. The answer is **Buggy**."
weka,25044.json,22fe9ffdd42d2d198ece80fef7d71fe91de6c1d6,"@@ -1,4 +1,4 @@
   public Clusterer getClusterer() {
 
-    return m_Clusterer;
+    return m_wrappedClusterer;
   }",NotBuggy,"Has been rewritten.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@1183 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",NotBuggy,"The commit message ""Has been rewritten"" is vague and doesn't provide specific details about the changes. The diff shows a change from `m_Clusterer` to `m_wrappedClusterer`. Without more context, it's impossible to determine if this change is related to a bug fix or simply a refactoring/renaming. Therefore, based on the available information, it's not possible to conclude that this commit fixes a bug. The answer is **NotBuggy**."
weka,9558.json,9632387e9fe84f73e41daa585774e86c8cc3c617,"@@ -1,92 +1,124 @@
   public void doGet(HttpServletRequest request, HttpServletResponse response)
     throws ServletException, IOException {
 
     if (!request.getRequestURI().startsWith(CONTEXT_PATH)) {
       return;
     }
 
     PrintWriter out = null;
     InputStream in = request.getInputStream();
     ObjectOutputStream oos = null;
 
-    String clientParam = request.getParameter(""client"");
-    boolean client = (clientParam != null && clientParam.equalsIgnoreCase(""y""));
+    String clientParamLegacy = request.getParameter(Legacy.LEGACY_CLIENT_KEY);
+    String clientParamNew = request.getParameter(JSONProtocol.JSON_CLIENT_KEY);
+    boolean clientLegacy =
+      clientParamLegacy != null && clientParamLegacy.equalsIgnoreCase(""y"");
+    boolean clientNew =
+      clientParamNew != null && clientParamNew.equalsIgnoreCase(""y"");
+
     String taskName = request.getParameter(""name"");
 
     NamedTask task = m_taskMap.getTask(taskName);
 
-    if (client) {
+    if (clientLegacy) {
       // response.setCharacterEncoding(""UTF-8"");
       // response.setContentType(""text/plain"");
       response.setContentType(""application/octet-stream"");
       OutputStream outS = response.getOutputStream();
       oos = new ObjectOutputStream(new BufferedOutputStream(outS));
+    } else if (clientNew) {
+      out = response.getWriter();
+      response.setCharacterEncoding(""UTF-8"");
+      response.setContentType(""application/json"");
     } else {
       out = response.getWriter();
       response.setCharacterEncoding(""UTF-8"");
       response.setContentType(""text/html;charset=UTF-8"");
       out.println(""<HTML>"");
       out.println(""<HEAD><TITLE>Schedule</TITLE></HEAD>"");
       out.println(""<BODY>"");
     }
 
     response.setStatus(HttpServletResponse.SC_OK);
 
     try {
       if (task == null) {
-        if (client) {
-          String errorResult = WekaServlet.RESPONSE_ERROR
-            + "": Can't find task "" + taskName;
+        if (clientLegacy) {
+          String errorResult =
+            WekaServlet.RESPONSE_ERROR + "": Can't find task "" + taskName;
           oos.writeObject(errorResult);
           oos.flush();
+        } else if (clientNew) {
+          Map<String, Object> errorJ =
+            JSONProtocol.createErrorResponseMap(""Can't find task "" + taskName);
+          String encodedResponse = JSONProtocol.encodeToJSONString(errorJ);
+          out.println(encodedResponse);
+          out.flush();
         } else {
           out
             .println(WekaServlet.RESPONSE_ERROR + "": Unknown task "" + taskName);
         }
       } else if (!(task instanceof Scheduled)) {
-        if (client) {
-          String errorResult = WekaServlet.RESPONSE_ERROR + ""'"" + taskName
-            + ""' "" + ""is not a scheduled task."";
+        if (clientLegacy) {
+          String errorResult =
+            WekaServlet.RESPONSE_ERROR + ""'"" + taskName + ""' ""
+              + ""is not a scheduled task."";
           oos.writeObject(errorResult);
           oos.flush();
+        } else if (clientNew) {
+          Map<String, Object> errorJ =
+            JSONProtocol.createErrorResponseMap(""'"" + taskName
+              + ""' is not a scheduled task"");
+          String encodedResponse = JSONProtocol.encodeToJSONString(errorJ);
+          out.println(encodedResponse);
+          out.flush();
         } else {
           out.println(WekaServlet.RESPONSE_ERROR + ""'"" + taskName + ""' ""
             + ""is not a scheduled task."");
         }
       } else {
         Schedule sched = ((Scheduled) task).getSchedule();
-        if (client) {
+        if (clientLegacy) {
           oos.writeObject(sched);
           oos.flush();
+        } else if (clientNew) {
+          Map<String, Object> scheduleJ = JSONProtocol.scheduleToJsonMap(sched);
+          Map<String, Object> jResponse =
+            JSONProtocol.createOKResponseMap(""OK. Schedule"");
+          JSONProtocol.addPayloadMap(jResponse, scheduleJ,
+            JSONProtocol.SCHEDULE_PAYLOAD_ID);
+
+          String encodedResonse = JSONProtocol.encodeToJSONString(jResponse);
+          out.println(encodedResonse);
+          out.flush();
         } else {
-          String optionsString = weka.core.Utils
-            .joinOptions(sched.getOptions());
+          String optionsString =
+            weka.core.Utils.joinOptions(sched.getOptions());
           out.println(optionsString + ""<p>"");
         }
       }
     } catch (Exception ex) {
-      if (client && oos != null) {
+      if (clientLegacy && oos != null) {
         oos.writeObject(WekaServlet.RESPONSE_ERROR + "" "" + ex.getMessage());
         oos.flush();
-      } else if (out != null) {
+      } else if (out != null && !clientNew) {
         out.println(""<p><pre>"");
         ex.printStackTrace(out);
         out.println(""</pre>\n"");
       }
       ex.printStackTrace();
     } finally {
-      if (!client && out != null) {
+      if (!clientLegacy && !clientNew && out != null) {
         out.println(""</BODY>\n</HTML>"");
       }
 
       if (out != null) {
         out.close();
         out = null;
       }
       if (oos != null) {
         oos.close();
         oos = null;
       }
     }
-
   }",NotBuggy,"Implementation for the new Knowledge Flow

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@12561 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",NotBuggy,"The commit message indicates the implementation of a new Knowledge Flow. The diff introduces changes to the `doGet` method in a servlet, adding support for a new JSON-based client (`clientNew`) in addition to the existing legacy client (`clientLegacy`). The code now handles requests from both types of clients, providing responses in either the old `application/octet-stream` format or the new `application/json` format. Error handling is updated to provide appropriate responses for both client types. This looks like a new feature implementation rather than a bug fix. The answer is **NotBuggy**."
xerces2-j,109.json,1281996de10d3af1b51f0b4d78cfcd75d8ade6fe,"@@ -1,4 +1,4 @@
-    public void endElement(QName element)
+    public void endElement(QName element, Augmentations augs)
         throws XNIException {
-        super.endElement(toUpperCase(element));
+        super.endElement(toUpperCase(element), augs);
     } // endElement(QName)",NotBuggy,"Modify XNI to include Augmentations parameter on each XMLDocumentHandler call.
Modify samples.
Modify docs http://nagoya.apache.org/bugzilla/show_bug.cgi?id=5058


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@317940 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates a modification to include an `Augmentations` parameter in the `endElement` method of the `XMLDocumentHandler`. The diff confirms this change, adding the `Augmentations augs` parameter to the method signature. This change doesn't inherently indicate a bug fix, but rather an enhancement or modification of the API. The commit message references a bugzilla entry, but without further context, it's impossible to determine if this change directly addresses a bug. The code change itself doesn't suggest any error handling or logical correction. Therefore, it's likely an API enhancement rather than a bug fix. The answer is **NotBuggy**."
xerces2-j,6765.json,62b65e27622f6a1657e2e6a6c0776c95151c9dd0,"@@ -1,24 +1,27 @@
     public Object getParameter(String name) throws DOMException {
-		if (name.equals(Constants.DOM_ERROR_HANDLER)){
-			if (fErrorHandler != null){
-				return fErrorHandler.getErrorHandler();
-			}
-		}
+        
+        if (name.equals(Constants.DOM_ERROR_HANDLER)){
+            return (fErrorHandler != null) ? fErrorHandler.getErrorHandler() : null;
+        }
+        else if (name.equals(Constants.DOM_RESOURCE_RESOLVER)) {
+            return (fResourceResolver != null) ? fResourceResolver.getEntityResolver() : null;
+        }
+        
         try {
             boolean feature = getFeature(name);
             return (feature) ? Boolean.TRUE : Boolean.FALSE;
         } catch (Exception e) {
             Object property;
             try {
                 property = getProperty(name);
                 return property;
             } catch (Exception ex) {
                 String msg =
                     DOMMessageFormatter.formatMessage(
                         DOMMessageFormatter.DOM_DOMAIN,
                         ""FEATURE_NOT_SUPPORTED"",
                         new Object[] { name });
                 throw new DOMException(DOMException.NOT_SUPPORTED_ERR, msg);
             }
         }
     }",NotBuggy,"The XML Schema API [1] states that the DOMConfiguration of an XSLoader may
allow setting of a resource-resolver (LSResourceResolver). Supporting this
parameter so that applications may use their own resolver to locate
schema imports and includes.

[1] http://www.w3.org/Submission/2004/SUBM-xmlschema-api-20040309/xml-schema-api.html#Interface-Loader


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@319940 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message describes the addition of support for a resource resolver in the XML Schema API, allowing applications to use custom resolvers for locating schema imports and includes. The diff shows the addition of a check for the `Constants.DOM_RESOURCE_RESOLVER` parameter in the `getParameter` method. If a resource resolver is set (`fResourceResolver != null`), the method returns the entity resolver associated with it. Otherwise, it returns `null`. This change implements the functionality described in the commit message, adding a new feature rather than fixing a bug. The answer is **NotBuggy**.
"
xerces2-j,5928.json,6fec9ee0c874aba003c6652a39c4de16036f6e4f,"@@ -1,7 +1,6 @@
-    public void startDocument() {
-
-        if (!canonical) {
-            out.println(""<?xml version=\""1.0\"" encoding=\""UTF-8\""?>"");
+    public void startDocument(XMLLocator locator, String encoding) 
+        throws XNIException {
+        if (fDocumentHandler != null && !fOnlyPassPrefixMappingEvents) {
+            fDocumentHandler.startDocument(locator, encoding);
         }
-
-    } // startDocument()

+    } // startDocument(XMLLocator,String)",NotBuggy,"Merge/move of Xerces 2 to the trunc!


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@317483 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates a merge or move of Xerces 2 to the trunk. The diff shows a change in the `startDocument` method signature and functionality. The original method `startDocument()` was replaced with `startDocument(XMLLocator locator, String encoding)`. The new method now calls `fDocumentHandler.startDocument(locator, encoding)` if certain conditions are met. This change suggests a modification to the document processing logic, likely to align with updated interfaces or requirements after the merge. It's difficult to determine if this is a bug fix without more context, but the change in method signature and the addition of a conditional call to `fDocumentHandler.startDocument` suggest a functional change rather than a simple refactoring. The change is related to how the document is started and processed, which could be related to fixing a bug or adding new functionality.

The change in method signature and the addition of a conditional call to `fDocumentHandler.startDocument` suggest a functional change rather than a simple refactoring. It is likely related to fixing a bug or adding new functionality.

**NotBuggy**
"
xerces2-j,6658.json,42ca322ea8ac4be5de3f277f111700384c06f8d7,"@@ -1,4 +1,4 @@
     public XSSimpleType createTypeRestriction(String name, String targetNamespace,
-                                              short finalSet, XSSimpleType base) {
-        return new XSSimpleTypeDecl((XSSimpleTypeDecl)base, name, targetNamespace, finalSet, false);
+                                              short finalSet, XSSimpleType base, XSObjectList annotations) {
+        return new XSSimpleTypeDecl((XSSimpleTypeDecl)base, name, targetNamespace, finalSet, false, annotations);
     }",NotBuggy,"as part of annotation support, enabling schema datatype factories to create XSSimpleType implementations containing lists of annotations


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@319276 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates the addition of annotation support, specifically enabling schema datatype factories to create XSSimpleType implementations containing lists of annotations. The diff shows a modification to the `createTypeRestriction` method, adding an `XSObjectList annotations` parameter and passing it to the `XSSimpleTypeDecl` constructor. This change directly relates to the commit message's stated goal of adding annotation support. This doesn't look like a bug fix. The answer is **NotBuggy**."
xerces2-j,3176.json,1130527a86137ec0fa5201483136b96ac2b25135,"@@ -1,8 +1,9 @@
     public Node getFirstChild() {
 
         if (needsSyncChildren()) {
             synchronizeChildren();
         }
-    	return firstChild;
+        makeChildNode();
+    	return (Node) value;
 
     }   // getFirstChild():Node",NotBuggy,"This commit brinds yet another optimization to this DOM implementation.
The whole idea is to try and avoid to always creating a Text node to hold
the value of an attribute. The DOM spec requires it, so we still have
to do it in case getFirstChild() is called for instance. The reason
attribute values are stored as a list of nodes is so that they can carry
more than a simple string. They can also contain EntityReference nodes.
However, most of the times people only have a single string that they
only set and get through Element.set/getAttribute or Attr.set/getValue.
In this new version, the Attr node has a value pointer which can either
be the String directly or a pointer to the first ChildNode. A flag tells
which one it currently is.
Note that while we try to stick with the direct String as much as possible
once we've switched to a node there is no going back. This is because we
have no way to know whether the application keeps referring to the node
we once returned.

The gain in memory varies on the density of attributes in the document.
But in the tests I've run I've seen up to 12% of memory gain. And the good
thing is that it also leads to a slight gain in speed because we allocate
fewer objects! I mean, that's until we have to actually create the node...

To avoid too much duplicated code, I got rid of ParentNode and renamed
ChildAndParentNode, which I never really liked, to ParentNode for
simplicity, this doesn't make much of a difference in memory usage because
there are only very objects that are only a Parent. This is only true now
because AttrImpl now inherits directly from NodeImpl and has its own
implementation of the ParentNode's node behavior. So there is still some
duplicated code there.


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@316776 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message describes a significant optimization in the DOM implementation, aiming to reduce memory usage and improve speed by avoiding the creation of Text nodes for attribute values when possible. The diff shows a modification to the `getFirstChild()` method. Instead of returning `firstChild` directly, it now calls `makeChildNode()` and returns `(Node) value`. This aligns with the commit message's intention to store attribute values directly as strings or as a pointer to the first ChildNode. The change suggests that `value` now holds either the string value or the first child node, and `makeChildNode()` likely handles the creation of the child node if it doesn't already exist. This optimization could potentially introduce bugs if not handled carefully, especially concerning the consistency of the DOM tree and the behavior of other methods that rely on the child nodes. The change is related to performance improvements, but it also touches the core logic of how attribute values are stored and accessed, which could lead to unexpected behavior if not implemented correctly. The answer is **Buggy**.
"
xerces2-j,6593.json,1b20fdfd0e92e36d5d66b17da8c7cbeb6e59c899,"@@ -1,4 +1,3 @@
-    public XSObjectList getAnnotations() {
-        // REVISIT: SCAPI: to implement
-        return fAnnotations;
-    }

+        public XSObjectList getAnnotations() {
+            return annotations;
+        }",NotBuggy,"Store and expose annotation for simpleTypes and facets. Add a new interface to
represent enumeration and pattern facets.


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@319311 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates the addition of functionality related to annotations for simpleTypes and facets. The diff shows the implementation of `getAnnotations()`, which previously returned `fAnnotations` (likely uninitialized or a placeholder) and now returns `annotations`. This change implements the intended functionality, but it doesn't fix a bug. The previous implementation was simply incomplete. The answer is **NotBuggy**.
"
xerces2-j,6062.json,5846060722ccc5d1a1b47182834caa718a3fa65b,"@@ -1,25 +1,25 @@
         protected boolean scanRootElementHook()
             throws IOException, XNIException {
-            if (fDTDValidator == null) {
-                fBindNamespaces = true;
-            } else if (!fDTDValidator.hasGrammar()) {
-                fBindNamespaces = true;
-                fPerformValidation = fDTDValidator.validate();
-                // re-configure pipeline
-                XMLDocumentSource source = fDTDValidator.getDocumentSource();
-                XMLDocumentHandler handler = fDTDValidator.getDocumentHandler();
-                source.setDocumentHandler(handler);
-                if (handler != null)
-                    handler.setDocumentSource(source);
-                fDTDValidator.setDocumentSource(null);
-                fDTDValidator.setDocumentHandler(null);
+            
+            if (fExternalSubsetResolver != null && !fSeenDoctypeDecl 
+                && (fValidation || fLoadExternalDTD)) {
+                scanStartElementName();
+                resolveExternalSubsetAndRead();
+                reconfigurePipeline();
+                if (scanStartElementAfterName()) {
+                    setScannerState(SCANNER_STATE_TRAILING_MISC);
+                    setDispatcher(fTrailingMiscDispatcher);
+                    return true;
+                }
             }
-
-            if (scanStartElement()) {
-                setScannerState(SCANNER_STATE_TRAILING_MISC);
-                setDispatcher(fTrailingMiscDispatcher);
-                return true;
+            else {
+                reconfigurePipeline();
+                if (scanStartElement()) {
+                    setScannerState(SCANNER_STATE_TRAILING_MISC);
+                    setDispatcher(fTrailingMiscDispatcher);
+                    return true;
+                }
             }
             return false;
 
         } // scanRootElementHook():boolean",NotBuggy,"Infrastructure work in the scanners to support the SAX2 Extensions 1.1
interface EntityResolver2, specifically getExternalSubset which
allows an application to provide an external subset for a document
which doesn't otherwise have one.

The scanners are now able to handle all three cases where a resolver
may be queried for external subset:

1) Neither an external or internal subset exist.
2) Only an internal subset exists.
3) No DOCTYPE declaration in the document.

In the third case, scanning of the root element is broken up into
three segments:

1) Scan the root element QName.
2) Query the resolver with the root name and base URI, if it 
locates an external subset then read it.
3) Scan the remainder of the start tag.

If an external subset is located the public id and system id
from the XMLInputSource returned from the resolver are
reported to doctypeDecl. This is what SAX expects.


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@319878 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message describes infrastructure work to support SAX2 Extensions 1.1, specifically the `getExternalSubset` method. It details how the scanner handles cases where an external subset is queried. The diff shows a modification to the `scanRootElementHook` method. The new code checks for an `fExternalSubsetResolver`, the absence of a DOCTYPE declaration (`!fSeenDoctypeDecl`), and whether validation or external DTD loading is enabled. If these conditions are met, it scans the start element name, resolves and reads the external subset, reconfigures the pipeline, and then scans the rest of the start element. Otherwise, it reconfigures the pipeline and scans the start element as before. This change introduces new logic to handle external subsets, which is in line with the commit message. The original code block was replaced with a conditional block that handles the cases described in the commit message. This looks like a new feature implementation rather than a bug fix. The answer is **NotBuggy**."
xerces2-j,8338.json,34aab31cc59d829a17b9f74ce192423af5788d9f,"@@ -1,3 +1,4 @@
-    public void startExternalSubset(Augmentations augs) throws XNIException {
+    public void startExternalSubset(XMLResourceIdentifier identifier,
+                                    Augmentations augs) throws XNIException {
         fReadingExternalDTD = true;
     } // startExternalSubset(Augmentations)",NotBuggy," Add implementation for baseURI and documentURI (DOM Level 3).
 Add XMLResourceIdentifier to startExternalSubset() method defined in  XNI XMLDTDHandler to
allow supporting baseURI for an external subset.


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@318382 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates the addition of functionality related to baseURI and documentURI for DOM Level 3, as well as the introduction of XMLResourceIdentifier to the startExternalSubset() method. The diff shows a change to the startExternalSubset method signature, adding an XMLResourceIdentifier parameter. This aligns with the commit message's description of adding support for baseURI for an external subset. The change introduces new functionality rather than fixing a bug. The answer is **NotBuggy**."
xerces2-j,1322.json,5453aaa1b3b312be38b47f18a2a5c6d9f874d3f0,"@@ -1,3 +1,3 @@
-    void setNextIndent( int indent )
+    public void setNextIndent( int indent )
     {
     }",NotBuggy,"Changed methods to public modifier


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@315918 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates a change in method visibility to `public`. The diff confirms this by showing a change from `void` to `public void` for the `setNextIndent` method. This change doesn't inherently indicate a bug fix. It's more likely a change in API design or accessibility. The answer is **NotBuggy**.
"
xerces2-j,29.json,e94cfece717a676ae2799d5c030d2ed5982d1e27,"@@ -1,19 +1,20 @@
     public void internalEntityDecl(String name, XMLString text,
-                                   XMLString nonNormalizedText)
+                                   XMLString nonNormalizedText,
+                                   Augmentations augs)
         throws XNIException {
 
         printIndent();
         fOut.print(""internalEntityDecl("");
         fOut.print(""name="");
         printQuotedString(name);
         fOut.print(',');
         fOut.print(""text="");
         printQuotedString(text.ch, text.offset, text.length);
         fOut.print(',');
         fOut.print(""nonNormalizedText="");
         printQuotedString(nonNormalizedText.ch, nonNormalizedText.offset,
                           nonNormalizedText.length);
         fOut.println(')');
         fOut.flush();
 
     } // internalEntityDecl(String,XMLString)",NotBuggy,"committing some (slightly modified to take care of regressions) massive XNI changes on behalf of Andy Clark.  These make use of XMLResourceIdentifier in start.*Entity calls, and add Augmentations to various DTD Handler callbacks.  Things yet to do:
1.  Make Augmentation callbacks consistent.  They are null sometimes, not others; need to make them null by default.
2.  Propagate XMLResourceIdentifier to remaining methods it could and is not being used in.
3.  Tie XMLResourceIdentifier and XMLLocator together.


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@318116 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message describes a large set of changes related to XNI (XML Native Interface), including the use of `XMLResourceIdentifier` and the addition of `Augmentations` to DTD handler callbacks. The diff shows the addition of an `Augmentations` parameter to the `internalEntityDecl` method. While the commit message mentions regressions being addressed, the specific change in the diff doesn't clearly indicate a bug fix. It seems to be more about adding functionality and propagating information. The commit message also lists future tasks, suggesting that the changes are part of a larger refactoring or enhancement effort. Therefore, based on the provided information, it is difficult to definitively say that this change is a bug fix.
**NotBuggy**
"
xerces2-j,6592.json,962564ff9af52f2561b55bcf3fc5f3f21e1f43e9,"@@ -1,3 +1,3 @@
-		public XSAnnotation getAnnotation() {
-			return annotation;
-		}

+        public XSAnnotation getAnnotation() {
+            return (XSAnnotation) annotations.item(0);
+        }",NotBuggy,"Committing code and unit tests for the following line items from the Xerces 2.8.0 release plan:
- Expose annotations in XSAttributeUse and XSParticle
- Support lists of annotations for other XSObjects 

git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@379204 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates the addition of functionality related to annotations in XSAttributeUse and XSParticle, as well as support for lists of annotations for other XSObjects. The diff shows a change in the `getAnnotation()` method. Instead of returning a direct `annotation` field, it now returns the first item from an `annotations` list. This suggests that the original code only supported a single annotation, and the change is to support multiple annotations. While this is a feature enhancement, it also implies that the previous implementation had a limitation (only supporting one annotation), which can be considered a bug in a broader sense. The answer is **Buggy**."
xerces2-j,3145.json,724c5904263e34696a38560774e273d8ba109a04,"@@ -1,8 +1,8 @@
     public void setNotationName(String name) {
         
-        if (syncData()) {
+        if (needsSyncData()) {
             synchronizeData();
         }
     	notationName = name;
 
     } // setNotationName(String)",NotBuggy,"global renaming of some internal methods,
I too wish I got them right in the first place...


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@315905 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates a renaming of internal methods. The diff shows a change from `syncData()` to `needsSyncData()`. This aligns with the commit message's description of renaming. There's no indication of a bug fix in either the message or the code change. The answer is **NotBuggy**.
"
xerces2-j,3103.json,724c5904263e34696a38560774e273d8ba109a04,"@@ -1,8 +1,8 @@
     public Node getFirstChild() {
 
-        if (syncChildren()) {
+        if (needsSyncChildren()) {
             synchronizeChildren();
         }
     	return firstChild;
 
     }   // getFirstChild():Node",NotBuggy,"global renaming of some internal methods,
I too wish I got them right in the first place...


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@315905 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates a renaming of internal methods. The diff shows a change from `syncChildren()` to `needsSyncChildren()`. This aligns with the commit message and suggests a refactoring or renaming rather than a bug fix. The answer is **NotBuggy**.
"
xerces2-j,7895.json,97033b9bcdb0ebcdfc9a24df498cab2d12790b44,"@@ -1,13 +1,19 @@
     synchronized SAXParser getSAXParser() {
-        if (fSAXParser != null) return fSAXParser;
+        if (fSAXParser != null) {
+            SAXParser parser = (SAXParser) fSAXParser.get();
+            if (parser != null) {
+                return parser;
+            }
+        }
         // REVISIT:  when schema handles XML 1.1, will need to 
         // revisit this (and the practice of not prepending an XML decl to the annotation string
-        IntegratedParserConfiguration config = new IntegratedParserConfiguration(fSymbolTable);
+        XML11Configuration config = new XML11Configuration(fSymbolTable);
         // note that this should never produce errors or require
         // entity resolution, so just a barebones configuration with
         // a couple of feature  set will do fine
         config.setFeature(Constants.SAX_FEATURE_PREFIX + Constants.NAMESPACES_FEATURE, true);
         config.setFeature(Constants.SAX_FEATURE_PREFIX + Constants.VALIDATION_FEATURE, false);
-        fSAXParser = new SAXParser(config);
-        return fSAXParser;
+        SAXParser parser = new SAXParser(config);
+        fSAXParser = new SoftReference(parser);
+        return parser;
     }",NotBuggy,"Improvement for writing annotations:
- IntegratedParserConfiguration is ancient and slow. Use the better performing XML11Configuration.
- Parser instances use quite a bit of memory. Store them in SoftReferences so they can be
  reclaimed if there is high demand for memory.
- Use adopt node when possible to avoid creating a copy of the DOM before attaching it to the target.
- Turn off the deferred DOM feature. There's no benefit to having it on since the target 
  is another document. We would end up traversing the entire DOM even when calling adoptNode.


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@348887 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message describes several improvements related to annotation writing, including switching to `XML11Configuration` for better performance, using `SoftReference` to store parser instances and reduce memory usage, using `adoptNode` to avoid DOM copying, and turning off the deferred DOM feature.

The code diff reflects these changes:
1. The code now uses `XML11Configuration` instead of `IntegratedParserConfiguration`.
2. The `SAXParser` is now stored in a `SoftReference`. The code checks if the `SoftReference` contains a valid parser before creating a new one.

These changes align with the commit message and are aimed at improving performance and reducing memory consumption. There is no indication of a bug fix. The answer is **NotBuggy**."
xerces2-j,8568.json,6fec9ee0c874aba003c6652a39c4de16036f6e4f,"@@ -1,3 +1,3 @@
-    public void writeChildrenXml (XmlWriteContext context) throws IOException
-    {
-    }

+    public void setProperty(String propertyId, Object value)
+        throws XMLConfigurationException {
+    } // setProperty(String,Object)",NotBuggy,"Merge/move of Xerces 2 to the trunc!


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@317483 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates a merge or move of Xerces 2 to the trunk, which is a large-scale operation. The provided diff shows the addition of a `setProperty` method that throws an `XMLConfigurationException`. This change doesn't inherently indicate a bug fix, but rather a potential addition of functionality or an interface implementation. The original code had a `writeChildrenXml` method that was empty. The new code adds a `setProperty` method. Without more context, it is difficult to determine if this change is related to a bug fix. The change is likely part of the merge/move operation. The answer is **NotBuggy**."
hibernate-orm,7648.json,ff4774ad7ef488ce7505784f9591684d096bdb99,"@@ -1,3 +1,3 @@
-	public Schema getDefaultSchema() {
-		return implicitSchema;
+	public Namespace getDefaultNamespace() {
+		return implicitNamespace;
 	}",NotBuggy,"HHH-9974 - Rename org.hibernate.boot.model.relational.Schema to Namespace
",NotBuggy,"The commit message indicates a renaming of a class from `Schema` to `Namespace`. The diff shows a corresponding change in the getter method name from `getDefaultSchema()` to `getDefaultNamespace()` and the return type from `Schema` to `Namespace`. This is a refactoring change, not a bug fix. The answer is **NotBuggy**.
"
hibernate-orm,19080.json,1e44e7420b7af10adc9547a461f02a6d979624f1,"@@ -1,17 +1,13 @@
 	public void afterReassociate(Object entity, SessionImplementor session) {
 		if ( getEntityMetamodel().getBytecodeEnhancementMetadata().isEnhancedForLazyLoading() ) {
 			LazyAttributeLoadingInterceptor interceptor = getEntityMetamodel().getBytecodeEnhancementMetadata().extractInterceptor( entity );
 			if ( interceptor == null ) {
-				getEntityMetamodel().getBytecodeEnhancementMetadata().injectInterceptor(
-						entity,
-						null,
-						session
-				);
+				getEntityMetamodel().getBytecodeEnhancementMetadata().injectInterceptor( entity, session );
 			}
 			else {
 				interceptor.setSession( session );
 			}
 		}
 
 		handleNaturalIdReattachment( entity, session );
 	}",NotBuggy,"HHH-10267 - Support defining lazy attribute fetch groups
",NotBuggy,"The commit message indicates support for defining lazy attribute fetch groups, which suggests a feature enhancement rather than a bug fix. The code diff modifies the `afterReassociate` method, specifically how the `LazyAttributeLoadingInterceptor` is handled. The change involves simplifying the `injectInterceptor` call by removing the `null` argument and directly passing the session. This adjustment seems related to how lazy attributes are loaded and managed within the Hibernate framework. There is no explicit error handling or logical correction that would indicate a bug fix. The changes are more aligned with feature enhancement or refactoring. The answer is **NotBuggy**."
hibernate-orm,1521.json,0e17f9adf85e14cb34c385c7886ed0cddfabe0f6,"@@ -1,3 +1,3 @@
 	public <X> ValueExtractor<X> getExtractor(final JavaTypeDescriptor<X> javaTypeDescriptor) {
-		return (ValueExtractor<X>) new PGGeometryValueExtractor();
+		return (ValueExtractor<X>) new PGGeometryValueExtractor(javaTypeDescriptor);
 	}",NotBuggy,"HHH-6509 - Prepare introduction of Geolatte-geom library as model

Updates testing approach
Refactors ValueBinders and ValueExtractors
",NotBuggy,"The commit message indicates refactoring and updates to the testing approach, specifically mentioning ValueBinders and ValueExtractors. The diff shows a change in the `getExtractor` method where a `PGGeometryValueExtractor` is instantiated. The change involves passing a `javaTypeDescriptor` to the constructor of `PGGeometryValueExtractor`. This suggests a modification to how the extractor is initialized, likely to accommodate the new Geolatte-geom library. There's no explicit indication of a bug fix in the traditional sense, but the refactoring might be addressing limitations or inconsistencies in the previous implementation. The change ensures the `PGGeometryValueExtractor` receives necessary type information during instantiation. The answer is **NotBuggy**."
hibernate-orm,19918.json,dc7cdf9d8803ff58191a35907414b7dd81210422,"@@ -1,3 +1,8 @@
 								public EntityDefinition toEntityDefinition() {
+									if ( getAssociationNature() != AssociationNature.ENTITY ) {
+										throw new WalkingException(
+												""Cannot build EntityDefinition from non-entity-typed attribute""
+										);
+									}
 									return (EntityPersister) aType.getAssociatedJoinable( ownerEntityPersister.getFactory() );
 								}",NotBuggy,"HHH-8276 - Integrate LoadPlans into UniqueEntityLoader (PoC)
",Buggy,"The commit message indicates the integration of LoadPlans into UniqueEntityLoader, which seems like a feature enhancement or refactoring. The code diff adds a check to ensure that an `EntityDefinition` is only built from an entity-typed attribute. If the attribute is not entity-typed, a `WalkingException` is thrown. This addition of exception handling suggests that there might have been cases where the code was trying to build an `EntityDefinition` from a non-entity-typed attribute, leading to unexpected behavior or errors. This can be considered a bug fix. The answer is **Buggy**.
"
hibernate-orm,14224.json,208e789bb107503a5df7a62531570ececaa4c0ed,"@@ -1,10 +1,10 @@
 	public Object getProxy() {
 		try {
 			final ProxyConfiguration proxy = (ProxyConfiguration) proxyClass.newInstance();
-			proxy.$$_hibernate_set_interceptor( new PassThroughInterceptor( proxy, proxyClass.getName() ) );
+			proxy.$$_hibernate_set_interceptor( this.interceptor );
 			return proxy;
 		}
 		catch (Throwable t) {
 			throw new HibernateException( ""Unable to instantiate proxy instance"", t );
 		}
 	}",NotBuggy,"HHH-12786 Improve the basic proxy interceptor

Apart from cosmetic changes, we were testing in the equals() method that the
instance == the proxied object which will always be true.

We should use the argument of the equals() method instead to do the
comparison.

And we can do the comparison on the instance, instead of requiring
passing the proxiedObject into the interceptor.
",Buggy,"The commit message indicates an improvement to the basic proxy interceptor, specifically addressing a flaw in the `equals()` method's testing logic. The original code was comparing the instance with the proxied object, which would always be true. The fix involves using the argument of the `equals()` method for comparison instead.

The provided diff snippet shows a change in the `getProxy()` method. It replaces the `PassThroughInterceptor` with `this.interceptor`. While the commit message focuses on the `equals()` method, this diff doesn't directly reflect that change. However, it's possible that the `this.interceptor` now contains the corrected `equals()` logic. Without seeing the changes to the interceptor itself, it's hard to definitively say if this diff directly addresses the bug described in the commit message. However, the commit message clearly states that there was a bug in the equals method.

The answer is **Buggy**."
hibernate-orm,19882.json,4620ff4b4fe670bbf121b709e9da6b418c24ee43,"@@ -1,3 +1,3 @@
 			public EntityDefinition getEntityDefinition() {
-				return AbstractEntityPersister.this;
+				return entityPersister;
 			}",NotBuggy,"HHH-7841 - Redesign Loader
",NotBuggy,"The commit message indicates a redesign of the Loader. The code change replaces `AbstractEntityPersister.this` with `entityPersister`. This change suggests a modification in how the `EntityDefinition` is accessed, likely as part of the Loader redesign. It's difficult to determine if this is a bug fix without more context, but it's likely a change related to the redesign.
The answer is **NotBuggy**."
hibernate-orm,12109.json,36dbd3a06e5867cc830b8537f6e24618a8a47137,"@@ -1,10 +1,12 @@
-	public InputStream getAsciiStream() throws SQLException {
+	private InputStream getStream() throws SQLException {
 		try {
-			if (needsReset) reader.reset();
+			if (needsReset) {
+				stream.reset();
+			}
 		}
-		catch (IOException ioe) {
+		catch ( IOException ioe) {
 			throw new SQLException(""could not reset reader"");
 		}
 		needsReset = true;
-		return new ReaderInputStream(reader);
+		return stream;
 	}",NotBuggy,"HHH-2412 - Support for JDBC4


git-svn-id: https://svn.jboss.org/repos/hibernate/core/trunk@17767 1b8cb986-b30d-0410-93ca-fae66ebed9b2
",Buggy,The commit message indicates support for JDBC4. The code diff modifies the `getAsciiStream` method (renamed to `getStream`) to reset the underlying stream and handle potential `IOExceptions` during the reset operation. The original code used a `ReaderInputStream` which is not present in the diff. The change includes exception handling with a `SQLException` which suggests a bug fix related to stream handling. The answer is **Buggy**.
hibernate-orm,12790.json,617975fefb42fe5bd6d3ae7893e96077cb44c84a,"@@ -1,17 +1,17 @@
 	public Blob createBlob(InputStream inputStream, long length) {
 		try {
 			Blob blob = createBlob();
 			OutputStream byteStream = blob.setBinaryStream( 1 );
 			StreamUtils.copy( inputStream, byteStream );
 			byteStream.flush();
 			byteStream.close();
 			// todo : validate length written versus length given?
 			return blob;
 		}
 		catch ( SQLException e ) {
-			throw new IllegalStateException( ""Unable to prepare BLOB binary stream for writing"", e );
+			throw new JDBCException( ""Unable to prepare BLOB binary stream for writing"",e );
 		}
 		catch ( IOException e ) {
-			throw new IllegalStateException( ""Unable to write stream contents to BLOB"", e );
+			throw new HibernateException( ""Unable to write stream contents to BLOB"", e );
 		}
 	}",NotBuggy,"HHH-4560 - JDBC4 support inadvertently missed 1.4 compatibility


git-svn-id: https://svn.jboss.org/repos/hibernate/core/trunk@17951 1b8cb986-b30d-0410-93ca-fae66ebed9b2
",Buggy,"The commit message indicates a fix related to JDBC4 support and compatibility with Java 1.4. The code changes involve exception handling within the `createBlob` method. Specifically, `IllegalStateException` is replaced with `JDBCException` and `HibernateException` for `SQLException` and `IOException` respectively. This suggests that the original exception handling was not appropriate for the context, and the changes aim to provide more specific and relevant exception types. This is a bug fix. The answer is **Buggy**."
hibernate-orm,3126.json,9e063ffa2577f06d98a9e912bb16d20424df8d6d,"@@ -1,3 +1,3 @@
 	public Getter getGetter(Class clazz) throws PropertyNotFoundException, MappingException {
-		return getPropertyAccessor(clazz).getGetter( clazz, name );
+		return getPropertyAccessStrategy( clazz ).buildPropertyAccess( clazz, name ).getGetter();
 	}",NotBuggy,"HHH-9837 - Remove reliance during annotation binding on org.hibernate.internal.util.ClassLoaderHelper
HHH-9841 - Redesign org.hibernate.property.PropertyAccessorFactory
",NotBuggy,"The commit message indicates a refactoring or redesign of the `PropertyAccessorFactory` and removal of reliance on `ClassLoaderHelper`. The code change replaces `getPropertyAccessor(clazz).getGetter( clazz, name )` with `getPropertyAccessStrategy( clazz ).buildPropertyAccess( clazz, name ).getGetter()`. This suggests a change in how the getter is obtained, likely part of the redesign mentioned in the commit message. There's no explicit bug fix indicated, but rather a change in implementation. The answer is **NotBuggy**.
"
hibernate-orm,30500.json,2f92109c48fb487870af6d93ba8dc884838179d0,"@@ -1,13 +1,11 @@
-	public static Map<String, ParsedPersistenceXmlDescriptor> parse(
+	public static Map<String,ParsedPersistenceXmlDescriptor> parse(
 			URL persistenceXmlUrl,
 			PersistenceUnitTransactionType transactionType,
-			Map integration
-	) {
+			Map integration) {
 		PersistenceXmlParser parser = new PersistenceXmlParser(
 				ClassLoaderServiceImpl.fromConfigSettings( integration ),
 				transactionType
 		);
 
-		parser.doResolve( integration );
-		return parser.persistenceUnits;
+		return parser.doResolve( integration );
 	}",NotBuggy,"Revert ""HHH-11845 - Warn user when multiple persistence-units use the same name""

This reverts commit 1242fd9580fdfb2a8b14ebb419ccc5d40ae01a01.
",Buggy,"The commit message indicates a revert of a previous commit related to warning users about multiple persistence units using the same name. The diff shows the removal of the `parser.doResolve( integration )` call and the return of `parser.persistenceUnits` directly. This suggests that the reverted commit introduced changes in how persistence units were resolved and returned, potentially causing issues that led to the revert. Since the commit reverts a previous change, it's likely that the original change introduced a bug or unintended behavior. The answer is **Buggy**."
hibernate-orm,24481.json,87e3f0fd28d7229b0a35032292ff32f542c22010,"@@ -1,3 +1,3 @@
-		public boolean isPassNullsEnabled() {
-			return nativeParamRegistration.isPassNullsEnabled();
-		}

+	public boolean isPassNullsEnabled() {
+		return nativeParamRegistration.isPassNullsEnabled();
+	}",NotBuggy,"HHH-10664 - Prep 6.0 feature branch - merge hibernate-entitymanager into hibernate-core (first sucessful full compile of consolidated hibernate-core)
",NotBuggy,The commit message indicates a merge of `hibernate-entitymanager` into `hibernate-core` and a successful compilation. The diff shows a minor change in indentation. This change does not indicate a bug fix. The answer is **NotBuggy**.
hibernate-orm,13970.json,ba3359fe62be258638554fe23a2a0a6a50f7e732,"@@ -1,3 +1,3 @@
-		public boolean isEntityClass(CtClass classDescriptor) {
-			return wrappedContext.isEntityClass( classDescriptor );
-		}

+	public boolean isEntityClass(CtClass classDescriptor) {
+		return enhancementContext.isEntityClass( new UnloadedCtClass( classDescriptor ) );
+	}",NotBuggy,"HHH-11152: Added BytecodeProvider based on Byte Buddy
",NotBuggy,"The commit message indicates the addition of a BytecodeProvider based on Byte Buddy. The diff shows a change in the `isEntityClass` method. Instead of directly using `wrappedContext.isEntityClass(classDescriptor)`, it now uses `enhancementContext.isEntityClass(new UnloadedCtClass(classDescriptor))`. This suggests a change in how entity classes are determined, likely related to the new BytecodeProvider. The change doesn't obviously indicate a bug fix, but rather an adaptation to the new BytecodeProvider. The answer is **NotBuggy**."
hibernate-orm,9325.json,08747fc2f4b33414ef05ac039b67e1d33c14db5f,"@@ -1,6 +1,6 @@
 	private Object resolveAny(String entityName, Serializable id, SharedSessionContractImplementor session)
 			throws HibernateException {
 		return entityName==null || id==null
 				? null
-				: session.internalLoad( entityName, id, false, false );
+				: session.internalLoad( entityName, id, eager, false );
 	}",NotBuggy,"HHH-13094 - Respect @Any.fetch setting to FetchType.EAGER
",Buggy,"The commit message indicates a fix related to the `@Any.fetch` setting, specifically ensuring it respects `FetchType.EAGER`. The code diff modifies the `resolveAny` method to pass the `eager` flag to the `session.internalLoad` method. This suggests that the previous implementation was not correctly handling the eager fetching configuration for `@Any` associations, which constitutes a bug fix. The answer is **Buggy**."
hibernate-orm,15177.json,ae43670290d05db28c322acb67431fe88db537d2,"@@ -1,3 +1,3 @@
-	public String getCreateTemporaryTablePostfix() {
+	public String getCreateIdTableStatementOptions() {
 		return "" on commit preserve rows"";
 	}",NotBuggy,"HHH-9724 - More complete ""temp table"" coverage and allow Dialects to influence which strategy is used
",NotBuggy,"The commit message indicates changes related to temporary tables and dialect-specific strategies. The code diff renames `getCreateTemporaryTablePostfix()` to `getCreateIdTableStatementOptions()`. This suggests a change in how temporary tables or similar constructs are handled, potentially to allow different dialects to customize the behavior. While not explicitly a bug fix, the change could be addressing limitations or inconsistencies in the previous implementation. The renaming and the context from the commit message suggest a functional change rather than a bug fix. The answer is **NotBuggy**.
"
hibernate-orm,11271.json,87e3f0fd28d7229b0a35032292ff32f542c22010,"@@ -1,3 +1,3 @@
 	public int executeNativeUpdate(NativeSQLQuerySpecification specification, QueryParameters queryParameters) throws HibernateException {
-		return sessionImplementor.executeNativeUpdate( specification, queryParameters );
+		return delegate.executeNativeUpdate( specification, queryParameters );
 	}",NotBuggy,"HHH-10664 - Prep 6.0 feature branch - merge hibernate-entitymanager into hibernate-core (first sucessful full compile of consolidated hibernate-core)
",NotBuggy,"The commit message indicates a merge of `hibernate-entitymanager` into `hibernate-core` and preparation for the 6.0 feature branch. The code change involves replacing `sessionImplementor` with `delegate` in the `executeNativeUpdate` method. This suggests a change in how the execution is delegated, likely due to the merging of the two projects and a change in the internal architecture. This doesn't appear to be a bug fix, but rather a consequence of the merge. The answer is **NotBuggy**.
"
presto,14100.json,8cb9c88804a6c7f7cc995fc729b862933e8d6b4d,"@@ -1,4 +1,4 @@
-    public static long cardinality(@SqlType(HyperLogLogType.NAME) Slice serializedHll)
+    public static long cardinality(@SqlType(StandardTypes.HYPER_LOG_LOG) Slice serializedHll)
     {
         return HyperLogLog.newInstance(serializedHll).cardinality();
     }",NotBuggy,"Move type names to their own class
",NotBuggy,"The commit message indicates a refactoring change, moving type names to their own class. The diff shows a change in the `@SqlType` annotation from `HyperLogLogType.NAME` to `StandardTypes.HYPER_LOG_LOG`. This change aligns with the commit message, suggesting that the type name `HyperLogLogType.NAME` was moved to `StandardTypes.HYPER_LOG_LOG`. There is no indication of a bug fix. The answer is **NotBuggy**.
"
presto,34438.json,279eb7e2af95bddf5d357327eaf249021ead9f8d,"@@ -1,10 +1,5 @@
     public <T> Optional<LocalProperty<T>> translate(Function<E, Optional<T>> translator)
     {
-        Optional<T> translated = translator.apply(column);
-
-        if (translated.isPresent()) {
-            return Optional.of(new SortingProperty<>(translated.get(), order));
-        }
-
-        return Optional.empty();
+        return translator.apply(column)
+                .map(translated -> new SortingProperty<>(translated, order));
     }",NotBuggy,"Simplify the use of Optionals
",NotBuggy,"The commit message indicates a simplification of Optional usage. The diff shows a change from a verbose if-present check and manual Optional creation to a more concise `map` operation on the Optional returned by the translator. This change streamlines the code and makes it more readable, but it doesn't inherently fix a bug. The original code was functionally correct, just less elegant. The answer is **NotBuggy**."
presto,26381.json,472538a02722c5409649533f9a602b5b76d9dd2d,"@@ -1,23 +1,23 @@
     public Configuration getConfiguration(FileSystemContext context, URI uri)
     {
         @SuppressWarnings(""resource"")
         Configuration config = new CachingJobConf((factoryConfig, factoryUri) -> {
             try {
                 FileSystem fileSystem = (new Path(factoryUri)).getFileSystem(hiveHdfsConfiguration.getConfiguration(context, factoryUri));
                 checkState(fileSystem instanceof ExtendedFileSystem);
-                return new CachingFileSystem(
+                return new FileMergeCachingFileSystem(
                         factoryUri,
                         factoryConfig,
                         cacheManager,
                         (ExtendedFileSystem) fileSystem,
                         cacheValidationEnabled);
             }
             catch (IOException e) {
                 throw new PrestoException(GENERIC_INTERNAL_ERROR, ""cannot create caching FS"", e);
             }
         });
         Configuration defaultConfig = hiveHdfsConfiguration.getConfiguration(context, uri);
 
         copy(defaultConfig, config);
         return config;
     }",NotBuggy,"Add Alluxio based data caching

This change enables Alluxio client-side local data cache on the worker.
This change also does a bit of refactoring of caching related pieces.
To enable Alluxio local cache, use following configuration
cache.enabled=true
cache.type=ALLUXIO

Co-authored-by: Bin Fan <fanbin103@gmail.com>
Co-authored-by: Bin Feng <binfeng@alluxio.com>
",NotBuggy,"The commit message indicates the addition of Alluxio-based data caching and some refactoring. The code diff shows a change from `CachingFileSystem` to `FileMergeCachingFileSystem`. This change suggests an enhancement or a refactoring related to the caching functionality, but it doesn't directly point to a bug fix. The change seems to be part of the new feature implementation. The answer is **NotBuggy**."
presto,37097.json,9ffb60c93e86e94c0c5a5a4bc21d61337344615d,"@@ -1,4 +1,4 @@
-            public MetadataReader createMetadataReader()
-            {
-                return new DwrfMetadataReader();
-            }

+        public MetadataWriter createMetadataWriter()
+        {
+            return new DwrfMetadataWriter();
+        }",NotBuggy,"Add OrcEncoding enum to differentiate between ORC and DWRF
",NotBuggy,"The commit message indicates the addition of an `OrcEncoding` enum to differentiate between ORC and DWRF. However, the provided diff shows a change in the `createMetadataWriter` method, switching from returning `DwrfMetadataReader` to `DwrfMetadataWriter`. This change doesn't directly relate to adding an enum for encoding differentiation. It seems more like a correction or update to the writer creation logic. The diff doesn't show any error handling or logical corrections, exception-handling improvements, or other signs of bug-related changes. The change is related to writer creation, not bug fixing. The answer is **NotBuggy**.
"
presto,26237.json,c8447e695d5434c811fc53d4920eaa8df9ec8bd0,"@@ -1,12 +1,4 @@
-    private void startBackupCleanup()
+    public void startBackupCleanup()
     {
-        scheduler.scheduleWithFixedDelay(() -> {
-            try {
-                cleanBackupShards();
-            }
-            catch (Throwable t) {
-                log.error(t, ""Error cleaning backup shards"");
-                backupJobErrors.update(1);
-            }
-        }, 0, backupCleanerInterval.toMillis(), MILLISECONDS);
+        scheduler.submit(this::runBackupCleanup);
     }",NotBuggy,"Expose recovery and cleaning methods through JMX
",NotBuggy,"The commit message indicates exposing methods through JMX. The diff changes the visibility of `startBackupCleanup` from private to public, making it accessible via JMX. Additionally, the scheduling mechanism is changed from `scheduleWithFixedDelay` to `scheduler.submit(this::runBackupCleanup)`. While the original code included error handling within the scheduled task, the modified code delegates the execution to `runBackupCleanup` without showing its implementation. Without further context on `runBackupCleanup`, it's difficult to determine if this change introduces or fixes a bug. However, the primary purpose seems to be exposing the method, not fixing a bug. The answer is **NotBuggy**.
"
presto,29934.json,db03518bf342c8fb74ace9ba25f2b0c78991953a,"@@ -1,24 +1,24 @@
-    protected void renameTable(String catalogName, SchemaTableName oldTable, SchemaTableName newTable)
+    protected void renameTable(JdbcIdentity identity, String catalogName, SchemaTableName oldTable, SchemaTableName newTable)
     {
-        try (Connection connection = connectionFactory.openConnection()) {
+        try (Connection connection = connectionFactory.openConnection(identity)) {
             DatabaseMetaData metadata = connection.getMetaData();
             String schemaName = oldTable.getSchemaName();
             String tableName = oldTable.getTableName();
             String newSchemaName = newTable.getSchemaName();
             String newTableName = newTable.getTableName();
             if (metadata.storesUpperCaseIdentifiers()) {
                 schemaName = schemaName.toUpperCase(ENGLISH);
                 tableName = tableName.toUpperCase(ENGLISH);
                 newSchemaName = newSchemaName.toUpperCase(ENGLISH);
                 newTableName = newTableName.toUpperCase(ENGLISH);
             }
             String sql = format(
                     ""ALTER TABLE %s RENAME TO %s"",
                     quoted(catalogName, schemaName, tableName),
                     quoted(catalogName, newSchemaName, newTableName));
             execute(connection, sql);
         }
         catch (SQLException e) {
             throw new PrestoException(JDBC_ERROR, e);
         }
     }",NotBuggy,"Pass JdbcIdentity To DriverConnectionFactory#openConnection

Co-Authored-By: Ke Wang <ke1024@fb.com>
",Buggy,The commit message indicates that the `JdbcIdentity` is passed to `DriverConnectionFactory#openConnection`. The diff confirms this change by adding `JdbcIdentity identity` as the first argument to the `renameTable` method and passing it to the `connectionFactory.openConnection()` method. This change likely fixes a bug or improves security by providing necessary identity information when opening a database connection. The answer is **Buggy**.
presto,17845.json,1e1df56d263e3338de3adc636df13ec5800e1413,"@@ -1,4 +1,4 @@
-    public ConnectorSplitManager getSplitManager()
+    public TransactionalConnectorSplitManager getSplitManager()
     {
         return splitManager;
     }",NotBuggy,"Make SystemTables transactional
",NotBuggy,"The commit message indicates a change to make SystemTables transactional. The diff shows a change in the return type of the `getSplitManager()` method from `ConnectorSplitManager` to `TransactionalConnectorSplitManager`. This suggests that the split manager is now transactional, aligning with the commit message. While this change might be part of a larger effort to fix a bug related to transactionality, the diff itself doesn't directly indicate a bug fix. It's more of an architectural or feature enhancement. The answer is **NotBuggy**.
"
presto,14579.json,7cf87bd218ba996089fd42fc2796e53c69d40d7b,"@@ -1,4 +1,4 @@
-    public static double log10(@SqlType(DoubleType.class) double num)
+    public static double log10(@SqlType(DoubleType.NAME) double num)
     {
         return Math.log10(num);
     }",NotBuggy,"Change @SqlType to take a String
",NotBuggy,"The commit message indicates a change in the `@SqlType` annotation to accept a String. The diff shows that the `DoubleType.class` is replaced with `DoubleType.NAME`. This change aligns with the commit message, suggesting that the annotation now uses a String representation of the type instead of the class itself. This could be to decouple the annotation from direct class references or to allow for more flexible type specifications. This change does not indicate a bug fix. The answer is **NotBuggy**.
"
presto,23057.json,41c77cf7606f1e111c66c9a5ee720577f6268bac,"@@ -1,8 +1,8 @@
     public String toString()
     {
-        return MoreObjects.toStringHelper(this)
+        return toStringHelper(this)
                 .add(""major"", major)
                 .add(""minor"", minor)
                 .add(""update"", update)
                 .toString();
     }",NotBuggy,"Static import toStringHelper
",NotBuggy,"The commit message indicates a change to use a static import for `toStringHelper`. The diff confirms this by replacing `MoreObjects.toStringHelper(this)` with `toStringHelper(this)`. This change simplifies the code by removing the need to qualify the `toStringHelper` method call. This is a refactoring change, not a bug fix. The answer is **NotBuggy**.
"
presto,18660.json,54478b6f30fab15d960676ad2dbc1c0005c1fb47,"@@ -1,8 +1,7 @@
-    public Optional<PlanNode> apply(PlanNode node, Context context)
+    public Optional<PlanNode> apply(ApplyNode applyNode, Captures captures, Context context)
     {
-        ApplyNode applyNode = (ApplyNode) node;
         if (applyNode.getSubqueryAssignments().isEmpty()) {
             return Optional.of(applyNode.getInput());
         }
         return Optional.empty();
     }",NotBuggy,"Parameterize Rule with pattern output type, remove root node casts
",NotBuggy,"The commit message indicates the removal of root node casts and parameterization of the Rule with a pattern output type. The diff shows that the `apply` method's signature has changed from accepting a generic `PlanNode` to `ApplyNode`, and it now also accepts `Captures`. The explicit cast `ApplyNode applyNode = (ApplyNode) node;` has been removed, which aligns with the commit message's intention to remove root node casts. The change in the apply method's parameters suggests a refactoring to make the rule more specific to `ApplyNode` and to utilize capture groups. There is no indication of bug fixing in this change. The answer is **NotBuggy**."
presto,10382.json,7c58b7cfb786bc2dc1309c42e18479577f0d2ffc,"@@ -1,4 +1,4 @@
-        public int size()
-        {
-            return delegate.size();
-        }

+    public int size()
+    {
+        return delegate.size();
+    }",NotBuggy,"Rename LinkedHashQueue to FifoQueue

Also make it a top-level class
",NotBuggy,"The commit message indicates a renaming of a class and making it a top-level class. The provided diff only shows a minor formatting change (removal of whitespace). This change does not indicate a bug fix. The answer is **NotBuggy**.
"
presto,11220.json,efd979afa5efcf616ac1ae0b806fcad3a0cf1ab0,"@@ -1,4 +1,4 @@
-    public void checkCanShowRoleGrants(TransactionId transactionId, Identity identity, String catalogName)
+    public void checkCanShowCurrentRoles(TransactionId transactionId, Identity identity, AccessControlContext context, String catalogName)
     {
-        denyShowRoleGrants(catalogName);
+        denyShowCurrentRoles(catalogName);
     }",NotBuggy,"Add AccessControlContext to store client information for security purposes
",NotBuggy,"The commit message indicates the addition of `AccessControlContext` for security purposes. The diff shows a change in the method signature from `checkCanShowRoleGrants` to `checkCanShowCurrentRoles`, including the addition of the `AccessControlContext` parameter. The method body also changes from `denyShowRoleGrants` to `denyShowCurrentRoles`. These changes align with the commit message and suggest an enhancement related to access control. There is no indication of a bug fix in the traditional sense. The answer is **NotBuggy**.
"
presto,32284.json,2d550a5123000f7965f16c0c5c594fc030e349a0,"@@ -1,4 +1,4 @@
     public List<String> listSchemaNames(ConnectorSession session)
     {
-        return ImmutableList.of(SCHEMA_NAME);
+        return ImmutableList.copyOf(schemas);
     }",NotBuggy,"Support CREATE SCHEMA in blackhole connector
",NotBuggy,"The commit message indicates support for `CREATE SCHEMA` in the blackhole connector. The diff changes the `listSchemaNames` method to return a list of schemas instead of a constant `SCHEMA_NAME`. This change is directly related to the feature described in the commit message, implying the previous implementation was limited and this change expands its functionality. The change doesn't inherently indicate a bug fix, but rather an enhancement. The answer is **NotBuggy**.
"
presto,28423.json,4edc6ea6dc2ee43794889122d51652a8c88be5a8,"@@ -1,4 +1,4 @@
     public String toString()
     {
-        return Objects.toStringHelper(this).toString();
+        return toStringHelper(this).toString();
     }",NotBuggy,"Update to Airbase 28 with Guava 18.0
",NotBuggy,"The commit message indicates an update to Airbase 28 with Guava 18.0. The diff shows a change from `Objects.toStringHelper(this)` to `toStringHelper(this)`. This suggests a change in how the `toString` method is implemented, likely due to an API change or a move of the `toStringHelper` method to the current class. This change doesn't inherently indicate a bug fix, but rather an adaptation to a new version of Guava. The answer is **NotBuggy**."
presto,11869.json,4ae5d1ecaf56ad7ef3d07bd86090eec806a9cf23,"@@ -1,6 +1,7 @@
     private void addRowsToAggregates(Page page, int startPosition, int endPosition)
     {
+        Page region = page.getRegion(startPosition, endPosition - startPosition + 1);
         for (Aggregator aggregator : aggregates) {
-            aggregator.processPage(page.getRegion(startPosition, endPosition - startPosition + 1));
+            aggregator.processPage(region);
         }
     }",NotBuggy,"Remove unnecessary copies in StreamingAggregationOperator

The previous implementation copied the aggregation group out of the
page for each aggregation operator which is wasteful and extremely
slow when the number of aggregates performed is large.

Performance improvement depends on the number of aggregations and
the number of groups contained in the page, but some workloads can
improve throughput by more than 100% and generate significantly
less garbage
",NotBuggy,"The commit message indicates a performance improvement by removing unnecessary copies of the aggregation group. The diff shows that the code now gets the region once and reuses it for all aggregators, instead of getting a new region for each aggregator. This aligns with the commit message's claim of removing unnecessary copies. While this change improves performance, it doesn't directly fix a bug in the sense of correcting incorrect behavior or preventing errors. It's an optimization. The answer is **NotBuggy**.
"
netty,14898.json,e6c9ac968d3923080822dc36fe14aa10e38af15b,"@@ -1,59 +1,60 @@
     public void channelRead(final ChannelHandlerContext ctx, final Object msg) throws Exception {
         long size = calculateSize(msg);
         long now = TrafficCounter.milliSecondFromNano();
         if (size > 0) {
             // compute the number of ms to wait before reopening the channel
             long waitGlobal = trafficCounter.readTimeToWait(size, getReadLimit(), maxTime, now);
             Integer key = ctx.channel().hashCode();
             PerChannel perChannel = channelQueues.get(key);
             long wait = 0;
             if (perChannel != null) {
                 wait = perChannel.channelTrafficCounter.readTimeToWait(size, readChannelLimit, maxTime, now);
                 if (readDeviationActive) {
                     // now try to balance between the channels
                     long maxLocalRead;
                     maxLocalRead = perChannel.channelTrafficCounter.cumulativeReadBytes();
                     long maxGlobalRead = cumulativeReadBytes.get();
                     if (maxLocalRead <= 0) {
                         maxLocalRead = 0;
                     }
                     if (maxGlobalRead < maxLocalRead) {
                         maxGlobalRead = maxLocalRead;
                     }
                     wait = computeBalancedWait(maxLocalRead, maxGlobalRead, wait);
                 }
             }
             if (wait < waitGlobal) {
                 wait = waitGlobal;
             }
             wait = checkWaitReadTime(ctx, wait, now);
             if (wait >= MINIMAL_WAIT) { // At least 10ms seems a minimal
                 // time in order to try to limit the traffic
                 // Only AutoRead AND HandlerActive True means Context Active
-                ChannelConfig config = ctx.channel().config();
+                Channel channel = ctx.channel();
+                ChannelConfig config = channel.config();
                 if (logger.isDebugEnabled()) {
                     logger.debug(""Read Suspend: "" + wait + ':' + config.isAutoRead() + ':'
                             + isHandlerActive(ctx));
                 }
                 if (config.isAutoRead() && isHandlerActive(ctx)) {
                     config.setAutoRead(false);
-                    ctx.attr(READ_SUSPENDED).set(true);
+                    channel.attr(READ_SUSPENDED).set(true);
                     // Create a Runnable to reactive the read if needed. If one was create before it will just be
                     // reused to limit object creation
-                    Attribute<Runnable> attr = ctx.attr(REOPEN_TASK);
+                    Attribute<Runnable> attr = channel.attr(REOPEN_TASK);
                     Runnable reopenTask = attr.get();
                     if (reopenTask == null) {
                         reopenTask = new ReopenReadTimerTask(ctx);
                         attr.set(reopenTask);
                     }
                     ctx.executor().schedule(reopenTask, wait, TimeUnit.MILLISECONDS);
                     if (logger.isDebugEnabled()) {
                         logger.debug(""Suspend final status => "" + config.isAutoRead() + ':'
                                 + isHandlerActive(ctx) + "" will reopened at: "" + wait);
                     }
                 }
             }
         }
         informReadOperation(ctx, now);
         ctx.fireChannelRead(msg);
     }",NotBuggy,"Cleanup: replaced deprecated ctx.attr() and ctx.hasAttr() methods usage with ch.attr() and ch.hasAttr().

Motivation:

Will allow easy removal of deprecated methods in future.

Modification:

Replaced ctx.attr(), ctx.hasAttr() with ctx.channel().attr(), ctx.channel().hasAttr().

Result:

No deprecated ctx.attr(), ctx.hasAttr() methods usage.
",NotBuggy,"The commit message indicates a cleanup task, replacing deprecated methods `ctx.attr()` and `ctx.hasAttr()` with `ctx.channel().attr()` and `ctx.channel().hasAttr()`. The diff confirms this change, showing that the code has been updated to use the `channel()` method to access the attributes. This change is a refactoring effort to remove deprecated methods, not a bug fix. The answer is **NotBuggy**."
netty,7557.json,ecfa241768499dae2bffa174a874e8f9d81d6d1f,"@@ -1,3 +1,3 @@
-        public void setTrailingHeaders(HttpHeaders trailingHeaders) {
+        void setTrailingHeaders(HttpHeaders trailingHeaders) {
             this.trailingHeaders = trailingHeaders;
         }",NotBuggy,"Make sure AggregatedFullHttpMessage.trailingHeaders() return non-null

When handling an oversized message, HttpObjectAggregator does not wait
until the last chunk is received to produce the failed message, making
AggregatedFullHttpMessage.trailingHeaders() return null.
",Buggy,"The commit message indicates a bug where `AggregatedFullHttpMessage.trailingHeaders()` could return null in certain scenarios (oversized messages). The diff changes the visibility of `setTrailingHeaders` from `public` to package-private (`void`). This change alone doesn't directly fix the null return issue described in the commit message. However, it's possible that this change is part of a larger fix, and the commit message focuses on the symptom rather than the root cause. Without more context, it's difficult to definitively say if this change fixes the bug. However, given the commit message's description of a bug and the change to the `setTrailingHeaders` method, it's likely that this change is related to fixing the bug. The answer is **Buggy**."
netty,525.json,00d2cea8bad744d15a5daed1e36590308f715ee3,"@@ -1,3 +1,3 @@
-    public QoS qualityOfService() {
+    public MqttQoS qualityOfService() {
         return qualityOfService;
     }",NotBuggy,"Overall clean-up on codec-mqtt

- Use simple string concatenation instead of String.format()
- Rewrite exception messages so that it follows our style
- Merge MqttCommonUtil and MqttValidationUtil into MqttCodecUtil
- Hide MqttCodecUtil from users
- Rename MqttConnectReturnCode.value to byteValue
- Rename MqttMessageFactory.create*() to new*()
- Rename QoS to MqttQoS
- Make MqttSubAckPayload.grantedQoSLevels immutable and add more useful
  constructor
",NotBuggy,"The commit message indicates a cleanup of the codec-mqtt module, including renaming `QoS` to `MqttQoS`. The diff shows a change from `QoS` to `MqttQoS` in the `qualityOfService()` method. This change aligns with the commit message and seems to be a renaming refactoring. There is no indication of a bug fix. The answer is **NotBuggy**.
"
netty,3934.json,3d81afb8a5ec31420fb503bc9476ecf23bb6bbfa,"@@ -1,9 +1,11 @@
         public void addTimeout(HashedWheelTimeout timeout) {
+            assert timeout.bucket == null;
+            timeout.bucket = this;
             if (head == null) {
                 head = tail = timeout;
             } else {
                 tail.next = timeout;
                 timeout.prev = tail;
                 tail = timeout;
             }
         }",NotBuggy,"Make sure cancelled Timeouts are able to be GC'ed fast.

Motivation:
At the moment the HashedWheelTimer will only remove the cancelled Timeouts once the HashedWheelBucket is processed again. Until this the instance will not be able to be GC'ed as there are still strong referenced to it even if the user not reference it by himself/herself. This can cause to waste a lot of memory even if the Timeout was cancelled before.

Modification:
Add a new queue which holds CancelTasks that will be processed on each tick to remove cancelled Timeouts. Because all of this is done only by the WorkerThread there is no need for synchronization and only one extra object creation is needed when cancel() is executed. For addTimeout(...) no new overhead is introduced.

Result:
Less memory usage for cancelled Timeouts.
",Buggy,"The commit message describes a memory leak issue where cancelled Timeouts are not garbage collected promptly due to strong references within the HashedWheelTimer. The diff shows the `addTimeout` method in `HashedWheelBucket` being modified to set the `bucket` field of the timeout and assert that it is null. While the diff itself doesn't show the complete fix (the addition of the CancelTasks queue and its processing), it does show a change related to the Timeout object's lifecycle and its association with the bucket. The assertion `assert timeout.bucket == null;` suggests that a timeout should not already be associated with a bucket when added, which is related to the fix described in the commit message. The change ensures that the timeout's bucket is correctly set, which is important for the subsequent removal of cancelled timeouts. This indicates that the change is part of a bug fix related to memory management. The answer is **Buggy**.
"
netty,17881.json,b9996908b1fac66526395414cc5c3a14a1567d4e,"@@ -1,3 +1,3 @@
-        public ByteBuf resumeIntermediaryDeallocations() {
-            return this;
-        }

+    public ByteBuf markWriterIndex() {
+        return this;
+    }",NotBuggy,"Implement reference counting

- Related: #1029
- Replace Freeable with ReferenceCounted
- Add AbstractReferenceCounted
- Add AbstractReferenceCountedByteBuf
- Add AbstractDerivedByteBuf
- Add EmptyByteBuf
",NotBuggy,"The commit message indicates a major change: implementing reference counting. The diff snippet shows a change from `resumeIntermediaryDeallocations()` to `markWriterIndex()`. This change doesn't seem directly related to fixing a bug but rather to implementing the new reference counting mechanism. The change in method name suggests a shift in functionality related to buffer management, aligning with the overall goal of implementing reference counting. The answer is **NotBuggy**."
netty,12649.json,17280116c48b91c2dd8988d625f749318150de3b,"@@ -1,11 +1,9 @@
-    private void fail(ChannelHandlerContext ctx, long frameLength) {
+    private void fail(long frameLength) {
         if (frameLength > 0) {
-            ctx.fireExceptionCaught(
-                    new TooLongFrameException(
-                            ""frame length exceeds "" + maxFrameLength + "": "" + frameLength + "" - discarded""));
+            throw new TooLongFrameException(
+                            ""frame length exceeds "" + maxFrameLength + "": "" + frameLength + "" - discarded"");
         } else {
-            ctx.fireExceptionCaught(
-                    new TooLongFrameException(
-                            ""frame length exceeds "" + maxFrameLength + "" - discarding""));
+            throw new TooLongFrameException(
+                            ""frame length exceeds "" + maxFrameLength + "" - discarding"");
         }
     }",NotBuggy,"[#2643] Throw TooLongFrameException instead of using fireExceptionCaught

Motivation:

It's not always the case that there is another handler in the pipeline that will intercept the exceptionCaught event because sometimes users just sub-class. In this case the exception will just hit the end of the pipeline.

Modification:
Throw the TooLongFrameException so that sub-classes can handle it in the exceptionCaught(...) method directly.

Result:
Sub-classes can correctly handle the exception,
",Buggy,"The commit message indicates a change in how exceptions are handled when a frame is too long. Instead of using `fireExceptionCaught`, the code now throws a `TooLongFrameException`. This allows subclasses to directly handle the exception in their `exceptionCaught(...)` method. The diff confirms this change, replacing calls to `ctx.fireExceptionCaught` with `throw new TooLongFrameException`. This change addresses a potential issue where exceptions might not be properly handled if there isn't a handler in the pipeline to intercept the `exceptionCaught` event. Therefore, this commit fixes a bug related to exception handling. The answer is **Buggy**."
netty,14065.json,59973e93dd7da715eee709788573e3515cc50238,"@@ -1,3 +1,3 @@
-    public String getPeerHost() {
+    public final String getPeerHost() {
         return wrapped.getPeerHost();
     }",NotBuggy,"Ensure X509KeyManager methods are called on the correct time when using server-side and support more methods of ExtendedSSLSession. (#8283)

Motivation:

Before when on server-side we just called the X509KeyManager methods when handshake() was called the first time which is not quite correct as we may not have received the full SSL hello / handshake and so could not extra for example the SNI hostname that was requested.
OpenSSL exposes the SSL_CTX_set_cert_cb function which allows to set a callback which is executed at the correct moment, so we should use it. This also allows us to support more methods of ExtendedSSLSession easily.

Modifications:

- Make use of new methods exposed by netty-tcnative since https://github.com/netty/netty-tcnative/pull/388 to ensure we select the key material at the correct time.
- Implement more methods of ExtendedOpenSslSession
- Add unit tests to ensure we are able to retrieve various things on server-side in the X509KeyManager and so verify it is called at the correct time.
- Simplify code by using new netty-tcnative methods.

Result:

More correct implementation for server-side usage and more complete implemented of ExtendedSSLSession.",Buggy,"The commit message indicates a bug fix related to the timing of X509KeyManager method calls on the server side. The original implementation called these methods too early in the handshake process, potentially before all necessary information (e.g., SNI hostname) was available. The fix involves using the SSL_CTX_set_cert_cb function in OpenSSL to ensure the methods are called at the correct time. The diff shows a change in the `getPeerHost()` method, adding `final`. While this specific change doesn't directly relate to the timing issue described in the commit message, the overall commit addresses a bug. The answer is **Buggy**.
"
netty,15217.json,7d4c077492ba5b40595b0742e7b22182c544f7b7,"@@ -1,5 +1,5 @@
-    public ByteBuf setInt(int index, int value) {
-        wrapped.checkIndex(index, 4);
-        _setInt(index, value);
+    public ByteBuf setByte(int index, int value) {
+        checkIndex(index);
+        _setByte(index, value);
         return this;
     }",NotBuggy,"Add *UnsafeHeapByteBuf for improve performance on systems with sun.misc.Unsafe

Motivation:

sun.misc.Unsafe allows us to handle heap ByteBuf in a more efficient matter. We should use special ByteBuf implementation when sun.misc.Unsafe can be used to increase performance.

Modifications:

- Add PooledUnsafeHeapByteBuf and UnpooledUnsafeHeapByteBuf that are used when sun.misc.Unsafe is ready to use.
- Add UnsafeHeapSwappedByteBuf

Result:

Better performance when using heap buffers and sun.misc.Unsafe is ready to use.
",NotBuggy,"The commit message indicates a performance improvement by leveraging `sun.misc.Unsafe` for heap `ByteBuf` operations. The diff shows a change in the `setInt` method to `setByte`. While the commit message focuses on performance, the code change suggests a modification in the API or functionality related to setting byte values. This could be part of the performance optimization strategy, but it's not immediately clear if it's fixing a bug. The change from `setInt` to `setByte` suggests a potential change in the intended data type being handled, which could indirectly address a bug if the original `setInt` was causing issues. However, without more context, it's difficult to definitively say if this is a bug fix. The commit message does not explicitly mention any bug being fixed.
The answer is **NotBuggy**."
netty,9425.json,c8ca3299324a1e2b9ce501c95715cf10ea70c238,"@@ -1,4 +1,4 @@
     public SpdyHeadersFrame setInvalid() {
-        super.setInvalid();
+        invalid = true;
         return this;
     }",NotBuggy,"SPDY: update object hierarchy
",NotBuggy,"The commit message indicates an update to the object hierarchy in SPDY. The diff shows a change in the `setInvalid` method, where `super.setInvalid()` is replaced with `invalid = true`. This suggests a modification in how the invalid state is managed within the class hierarchy. It's likely related to how the invalid flag is propagated or handled in the SPDY framework. While it's a change in logic, it doesn't explicitly indicate a bug fix. It's more likely a refactoring or an update to the object hierarchy as the commit message suggests. The answer is **NotBuggy**.
"
netty,1157.json,dfa3bbbf0035e6eb39403194c9aabae0f9c2c1a3,"@@ -1,3 +1,3 @@
-    public List<DnsCacheEntry> get(String hostname) {
+    public List<DnsCacheEntry> get(String hostname, DnsRecord[] additionals) {
         return Collections.emptyList();
     }",NotBuggy,"Add support for Client Subnet in DNS Queries (RFC7871)

Motivation:

RFC7871 defines an extension which allows to request responses for a given subset.

Modifications:

- Add DnsOptPseudoRrRecord which can act as base class for extensions based on EDNS(0) as defined in RFC6891
- Add DnsOptEcsRecord to support the Client Subnet in DNS Queries extension
- Add tests

Result:

Client Subnet in DNS Queries extension is now supported.
",NotBuggy,"The commit message indicates the addition of a new feature: support for Client Subnet in DNS Queries (RFC7871). The modifications listed include adding new classes (DnsOptPseudoRrRecord, DnsOptEcsRecord) and tests. The diff shows a change in the signature of the `get` method in what appears to be a DNS cache, adding an `additionals` parameter. This change is likely related to passing additional DNS records, potentially including the new ECS record, during a DNS query. This doesn't appear to be a bug fix, but rather an enhancement to support the new feature. The answer is **NotBuggy**.
"
netty,12457.json,3a9f47216143082bdfba62e8940160856767d672,"@@ -1,4 +1,3 @@
     public ByteBuf setShort(int index, int value) {
-        reject();
-        return this;
+        throw reject();
     }",NotBuggy,"Make retained derived buffers recyclable

Related: #4333 #4421 #5128

Motivation:

slice(), duplicate() and readSlice() currently create a non-recyclable
derived buffer instance. Under heavy load, an application that creates a
lot of derived buffers can put the garbage collector under pressure.

Modifications:

- Add the following methods which creates a non-recyclable derived buffer
  - retainedSlice()
  - retainedDuplicate()
  - readRetainedSlice()
- Add the new recyclable derived buffer implementations, which has its
  own reference count value
- Add ByteBufHolder.retainedDuplicate()
- Add ByteBufHolder.replace(ByteBuf) so that..
  - a user can replace the content of the holder in a consistent way
  - copy/duplicate/retainedDuplicate() can delegate the holder
    construction to replace(ByteBuf)
- Use retainedDuplicate() and retainedSlice() wherever possible
- Miscellaneous:
  - Rename DuplicateByteBufTest to DuplicatedByteBufTest (missing 'D')
  - Make ReplayingDecoderByteBuf.reject() return an exception instead of
    throwing it so that its callers don't need to add dummy return
    statement

Result:

Derived buffers are now recycled when created via retainedSlice() and
retainedDuplicate() and derived from a pooled buffer
",Buggy,"The commit message indicates a performance improvement by making derived buffers recyclable, which can reduce garbage collection pressure. The diff shows a change in `setShort` method where `reject()` now throws an exception instead of simply calling it. This change, along with the commit message, suggests that the original implementation might have had a flaw where it wasn't handling the rejection properly, potentially leading to unexpected behavior or resource leaks. By throwing an exception, the code ensures that the caller is aware of the rejection and can handle it appropriately. This can be considered a bug fix, as it addresses a potential issue in the original implementation. The answer is **Buggy**.
"
netty,14470.json,5ac84760c4f5d4f09a61fb5cd2cac7d95a782f9f,"@@ -1,11 +1,15 @@
     public SslContextBuilder keyManager(File keyCertChainFile, File keyFile, String keyPassword) {
-        if (forServer) {
-            checkNotNull(keyCertChainFile, ""keyCertChainFile required for servers"");
-            checkNotNull(keyFile, ""keyFile required for servers"");
+        X509Certificate[] keyCertChain;
+        PrivateKey key;
+        try {
+            keyCertChain = SslContext.toX509Certificates(keyCertChainFile);
+        } catch (Exception e) {
+            throw new IllegalArgumentException(""File does not contain valid certificates: "" + keyCertChainFile, e);
         }
-        this.keyCertChainFile = keyCertChainFile;
-        this.keyFile = keyFile;
-        this.keyPassword = keyPassword;
-        this.keyManagerFactory = null;
-        return this;
+        try {
+            key = SslContext.toPrivateKey(keyFile, keyPassword);
+        } catch (Exception e) {
+            throw new IllegalArgumentException(""File does not contain valid private key: "" + keyFile, e);
+        }
+        return keyManager(key, keyPassword, keyCertChain);
     }",NotBuggy,"Allow to create SslContext from existing PrivateKey / X509Certificate

Motivation:

Sometimes the user already has a PrivateKey / X509Certificate which should be used to create a new SslContext. At the moment we only allow to construct it via Files.

Modifications:

- Add new methods to the SslContextBuilder to allow creating a SslContext from PrivateKey / X509Certificate
- Mark all public constructors of *SslContext as @Deprecated, the user should use SslContextBuilder
- Update tests to us SslContextBuilder.

Result:

Creating of SslContext is possible with PrivateKay/X509Certificate
",NotBuggy,"The commit message describes adding new methods to `SslContextBuilder` to allow creating an `SslContext` from `PrivateKey` and `X509Certificate` objects, instead of just files. The diff shows that the `keyManager` method now takes `PrivateKey` and `X509Certificate` objects as input. The code now converts the files to the required objects and calls the new method. This change introduces new functionality and does not fix a bug. The answer is **NotBuggy**."
netty,11382.json,f9001b9fc07a71a9d6eaf0462470416780302107,"@@ -1,21 +1,21 @@
-    private void encodeLiteral(ByteBuf out, CharSequence name, CharSequence value, HpackUtil.IndexType indexType,
+    private void encodeLiteral(ByteBuf out, CharSequence name, CharSequence value, IndexType indexType,
                                int nameIndex) {
         boolean nameIndexValid = nameIndex != -1;
         switch (indexType) {
             case INCREMENTAL:
                 encodeInteger(out, 0x40, 6, nameIndexValid ? nameIndex : 0);
                 break;
             case NONE:
                 encodeInteger(out, 0x00, 4, nameIndexValid ? nameIndex : 0);
                 break;
             case NEVER:
                 encodeInteger(out, 0x10, 4, nameIndexValid ? nameIndex : 0);
                 break;
             default:
                 throw new Error(""should not reach here"");
         }
         if (!nameIndexValid) {
             encodeStringLiteral(out, name);
         }
         encodeStringLiteral(out, value);
     }",NotBuggy,"HTTP/2 move internal HPACK classes to the http2 package

Motivation:
The internal.hpack classes are no longer exposed in our public APIs and can be made package private in the http2 package.

Modifications:
- Make the hpack classes package private in the http2 package

Result:
Less APIs exposed as public.
",NotBuggy,"The commit message indicates a refactoring to reduce the number of exposed public APIs by making internal HPACK classes package private. The diff shows a change in the import statement for `HpackUtil.IndexType` to `IndexType`, which is consistent with moving the class to a different package and adjusting its visibility. This change does not appear to be related to fixing a bug, but rather to improving the internal structure and reducing the public API surface. The answer is **NotBuggy**.
"
netty,16567.json,57063b6db0193f3d53847f3754005a1cd63d463a,"@@ -1,3 +1,3 @@
     protected int _getIntLE(int index) {
-        return buffer.getIntLE(index);
+        return unwrap().getIntLE(index);
     }",NotBuggy,"Implement the ByteBuf methods that should have been overridden

Motivation:

Some ByteBuf implementations do not override all necessary methods,
which can lead to potentially sub-optimal behavior.

Also, SlicedByteBuf does not perform the range check correctly due to
missing overrides.

Modifications:

- Add missing overrides
- Use unwrap() instead of direct member access in derived buffers for
  consistency
- Merge unwrap0() into unwrap() using covariant return type
- Deprecate AbstractDerivedByteBuf and its subtypes, because they were
  not meant to be public

Result:

Correctness
",Buggy,"The commit message indicates that some ByteBuf implementations do not override all necessary methods, which can lead to potentially sub-optimal behavior. The diff shows that the `_getIntLE` method in a class is modified to use `unwrap()` instead of `buffer` to access the underlying buffer. This change is part of the effort to ensure consistent behavior across different ByteBuf implementations by using the `unwrap()` method. This suggests a bug fix related to incorrect or inconsistent behavior due to missing overrides. The answer is **Buggy**."
netty,17888.json,b9996908b1fac66526395414cc5c3a14a1567d4e,"@@ -1,3 +1,3 @@
-        public byte getByte(int index) {
-            throw new IndexOutOfBoundsException();
-        }

+    public byte getByte(int index) {
+        throw new IndexOutOfBoundsException();
+    }",NotBuggy,"Implement reference counting

- Related: #1029
- Replace Freeable with ReferenceCounted
- Add AbstractReferenceCounted
- Add AbstractReferenceCountedByteBuf
- Add AbstractDerivedByteBuf
- Add EmptyByteBuf
",NotBuggy,"The commit message indicates a major change: implementing reference counting. The diff itself shows a very minor change: a formatting adjustment (removing leading whitespace) in an `IndexOutOfBoundsException` throw. While the commit message describes a significant feature implementation, the provided diff doesn't directly reflect bug fixing. The formatting change alone doesn't suggest a bug fix. The answer is **NotBuggy**.
"
argouml,2154.json,992cfe1a2840c838a401450e5232b03826cb956c,"@@ -1,15 +1,22 @@
-    public void actionPerformed(ActionEvent e) {
-        super.actionPerformed(e);
-        Object source = e.getSource();
-        if (source instanceof UMLComboBox2) {
-            Object selected = ((UMLComboBox2) source).getSelectedItem();
-            Object target = ((UMLComboBox2) source).getTarget();
-            if (Model.getFacade().isACallAction(target)
-                    && Model.getFacade().isAOperation(selected)) {
-                if (Model.getFacade().getOperation(target) != selected) {
-                    Model.getCommonBehaviorHelper()
-                    .setOperation(target, selected);
-                }
-            }
-        }
-    }

+       public void actionPerformed(ActionEvent e) {
+           super.actionPerformed(e);
+           Object source = e.getSource();
+           Object oldAssoc = null;
+           Object newAssoc = null;
+           Object link = null;
+           if (source instanceof UMLComboBox2) {
+               UMLComboBox2 box = (UMLComboBox2) source;
+               Object o = box.getTarget();
+               if (Model.getFacade().isALink(o)) {
+                   link = o;
+                   oldAssoc = Model.getFacade().getAssociation(o);
+               }
+               Object n = box.getSelectedItem();
+               if (Model.getFacade().isAAssociation(n)) {
+                   newAssoc = n;
+               }
+           }
+           if (newAssoc != oldAssoc && link != null && newAssoc != null) {
+               Model.getCoreHelper().setAssociation(link, newAssoc);
+           }
+       }",NotBuggy,"Change the model when the combo selection is changed

git-svn-id: http://argouml.tigris.org/svn/argouml/trunk@17668 a161b567-7d1e-0410-9ef9-912c70fedb3f
",Buggy,"The commit message indicates a change in the model based on combo box selection. The diff modifies the `actionPerformed` method to handle the selection of a new association in a combo box. It retrieves the old and new associations, and if they are different, it updates the model to reflect the new association. This suggests the original code might not have correctly updated the model when the combo box selection changed, which can be considered a bug. The answer is **Buggy**."
argouml,2119.json,9d400c77ecb6e128d8d0867107f60f1ea00b81d3,"@@ -1,7 +1,7 @@
-    private static Object getType(Object target) {
-        Object type = Model.getFacade().getType(target);
-        if (Model.getFacade().isAClassifierInState(type)) {
-            type = Model.getFacade().getType(type);
-        }
-        return type;
-    }

+        private static Object getType(Object target) {
+            Object type = Model.getFacade().getType(target);
+            if (Model.getFacade().isAClassifierInState(type)) {
+                type = Model.getFacade().getType(type);
+            }
+            return type;
+        }",NotBuggy,"Make Actions inner classes of whatever uses them

git-svn-id: http://argouml.tigris.org/svn/argouml/trunk@17684 a161b567-7d1e-0410-9ef9-912c70fedb3f
",NotBuggy,The diff moves the `getType` method into another class. This change does not indicate a bug fix. The answer is **NotBuggy**.
argouml,5773.json,026fc657d606390b8457286c18c7dc5b9fe57803,"@@ -1,3 +1,4 @@
             public void undo() {
-                showStereotypes = !showem;
+                showVisibilities = !showem;
+                showVisibilitiesSet = oldValid;
             }",NotBuggy,"showStereotype setting is for the explorer (ie project-wide).  move it back to the ProjectSettings and undeprecate the related methods.


git-svn-id: http://argouml.tigris.org/svn/argouml/trunk@16435 a161b567-7d1e-0410-9ef9-912c70fedb3f
",NotBuggy,"The commit message indicates a change in the scope of the `showStereotype` setting and the undeprecation of related methods. The provided diff snippet, however, shows changes related to `showVisibilities` and `showVisibilitiesSet`. This discrepancy suggests the diff is either incomplete or unrelated to the commit message. The code snippet shows an undo operation that toggles `showVisibilities` and sets `showVisibilitiesSet` to `oldValid`. Without more context, it's difficult to determine if this change is a bug fix. However, given the commit message's focus on `showStereotype` and the diff's focus on `showVisibilities`, and the lack of clear evidence of a bug fix in the provided snippet, it's more likely a refactoring or feature adjustment.

**NotBuggy**
"
argouml,2643.json,2a18b45c346f54b0340fd7f7cf7fe8644919d2f0,"@@ -1,47 +1,36 @@
-    public void setTarget(Object theNewTarget) {
+    protected void setTarget(Object theNewTarget) {
+        assert (getTarget() == null);
+        assert (!(getTarget() instanceof Fig));
+        
         theNewTarget = theNewTarget instanceof Fig
             ? ((Fig) theNewTarget).getOwner() : theNewTarget;
         if (Model.getFacade().isAModelElement(theNewTarget)
                 || theNewTarget instanceof Diagram) {
-            if (getTarget() != null) {
-                Enumeration enumeration = elements();
-                while (enumeration.hasMoreElements()) {
-                    Object base = enumeration.nextElement();
-                    Model.getPump().removeModelEventListener(
-                        this,
-                        base,
-                        ""ownedElement"");
-                }
-                Model.getPump().removeModelEventListener(
-                    this,
-                    getTarget(),
-                    ""base"");
-            }
             setListTarget(theNewTarget);
             if (getTarget() != null) {
                 Collection bases = Model.getFacade().getBases(getTarget());
                 Iterator it = bases.iterator();
                 while (it.hasNext()) {
                     Object base =  it.next();
                     Model.getPump().addModelEventListener(
                         this,
                         base,
                         ""ownedElement"");
                 }
                 // make sure we know it when a classifier is added as a base
                 Model.getPump().addModelEventListener(
                     this,
                     getTarget(),
                     ""base"");
             }
             if (getTarget() != null) {
                 removeAllElements();
                 setBuildingModel(true);
                 buildModelList();
                 setBuildingModel(false);
                 if (getSize() > 0) {
                     fireIntervalAdded(this, 0, getSize() - 1);
                 }
             }
         }
     }",NotBuggy,"Get rid of any target listeners

git-svn-id: http://argouml.tigris.org/svn/argouml/trunk@17506 a161b567-7d1e-0410-9ef9-912c70fedb3f
",Buggy,"The commit message ""Get rid of any target listeners"" suggests the removal of code related to listening for changes on a target object. The diff shows the removal of code that removes model event listeners when the target is changed. The assertions added at the beginning of the method indicate a specific context or constraint related to the target. The removal of the listener code and the addition of assertions suggest a change in how the target is handled, potentially to fix a bug or prevent a specific issue. The answer is **Buggy**."
argouml,13058.json,c594f19e891bb817975f314e7f9a190bf073e003,"@@ -1,12 +1,10 @@
     private boolean isDeleteEventRequired(
             final Object element) {
-        if (element instanceof Property) {
-            synchronized (deleteEventIgnoreList) {
-                if (deleteEventIgnoreList.contains(element)) {
-                    deleteEventIgnoreList.remove(element);
-                    return false;
-                }
+        synchronized (deleteEventIgnoreList) {
+            if (deleteEventIgnoreList.contains(element)) {
+                deleteEventIgnoreList.remove(element);
+                return false;
             }
         }
         return true;
     }",NotBuggy,"Make sure that changing namespace or region does not fire a delete event

git-svn-id: http://argouml.tigris.org/svn/argouml/trunk@19406 a161b567-7d1e-0410-9ef9-912c70fedb3f
",Buggy,"The code removes the check `element instanceof Property` before accessing and potentially removing the element from `deleteEventIgnoreList`. This change ensures that the `deleteEventIgnoreList` is checked and modified regardless of the element's type. The commit message indicates that the change is intended to prevent delete events from being fired when the namespace or region is changed. This suggests that there was a bug where delete events were incorrectly being fired in these scenarios. The code change is designed to fix this bug by ensuring that the `deleteEventIgnoreList` is always checked, and the element is removed if present, thus preventing the delete event. The answer is **Buggy**."
argouml,2284.json,5df7baaa147ae1e07eda64a63bf1a7bd187a5176,"@@ -1,14 +1,20 @@
     protected void addAll(Collection col) {
         Object selected = getSelectedItem();
         fireListEvents = false;
         int oldSize = objects.size();
         for (Object o : col) {
-            addElement(o);
+            if (filter != null && filter.length() > 0) {
+        	if (Model.getFacade().getName(o).toLowerCase().startsWith(filter.toLowerCase())) {
+                    addElement(o);
+        	}
+            } else {
+                addElement(o);
+            }
         }
         setSelectedItem(external2internal(selected));
         fireListEvents = true;
         if (objects.size() != oldSize) {
             fireIntervalAdded(this, oldSize == 0 ? 0 : oldSize - 1, 
                     objects.size() - 1);
         }
     }",NotBuggy,"Allow filtering of namespaces in combo

git-svn-id: http://argouml.tigris.org/svn/argouml/trunk@18737 a161b567-7d1e-0410-9ef9-912c70fedb3f
",NotBuggy,"The code adds a filtering mechanism to the `addAll` method. It checks if a filter is set and, if so, only adds elements whose names (converted to lowercase) start with the filter string (also converted to lowercase). If no filter is set, it adds all elements. This change introduces a new feature, allowing users to filter namespaces in a combo box. The commit message aligns with the code changes. There is no indication of a bug fix. The answer is **NotBuggy**.
"
argouml,1917.json,02860b3ad2e8258250f31a3ea52907e4e4df61b0,"@@ -1,3 +1,3 @@
-    public Fig getNameDisplay() {
-        return nameFig;
+    public DiagramElement getNameDisplay() {
+        return nameDisplay;
     }",NotBuggy,"Remove all knowledge of FigNodeModelElement

git-svn-id: http://argouml.tigris.org/svn/argouml/trunk@19249 a161b567-7d1e-0410-9ef9-912c70fedb3f
",NotBuggy,"The commit message indicates the removal of `FigNodeModelElement`. The diff shows a change in the return type of the `getNameDisplay` method from `Fig` to `DiagramElement`. This change aligns with the commit message, suggesting the removal of a specific class dependency. While it's a significant change, it doesn't inherently indicate a bug fix. It's more likely a refactoring or code cleanup task. The answer is **NotBuggy**.
"
argouml,14398.json,c1eb0dea7c36300fe037cd721449f3f0e45449ce,"@@ -1,10 +1,16 @@
     public void selectionChanged(GraphSelectionEvent selectionEvent) {
         removeAll();
         if (selectionEvent.getSelections().size() == 1) {
             Fig selectedFig = (Fig)selectionEvent.getSelections().get(0);
             DefaultMutableTreeNode tn = new DefaultMutableTreeNode(getDescr(selectedFig));
             buildTree(selectedFig, tn);
-            JTree tree = new JTree(tn);
-            add(tree);
+            if (selectedFig instanceof FigClassifierRole) {
+                MessageNodeBuilder.addNodeTree(tn, (FigClassifierRole)selectedFig);
+            }
+            FigTree tree = new FigTree(tn);
+            tree.expandAll();
+            
+            JScrollPane scroller = new JScrollPane(tree);
+            add(scroller);
         }
     }",NotBuggy,"Specialize FigInspector for FigClassifierRole so I can see the contained MessageNodes


git-svn-id: http://argouml.tigris.org/svn/argouml/trunk@10031 a161b567-7d1e-0410-9ef9-912c70fedb3f
",NotBuggy,"The commit message indicates a specialization of `FigInspector` for `FigClassifierRole` to enable viewing contained `MessageNodes`. The code diff confirms this by adding a conditional block that, when the selected `Fig` is an instance of `FigClassifierRole`, calls `MessageNodeBuilder.addNodeTree` to build the tree with the message nodes. Additionally, the code now uses a `FigTree` and wraps it in a `JScrollPane`. These changes directly support the commit message's goal of providing specialized inspection for `FigClassifierRole` and viewing its contained `MessageNodes`. There is no indication of a bug fix. The answer is **NotBuggy**."
argouml,3102.json,e5ccb0e741ae79fe74f24969ededccffcb04652e,"@@ -1,3 +1,3 @@
     public String getLabel() {
-        return name;
+        return propertyName;
     }",NotBuggy,"Refactor getName() -> getPropertyName()

git-svn-id: http://argouml.tigris.org/svn/argouml/trunk@18704 a161b567-7d1e-0410-9ef9-912c70fedb3f
",NotBuggy,"The commit message indicates a refactoring, specifically renaming a method from `getName()` to `getPropertyName()`. The diff shows a change within the `getLabel()` method, where the return value is switched from `name` to `propertyName`. This aligns with the commit message, suggesting a simple renaming refactoring. There's no indication of a bug fix in this change. The answer is **NotBuggy**.
"
argouml,1992.json,a09bb80652d16906a2c08e561094768d32d17431,"@@ -1,3 +1,10 @@
     public boolean disable() {
+
+        PropPanelFactoryManager.removePropPanelFactory(propPanelFactory);
+
+        // TODO: Remove the casting to DiagramFactoryInterface2
+        // as soon as DiagramFactoryInterface is removed.
+        DiagramFactory.getInstance().registerDiagramFactory(
+                DiagramType.Activity, (DiagramFactoryInterface2) null);
         return true;
     }",NotBuggy,"Move all files down (I seem to created a rogue directory in previous commit)

git-svn-id: http://argouml.tigris.org/svn/argouml/trunk@18519 a161b567-7d1e-0410-9ef9-912c70fedb3f
",NotBuggy,"The commit message indicates a reorganization of the file structure within the repository, specifically moving files from a mistakenly created directory to the correct location. The diff shows code being added to the `disable()` method. This code removes a `PropPanelFactory` and unregisters a `DiagramFactory`. This change is related to disabling or removing a feature, which could be part of the file reorganization. However, the commit message does not suggest a bug fix. The answer is **NotBuggy**.
"
argouml,2047.json,9d400c77ecb6e128d8d0867107f60f1ea00b81d3,"@@ -1,3 +1,3 @@
-    protected String getDialogTitle() {
-        return Translator.localize(""dialog.title.add-contents"");
-    }

+        protected String getDialogTitle() {
+            return Translator.localize(""dialog.title.add-baseclasses"");
+        }",NotBuggy,"Make Actions inner classes of whatever uses them

git-svn-id: http://argouml.tigris.org/svn/argouml/trunk@17684 a161b567-7d1e-0410-9ef9-912c70fedb3f
",NotBuggy,"The commit message indicates a structural change, moving `Actions` to be inner classes. The diff shows a change in the string returned by `getDialogTitle()`. The change from `""dialog.title.add-contents""` to `""dialog.title.add-baseclasses""` suggests a change in the dialog's purpose, but it doesn't inherently indicate a bug fix. The change is more likely related to the refactoring of the `Actions` classes and the dialogs they control. Therefore, the answer is **NotBuggy**.
"
argouml,11027.json,c82d790257c6dbfd74c4ead021354b2d7a75c6be,"@@ -1,3 +1,2 @@
     public void popupMenuCanceled(PopupMenuEvent e) {
-        LOG.info(""popupMenuCanceled"");
     }",NotBuggy,"Remove logging commited accidentally

git-svn-id: http://argouml.tigris.org/svn/argouml/trunk@15539 a161b567-7d1e-0410-9ef9-912c70fedb3f
",Buggy,"The commit message indicates the removal of accidentally committed logging statements. The diff confirms the removal of a logging statement (`LOG.info(""popupMenuCanceled"");`) from the `popupMenuCanceled` method. This aligns with the commit message and suggests the removal of unintended debugging code, which can be considered a bug fix in the sense of removing unwanted behavior. The answer is **Buggy**.
"
argouml,14041.json,280cf1fa18a226a8dbf707f91b08ae68097be66c,"@@ -1,4 +1,3 @@
     public Object getCollaborationInstanceSet() {
-        // TODO: Need UML 2.x equivalent
-        return null /*CollaborationInstanceSet.class*/;
+        throw new NotYetImplementedException();
     }",NotBuggy,"Define more model element types that are used by activity diagrams

git-svn-id: http://argouml.tigris.org/svn/argouml/trunk@18525 a161b567-7d1e-0410-9ef9-912c70fedb3f
",NotBuggy,"The commit message indicates the definition of more model element types for activity diagrams. The code change replaces a `return null` with a `throw new NotYetImplementedException()`. This suggests that the original code was a placeholder or incomplete implementation. The change indicates that the functionality was not fully implemented and is now explicitly marked as such. This is not necessarily a bug fix, but rather an indication of incomplete functionality. The answer is **NotBuggy**."
argouml,2590.json,e7865aa82550c72061b5ffc4c566f5b716ae8299,"@@ -1,25 +1,40 @@
     public void buildPanel() {
 	// ///////////////////////////////////////
 	// Build the field
 	// ///////////////////////////////////////
 
-	checkbox = new JCheckBox();
-	// TODO ? find a Tool tips, add a label
-	// checkbox.setToolTipText(Translator.localize(""label.body.tooltip""));
-	checkbox.setFont(LookAndFeelMgr.getInstance().getStandardFont());
+	trueButton = new JRadioButton(Translator.localize(""misc.boolean.true""));
+	falseButton = new JRadioButton(Translator
+		.localize(""misc.boolean.false""));
+
+	trueFalseGroup = new ButtonGroup();
+	trueFalseGroup.add(trueButton);
+	trueFalseGroup.add(falseButton);
+
+	trueButton.setFont(LookAndFeelMgr.getInstance().getStandardFont());
+	falseButton.setFont(LookAndFeelMgr.getInstance().getStandardFont());
 
 	/**
 	 * on change : Change the value in the model
 	 */
-	checkbox.addActionListener(new ActionListener() {
+	trueButton.addActionListener(new ActionListener() {
+	    public void actionPerformed(ActionEvent e) {
+		updateModel();
+	    }
+	});
+
+	falseButton.addActionListener(new ActionListener() {
 	    public void actionPerformed(ActionEvent e) {
 		updateModel();
 	    }
 	});
 
 	// ///////////////////////////////////////
 	// Add field(s) to panel
 	// ///////////////////////////////////////
-	this.allField = new JScrollPane(checkbox);
+	JPanel panel = new JPanel();
+	panel.add(trueButton);
+	panel.add(falseButton);
+	this.allField = new JScrollPane(panel);
 
     }",NotBuggy,"Allow different types of initial value to be specified

git-svn-id: http://argouml.tigris.org/svn/argouml/trunk@19429 a161b567-7d1e-0410-9ef9-912c70fedb3f
",NotBuggy,"The commit message ""Allow different types of initial value to be specified"" suggests a feature enhancement rather than a bug fix. The diff replaces a JCheckBox with a JRadioButton group (trueButton and falseButton), allowing the user to explicitly select either true or false as the initial value. This change introduces new UI elements and functionality, aligning with the commit message's intention to allow different types of initial values. Therefore, the changes do not indicate a bug fix. The answer is **NotBuggy**."
argouml,10209.json,604299fb46334e559a0cbc4aaf66e7471c0e693f,"@@ -1,12 +1,10 @@
     private void initFigs() {
         setEditable(false);
-        setBigPort(new FigRect(X, Y, STATE_WIDTH, HEIGHT, DEBUG_COLOR,
-                DEBUG_COLOR));
         head = new FigRect(X, Y, STATE_WIDTH, HEIGHT, LINE_COLOR,
                 SOLID_FILL_COLOR);
         // add Figs to the FigNode in back-to-front order
         addFig(getBigPort());
         addFig(head);
 
         setBlinkPorts(false); //make port invisible unless mouse enters
     }",NotBuggy,"Make code consistent. 
Remove the use of deprecated method setBigPort().

git-svn-id: http://argouml.tigris.org/svn/argouml/trunk@18728 a161b567-7d1e-0410-9ef9-912c70fedb3f
",NotBuggy,"The commit message indicates a change to make the code consistent and remove the use of a deprecated method `setBigPort()`. The diff shows the removal of the line `setBigPort(new FigRect(X, Y, STATE_WIDTH, HEIGHT, DEBUG_COLOR, DEBUG_COLOR));`. This aligns with the commit message's intention to remove the deprecated method. There is no indication of a bug fix. The answer is **NotBuggy**."
docx4j,1091.json,e29924b33ec0c0298ba4fc3f7a8c218c8e6cfa0c,"@@ -1,26 +1,16 @@
     public static void main(String args[])
         throws IOException
     {
         if (args.length != 2)
         {
             System.err.println(
                 ""two arguments required: input filename and output filename"");
             System.exit(1);
         }
         FileInputStream  istream = new FileInputStream(args[ 0 ]);
-        try {
-            FileOutputStream ostream = new FileOutputStream(args[ 1 ]);
-            try {
-                NPOIFSFileSystem fs = new NPOIFSFileSystem(istream);
-                try {
-                    fs.writeFilesystem(ostream);
-                } finally {
-                    fs.close();
-                }
-            } finally {
-                ostream.close();
-            }
-        } finally {
-            istream.close();
-        }
+        FileOutputStream ostream = new FileOutputStream(args[ 1 ]);
+
+        new OPOIFSFileSystem(istream).writeFilesystem(ostream);
+        istream.close();
+        ostream.close();
     }",NotBuggy,"repackaged poifs; password encryption (load/save) now works
",Buggy,"The commit message indicates that password encryption for loading and saving now works. The code diff simplifies the main method by removing nested try-finally blocks and directly using `OPOIFSFileSystem`. The original code used `NPOIFSFileSystem`, which may not have supported password encryption correctly, leading to a bug. The change to `OPOIFSFileSystem` suggests a fix related to password encryption functionality. The answer is **Buggy**."
docx4j,6115.json,6fc95a3b720163091946b0c51ad6200c6d780fb4,"@@ -1,7 +1,7 @@
 	public boolean shouldTraverse(Object o) {
 		
-		System.out.println(o.getClass().getName() );
+//		System.out.println(o.getClass().getName() );
 		
 		return !(o instanceof P
 				|| o instanceof org.docx4j.wml.Tbl); 
 	}",NotBuggy,"trivial
",NotBuggy,"The commit message ""trivial"" is vague and uninformative. The code change consists of commenting out a `System.out.println` statement. This suggests the removal of debugging code, which doesn't inherently indicate a bug fix. It's more likely a cleanup or minor adjustment. The answer is **NotBuggy**."
docx4j,25366.json,a440fc994771f87a66c1186134e990c646661e13,"@@ -1,3 +1,3 @@
-    public JAXBElement<P.Dir> createPDir(P.Dir value) {
-        return new JAXBElement<P.Dir>(_PDir_QNAME, P.Dir.class, P.class, value);
+    public JAXBElement<XMLGregorianCalendar> createDate(XMLGregorianCalendar value) {
+        return new JAXBElement<XMLGregorianCalendar>(_Date_QNAME, XMLGregorianCalendar.class, null, value);
     }",NotBuggy,"Move to maven multi-module project (Work in progress)
",NotBuggy,"The commit message indicates a structural change to the project (moving to a multi-module Maven project). The diff shows a change in the type of JAXBElement being created, from `P.Dir` to `XMLGregorianCalendar`. This change doesn't seem directly related to bug fixing but rather to adapting the code to the new project structure or data model. The change in the JAXBElement type suggests a modification in how data is handled, but without more context, it's difficult to determine if it's a bug fix or a feature change. Given the commit message and the nature of the code change, it's more likely related to the project restructuring. The answer is **NotBuggy**."
docx4j,20658.json,6e4c47a7dae3ec85edb61ade5dd7b49030cc641d,"@@ -1,3 +1,3 @@
-    public CTWrapSquare createCTWrapSquare() {
-        return new CTWrapSquare();
+    public CTPageSetup createCTPageSetup() {
+        return new CTPageSetup();
     }",NotBuggy,"More complete DML, generated from TC45 1.0 final, using dml__ROOT.xsd
",NotBuggy,"The commit message indicates an update to DML (DrawingML) based on a new schema. The code change replaces the creation of a `CTWrapSquare` object with the creation of a `CTPageSetup` object. This suggests a change in the structure or elements of the DML being generated, likely to align with the updated schema. This doesn't look like a bug fix, but rather a schema update. The answer is **NotBuggy**."
docx4j,14816.json,5eb0d359e63a65813137778bde2398fee5fa64af,"@@ -1,28 +1,26 @@
   public static String toShortString(DiffXEvent e) {
     // an element to open
-    if (e instanceof OpenElementEvent) {
+    if (e instanceof OpenElementEvent)
       return '<'+((OpenElementEvent)e).getName()+'>';
     // an element to close
-    } else if (e instanceof CloseElementEvent) {
+    else if (e instanceof CloseElementEvent)
       return ""</""+((CloseElementEvent)e).getName()+'>';
     // an attribute
-    } else if (e instanceof AttributeEvent) {
+    else if (e instanceof AttributeEvent)
       return ""@""+((AttributeEvent)e).getName();
     // a word
-    } else if (e instanceof WordEvent) {
+    else if (e instanceof WordEvent)
       return '""'+((CharactersEventBase)e).getCharacters()+'""';
     // a white space event
-    } else if (e instanceof SpaceEvent) {
+    else if (e instanceof SpaceEvent)
       return ""_s_"";
     // a single character
-    } else if (e instanceof CharEvent) {
+    else if (e instanceof CharEvent)
       return '\''+((CharactersEventBase)e).getCharacters()+'\'';
     // an ignorable space event
-    } else if (e instanceof IgnorableSpaceEvent) {
+    else if (e instanceof IgnorableSpaceEvent)
       return ""_i_"";
     // a single line
-    } else if (e instanceof LineEvent) {
-      return ""L#""+((LineEvent)e).getLineNumber();
-    }
+    else if (e instanceof LineEvent) return ""L#""+((LineEvent)e).getLineNumber();
     return ""???"";
   }",NotBuggy,"update to diffx-0.7.4
",NotBuggy,"The commit message indicates an update to a library (diffx). The code changes involve simplifying the `if-else` structure in the `toShortString` method. The functionality remains the same, but the code is more concise. There's no indication of a bug fix in either the commit message or the code changes.
The answer is **NotBuggy**."
docx4j,18855.json,beb51743e46de6c74bf4a56faa3fa1a4322fea8a,"@@ -1,3 +1,3 @@
-    public Theme.ExtLst createThemeExtLst() {
-        return new Theme.ExtLst();
+    public CTTextBody createCTTextBody() {
+        return new CTTextBody();
     }",NotBuggy,"Map the non-fonts bit to xsd:any, doesn't marshall namespaces properly with JAXB in Java 6, so model the non-font bits properly.
",Buggy,"The commit message indicates a fix related to how namespaces are handled with JAXB in Java 6, and the code change replaces `Theme.ExtLst` with `CTTextBody`. This suggests a modification to the data model to correctly represent ""non-font bits,"" which aligns with the commit message's intent to ""model the non-font bits properly."" While it's not immediately clear if this is a bug fix in the traditional sense, the change addresses an issue with namespace marshalling and data representation, which can be considered a bug fix in the context of data handling and serialization. The answer is **Buggy**."
docx4j,25367.json,a440fc994771f87a66c1186134e990c646661e13,"@@ -1,3 +1,3 @@
-    public JAXBElement<Drawing> createRDrawing(Drawing value) {
-        return new JAXBElement<Drawing>(_RDrawing_QNAME, Drawing.class, R.class, value);
+    public JAXBElement<Variant> createVariant(Variant value) {
+        return new JAXBElement<Variant>(_Variant_QNAME, Variant.class, null, value);
     }",NotBuggy,"Move to maven multi-module project (Work in progress)
",NotBuggy,"The commit message indicates a structural change to the project (moving to a multi-module Maven project). The diff shows a change in the type of JAXBElement being created, from `Drawing` to `Variant`. The change also involves the QNAME and the class associated with the JAXBElement. Since the commit message describes a project restructuring, and the code change reflects modifications related to JAXB element creation, it's unlikely this is a bug fix. The changes seem to be related to adapting the code to the new project structure. The answer is **NotBuggy**.
"
docx4j,6963.json,c9021fdc912cccadd839aea24b132ca922147258,"@@ -1,40 +1,40 @@
-	private PhysicalFont getAssociatedPhysicalFont(String documentFontName, String orignalKey, org.foray.font.format.Panose soughtPanose) {
+	private PhysicalFont getAssociatedPhysicalFont(String documentFontName, String orignalKey, org.docx4j.fonts.foray.font.format.Panose soughtPanose) {
 
 		log.debug(""Looking for "" + soughtPanose);
 		
 		String resultingPanoseKey;
 		
 //		// First try panose space restricted to this font family
 //		2009 03 22 - we don't have physicalFontFamiliesMap any more		
 //		if (orignalKey!=null) {
 //			PhysicalFontFamily thisFamily = 
 //				physicalFontFamiliesMap.get( PhysicalFonts.getPhysicalFonts().get(orignalKey).getName() );					
 //			
 //			log.debug(""Searching within family:"" + thisFamily.getFamilyName() );
 //			
 //			resultingPanoseKey = findClosestPanoseMatch(documentFontName, soughtPanose, 
 //					thisFamily.getPhysicalFonts(), MATCH_THRESHOLD_INTRA_FAMILY);    
 //			if ( resultingPanoseKey!=null ) {
 //				log.info(""--> "" + PhysicalFonts.getPhysicalFonts().get(resultingPanoseKey).getEmbeddedFile() );
 //	        	fm.setPhysicalFont( PhysicalFonts.getPhysicalFonts().get(resultingPanoseKey) );													
 //				return fm;
 //			}  else {
 //				log.warn(""No match in immediate font family"");
 //			}
 //		} else {
 //			log.debug(""originalKey was null."");
 //		}
 		
 		// Well, that failed, so search the whole space
 		
 		//fm.setDocumentFont(documentFontName); ???
 		resultingPanoseKey = findClosestPanoseMatch(documentFontName, soughtPanose, PhysicalFonts.getPhysicalFonts(),
 				MATCH_THRESHOLD); 
 		if ( resultingPanoseKey!=null ) {
 			log.info(""--> "" + PhysicalFonts.getPhysicalFonts().get(resultingPanoseKey).getEmbeddedFile() );
         	return PhysicalFonts.getPhysicalFonts().get(resultingPanoseKey);													
 		}  else {
 			log.warn(""No match in panose space"");
 			return null;
 		}
 	}",NotBuggy,"Include source code for fop-fonts, as org.docx4j.fonts.fop.
Use FOP 1.0.
Move panose to org.docx4j.fonts.foray
Add LICENSE, NOTICE
",NotBuggy,"The commit message indicates a reorganization of source code, the inclusion of a new library (fop-fonts), an upgrade of FOP to version 1.0, and the relocation of the panose class. The diff shows a change in the import statement for the Panose class, moving it from `org.foray.font.format.Panose` to `org.docx4j.fonts.foray.font.format.Panose`. This aligns with the commit message's description of moving panose. There is no indication of a bug fix in either the commit message or the code diff. The changes are related to code organization and dependency updates. The answer is **NotBuggy**."
docx4j,25396.json,a440fc994771f87a66c1186134e990c646661e13,"@@ -1,3 +1,3 @@
-    public JAXBElement<P.Dir> createPDir(P.Dir value) {
-        return new JAXBElement<P.Dir>(_PDir_QNAME, P.Dir.class, P.class, value);
+    public JAXBElement<BigInteger> createUi8(BigInteger value) {
+        return new JAXBElement<BigInteger>(_Ui8_QNAME, BigInteger.class, null, value);
     }",NotBuggy,"Move to maven multi-module project (Work in progress)
",NotBuggy,"The commit message indicates a move to a Maven multi-module project, which is a structural change. The diff shows a change in the type of JAXBElement being created, from `P.Dir` to `BigInteger`. This change doesn't obviously indicate a bug fix, but rather a modification of the data model or schema being used. It's likely related to the project restructuring. The answer is **NotBuggy**."
docx4j,14694.json,5eb0d359e63a65813137778bde2398fee5fa64af,"@@ -1,6 +1,7 @@
   public void delete(DiffXEvent e) {
-    if (e instanceof OpenElementEvent)
+    if (e instanceof OpenElementEvent) {
       push((OpenElementEvent)e, '-');
-    else if (e instanceof CloseElementEvent)
+    } else if (e instanceof CloseElementEvent) {
       pop();
+    }
   }",NotBuggy,"update to diffx-0.7.4
",NotBuggy,The commit message indicates an update to a dependency (diffx-0.7.4). The code changes involve adding braces to `if` and `else if` blocks in the `delete` method. This change improves code readability and maintainability but doesn't inherently fix a bug. It's a refactoring change. The answer is **NotBuggy**.
docx4j,27318.json,fcdd0ece2e6c040ce5b870108f0e83bef14bce58,"@@ -1,7 +1,3 @@
     public String getDashstyle() {
-        if (dashstyle == null) {
-            return ""solid"";
-        } else {
-            return dashstyle;
-        }
+        return dashstyle;
     }",NotBuggy,"VML generated classes, based on ECMA 376 1ed (rather than earlier draft).  
Includes @XmlType namespace annotations from revs 940 and 1129.
",NotBuggy,"The commit message indicates a change related to VML generated classes and updates based on a specific ECMA standard. The diff removes a conditional statement that provided a default value for the `dashstyle` property when it was null. Now, if `dashstyle` is null, the getter will return null. This change could potentially expose null pointer exceptions if the calling code expects a non-null value. However, without more context, it's difficult to determine if this is a bug fix or a change in behavior. The commit message doesn't explicitly mention a bug fix. Therefore, the answer is **NotBuggy**."
docx4j,22538.json,6e4c47a7dae3ec85edb61ade5dd7b49030cc641d,"@@ -1,3 +1,3 @@
-    public void setExt(CTOfficeArtExtension value) {
-        this.ext = value;
+    public void setExtLst(CTOfficeArtExtensionList value) {
+        this.extLst = value;
     }",NotBuggy,"More complete DML, generated from TC45 1.0 final, using dml__ROOT.xsd
",Buggy,"The commit message indicates a more complete Data Manipulation Language (DML) implementation, generated from a specific schema. The diff shows a change from `setExt(CTOfficeArtExtension value)` to `setExtLst(CTOfficeArtExtensionList value)`. This suggests a change in the data model, where a single extension (`Ext`) is replaced by a list of extensions (`ExtLst`). This could be due to a bug or limitation in the previous model where only one extension was supported. The change aligns with the commit message's intention to provide a more complete DML implementation, which likely involves correcting or enhancing the data model. The answer is **Buggy**.
"
docx4j,8715.json,a440fc994771f87a66c1186134e990c646661e13,"@@ -1,3 +1,3 @@
-    public R createR() {
-        return new R();
+    public CTDefinedName createCTDefinedName() {
+        return new CTDefinedName();
     }",NotBuggy,"Move to maven multi-module project (Work in progress)
",NotBuggy,"The commit message indicates a restructuring of the project into a multi-module Maven project, which is a build system and project management tool. The code diff shows a change in a method name from `createR()` to `createCTDefinedName()` and a corresponding change in the return type from `R` to `CTDefinedName`. This change suggests a renaming or refactoring of a class or interface, likely as part of the module reorganization. This doesn't look like a bug fix. The answer is **NotBuggy**.
"
docx4j,31212.json,ed8d15d352d0a2eaeac306e4ba9a3c83a3547558,"@@ -1,3 +1,3 @@
-        public BooleanDefaultTrue getSemiHidden() {
-            return semiHidden;
-        }

+    public BooleanDefaultTrue getSemiHidden() {
+        return semiHidden;
+    }",NotBuggy,"Change CT_FldChar to FldChar;
Style is freestanding, rather than an inner class of Styles.
(Other changes largely whitespace in license header)
",NotBuggy,"The commit message indicates a change in class naming and style adjustments, specifically moving `CT_FldChar` to `FldChar` and refactoring its location. The provided diff shows a minor change in indentation within the `getSemiHidden()` method. This change does not appear to be related to a bug fix, but rather a style or refactoring change. The answer is **NotBuggy**.
"
docx4j,1661.json,e29924b33ec0c0298ba4fc3f7a8c218c8e6cfa0c,"@@ -1,22 +1,48 @@
-   public static void main(final String args[]) throws IOException {
-      if (args.length == 0) {
-         System.err.println(""Must specify at least one file to view"");
-         System.exit(1);
-      }
-
-      boolean withSizes = false;
-      boolean newPOIFS = true;
-      for (int j = 0; j < args.length; j++) {
-         if (args[j].equalsIgnoreCase(""-size"") || args[j].equalsIgnoreCase(""-sizes"")) {
-            withSizes = true;
-         } else if (args[j].equalsIgnoreCase(""-old"") || args[j].equalsIgnoreCase(""-old-poifs"")) {
-            newPOIFS = false;
-         } else {
-            if(newPOIFS) {
-               viewFile(args[j], withSizes);
-            } else {
-               viewFileOld(args[j], withSizes);
+    public static void main(String[] args) throws Exception {
+        if (args.length == 0) {
+            System.err.println(""Must specify at least one file to dump"");
+            System.exit(1);
+        }
+        
+        boolean dumpProps = false, dumpMini = false;
+        for (int i = 0; i < args.length; i++) {
+            if (args[i].equalsIgnoreCase(""-dumprops"") ||
+                args[i].equalsIgnoreCase(""-dump-props"") ||
+                args[i].equalsIgnoreCase(""-dump-properties"")) {
+                dumpProps = true;
+                continue;
             }
-         }
-      }
+            if (args[i].equalsIgnoreCase(""-dumpmini"") ||
+                args[i].equalsIgnoreCase(""-dump-mini"") ||
+                args[i].equalsIgnoreCase(""-dump-ministream"") ||
+                args[i].equalsIgnoreCase(""-dump-mini-stream"")) {
+                dumpMini = true;
+                continue;
+            }
+            
+            System.out.println(""Dumping "" + args[i]);
+            FileInputStream is = new FileInputStream(args[i]);
+            NPOIFSFileSystem fs = new NPOIFSFileSystem(is);
+            is.close();
+
+            DirectoryEntry root = fs.getRoot();
+            File file = new File(root.getName());
+            file.mkdir();
+
+            dump(root, file);
+            
+            if (dumpProps) {
+                HeaderBlock header = getHeaderBlock(fs);
+                dump(fs, header.getPropertyStart(), ""properties"", file);
+            }
+            if (dumpMini) {
+                NPropertyTable props = getPropertyTable(fs);
+                int startBlock = props.getRoot().getStartBlock(); 
+                if (startBlock == POIFSConstants.END_OF_CHAIN) {
+                    System.err.println(""No Mini Stream in file"");
+                } else {
+                    dump(fs, startBlock, ""mini-stream"", file);
+                }
+            }
+        }
    }",NotBuggy,"repackaged poifs; password encryption (load/save) now works
",Buggy,"The commit message indicates a repackaging of poifs and the enablement of password encryption for loading and saving files. The provided diff shows a complete rewrite of the `main` method, changing its functionality from viewing files to dumping file contents, properties, and mini streams based on command-line arguments. This suggests a significant change in functionality, potentially addressing issues or limitations in the previous implementation. The changes are more than just a simple refactor; they introduce new features and modify existing ones. Therefore, the changes indicate a bug fix or enhancement related to the original functionality. The answer is **Buggy**."
atmosphere,414.json,843374c2f23f04dfeca68025d80a9fc5c89846b5,"@@ -1,15 +1,15 @@
     public final static AtmosphereRequest wrap(HttpServletRequest request) {
         // Do not rewrap.
         if (AtmosphereRequestImpl.class.isAssignableFrom(request.getClass())) {
-            return AtmosphereRequestImpl.class.cast(request);
+            return (AtmosphereRequestImpl) request;
         }
 
         Builder b = new Builder();
         Enumeration<String> e = request.getAttributeNames();
         String s;
         while (e.hasMoreElements()) {
             s = e.nextElement();
             b.localAttributes.put(s, attributeWithoutException(request, s));
         }
         return b.request(request).build();
     }",NotBuggy,"Update code to JDK8
",NotBuggy,The commit message indicates an update to JDK8. The code changes involve casting using `(AtmosphereRequestImpl) request` instead of `AtmosphereRequestImpl.class.cast(request)`. This change is related to adapting the code to JDK8's syntax and features. There's no indication of a bug fix in the traditional sense. The change seems to be more about compatibility and leveraging newer language features. The answer is **NotBuggy**.
atmosphere,1507.json,add336c1cc60c8189b00d21dd1a7b3408bc4754d,"@@ -1,6 +1,6 @@
     public void reconfigureInitParams(boolean reconfigureInitParams) {
         if (reconfigureInitParams) {
-            doInitParams(servletConfig);
+            doInitParams(servletConfig, reconfigureInitParams);
             doInitParamsForWebSocket(servletConfig);
         }
     }",NotBuggy,"Do not allow reloading the AsyncSupport from init-param
",Buggy,"The commit message indicates a restriction on reloading AsyncSupport from init-param. The code change modifies the `reconfigureInitParams` method to pass a boolean flag `reconfigureInitParams` to the `doInitParams` method. This suggests that the `doInitParams` method now takes into account whether it should reload the AsyncSupport based on this flag. This change is likely related to preventing unintended or incorrect behavior when reloading AsyncSupport, which can be considered a bug fix or a preventative measure against potential bugs. The answer is **Buggy**.
"
atmosphere,1267.json,5241afdf818b484412c44d27d97a2c7e4dacbb71,"@@ -1,11 +1,11 @@
     private static void fallbackToManualAnnotatedClasses(Class<?> mainClass, AtmosphereFramework f, AnnotationHandler handler) {
         logger.warn(""Unable to detect annotations. Application may fail to deploy."");
         f.annotationScanned(true);
-        for (Class a : coreAnnotations) {
+        for (Class<?> a : coreAnnotations) {
             try {
                 handler.handleProcessor(loadClass(mainClass, a.getName()));
             } catch (Exception e) {
                 logger.trace("""", e);
             }
         }
     }",NotBuggy,"Refactor
",NotBuggy,The commit message indicates a refactoring. The code changes involve using the generic type `Class<?>` instead of the raw type `Class` in the enhanced for loop. This change improves type safety and eliminates the need for casting when accessing elements within the loop. The functionality of the code remains the same. The answer is **NotBuggy**.
atmosphere,2813.json,a04f6f742d7aca4e198aca78b48f75eb4ea4bc3a,"@@ -1,9 +1,9 @@
         public void onMessage(WebSocket socket, String text) {
             super.onMessage(socket, text);
             LOGGER.trace(""onMessage(String) {} "", socket);
-            DefaultWebSocket g2w = DefaultWebSocket.class.cast(socket);
+            DefaultWebSocket g2w = (DefaultWebSocket) socket;
             org.atmosphere.websocket.WebSocket webSocket = (org.atmosphere.websocket.WebSocket) g2w.getUpgradeRequest().getAttribute(""grizzly.webSocket"");
             if (webSocket != null) {
                 webSocketProcessor.invokeWebSocketProtocol(webSocket, text);
             }
         }",NotBuggy,"Update code to JDK8
",NotBuggy,"The commit message indicates an update to JDK8. The code change involves casting a `WebSocket` object to `DefaultWebSocket`. The original code used `DefaultWebSocket.class.cast(socket)`, while the updated code uses `(DefaultWebSocket) socket`. This change reflects the updated syntax and practices encouraged by JDK8. The functionality remains the same. The answer is **NotBuggy**."
atmosphere,588.json,b5b88b095c89dbb9914fd9ec8e0d999e2007fc55,"@@ -1,3 +1,7 @@
     public static AtmosphereResourceFactory resourceFactory() {
+        if (resourceFactoryDuplicate) {
+            throw new IllegalStateException(
+                    ""More than one instance has been stored. Universe cannot be used."");
+        }
         return resourceFactory;
     }",NotBuggy,"Throw exception if Universe methods are used when they are not reliable

The previous version causes a lot of logging about Universe not being reliable
even though you never use Universe. This changes the behavior so that you will
instead get an exception only when actually using the methods, when they will not
work properly.
",Buggy,"The commit message indicates a change in behavior related to the use of ""Universe"" methods. Instead of logging when ""Universe"" is not reliable, the code now throws an `IllegalStateException` when these methods are used improperly. The diff shows that an `IllegalStateException` is thrown if `resourceFactoryDuplicate` is true when `resourceFactory()` is called. This aligns with the commit message's intention to throw an exception when the methods are used in an unreliable state. The change addresses a situation where the system was logging errors even when the problematic functionality wasn't being used, and now throws an exception only when the problematic methods are actually called, indicating a bug fix. The answer is **Buggy**."
atmosphere,958.json,e1a548d5c483168f07249763b2b5e1971bfeaf1e,"@@ -1,5 +1,5 @@
-    protected void endRequest(AtmosphereResourceImpl r, boolean cancel) {
+    public void endRequest(AtmosphereResourceImpl r, boolean cancel) {
         if (completeLifecycle(r, cancel)) {
             config.framework().notify(Action.TYPE.CANCELLED, r.getRequest(false), r.getResponse(false));
         }
     }",NotBuggy,"Improve Logging, make method public
",NotBuggy,"The commit message indicates improvements to logging and a change in method visibility from protected to public. The diff shows a change in the `endRequest` method's access modifier from `protected` to `public`. This change in visibility doesn't inherently indicate a bug fix. It's more likely a feature enhancement or API adjustment. The answer is **NotBuggy**.
"
atmosphere,2102.json,6603c01d44e431a43c5ae468feb76b09243d72d5,"@@ -1,35 +1,36 @@
     protected void retryInjection(AtmosphereFramework framework){
+        int maxTryPerCycle = maxTry;
         // Give another chance to injection in case we failed at first place. We may still fail if there is a strong
         // dependency between Injectable, e.g one depend on other, or if the Injectable is not defined at the right place
         // in META-INF/services/org/atmosphere/inject.Injectable
         Set<Field> fields = new HashSet<Field>();
         Object instance = null;
         final LinkedHashSet<Object> postponedMethodExecution = new LinkedHashSet<>(pushBackInjection);
-        while (!pushBackInjection.isEmpty() & maxTry-- > 0) {
+        while (!pushBackInjection.isEmpty() & maxTryPerCycle-- > 0) {
             Iterator<Object> t = new LinkedList(pushBackInjection).iterator();
             pushBackInjection.clear();
             while (t.hasNext()) {
                 instance = t.next();
                 fields.addAll(getInheritedPrivateFields(instance.getClass()));
                 try {
                     injectFields(fields, instance, framework, injectables);
                 } catch (IllegalAccessException e) {
                     logger.warn("""", e);
                 } finally {
                     fields.clear();
                 }
             }
         }
 
         if (!pushBackInjection.isEmpty()) {
             injectionFailed();
         }
 
         for (Object o : postponedMethodExecution) {
             try {
                 applyMethods(o, (Class<Object>) o.getClass());
             } catch (IllegalAccessException e) {
                 logger.warn("""", e);
             }
         }
     }",NotBuggy,"Re-try Injectable Injection: reset counter
",Buggy,"The commit message indicates a retry mechanism for injectable injection, specifically resetting a counter. The diff shows that the `maxTry` counter is now copied to `maxTryPerCycle` before the retry loop. The loop condition now uses `maxTryPerCycle` instead of the original `maxTry`. This ensures that the original `maxTry` value is preserved, and each retry cycle uses the initial maximum number of attempts. This change addresses a potential issue where the `maxTry` counter was not being reset properly, leading to fewer retry attempts than intended. The answer is **Buggy**."
atmosphere,3067.json,8c46e966a9a873af2cf4d6b4f7577d7975d62d40,"@@ -1,3 +1,3 @@
     public String getContainerName() {
-        return super.getContainerName() + "" using javax.servlet/3.0 and jsr356/WebSocket API"";
+        return super.getContainerName() + "" and jsr356/WebSocket API"";
     }",NotBuggy,"Cosmetic
",NotBuggy,"The commit message ""Cosmetic"" suggests a change that is primarily for appearance or style, without altering the underlying functionality. The diff removes "" using javax.servlet/3.0"", which seems to be a minor textual adjustment. This change doesn't appear to fix any bug or introduce new functionality. The answer is **NotBuggy**.
"
atmosphere,2596.json,8c2994c415cab838ba6a4571eecb8934cf81c34b,"@@ -1,2 +1,2 @@
-                public void shutdown() {
-                }

+    public void shutdown() {
+    }",NotBuggy,"Refactor, no functional change
",NotBuggy,The commit message indicates a refactoring with no functional change. The diff shows a minor change in indentation. This change doesn't fix any bug. The answer is **NotBuggy**.
atmosphere,415.json,b9625cac7eaae32687fa4f10939f9d6cbb9749e5,"@@ -1,63 +1,63 @@
     public final static AtmosphereRequest cloneRequest(HttpServletRequest request, boolean loadInMemory, boolean copySession, boolean isDestroyable, boolean createSession) {
         Builder b;
         HttpServletRequest r;
 
         Cookie[] cs = request.getCookies();
         Set<Cookie> hs = Collections.synchronizedSet(new HashSet());
         if (cs != null) {
             for (Cookie c : cs) {
                 hs.add(c);
             }
         }
 
         boolean isWrapped = false;
         if (AtmosphereRequestImpl.class.isAssignableFrom(request.getClass())) {
             b = AtmosphereRequestImpl.class.cast(request).b;
             isWrapped = true;
         } else {
             b = new Builder();
             b.request(request);
         }
 
-        HttpSession session = null;
+        HttpSession session = request.getSession(false);
         if (copySession) {
             session = request.getSession(createSession);
             if (session != null) {
                 session = new FakeHttpSession(session);
             } else {
                 session = new FakeHttpSession("""", null, System.currentTimeMillis(), -1);
             }
         }
 
         b.servletPath(request.getServletPath())
                 .pathInfo(request.getPathInfo())
                 .contextPath(request.getContextPath())
                 .requestURI(request.getRequestURI())
                 .requestURL(request.getRequestURL().toString())
                 .method(request.getMethod())
                 .serverName(request.getServerName())
                 .serverPort(request.getServerPort())
                 .remoteAddr(request.getRemoteAddr())
                 .remoteHost(request.getRemoteHost())
                 .remotePort(request.getRemotePort())
                 .destroyable(isDestroyable)
                 .cookies(hs)
                 .session(session)
                 .principal(request.getUserPrincipal())
                 .authType(request.getAuthType())
                 .isSSecure(request.isSecure());
 
         if (loadInMemory) {
             String s = (String) attributeWithoutException(request, FrameworkConfig.THROW_EXCEPTION_ON_CLONED_REQUEST);
             boolean throwException = s != null && Boolean.parseBoolean(s);
             r = new NoOpsRequest(throwException);
             if (isWrapped) {
                 load(b.request, b);
             } else {
                 load(request, b);
             }
             b.request(r);
         }
 
         return isWrapped ? AtmosphereRequestImpl.class.cast(request) : b.build();
     }",NotBuggy," #2157 use the original session by default when cloning a request
",Buggy,"The commit message indicates that the original session should be used by default when cloning a request. The code diff shows that the session is now retrieved using `request.getSession(false)` initially, which retrieves the session if it exists without creating a new one. The subsequent `if (copySession)` block then handles the case where a new session needs to be created or copied. This change ensures that the original session is used by default unless `copySession` is true. This looks like a fix to ensure that the original session is used when cloning a request. The answer is **Buggy**."
atmosphere,2073.json,894a09e40995ab99985763f1cdd1e05fed0fd18b,"@@ -1,3 +1,3 @@
-    public boolean supportedType(Type t) {
-        return false;
+    public Object injectable(AtmosphereResource resource) {
+        return null;
     }",NotBuggy,"Refactor Injection
",NotBuggy,"The commit message indicates a refactoring. The code changes the method `supportedType` to `injectable`. This is a significant change in the method signature and likely its purpose. However, without more context, it's difficult to determine if this change is related to fixing a bug. Since the commit message explicitly states ""Refactor,"" it's less likely to be a bug fix. The answer is **NotBuggy**."
atmosphere,11.json,1894ff3002e438f057a2e01b781ba5b538d6ca2b,"@@ -1,10 +1,9 @@
     public Object broadcast() {
         try {
-            return (Object) b.broadcast(message).get();
-        } catch (InterruptedException ex) {
-            LoggerUtils.getLogger().log(Level.SEVERE, null, ex);
-        } catch (ExecutionException ex) {
-            LoggerUtils.getLogger().log(Level.SEVERE, null, ex);
+            return b.broadcast(message).get();
+        }
+        catch (Exception ex) {
+            logger.error(""failed to broadcast message: "" + message, ex);
         }
         return null;
     }",NotBuggy,"migrated from jul logging to SLF4J API.  converted most usages of Exception.printStackTrace() and calls to System.out and System.err to logging calls.  tests now run with logback as the logging implementation and write output to stdout and log file, adjust logback-test.xml in test resources directory as appropriate.
",Buggy,"The commit message indicates a migration to SLF4J for logging and the replacement of `Exception.printStackTrace()` and `System.out/err` calls with logging calls. The diff shows a change from using `LoggerUtils.getLogger().log(Level.SEVERE, null, ex)` to `logger.error(""failed to broadcast message: "" + message, ex)`. Also, the catch block is simplified to catch `Exception` instead of `InterruptedException` and `ExecutionException` separately. This change aligns with the commit message's description of converting exception handling to use the SLF4J logging API. The original code used `printStackTrace()` which is not ideal for production environments. The new code uses a logger, which is better practice. The change also simplifies the exception handling, which could be seen as a bug fix if the original code was not handling all possible exceptions correctly. The answer is **Buggy**."
atmosphere,3036.json,a04f6f742d7aca4e198aca78b48f75eb4ea4bc3a,"@@ -1,9 +1,9 @@
         public void onMessage(WebSocket socket, String text) {
             super.onMessage(socket, text);
             LOGGER.trace(""onMessage(String) {} "", socket);
-            DefaultWebSocket g2w = DefaultWebSocket.class.cast(socket);
+            DefaultWebSocket g2w = (DefaultWebSocket) socket;
             org.atmosphere.websocket.WebSocket webSocket = (org.atmosphere.websocket.WebSocket) g2w.getUpgradeRequest().getAttribute(""grizzly.webSocket"");
             if (webSocket != null) {
                 webSocketProcessor.invokeWebSocketProtocol(webSocket, text);
             }
         }",NotBuggy,"Update code to JDK8
",NotBuggy,The commit message indicates an update to JDK8. The code change involves casting `socket` to `DefaultWebSocket` using `(DefaultWebSocket) socket` instead of `DefaultWebSocket.class.cast(socket)`. This is a change in syntax to align with JDK8 conventions. This change does not indicate a bug fix. The answer is **NotBuggy**.
atmosphere,2900.json,a04f6f742d7aca4e198aca78b48f75eb4ea4bc3a,"@@ -1,5 +1,5 @@
-    public AsyncSupport complete(AtmosphereResourceImpl r) {
+    public AsyncSupport<AtmosphereResourceImpl> complete(AtmosphereResourceImpl r) {
         CometContext ctx = CometEngine.getEngine().getCometContext(atmosphereCtx);
         resume(r.getRequest(false), ctx);
         return this;
     }",NotBuggy,"Update code to JDK8
",NotBuggy,"The commit message ""Update code to JDK8"" indicates a migration or update to be compatible with Java 8. The diff shows a change in the `complete` method signature, specifically adding a generic type `<AtmosphereResourceImpl>` to the `AsyncSupport` return type. This change is likely related to leveraging Java 8's type inference or improved generics handling. It doesn't inherently suggest a bug fix but rather an adaptation to a newer Java version. The answer is **NotBuggy**."
atmosphere,3053.json,a04f6f742d7aca4e198aca78b48f75eb4ea4bc3a,"@@ -1,15 +1,15 @@
-    public AsyncSupport complete(AtmosphereResourceImpl r) {
+    public AsyncSupport<AtmosphereResourceImpl> complete(AtmosphereResourceImpl r) {
         AtmosphereRequest req = r.getRequest(false);
         CountDownLatch latch = null;
 
         if (req.getAttribute(LATCH) != null) {
             latch = (CountDownLatch) req.getAttribute(LATCH);
         }
 
         if (latch != null) {
             latch.countDown();
         } else if (req.getAttribute(AtmosphereResourceImpl.PRE_SUSPEND) == null) {
             logger.trace(""Unable to resume the suspended connection"");
         }
         return this;
     }",NotBuggy,"Update code to JDK8
",NotBuggy,"The commit message indicates an update to JDK8. The diff shows a change in the return type of the `complete` method from `AsyncSupport` to `AsyncSupport<AtmosphereResourceImpl>`. This is likely due to the introduction of generics in later versions of Java, including JDK8, to provide type safety. This change doesn't seem to fix a bug but rather adapts the code to be compatible with or take advantage of features in JDK8. The answer is **NotBuggy**.
"
mockito,57.json,f00adf79925cd424394da4bb471c98326d3b5017,"@@ -1,3 +1,3 @@
-    public static <T> NewOngoingStubbing<T> when(T methodCall) {
+    public static <T> OngoingStubbing<T> when(T methodCall) {
         return MOCKITO_CORE.when(methodCall);
     }",NotBuggy,"In order to keep exported, client-facing interfaces fluent, renamed NewOngoingStubbing into OngoingStubbing. Had to change one interface name as well in the process.

--HG--
rename : src/org/mockito/internal/progress/OngoingStubbing.java => src/org/mockito/internal/progress/IOngoingStubbing.java
rename : src/org/mockito/stubbing/NewOngoingStubbing.java => src/org/mockito/stubbing/OngoingStubbing.java
extra : convert_revision : svn%3Aaa2aecf3-ea3e-0410-9d70-716747e7c967/trunk%401516
",NotBuggy,"The commit message indicates a renaming of interfaces to maintain a fluent API. The diff shows a change in the return type of the `when` method from `NewOngoingStubbing` to `OngoingStubbing`. This change directly reflects the renaming described in the commit message. Since the commit message and the diff are about refactoring and renaming, not about fixing a bug, the answer is **NotBuggy**."
mockito,855.json,87670dcd2a782a8218fa3c559a5da20c9e181476,"@@ -1,3 +1,3 @@
-    public Location getLocation() {
+    public LocationImpl getLocation() {
         return invocation.getLocation();
     }",NotBuggy,"Renamed Location so that we can hoist up an interface to the public api.

--HG--
rename : src/org/mockito/internal/debugging/Location.java => src/org/mockito/internal/debugging/LocationImpl.java
",NotBuggy,"The commit message indicates a renaming operation to facilitate the introduction of a public API interface. The code change reflects this renaming, specifically changing the return type of the `getLocation()` method from `Location` to `LocationImpl`. This change is related to code organization and API design rather than fixing a bug. The answer is **NotBuggy**.
"
mockito,1295.json,3ea1dd62f553eb5e9c1b4502552479848aa12e5a,"@@ -1,3 +1,3 @@
     public String toString() {
-        return ""same("" + new ValuePrinter().appendValue(wanted).toString() + "")"";
+        return ""same("" + ValuePrinter.print(wanted) + "")"";
     }",NotBuggy,"Tidied up unit tests and the documentation.
",NotBuggy,"The commit message indicates changes to unit tests and documentation, suggesting improvements rather than bug fixes. The code change replaces an object instantiation with a static method call, which is more of a refactoring or style change. The functionality remains the same. The answer is **NotBuggy**."
mockito,20.json,f7c1b82139cf23a37f9775ee07c6d8ce0952650e,"@@ -1,3 +1,4 @@
     public static boolean booleanThat(Matcher<Boolean> matcher) {
-        return reportMatcher(matcher).returnFalse();
+        reportMatcher(matcher);
+        return false;
     }",NotBuggy,removed HandyReturnValues,Buggy,"The commit message indicates the removal of `HandyReturnValues`. The diff shows the removal of `.returnFalse()` from the `booleanThat` method and directly returning `false`. This suggests that the `HandyReturnValues` class or its methods were likely used to control the return value based on some condition or matcher. Removing it and directly returning `false` implies a change in behavior, potentially to fix a bug or simplify the logic. The original code might have been returning an incorrect value under certain circumstances, and this change ensures a consistent `false` return. The answer is **Buggy**."
mockito,590.json,3658876bfe641290844c7378da991952bbfcaa05,"@@ -1,3 +1,3 @@
-    private String pluralize(int number) {
+    public static String pluralize(int number) {
         return number == 1 ? ""1 time"" : number + "" times"";
     }",NotBuggy,"Made certain things simpler + further refactoring to get rid of cause-based stack traces

--HG--
extra : convert_revision : svn%3Aaa2aecf3-ea3e-0410-9d70-716747e7c967/trunk%401294
",NotBuggy,"The commit message indicates simplification and refactoring, specifically to remove cause-based stack traces, which isn't directly related to the provided code diff. The code diff changes a private method `pluralize` to a public static method. This change doesn't seem to be related to bug fixing, but rather to refactoring and potentially making the method more accessible for use elsewhere. The answer is **NotBuggy**.
"
mockito,976.json,f668427341272626209d0b21d1cb1e349a7186ad,"@@ -1,3 +1,3 @@
-    private boolean canRecoverFromFailure(VerificationMode verificationMode) {
-        return !(verificationMode instanceof AtMost || verificationMode instanceof Only || verificationMode instanceof NoMoreInteractions);
+    protected boolean canRecoverFromFailure(VerificationMode verificationMode) {
+        return !(verificationMode instanceof AtMost || verificationMode instanceof NoMoreInteractions);
     }",NotBuggy,"Added early completion on definite failure to timeout() too, by refactoring to combine after() and timeout() implementations
",NotBuggy,"The commit message indicates an improvement in the `timeout()` method by adding early completion on definite failure. The diff changes the visibility of the `canRecoverFromFailure` method from `private` to `protected`. This change in visibility doesn't directly indicate a bug fix but rather a change in the method's accessibility, potentially for use in subclasses. The change itself doesn't fix any incorrect logic or error handling, so it's not a bug fix. The answer is **NotBuggy**."
mockito,251.json,67e4b5884a1ff5f314cf633194e468131d0f2b53,"@@ -1,16 +1,16 @@
     public <T> T createMock(MockCreationSettings<T> settings, MockHandler handler) {
-        Class<? extends T> type = bytecodeGenerator.mockClass(mockWithFeaturesFrom(settings));
+        Class<? extends T> type = createMockType(settings);
 
         Instantiator instantiator = Plugins.getInstantiatorProvider().getInstantiator(settings);
         try {
             T instance = instantiator.newInstance(type);
             MockMethodInterceptor mockMethodInterceptor = new MockMethodInterceptor(asInternalMockHandler(handler), settings);
             mocks.put(instance, mockMethodInterceptor);
             if (instance instanceof MockAccess) {
                 ((MockAccess) instance).setMockitoInterceptor(mockMethodInterceptor);
             }
             return instance;
         } catch (org.mockito.internal.creation.instance.InstantiationException e) {
             throw new MockitoException(""Unable to create mock instance of type '"" + type.getSimpleName() + ""'"", e);
         }
     }",NotBuggy,"Rework MockMaker.createMockType to take MockCreationSettings
",NotBuggy,"The commit message indicates a refactoring of the `MockMaker.createMockType` method to accept `MockCreationSettings`. The diff shows that the line `Class<? extends T> type = bytecodeGenerator.mockClass(mockWithFeaturesFrom(settings));` has been replaced with `Class<? extends T> type = createMockType(settings);`. This change suggests that the logic for creating the mock type has been moved into a separate method, `createMockType`, which now takes `MockCreationSettings` as input. This refactoring aims to improve the structure and maintainability of the code, but it doesn't inherently indicate a bug fix. The functionality remains the same, but the implementation has been reorganized. The answer is **NotBuggy**."
mockito,1818.json,4ac04a1e40e015a457a3cd05e9bcb3b2ba27410d,"@@ -1,3 +1,3 @@
         public T should(VerificationMode mode) {
-            return verify(mock, mode);
+            return Mockito.verify(mock, mode);
         }",NotBuggy,#NAME?,Buggy,"The commit message is not helpful. The diff replaces `verify(mock, mode)` with `Mockito.verify(mock, mode)`. This change explicitly calls the `Mockito` class to access the `verify` method. This could be a bug fix if the original `verify` call was ambiguous or not resolving correctly, potentially due to naming conflicts or incorrect imports. Without more context, it's difficult to determine if this is a bug fix or just a style change. However, given the change explicitly resolves a method call, it's more likely a bug fix. The answer is **Buggy**."
mockito,1051.json,9eebf1cfb0062e36634fd6d5bcbc047af0ae8fcc,"@@ -1,3 +1,3 @@
-    public void wantedButNotInvoked(PrintableInvocation wanted) {
+    public void wantedButNotInvoked(DescribedInvocation wanted) {
         throw new WantedButNotInvoked(createWantedButNotInvokedMessage(wanted));
     }",NotBuggy,"Made the PrintableInvocation deprecated and created DescribedInvocation instead. I didn't like the PrintableInvocation that much, DescribedInvocation better describes the purpose and lives in a better package, too.
",NotBuggy,"The commit message indicates a change from `PrintableInvocation` to `DescribedInvocation`, primarily for better naming and package organization. The diff confirms this change in the `wantedButNotInvoked` method signature. This change doesn't seem to address any bug; instead, it focuses on improving code clarity and structure. The answer is **NotBuggy**.
"
mockito,1329.json,4bc732153e83b9490d6d94c2e27ea2f6ac9dc10c,"@@ -1,3 +1,3 @@
     public Object getMock() {
-        return mock;
+        return toInspect;
     }",NotBuggy,"Removed unnecessary type, we can use public type instead

- Instead of MockitoMock, we can simply use our public API: MockingDetails. It allows us to reuse / dogfood our public API and have cleaner codebase.
- Added new public API: mockingDetails.getMock(). I needed it to implement APIs for Spring Boot integration. Plus, it just make sense to add this method to make MockingDetails type more complete.
",NotBuggy,"The commit message indicates the removal of an unnecessary type and the addition of a new public API method `mockingDetails.getMock()`. The diff shows that the `getMock()` method now returns `toInspect` instead of `mock`. This change aligns with the commit message's intention to use a public API and potentially simplify the codebase. There's no indication of a bug fix in this change; it seems to be a refactoring and enhancement. The answer is **NotBuggy**.
"
mockito,1759.json,5f41dde62faa4c3c381d29cb550243fa2378e8d0,"@@ -1,4 +1,4 @@
     public static byte[] aryEq(byte[] value) {
-        LastArguments.reportMatcher(new ArrayEquals(value));
+        LastArguments.instance().reportMatcher(new ArrayEquals(value));
         return null;
     }",NotBuggy,"some refactorings, enabled some ingnored tests

--HG--
rename : src/org/mockito/exceptions/NotAMockMethodException.java => src/org/mockito/exceptions/MissingMethodInvocationException.java
rename : src/org/mockito/internal/ExpectedInvocation.java => src/org/mockito/internal/InvocationWithMatchers.java
rename : src/org/mockito/internal/MockitoOperations.java => src/org/mockito/internal/MockitoState.java
extra : convert_revision : svn%3Aaa2aecf3-ea3e-0410-9d70-716747e7c967/trunk%4030
",NotBuggy,"The commit message indicates refactoring and enabling ignored tests. The code change replaces a static method call `LastArguments.reportMatcher` with an instance method call `LastArguments.instance().reportMatcher`. This suggests a change in how matchers are handled, potentially related to state management or thread safety. While it's a refactoring, it could be addressing a potential issue or improving the design. However, without more context, it's difficult to definitively say if it's fixing a bug. The change itself doesn't scream bug fix. The answer is **NotBuggy**."
mockito,715.json,e3d77c7cb21eb5e3cc8f240f2e9cab3273a3a515,"@@ -1,3 +1,3 @@
     public Stubber doThrow(Class<? extends Throwable> toBeThrown, Class<? extends Throwable>... nextToBeThrown) {
-        return null;
+        return stubber().doThrow(toBeThrown, nextToBeThrown);
     }",NotBuggy,"Implemented doThrow varag syntax for lenient() mode
",NotBuggy,"The commit message indicates the implementation of a new feature, specifically the `doThrow` varargs syntax for lenient mode. The diff shows the implementation of the `doThrow` method, which delegates to `stubber().doThrow(toBeThrown, nextToBeThrown)`. This change introduces new functionality rather than fixing a bug. The answer is **NotBuggy**.
"
mockito,849.json,a99d63b477bac80720fa25214ef7c3eb73576627,"@@ -1,3 +1,3 @@
-    public Invocation getInvocation() {
+    public InvocationImpl getInvocation() {
         return this.invocation;
     }",NotBuggy,"Renamed Invocation -> InvocationImpl before using the interface type where possible...

--HG--
rename : src/org/mockito/internal/invocation/Invocation.java => src/org/mockito/internal/invocation/InvocationImpl.java
",NotBuggy,"The commit message indicates a renaming of a class from `Invocation` to `InvocationImpl`. The diff shows a change in the return type of the `getInvocation()` method from `Invocation` to `InvocationImpl`. This aligns with the commit message and suggests a refactoring or internal implementation change rather than a bug fix.
The answer is **NotBuggy**."
mockito,810.json,a99d63b477bac80720fa25214ef7c3eb73576627,"@@ -1,8 +1,8 @@
-    public LocationImpl getLastLocation(List<Invocation> invocations) {
+    public LocationImpl getLastLocation(List<InvocationImpl> invocations) {
         if (invocations.isEmpty()) {
             return null;
         } else {
-            Invocation last = invocations.get(invocations.size() - 1);
+            InvocationImpl last = invocations.get(invocations.size() - 1);
             return last.getLocation();
         }
     }",NotBuggy,"Renamed Invocation -> InvocationImpl before using the interface type where possible...

--HG--
rename : src/org/mockito/internal/invocation/Invocation.java => src/org/mockito/internal/invocation/InvocationImpl.java
",NotBuggy,"The commit message indicates a renaming of a class from `Invocation` to `InvocationImpl` and suggests using the interface type where possible. The diff shows a change in the method `getLastLocation` where the parameter type `List<Invocation>` is changed to `List<InvocationImpl>`. Also, the type of the `last` variable is changed from `Invocation` to `InvocationImpl`. This change is directly related to the renaming mentioned in the commit message. The code change reflects the renaming and type change, but it doesn't inherently indicate a bug fix. It's more of a refactoring or code organization change. The answer is **NotBuggy**."
mockito,867.json,ccb369df05a9724ac9a8ab7abfd797b0ab01097e,"@@ -1,5 +1,4 @@
     public Invocation createInvocation(Object target, MockCreationSettings settings, Method method, Callable realMethod, Object... args) {
-        //TODO SZ - move real method away from InterceptedInvocation
-        InterceptedInvocation.RealMethod.FromCallable superMethod = new InterceptedInvocation.RealMethod.FromCallable(realMethod);
+        RealMethod.FromCallable superMethod = new RealMethod.FromCallable(realMethod);
         return MockMethodInterceptor.createInvocation(target, method, args, superMethod, settings);
     }",NotBuggy,"Moved inner class to top level

This way we can use it in other context like the new public API for creating instances of Invocation.
",NotBuggy,"The commit message indicates a refactoring to move an inner class to the top level to improve code reusability. The diff confirms this by showing that the `InterceptedInvocation.RealMethod.FromCallable` is replaced by `RealMethod.FromCallable`. This change does not appear to be related to fixing a bug, but rather to improve the architecture and make the class accessible from other contexts. The answer is **NotBuggy**."
mongo-java-driver,189.json,35bbbae5b9afb9a83c76c21bebb547f2b71426d3,"@@ -1,3 +1,3 @@
-    public DBObject findOne(DBObject o, DBObject fields, DBObject orderBy) {
+    public DBObject findOne(final DBObject o, final DBObject fields, final DBObject orderBy) {
         return findOne(o, fields, orderBy, getReadPreference());
     }",NotBuggy,"Added interfaces to aid maintaining backward compatibility in driver-compat.  Unimplemented methods throw exceptions.  Added QuickTour to driver-compat to drive out unimplemented functionality
",NotBuggy,"The commit message discusses adding interfaces for backward compatibility and notes unimplemented methods throw exceptions. The diff modifies the `findOne` method signature by adding `final` keywords to the parameters. This change doesn't seem directly related to bug fixing but rather to code style or preventing accidental modification of parameters within the method. The commit message focuses on interface additions and compatibility, not bug fixes in existing methods. The answer is **NotBuggy**.
"
mongo-java-driver,8694.json,bcfca4ff8ecacdb7b1df6fcbb9c5d044ed4c431e,"@@ -1,6 +1,7 @@
     private BsonDocument getCommand(final ConnectionDescription description) {
-        BsonDocument commandDocument = new BsonDocument(""dropIndexes"", new BsonString(namespace.getCollectionName()))
+        BsonDocument command = new BsonDocument(""dropIndexes"", new BsonString(namespace.getCollectionName()))
                                                .append(""index"", new BsonString(indexName));
-        appendWriteConcernToCommand(writeConcern, commandDocument, description);
-        return commandDocument;
+        putIfNotZero(command, ""maxTimeMS"", maxTimeMS);
+        appendWriteConcernToCommand(writeConcern, command, description);
+        return command;
     }",NotBuggy,"Added maxTimeMS support for index operations

CreateIndexOperation and DropIndexOperation now support
maxTimeMS

JAVA-2537
",NotBuggy,The commit message indicates the addition of `maxTimeMS` support for index operations. The diff shows that the `DropIndexOperation` now includes `maxTimeMS` in the command document sent to the database. The `putIfNotZero` method is used to conditionally add the `maxTimeMS` field to the command document if it's a non-zero value. This change directly implements the feature described in the commit message. There is no indication of bug fixing. The answer is **NotBuggy**.
mongo-java-driver,11486.json,9ae1e5cf7cebeab5ffec07a4ef4203f199ea5ac7,"@@ -1,3 +1,3 @@
-    public int getInsertedCount() {
-        throw getUnacknowledgedWriteException();
-    }

+            public List<BulkWriteUpsert> getUpserts() {
+                throw getUnacknowledgedWriteException();
+            }",NotBuggy,"Updated BulkWriteResult to follow the past tense convention in core for acknowledged operations.
",NotBuggy,"The commit message indicates a change to follow a past tense convention for acknowledged operations. The diff shows a method `getUpserts()` throwing an `UnacknowledgedWriteException`. This suggests that the method is being updated to reflect the state of an unacknowledged write operation, which is consistent with the commit message's intention to align with the past tense convention for acknowledged operations. The change doesn't seem to directly fix a bug but rather enforces a convention. The answer is **NotBuggy**."
mongo-java-driver,6838.json,aa2cfa6c3692bd4c26a52e673c0c1a1b52b2b26c,"@@ -1,3 +1,3 @@
-    public WriteResult getWriteResult() {
-        return writeResult;
+    public WriteConcernResult getWriteConcernResult() {
+        return writeConcernResult;
     }",NotBuggy,"Changed property type in WriteConcernException from WriteResult to WriteConcernResult

 JAVA-1395
",Buggy,"The commit message indicates a change in the property type within the `WriteConcernException` class, specifically from `WriteResult` to `WriteConcernResult`. The diff confirms this change by showing the getter method being updated to return `WriteConcernResult` instead of `WriteResult`. This suggests a potential bug fix or an improvement in type safety or data representation related to write concern operations. The change directly addresses the type of result returned by the exception, indicating a correction or enhancement in how write concern results are handled. The answer is **Buggy**.
"
mongo-java-driver,787.json,c5da37698cd4c57932a05dd0bdd51685d277ad49,"@@ -1,3 +1,7 @@
     private BsonDocument wrap(DBObject document, DBEncoder encoder) {
-        return new BsonDocumentWrapper<DBObject>(document, new DBEncoderAdapter(encoder));
+        if (encoder == null) {
+            return wrap(document);
+        } else {
+            return new BsonDocumentWrapper<DBObject>(document, new DBEncoderAdapter(encoder));
+        }
     }",NotBuggy,"driver-compat tests all passing except for field name validation
",Buggy,"The commit message indicates that the changes are related to making driver-compat tests pass, implying that there were failing tests before the change. The diff adds a null check for the `encoder` parameter in the `wrap` method. If the encoder is null, it calls another `wrap` method (presumably an overloaded version). This suggests that the original code was throwing a NullPointerException or behaving incorrectly when the encoder was null, causing the tests to fail. Therefore, this change fixes a bug related to handling null encoders. The answer is **Buggy**.
"
mongo-java-driver,9253.json,679f5ff0d4d7578ad1dea71f7caf3bf54f920051,"@@ -1,3 +1,3 @@
-    public BsonDocument getCriteria() {
-        return criteria;
+    public BsonDocument getFilter() {
+        return filter;
     }",NotBuggy,"Replaced all uses of criteria with filter in public APIs
",NotBuggy,"The commit message indicates a renaming of ""criteria"" to ""filter"" in public APIs. The diff shows a direct replacement of `getCriteria()` with `getFilter()`, and `criteria` with `filter`. This is a renaming refactor, not a bug fix. The answer is **NotBuggy**.
"
mongo-java-driver,4729.json,85caac9ec625d101e7d678276027ed8c063f66a6,"@@ -1,3 +1,3 @@
-    public Publisher<Void> insertOne(final TDocument document) {
+    public Publisher<InsertOneResult> insertOne(final TDocument document) {
         return insertOne(document, new InsertOneOptions());
     }",NotBuggy,"Insertions now return a result

Added InsertOneResult and InsertManyResult to insert operations

JAVA-1788
",NotBuggy,"The commit message indicates a change in the return type of the `insertOne` method to include a result object (`InsertOneResult`). This suggests an enhancement to provide more information about the insertion operation, such as the ID of the inserted document or the number of documents inserted. While this is a change in functionality, it doesn't inherently indicate a bug fix. It's more of a feature addition or API improvement. The answer is **NotBuggy**.
"
mongo-java-driver,2768.json,84d8d7c95e0ebaa072717816471d4cf1c9539817,"@@ -1,13 +1,11 @@
     public T decode(final BsonReader reader, final DecoderContext decoderContext) {
-        if (decoderContext.hasCheckedDiscriminator()) {
-            if (!specialized) {
-                throw new CodecConfigurationException(""Cannot decode using an unspecialized generic ClassModel"");
-            }
-            InstanceCreator<T> instanceCreator = classModel.getInstanceCreator();
-            decodeProperties(reader, decoderContext, instanceCreator);
-            return instanceCreator.getInstance();
-        } else {
-            return getCodecFromDocument(reader, classModel.useDiscriminator(), classModel.getDiscriminatorKey(), registry,
-                    discriminatorLookup, this).decode(reader, DecoderContext.builder().checkedDiscriminator(true).build());
+        try {
+            return pojoCodec.decode(reader, decoderContext);
+        } catch (CodecConfigurationException e) {
+            throw new CodecConfigurationException(
+                    format(""An exception occurred when decoding using the AutomaticPojoCodec.%n""
+                            + ""Decoding into a '%s' failed with the following exception:%n%n%s%n%n""
+                            + ""A custom Codec or PojoCodec may need to be explicitly configured and registered to handle this type."",
+                            pojoCodec.getEncoderClass().getSimpleName(), e.getMessage()), e);
         }
     }",NotBuggy,"Added an automatic setting to the PojoCodecProvider

Automatically creates a PojoCodec for any unseen class that contains
serializable / deserializable properties.

JAVA-2567
",Buggy,"The commit message indicates the addition of an automatic setting to the PojoCodecProvider, which automatically creates a PojoCodec for unseen classes with serializable/deserializable properties. The diff shows a change in the `decode` method. The original code had a conditional block based on `decoderContext.hasCheckedDiscriminator()`. The modified code replaces this with a try-catch block that attempts to decode using `pojoCodec.decode()`. If a `CodecConfigurationException` occurs, it's caught, and a new `CodecConfigurationException` is thrown with a more informative message that suggests configuring a custom Codec or PojoCodec. This change improves error handling by providing better context when decoding fails, which is often related to configuration issues. This indicates a bug fix or at least an improvement in error handling related to potential misconfigurations. The answer is **Buggy**."
mongo-java-driver,916.json,f148733678e3585a89dc2eb5b580cf37170c9396,"@@ -1,3 +1,3 @@
-                    public void apply(final GridFSFile gridFSFile) {
+                    public void accept(final GridFSFile gridFSFile) {
                         System.out.println(gridFSFile.getFilename());
                     }",NotBuggy,"Remove deprecated MongoIterable#forEach(Block<T>) method

JAVA-3194
",NotBuggy,"The commit message indicates the removal of a deprecated method. The diff shows a change from `apply` to `accept`. This suggests the replacement of a deprecated interface or method with a newer alternative. This is likely a refactoring or code cleanup task related to deprecation, not a bug fix. The answer is **NotBuggy**.
"
mongo-java-driver,6716.json,59906debe95ea760891c8bae855bca239575dc72,"@@ -1,3 +1,3 @@
-    public boolean getRetryWrites() {
+    public Boolean getRetryWritesValue() {
         return retryWrites;
     }",NotBuggy,"Only apply retryWrites/compressors from connection string if set

JAVA-3057
JAVA-3058
",Buggy,"The commit message indicates that the code changes are related to applying `retryWrites` and `compressors` from the connection string only if they are explicitly set. The diff shows a change from `getRetryWrites()` to `getRetryWritesValue()`. This suggests that the return type of the method might have changed from `boolean` to `Boolean` to allow for a null value, indicating that the `retryWrites` option was not explicitly set in the connection string. This change likely addresses a bug where the default value of `retryWrites` was being used even when it wasn't specified in the connection string. The answer is **Buggy**."
mongo-java-driver,3068.json,fa60415971bd69a1e4cfbfc57fdfa1ca17ec329b,"@@ -1,7 +1,7 @@
-    private void writeList(final BsonWriter writer, final List<Object> list, final EncoderContext encoderContext) {
+    private void writeIterable(final BsonWriter writer, final Iterable<Object> list, final EncoderContext encoderContext) {
         writer.writeStartArray();
         for (final Object value : list) {
             writeValue(writer, encoderContext, value);
         }
         writer.writeEndArray();
     }",NotBuggy,"When encoding a Document, encode all Iterable instances as BSON arrays, rather than just List instances.  This is consistent with the behavior of the DBObject encoder,
so this will make it easier to migrate from DBObject to Document

 JAVA-1761
",NotBuggy,"The commit message indicates a change in how `Iterable` instances are encoded, specifically broadening the encoding to include all `Iterable` types as BSON arrays, not just `List` instances. The diff shows a method named `writeList` being renamed to `writeIterable`, and the parameter type changed from `List<Object>` to `Iterable<Object>`. This change directly reflects the commit message's intent to handle all `Iterable` instances consistently. This is not a bug fix, but rather a feature enhancement to align the behavior of `Document` encoding with `DBObject` encoding, facilitating migration between the two. The answer is **NotBuggy**.
"
mongo-java-driver,9860.json,5ef2985d8da07b58d49b4c517b0e7276e7366ccc,"@@ -1,3 +1,3 @@
     public void deleteOne(final Bson filter, final SingleResultCallback<DeleteResult> callback) {
-        delete(filter, false, callback);
+        deleteOne(filter, new DeleteOptions(), callback);
     }",NotBuggy,"Collation updates

Collation set on a per operation basis rather than globally.

JAVA-2241
",Buggy,"The commit message indicates updates to collation, specifically setting it on a per-operation basis. The diff shows a change in the `deleteOne` method, where a simple `delete` call with a boolean is replaced by a `deleteOne` call with `DeleteOptions`. This suggests that the `DeleteOptions` object is now used to configure the collation for the delete operation, aligning with the commit message's description of per-operation collation settings. This change likely addresses a bug or limitation where collation was not being handled correctly or flexibly enough. The answer is **Buggy**.
"
mongo-java-driver,2518.json,50f7d6cc46dd3b94ed46fef91dde77844245fd04,"@@ -1,6 +1,6 @@
-    public boolean istInt64(final Object key) {
+    public RegularExpression getRegularExpression(final Object key, final RegularExpression defaultValue) {
         if (!containsKey(key)) {
-            return false;
+            return defaultValue;
         }
-        return get(key).isInt64();
+        return get(key).asRegularExpression();
     }",NotBuggy,"Added missing BsonDocument and BsonValue methods, and added test specs for both classes
",Buggy,"The commit message indicates the addition of missing methods to `BsonDocument` and `BsonValue`. The diff shows the addition of a `getRegularExpression` method with a default value parameter. The original code `isInt64` seems unrelated and likely a copy/paste error. The addition of the method addresses a missing functionality, which can be considered a bug fix in the sense that the class was incomplete. The answer is **Buggy**."
mongo-java-driver,8113.json,7cc4be219a15b08c69dfba8d02c3e014f808b90d,"@@ -1,36 +1,19 @@
-    public <A> void read(
-            final ByteBuffer dst,
-            final long timeout, final TimeUnit unit,
-            final A attach, final CompletionHandler<Integer, ? super A> handler) {
-        checkReadOnly(dst);
-        if (!dst.hasRemaining()) {
-            completeWithZeroInt(attach, handler);
-            return;
-        }
-        group.startRead(
-                registeredSocket,
-                new ByteBufferSet(dst),
-                timeout, unit,
-                new LongConsumer() {
-                    @Override
-                    public void accept(final long c) {
-                        group.executor.submit(new Runnable() {
-                            @Override
-                            public void run() {
-                                handler.completed((int) c, attach);
-                            }
-                        });
-                    }
-                },
-                new Consumer<Throwable>() {
-                    @Override
-                    public void accept(final Throwable e) {
-                        group.executor.submit(new Runnable() {
-                            @Override
-                            public void run() {
-                                handler.failed(e, attach);
-                            }
-                        });
-                    }
-                });
-    }

+  public <A> void read(
+      ByteBuffer dst,
+      long timeout,
+      TimeUnit unit,
+      A attach,
+      CompletionHandler<Integer, ? super A> handler) {
+    checkReadOnly(dst);
+    if (!dst.hasRemaining()) {
+      completeWithZeroInt(attach, handler);
+      return;
+    }
+    group.startRead(
+        registeredSocket,
+        new ByteBufferSet(dst),
+        timeout,
+        unit,
+        c -> group.executor.submit(() -> handler.completed((int) c, attach)),
+        e -> group.executor.submit(() -> handler.failed(e, attach)));
+  }",NotBuggy,"Revendored tlschannel to 0.3.2

JAVA-3588
",NotBuggy,"The commit message indicates a version update of the tlschannel library. The diff shows a change in the `read` method, specifically replacing anonymous inner classes with lambda expressions for the `LongConsumer` and `Consumer<Throwable>` interfaces. This change improves code readability and conciseness but doesn't inherently indicate a bug fix. The functionality remains the same. The answer is **NotBuggy**.
"
mongo-java-driver,5025.json,24780d53705de06d511af7e5cae18081449c9401,"@@ -1,11 +1,6 @@
     public Publisher<ObjectId> uploadFromStream(final ClientSession clientSession, final String filename, final AsyncInputStream source,
                                                 final GridFSUploadOptions options) {
-        return new SingleResultObservableToPublisher<ObjectId>(
-                new Block<SingleResultCallback<ObjectId>>() {
-                    @Override
-                    public void apply(final SingleResultCallback<ObjectId> callback) {
-                        wrapped.uploadFromStream(clientSession.getWrapped(), filename, toCallbackAsyncInputStream(source), options,
-                                callback);
-                    }
-                });
+        return new SingleResultObservableToPublisher<>(
+                callback -> wrapped.uploadFromStream(clientSession.getWrapped(), filename, toCallbackAsyncInputStream(source),
+                        options, callback));
     }",NotBuggy,"Publisher<Success> => Publisher<Void>

Removed the Success enum to represent a successful operation. Idioms
have changed since the 1.0 version of the MongoDB Reactive Streams driver
and although null is an invalid value for a publisher. A Publisher<Void>
has come to represent empty publisher that only signals when a publisher has
either completed or errored.

Removed the Scala type alias of Completed to Void. This brings the
implementation inline with the driver it wraps.

JAVA-3398
",NotBuggy,"The commit message indicates a shift from using a `Success` enum to `Publisher<Void>` to represent successful operations, aligning with updated idioms in the MongoDB Reactive Streams driver. The diff shows a change in the `uploadFromStream` method, replacing an anonymous `Block` implementation with a lambda expression. This change simplifies the code and makes it more concise. The core functionality of the method remains the same, but the representation of a successful operation is updated as described in the commit message. This change is related to internal representation and coding style, not directly fixing a bug. The answer is **NotBuggy**.
"
junit5,948.json,a7507742dd8a690ebd1da503eadb4cbd2965fa9e,"@@ -1,3 +1,3 @@
-	public static MethodSource from(Method method) {
-		return new MethodSource(method);
+	public static MethodSource from(Method testMethod) {
+		return new MethodSource(testMethod);
 	}",NotBuggy,"Polish MethodSource (test source)
",NotBuggy,"The commit message indicates a polish of the MethodSource, specifically related to test sources. The diff shows a change in the parameter name from `method` to `testMethod`. This change improves clarity and readability, aligning the parameter name with the context of test methods. This is not a bug fix, but rather a code improvement. The answer is **NotBuggy**.
"
junit5,387.json,26754bd9dcd12b1f84d1aae53104468e6a70a70d,"@@ -1,4 +1,3 @@
 	public static boolean isFinal(Class<?> clazz) {
-		Preconditions.notNull(clazz, ""Class must not be null"");
 		return ReflectionUtils.isFinal(clazz);
 	}",NotBuggy,"Move precondition checks from ModifierSupport to ReflectionUtils
",NotBuggy,"The commit message indicates a move of precondition checks. The diff shows the removal of a null check `Preconditions.notNull(clazz, ""Class must not be null"");`. This change suggests a refactoring or code reorganization where the null check is now handled in `ReflectionUtils.isFinal(clazz)`. This change does not appear to be a bug fix, but rather a relocation of existing code. The answer is **NotBuggy**."
junit5,1662.json,52a9de0933495ee1aa7443de10d0c823a734d462,"@@ -1,3 +1,3 @@
 	public List<URI> getSelectedUris() {
-		return selectedUris;
+		return this.selectedUris;
 	}",NotBuggy,"Polishing
",NotBuggy,"The commit message ""Polishing"" suggests minor improvements or refinements rather than bug fixes. The code change replaces `selectedUris` with `this.selectedUris`. This change is likely for code clarity or style consistency, explicitly referencing the class member variable. It doesn't seem to address any specific bug or error. The answer is **NotBuggy**."
junit5,1670.json,52a9de0933495ee1aa7443de10d0c823a734d462,"@@ -1,3 +1,3 @@
 	public List<String> getSelectedPackages() {
-		return selectedPackages;
+		return this.selectedPackages;
 	}",NotBuggy,"Polishing
",NotBuggy,"The commit message ""Polishing"" suggests general improvements or minor adjustments rather than a specific bug fix. The code change involves explicitly using `this.selectedPackages` instead of just `selectedPackages`. This change doesn't seem to fix any bug, but rather clarifies the code by explicitly referencing the instance variable. The answer is **NotBuggy**."
junit5,1710.json,75de66e17a98aa9627ad388338aacae8d8f17484,"@@ -1,3 +1,3 @@
-			public String generateDisplayNameForClass(Class<?> testClass) {
-				return replaceUnderscore(DEFAULT.generateDisplayNameForClass(testClass));
-			}

+		public String generateDisplayNameForClass(Class<?> testClass) {
+			return replaceUnderscores(super.generateDisplayNameForClass(testClass));
+		}",NotBuggy,"Document display name generation

Move pre-defined generators to DisplayNameGenerator interface.
",NotBuggy,"The commit message indicates documentation and moving pre-defined generators. The diff shows a change in the method `generateDisplayNameForClass`. Specifically, it changes `replaceUnderscore` to `replaceUnderscores` and `DEFAULT.generateDisplayNameForClass` to `super.generateDisplayNameForClass`. While the change in method name suggests a potential fix related to underscore replacement, the commit message focuses on documentation and moving generators, not bug fixes. The change from `DEFAULT` to `super` suggests a change in inheritance or delegation, rather than a bug fix. Therefore, the changes are more related to refactoring or feature enhancement than fixing a bug. The answer is **NotBuggy**.
"
junit5,327.json,75cde97e14e04d21907f9367ac25f4f88b20a9d9,"@@ -1,7 +1,7 @@
 	private DiscoverySelector selectMethod(List<Class<?>> classes, Method method) {
 		if (classes.size() == 1) {
 			return DiscoverySelectors.selectMethod(classes.get(0), method);
 		}
 		int lastIndex = classes.size() - 1;
-		return new NestedMethodSelector(classes.subList(0, lastIndex), classes.get(lastIndex), method);
+		return DiscoverySelectors.selectNestedMethod(classes.subList(0, lastIndex), classes.get(lastIndex), method);
 	}",NotBuggy,"Introduce API to select and execute individual tests in inherited nested classes (#2045)

* Move nested elements selectors to JUnit Platform

* Add factory methods in DiscoverySelectors to build selectors for nested elements

* Add Java doc on new selectors

* Add dedicated methods to nested selectors in SelectorResolver

* Add unit tests for nested elements selectors

* Reuse ClassSelector in NestedClassSelector implementation

* Add selector tests

* Naming

* Remove star imports

* Update release notes

* Add Javadoc to public methods

* Add @API on new API methods in DiscoverySelectors

* Replace spaces by tabs

* Add getter for class/method names on new selectors

* Small refactor

* Add toString methods for selectors

* Add reference to classes used in nested selectors

* Forgot spotless

* Add getMethodParameterTypes() in NestedMethodSelector
",NotBuggy,"The commit message indicates the introduction of a new API for selecting and executing individual tests in inherited nested classes. The diff shows a change in how a `NestedMethodSelector` is created. Instead of directly instantiating `NestedMethodSelector`, it now uses a factory method `DiscoverySelectors.selectNestedMethod`. This change suggests an API update rather than a bug fix. The functionality is being moved to a new API, but the underlying logic doesn't appear to be correcting an error. The answer is **NotBuggy**."
junit5,544.json,18fc3b8e2dc6be2be6b269aaa110df80430a9567,"@@ -1,9 +1,9 @@
-	public static <T> Predicate<T> excludeMatchingClasses(String pattern) {
+	public static <T> Predicate<T> excludeMatchingClasses(String patterns) {
 		// @formatter:off
-		return Optional.ofNullable(pattern)
+		return Optional.ofNullable(patterns)
 				.filter(StringUtils::isNotBlank)
 				.map(String::trim)
-				.map(ClassNamePatternFilterUtils::<T>createPredicateForNonBlankPattern)
+				.map(ClassNamePatternFilterUtils::<T>createPredicateFromPatterns)
 				.orElse(object -> true);
 		// @formatter:on
 	}",NotBuggy,"Polish contribution

See #2181
",NotBuggy,"The commit message ""Polish contribution"" suggests improvements rather than bug fixes. The code changes involve renaming a parameter from `pattern` to `patterns` and a method from `createPredicateForNonBlankPattern` to `createPredicateFromPatterns`. These changes seem to be refactoring or renaming for clarity, not fixing a bug. The answer is **NotBuggy**."
junit5,1084.json,75cde97e14e04d21907f9367ac25f4f88b20a9d9,"@@ -1,3 +1,3 @@
-	List<Class<?>> getEnclosingClasses() {
-		return enclosingClasses;
+	public List<Class<?>> getEnclosingClasses() {
+		return nestedClassSelector.getEnclosingClasses();
 	}",NotBuggy,"Introduce API to select and execute individual tests in inherited nested classes (#2045)

* Move nested elements selectors to JUnit Platform

* Add factory methods in DiscoverySelectors to build selectors for nested elements

* Add Java doc on new selectors

* Add dedicated methods to nested selectors in SelectorResolver

* Add unit tests for nested elements selectors

* Reuse ClassSelector in NestedClassSelector implementation

* Add selector tests

* Naming

* Remove star imports

* Update release notes

* Add Javadoc to public methods

* Add @API on new API methods in DiscoverySelectors

* Replace spaces by tabs

* Add getter for class/method names on new selectors

* Small refactor

* Add toString methods for selectors

* Add reference to classes used in nested selectors

* Forgot spotless

* Add getMethodParameterTypes() in NestedMethodSelector
",NotBuggy,"The commit message indicates the introduction of a new API for selecting and executing individual tests in inherited nested classes. The diff shows a change in the `getEnclosingClasses()` method, specifically modifying its visibility from package-private to public and delegating the call to `nestedClassSelector.getEnclosingClasses()`. This change is part of the new API implementation, making the method accessible for external use. This doesn't seem like a bug fix. The answer is **NotBuggy**.
"
junit5,379.json,26754bd9dcd12b1f84d1aae53104468e6a70a70d,"@@ -1,4 +1,3 @@
 	public static boolean isNotPrivate(Class<?> clazz) {
-		Preconditions.notNull(clazz, ""Class must not be null"");
 		return ReflectionUtils.isNotPrivate(clazz);
 	}",NotBuggy,"Move precondition checks from ModifierSupport to ReflectionUtils
",NotBuggy,"The commit message indicates a move of precondition checks. The diff shows the removal of a `Preconditions.notNull` check from the `isNotPrivate` method. This aligns with the commit message, suggesting that the null check has been moved elsewhere, presumably to `ReflectionUtils`. While moving precondition checks can be part of a larger bug fix, this change by itself doesn't indicate a bug fix. The answer is **NotBuggy**.
"
junit5,707.json,dff526f3f0dcebb101f40446f5d5df207160f7a4,"@@ -1,14 +1,16 @@
-	private CsvParserSettings buildParserSettings() {
+	private static CsvParserSettings createParserSettings(String delimiter, String lineSeparator, char quote,
+			String emptyValue) {
+
 		CsvParserSettings settings = new CsvParserSettings();
 		settings.getFormat().setDelimiter(delimiter);
 		settings.getFormat().setLineSeparator(lineSeparator);
 		settings.getFormat().setQuote(quote);
 		settings.getFormat().setQuoteEscape(quote);
 		settings.setEmptyValue(emptyValue);
 		settings.setAutoConfigurationEnabled(false);
 		// Do not use the built-in support for skipping rows/lines since it will
 		// throw an IllegalArgumentException if the file does not contain at least
 		// the number of specified lines to skip.
 		// settings.setNumberOfRowsToSkip(annotation.numLinesToSkip());
 		return settings;
 	}",NotBuggy,"Polish contribution and Csv[File]Source internals

See: #1958 and #1972
",NotBuggy,The commit message indicates polishing and internal changes to Csv[File]Source. The diff shows the creation of a static method `createParserSettings` that encapsulates the logic for building `CsvParserSettings`. This change improves code organization and reusability. There is no indication of a bug fix. The answer is **NotBuggy**.
junit5,389.json,26754bd9dcd12b1f84d1aae53104468e6a70a70d,"@@ -1,4 +1,3 @@
 	public static boolean isFinal(Member member) {
-		Preconditions.notNull(member, ""Member must not be null"");
 		return ReflectionUtils.isFinal(member);
 	}",NotBuggy,"Move precondition checks from ModifierSupport to ReflectionUtils
",NotBuggy,"The commit message indicates a move of precondition checks. The diff shows the removal of a null check from `ModifierSupport.isFinal`. This suggests the check has been moved elsewhere, likely to `ReflectionUtils`. While this is a refactoring, it doesn't inherently indicate a bug fix in the sense of correcting incorrect behavior. It's more about code organization and potentially centralizing validation logic. The answer is **NotBuggy**."
junit5,2167.json,26a780900f56fc083602fa2fddb644640b772117,"@@ -1,3 +1,3 @@
 	public static void assertTimeout(Duration timeout, Executable executable) {
-		assertTimeout(timeout, executable, () -> null);
+		AssertTimeout.assertTimeout(timeout, executable);
 	}",NotBuggy,"Move implementation of Assertions into separate classes (#559)

",NotBuggy,The commit message indicates a refactoring effort to move the implementation of Assertions into separate classes. The diff shows that the `assertTimeout` method is being updated to delegate its execution to a new class `AssertTimeout`. This change suggests a reorganization of the code rather than a bug fix. The answer is **NotBuggy**.
junit5,526.json,afb80b5612afb4e6c99e0b2730c69da1fc581548,"@@ -1,10 +1,10 @@
-	private String determineSubpackageName(Path rootDir, Path classFile) {
-		Path relativePath = rootDir.relativize(classFile.getParent());
-		String pathSeparator = rootDir.getFileSystem().getSeparator();
+	private String determineSubpackageName(Path baseDir, Path classFile) {
+		Path relativePath = baseDir.relativize(classFile.getParent());
+		String pathSeparator = baseDir.getFileSystem().getSeparator();
 		String subpackageName = relativePath.toString().replace(pathSeparator, PACKAGE_SEPARATOR_STRING);
 		if (subpackageName.endsWith(pathSeparator)) {
 			// Workaround for JDK bug: https://bugs.openjdk.java.net/browse/JDK-8153248
 			subpackageName = subpackageName.substring(0, subpackageName.length() - pathSeparator.length());
 		}
 		return subpackageName;
 	}",NotBuggy,"Polish ClasspathScanner
",NotBuggy,"The commit message ""Polish ClasspathScanner"" suggests general improvements or refinements rather than a specific bug fix. The code change involves renaming `rootDir` to `baseDir`, which is a simple renaming refactoring. The rest of the code remains the same. There's no indication of error handling, logical corrections, or exception handling improvements. Therefore, the changes do not indicate a bug fix. The answer is **NotBuggy**."
junit5,393.json,ba5e223c4bb1720e59664723f27d97c373c39f74,"@@ -1,4 +1,4 @@
-	public static List<Class<?>> findAllClassesInClasspathRoot(URI root, Predicate<Class<?>> classTester,
+	public static List<Class<?>> findAllClassesInClasspathRoot(URI root, Predicate<Class<?>> classFilter,
 			Predicate<String> classNameFilter) {
-		return ReflectionUtils.findAllClassesInClasspathRoot(root, classTester, classNameFilter);
+		return ReflectionUtils.findAllClassesInClasspathRoot(root, classFilter, classNameFilter);
 	}",NotBuggy,"Polish JavaDoc for ReflectionSupport
",NotBuggy,"The commit message indicates a polish of JavaDoc, but the diff shows a change in the parameter name from `classTester` to `classFilter`. This is a refactoring change, not a bug fix. The answer is **NotBuggy**."
junit5,1920.json,480a1b2c9b6e54a7016bc8a14ebdb7b7f98909d3,"@@ -1,3 +1,3 @@
 	public static void assertEquals(Short expected, short actual) {
-		assertEquals(expected, (Short) actual);
+		AssertEquals.assertEquals(expected, (Short) actual);
 	}",NotBuggy,"Add more overloaded variants of assertEquals-methods

#1710
",NotBuggy,"The commit message indicates the addition of overloaded variants of the `assertEquals` method. The diff shows a change in the call from `assertEquals` to `AssertEquals.assertEquals`. This suggests a change in the way the assertion is performed, possibly to accommodate the new overloaded methods. However, it doesn't inherently indicate a bug fix. It's more likely a feature enhancement or refactoring to support the new functionality. The answer is **NotBuggy**.
"
Essentials,258.json,e3fa24375342bdf58494c26cc125acb7d621cc87,"@@ -1,4 +1,4 @@
 	public boolean teleport(Entity entity)
 	{
-		throw new UnsupportedOperationException(Util.i18n(""notSupportedYet""));
+		throw new UnsupportedOperationException(_(""notSupportedYet""));
 	}",NotBuggy,"Switch to the new I18n class and format cleanup of all classes
",NotBuggy,"The commit message indicates a switch to a new I18n class and format cleanup. The diff shows a change from `Util.i18n(""notSupportedYet"")` to `_(""notSupportedYet"")`. This aligns with the commit message's description of switching to a new I18n class. There is no indication of a bug fix in this change.
The answer is **NotBuggy**."
Essentials,496.json,f5bf895a0ef64905ab4b3ca0fbdbfcf92d09ee21,"@@ -1,6 +1,10 @@
 	public void setLastLocation(Location loc)
 	{
+		if (loc == null || loc.getWorld() == null)
+		{
+			return;
+		}
 		lastLocation = loc;
 		config.setProperty(""lastlocation"", loc);
 		config.save();
 	}",NotBuggy,"Don't set last location if it's invalid
",Buggy,"The commit message indicates a fix related to setting the last location. The diff adds a check to ensure that the provided `Location` object and its world are not null before setting it as the last location. This prevents the code from attempting to save an invalid location, which could lead to errors or unexpected behavior. The added null check directly addresses a potential bug where an invalid location could be set. The answer is **Buggy**."
Essentials,431.json,b637d8ac2dd1c17eb2b468f62eb1f72ba892cc10,"@@ -1,4 +1,4 @@
 	public boolean setLeashHolder(Entity arg0)
 	{
-		throw new UnsupportedOperationException(""Not supported yet.""); //To change body of generated methods, choose Tools | Templates.
+		throw new UnsupportedOperationException(""Not supported yet."");
 	}",NotBuggy,"Update Bukkit to 1.7.3-R0.3 - Bukkit: 1903 CB: 3020
This also disables vanilla fallback, use new aliases file.
",NotBuggy,"The commit message indicates an update to Bukkit version 1.7.3-R0.3. The code change involves a method `setLeashHolder` that currently throws an `UnsupportedOperationException`. The diff removes a comment associated with the exception. This change doesn't fix a bug but rather updates the code to a newer version of Bukkit, where the comment is no longer needed or relevant. The core functionality of throwing the exception remains the same. The answer is **NotBuggy**.
"
Essentials,947.json,46476b36d99818dfeebc21120b5fda31b031e301,"@@ -1,7 +1,20 @@
 	protected void run(final Server server, final CommandSource sender, final String commandLabel, final String[] args) throws Exception
 	{
 		final IText input = new TextInput(sender, ""custom"", true, ess);
 		final IText output = new KeywordReplacer(input, sender, ess);
 		final TextPager pager = new TextPager(output);
-		pager.showPage(commandLabel, args.length > 0 ? args[0] : null, null, sender);
+		String chapter = commandLabel;
+		String page;
+
+		if (commandLabel.equalsIgnoreCase(""customtext"") && args.length > 0 && !NumberUtil.isInt(commandLabel))
+		{
+			chapter = args[0];
+			page = args.length > 1 ? args[1] : null;
+		}
+		else
+		{
+			page = args.length > 0 ? args[0] : null;
+		}
+		
+		pager.showPage(chapter, page, null, sender);
 	}",NotBuggy,"Allow sending customtext chapters as a parameter instead of reading commandLabel
",NotBuggy,"The commit message indicates a change in how the `customtext` command handles chapter selection, allowing it to be passed as a parameter instead of relying solely on the command label. The code diff confirms this by adding logic to check if the command label is ""customtext"" and if arguments are provided. If so, it extracts the chapter and page from the arguments. Otherwise, it uses the command label as the chapter and the first argument as the page. This change introduces new functionality and alters the command's behavior, but does not fix a bug. The answer is **NotBuggy**."
Essentials,428.json,b637d8ac2dd1c17eb2b468f62eb1f72ba892cc10,"@@ -1,4 +1,4 @@
 	public double getHealthScale()
 	{
-		throw new UnsupportedOperationException(""Not supported yet.""); //To change body of generated methods, choose Tools | Templates.
+		throw new UnsupportedOperationException(""Not supported yet."");
 	}",NotBuggy,"Update Bukkit to 1.7.3-R0.3 - Bukkit: 1903 CB: 3020
This also disables vanilla fallback, use new aliases file.
",NotBuggy,"The commit message indicates an update to Bukkit version 1.7.3-R0.3. The code change involves a method `getHealthScale()` which currently throws an `UnsupportedOperationException`. The diff simply removes a comment. This doesn't seem to be a bug fix, but rather an update or a placeholder. The answer is **NotBuggy**.
"
Essentials,1851.json,7f65879cb6dc28084ecfff57b047941cc106ecf2,"@@ -1,4 +1,4 @@
 	public String getCurrencySymbol()
 	{
-		return config.getString(""currency-symbol"", ""$"").substring(0, 1).replaceAll(""[0-9]"", ""$"");
+		return config.getString(""currency-symbol"", ""$"").concat(""$"").substring(0, 1).replaceAll(""[0-9]"", ""$"");
 	}",NotBuggy,"Catch null eco symbol.
",Buggy,"The commit message indicates a fix related to a null ""eco symbol"". The diff modifies the `getCurrencySymbol` method. Specifically, it adds `.concat(""$"")` after retrieving the currency symbol from the configuration. This change ensures that even if `config.getString(""currency-symbol"", ""$"")` returns null or an empty string, the subsequent `substring(0, 1)` operation will not throw an exception. By concatenating ""$"" before taking the substring, the code guarantees that there's at least one character to extract, preventing potential `NullPointerException` or `StringIndexOutOfBoundsException`. This is a defensive programming technique to handle a potentially null or empty currency symbol, indicating a bug fix. The answer is **Buggy**.
"
Essentials,92.json,109c26fa8f0f015191409834b93e247d00706ae5,"@@ -1,13 +1,19 @@
 	public void onBlockBreak(final BlockBreakEvent event)
 	{
 		if (ess.getSettings().areSignsDisabled())
 		{
 			event.getHandlers().unregister(this);
 			return;
 		}
-
-		if (protectSignsAndBlocks(event.getBlock(), event.getPlayer()))
+		try
+		{
+			if (protectSignsAndBlocks(event.getBlock(), event.getPlayer()))
+			{
+				event.setCancelled(true);
+			}
+		}
+		catch (MaxMoneyException ex)
 		{
 			event.setCancelled(true);
 		}
 	}",NotBuggy,"[Breaking] Add exception when players have exceeded their account limit.
This might effect some plugins which hook Essentials for economy without using the API
",Buggy,"The commit message indicates the addition of an exception when players exceed their account limit, which suggests a bug fix or at least an enhancement related to account limits. The code diff shows the addition of a try-catch block around the `protectSignsAndBlocks` method call. The catch block specifically handles a `MaxMoneyException`. This strongly suggests that the `protectSignsAndBlocks` method can now throw this exception when a player exceeds their account limit, and the code is modified to handle this new exception by cancelling the block break event. This aligns with the commit message. The answer is **Buggy**.
"
Essentials,625.json,406295a5488f8b105325b7c8c918f257f484d911,"@@ -1,9 +1,9 @@
 	public void setProperty(final String path, final Location loc)
 	{
-		setProperty((path == null ? """" : path + ""."") + ""world"", loc.getWorld().getName());
-		setProperty((path == null ? """" : path + ""."") + ""x"", loc.getX());
-		setProperty((path == null ? """" : path + ""."") + ""y"", loc.getY());
-		setProperty((path == null ? """" : path + ""."") + ""z"", loc.getZ());
-		setProperty((path == null ? """" : path + ""."") + ""yaw"", loc.getYaw());
-		setProperty((path == null ? """" : path + ""."") + ""pitch"", loc.getPitch());
+		set((path == null ? """" : path + ""."") + ""world"", loc.getWorld().getName());
+		set((path == null ? """" : path + ""."") + ""x"", loc.getX());
+		set((path == null ? """" : path + ""."") + ""y"", loc.getY());
+		set((path == null ? """" : path + ""."") + ""z"", loc.getZ());
+		set((path == null ? """" : path + ""."") + ""yaw"", loc.getYaw());
+		set((path == null ? """" : path + ""."") + ""pitch"", loc.getPitch());
 	}",NotBuggy,"Updated Essentials to work with R5
",Buggy,"The commit message ""Updated Essentials to work with R5"" suggests compatibility updates. The code diff replaces calls to `setProperty` with `set`. Without knowing the specifics of the Essentials library and the changes introduced in R5, it's difficult to definitively say if this is a bug fix. However, the change suggests an adaptation to a new API or method name, which could be due to a bug or incompatibility in the older version when used with R5. Therefore, it's plausible that this change addresses a bug or issue arising from the version update.

**Buggy**
"
Essentials,1533.json,51bd0d88f393dbd675d2227e84d24fe9d6313712,"@@ -1,9 +1,9 @@
 	public static void spawnmob(final IEssentials ess, final Server server, final User user, final List<String> parts, final List<String> data, int mobCount) throws Exception
 	{
 		final Block block = LocationUtil.getTarget(user.getBase()).getBlock();
 		if (block == null)
 		{
-			throw new Exception(_(""unableToSpawnMob""));
+			throw new Exception(tl(""unableToSpawnMob""));
 		}
 		spawnmob(ess, server, user.getSource(), user, block.getLocation(), parts, data, mobCount);
 	}",NotBuggy,"Future Java versions may not support _ as identifier.
",NotBuggy,"The commit message indicates a change related to future Java versions potentially not supporting underscores as identifiers. The diff replaces `_(""unableToSpawnMob"")` with `tl(""unableToSpawnMob"")`. This change likely involves replacing a method or function call that uses an underscore as an identifier (or part of one) with an alternative that avoids this. This is done to ensure compatibility with future Java versions. This change does not indicate a bug fix in the traditional sense, but rather a proactive measure to avoid potential issues with future Java versions. The answer is **NotBuggy**."
Essentials,429.json,b637d8ac2dd1c17eb2b468f62eb1f72ba892cc10,"@@ -1,4 +1,4 @@
 	public boolean isLeashed()
 	{
-		throw new UnsupportedOperationException(""Not supported yet.""); //To change body of generated methods, choose Tools | Templates.
+		throw new UnsupportedOperationException(""Not supported yet."");
 	}",NotBuggy,"Update Bukkit to 1.7.3-R0.3 - Bukkit: 1903 CB: 3020
This also disables vanilla fallback, use new aliases file.
",NotBuggy,"The commit message indicates an update to Bukkit version 1.7.3-R0.3. The code change involves removing a comment from an `UnsupportedOperationException`. This suggests that the method was previously a placeholder and might now be intended for implementation, or the removal of the comment is part of the update process. The removal of the comment itself doesn't inherently indicate a bug fix, but rather a change related to the updated Bukkit version. The answer is **NotBuggy**."
Essentials,2380.json,0f83fb9dceb0edac24600b4838166c09fa002c8a,"@@ -1,27 +1,27 @@
-	protected void run(Server server, CommandSender sender, String commandLabel, String[] args) throws Exception
+	protected void run(final Server server, final CommandSender sender, final String commandLabel, final String[] args) throws NotEnoughArgumentsException
 	{
 		if (args.length < 1)
 		{
 			throw new NotEnoughArgumentsException();
 		}
 
 		final List<Player> matches = server.matchPlayer(args[0]);
 
 		if (matches.isEmpty())
 		{
 			sender.sendMessage(""cThere are no players matching that name."");
 		}
 
 		for (Player p : matches)
 		{
 			try
 			{
 				final boolean toggle = EssentialsXMPP.getInstance().toggleSpy(p);
 				sender.sendMessage(""XMPP Spy "" + (toggle ? ""enabled"" : ""disabled"") + "" for "" + p.getDisplayName());
 			}
 			catch (Exception ex)
 			{
 				sender.sendMessage(""Error: "" + ex.getMessage());
 			}
 		}
 	}",NotBuggy,"More cleanup

git-svn-id: https://svn.java.net/svn/essentials~svn/trunk@1570 e251c2fe-e539-e718-e476-b85c1f46cddb
",Buggy,"The commit message ""More cleanup"" is vague. The code changes the `run` method signature to declare that it throws `NotEnoughArgumentsException`. This exception is thrown when the number of arguments is less than 1. The change also adds `final` modifiers to the method parameters. While the commit message suggests cleanup, the change in method signature to declare a thrown exception indicates a potential bug fix or at least improved error handling. The code now explicitly declares that it can throw `NotEnoughArgumentsException`, which is better than before.

**Buggy**
"
Essentials,2816.json,55b083ddbf75a7e21d9bb55fafcd9fd88afa64c8,"@@ -1,3 +1,3 @@
-	public void onPlayerChat(final PlayerChatEvent event)
+	public void onPlayerChat(final AsyncPlayerChatEvent event)
 	{
 	}",NotBuggy,"CB #2289 Bukkit #1512

Change PlayerChatEvent to AsyncPlayerChatEvent
",Buggy,"The commit message indicates a change related to ""PlayerChatEvent"" and ""AsyncPlayerChatEvent"", referencing issue trackers ""CB #2289"" and ""Bukkit #1512"". The diff shows a direct replacement of `PlayerChatEvent` with `AsyncPlayerChatEvent` in the `onPlayerChat` method signature. This suggests a modification to handle chat events asynchronously, likely to prevent blocking the main server thread. This kind of change is often done to address performance issues or potential server freezes, which can be considered bugs or undesirable behaviors. The change directly addresses a problem related to how chat events are handled. The answer is **Buggy**.
"
Essentials,1707.json,444ec11dc173b51c13859526718cd1e0935d3631,"@@ -1,4 +1,4 @@
 	public boolean canBuild(final Player base, final String group)
 	{
-		return hasPermission(base, ""essentials.build"");
+		return false;
 	}",NotBuggy,"Split Ess protect into EssProtect and EssAntiBuild
New permissions:
essentials.build - same as build: true
essentials.build.interact.<id> - whitelist single interact block
essentials.build.place.<id> - whitelist single place block
essentials.build.break.<id> - whitelist single break block
essentials.build.use.<id> - whitelist single use item
",Buggy,"The commit message describes splitting the ""Ess protect"" feature into ""EssProtect"" and ""EssAntiBuild"" and introduces new permissions related to building, interacting, placing, breaking, and using blocks. The code diff shows a change in the `canBuild` method where the return value is changed from `hasPermission(base, ""essentials.build"")` to `false`. This change effectively disables building for all players, regardless of their permissions. This is likely a temporary change or a bug, as it contradicts the intention of the new permissions described in the commit message. The answer is **Buggy**.
"
Essentials,316.json,15c088e6dee3b470b7de80c95f7ec639613593e8,"@@ -1,4 +1,3 @@
 	public void recalculatePermissions()
 	{
-		throw new UnsupportedOperationException(""Not supported yet."");
 	}",NotBuggy,"Bukkit Permission system
New config setting: use-bukkit-permissions
",Buggy,"The commit message indicates the introduction of a new configuration setting related to the Bukkit permission system. The code diff shows the removal of an `UnsupportedOperationException` from the `recalculatePermissions` method. This suggests that the method was previously a placeholder and is now being implemented, likely to support the new Bukkit permission system. While it's not a direct bug fix in the traditional sense, the removal of the exception implies that a previously unimplemented feature is now functional, which could be considered resolving a bug where the permission recalculation was not working. The answer is **Buggy**.
"
Essentials,814.json,ff46b39f2b42abbe4ce8dc39e564f3dc71a2a104,"@@ -1,4 +1,4 @@
-	protected void run(final Server server, final CommandSender sender, final String commandLabel, final String[] args) throws Exception
+	protected void run(final Server server, final CommandSource sender, final String commandLabel, final String[] args) throws Exception
 	{
 		sender.sendMessage(_(""helpConsole""));
 	}",NotBuggy,"Extract CommandSender to CommandSource, this should prevent Ess user object leaks.
",Buggy,"The commit message indicates a change to prevent user object leaks by extracting `CommandSender` to `CommandSource`. The diff shows a change in the `run` method signature, replacing `CommandSender` with `CommandSource`. This suggests a refactoring to improve security and prevent potential leaks, which can be considered a bug fix in a broader sense. The answer is **Buggy**."
commons-io,729.json,55bae88d398feed4a3008ec5e97eb8b85c983b1e,"@@ -1,3 +1,3 @@
     public static String toString(URI uri, Charset encoding) throws IOException {
-        return toString(uri.toURL(), toCharset(encoding));
+        return toString(uri.toURL(), Charsets.toCharset(encoding));
     }",NotBuggy,"[IO-318] Add Charset sister APIs to method that take a String charset name. FileUtils.

git-svn-id: https://svn.apache.org/repos/asf/commons/proper/io/trunk@1307412 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates the addition of Charset sister APIs to methods that take a String charset name, specifically in FileUtils. The diff shows a change in the `toString` method where `toCharset(encoding)` is replaced with `Charsets.toCharset(encoding)`. This suggests a change in how the charset is handled, likely to use the new Charset sister APIs. This change doesn't directly indicate a bug fix, but rather an enhancement or refactoring to use the new APIs. The answer is **NotBuggy**."
commons-io,677.json,6aa00766b9103d75c763160280429af8e1f0549e,"@@ -1,6 +1,6 @@
-    public static void readFully(InputStream input, byte[] buffer, int offset, int length) throws IOException {
-        int actual = read(input, buffer, offset, length);
+    public static void readFully(final InputStream input, final byte[] buffer, final int offset, final int length) throws IOException {
+        final int actual = read(input, buffer, offset, length);
         if (actual != length) {
             throw new EOFException(""Length to read: "" + length + "" actual: "" + actual);
         }
     }",NotBuggy,"Use final where possible.

git-svn-id: https://svn.apache.org/repos/asf/commons/proper/io/trunk@1415850 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates a change to use the `final` keyword where possible. The diff shows that the `final` keyword has been added to the method parameters `input`, `buffer`, `offset`, and `length`, and to the local variable `actual`. This change does not indicate a bug fix, but rather an improvement in code clarity and potentially performance by preventing accidental modification of these variables within the method. The answer is **NotBuggy**."
commons-io,545.json,14b52eb4f630f32c66122977331bd894b9474978,"@@ -1,4 +1,4 @@
-    public ValidatingObjectInputStream accept(ClassNameMatcher m) {
+    public ValidatingObjectInputStream accept(final ClassNameMatcher m) {
         acceptMatchers.add(m);
         return this;
     }",NotBuggy,Use final for parameters.,NotBuggy,"The commit message indicates that the change is to add the `final` keyword to parameters. The diff confirms this change. Adding `final` to parameters prevents them from being reassigned within the method, which can improve code clarity and prevent accidental modification. This is a refactoring change, not a bug fix. The answer is **NotBuggy**.
"
commons-io,1030.json,7c111fecd837f22f1f8c1faa38dbef0f4a0e91ee,"@@ -1,5 +1,5 @@
-    public static <T extends FileVisitor<? super Path>> T visitFileTree(final Path directory, final T visitor)
+    public static <T extends FileVisitor<? super Path>> T visitFileTree(final T visitor, final Path directory)
             throws IOException {
         Files.walkFileTree(directory, visitor);
         return visitor;
     }",NotBuggy,"[IO-632] Add PathUtils for operations on NIO Path.

Refactor for counting longs and big integers. Add cleaning visitor.
",Buggy,"The commit message indicates the addition of PathUtils for NIO Path operations and refactoring for counting longs and big integers, along with adding a cleaning visitor. The provided diff shows a change in the order of arguments in the `visitFileTree` method, swapping the `Path directory` and `T visitor` arguments. This change in argument order could potentially break existing code that relies on the original order, indicating a possible bug fix or at least a change in behavior. While the commit message doesn't explicitly mention a bug fix related to argument order, the refactoring and the potential for breakage suggest that the original argument order might have been incorrect or problematic.

The change in argument order can be considered a bug fix if the original order was causing issues or was inconsistent with other similar methods. Without more context, it's difficult to definitively say if this is a bug fix, but the potential for breakage and the refactoring aspect suggest it might be.

The answer is **Buggy**.
"
commons-io,763.json,b7ec4f7e855ecb77c8bf42fdd97f123d4588edd2,"@@ -1,3 +1,3 @@
-    public <T extends Throwable> List<T> getCauseList(Class<T> clazz) {
+    public <T extends Throwable> List<T> getCauseList(final Class<T> clazz) {
         return (List<T>) causeList;
     }",NotBuggy,Use final.,NotBuggy,"The commit message ""Use final"" suggests that the change involves adding the `final` keyword to a variable or parameter. The diff shows that the `final` keyword has been added to the `clazz` parameter of the `getCauseList` method. This change makes the `clazz` parameter immutable within the scope of the method. This change doesn't fix a bug, but rather improves code clarity and prevents accidental modification of the parameter. The answer is **NotBuggy**.
"
commons-io,1152.json,8db20da2ea53b8598e92fc0672482a74acb06845,"@@ -1,11 +1,7 @@
-    private static void validateMoveParameters(final File src, final File dest) throws FileNotFoundException {
-        if (src == null) {
-            throw new NullPointerException(""Source must not be null"");
-        }
-        if (dest == null) {
-            throw new NullPointerException(""Destination must not be null"");
-        }
-        if (!src.exists()) {
-            throw new FileNotFoundException(""Source '"" + src + ""' does not exist"");
+    private static void validateMoveParameters(final File source, final File destination) throws FileNotFoundException {
+        Objects.requireNonNull(source, ""source"");
+        Objects.requireNonNull(destination, ""destination"");
+        if (!source.exists()) {
+            throw new FileNotFoundException(""Source '"" + source + ""' does not exist"");
         }
     }",NotBuggy,"Use Objects.requireNonNull() instead of custom check.
",NotBuggy,"The commit message indicates a change from a custom null check to using `Objects.requireNonNull()`. The diff confirms this change, replacing the explicit null checks for `src` and `dest` with calls to `Objects.requireNonNull()`. This change simplifies the code and leverages a standard library function for null checking. While it improves code readability and maintainability, it doesn't directly address a bug. It's a refactoring to use a more concise and standard way of performing null checks. The answer is **NotBuggy**."
commons-io,543.json,14b52eb4f630f32c66122977331bd894b9474978,"@@ -1,4 +1,4 @@
-    public ValidatingObjectInputStream accept(Pattern pattern) {
+    public ValidatingObjectInputStream accept(final Pattern pattern) {
         acceptMatchers.add(new RegexpClassNameMatcher(pattern));
         return this;
     }",NotBuggy,Use final for parameters.,NotBuggy,"The commit message indicates a change to use the `final` keyword for parameters. The diff confirms this change by adding `final` to the `pattern` parameter in the `accept` method. This change doesn't fix a bug but rather improves code clarity and prevents accidental modification of the parameter within the method. The answer is **NotBuggy**.
"
commons-io,336.json,46ab2532070e129ac54508bfb7a5be33a6b797bc,"@@ -1,12 +1,12 @@
     public int read() throws IOException {
         int c = EOF;
         while (reader != null) {
             c = reader.read();
             if (c == EOF) {
-                reader = readers.hasNext() ? readers.next() : null;
+                reader = nextReader();
             } else {
                 break;
             }
         }
         return c;
     }",NotBuggy,"[IO-458] Add a SequenceReader similar to java.io.SequenceInputStream.

Refactor commmon code in a private method.",NotBuggy,"The commit message indicates the addition of a `SequenceReader` and a refactoring of common code into a private method. The diff shows the `read()` method being modified to use `nextReader()` instead of directly accessing the `readers` iterator. This suggests that the logic for obtaining the next reader has been encapsulated within the `nextReader()` method, which aligns with the refactoring mentioned in the commit message. The change itself doesn't inherently indicate a bug fix, but rather an improvement in code organization and maintainability. Therefore, the answer is **NotBuggy**.
"
commons-io,475.json,1170279dc85c2bbdea594b053c56b379a72a2104,"@@ -1,53 +1,53 @@
         private String readLine() throws IOException {
 
             String line = null;
             int newLineMatchByteCount;
 
-            boolean isLastFilePart = (no == 1);
+            boolean isLastFilePart = no == 1;
 
             int i = currentLastBytePos;
             while (i > -1) {
 
                 if (!isLastFilePart && i < avoidNewlineSplitBufferSize) {
                     // avoidNewlineSplitBuffer: for all except the last file part we
                     // take a few bytes to the next file part to avoid splitting of newlines
                     createLeftOver();
                     break; // skip last few bytes and leave it to the next file part
                 }
 
                 // --- check for newline ---
                 if ((newLineMatchByteCount = getNewLineMatchByteCount(data, i)) > 0 /* found newline */) {
                     final int lineStart = i + 1;
                     int lineLengthBytes = currentLastBytePos - lineStart + 1;
 
                     if (lineLengthBytes < 0) {
                         throw new IllegalStateException(""Unexpected negative line length=""+lineLengthBytes);
                     }
                     byte[] lineData = new byte[lineLengthBytes];
                     System.arraycopy(data, lineStart, lineData, 0, lineLengthBytes);
 
                     line = new String(lineData, encoding);
 
                     currentLastBytePos = i - newLineMatchByteCount;
                     break; // found line
                 }
 
                 // --- move cursor ---
                 i -= byteDecrement;
 
                 // --- end of file part handling ---
                 if (i < 0) {
                     createLeftOver();
                     break; // end of file part
                 }
             }
 
             // --- last file part handling ---
             if (isLastFilePart && leftOver != null) {
                 // there will be no line break anymore, this is the first line of the file
                 line = new String(leftOver, encoding);
                 leftOver = null;
             }
 
             return line;
         }",NotBuggy,"Remove noisy and unnecessary parens. Especially painful in return statements: ""return (foo + bar);"" vs. ""return foo + bar;""

git-svn-id: https://svn.apache.org/repos/asf/commons/proper/io/trunk@1302748 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates the removal of unnecessary parentheses, which is a code style improvement rather than a bug fix. The diff shows several instances where parentheses around boolean expressions have been removed. These changes improve readability but do not address any underlying bugs. The answer is **NotBuggy**."
commons-io,1046.json,154cb7648ffde863b2548eba10b330c51a103c6a,"@@ -1,4 +1,4 @@
-    public FileVisitResult preVisitDirectory(final Path dir, final BasicFileAttributes attrs) throws IOException {
-        super.preVisitDirectory(dir, attrs);
+    public FileVisitResult preVisitDirectory(final Path dir, final BasicFileAttributes attributes) throws IOException {
+        super.preVisitDirectory(dir, attributes);
         return accept(dir) ? FileVisitResult.CONTINUE : FileVisitResult.SKIP_SUBTREE;
     }",NotBuggy,"[IO-632] Add PathUtils for operations on NIO Path.

Refactor to add PathUtils.copyDirectory(Path, Path, CopyOption...)
",NotBuggy,"The commit message indicates the addition of `PathUtils` for NIO `Path` operations, specifically `copyDirectory`. The diff shows a change in the `preVisitDirectory` method, where the parameter name `attrs` is changed to `attributes`. This is a simple refactoring of the variable name and does not indicate a bug fix. The functionality remains the same. The answer is **NotBuggy**."
commons-io,627.json,3e947380d02dbd6d8f4e7590e3282498a16d6d32,"@@ -1,3 +1,3 @@
-    public static BufferedWriter asBufferedWriter(final Writer writer) {
+    public static BufferedWriter buffer(final Writer writer) {
         return writer instanceof BufferedWriter ? (BufferedWriter) writer : new BufferedWriter(writer);
     }",NotBuggy,"IO-233 IO-330 Add Methods for Buffering Streams/Writers To IOUtils

git-svn-id: https://svn.apache.org/repos/asf/commons/proper/io/trunk@1483859 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates the addition of methods for buffering streams/writers to IOUtils. The code change renames the method `asBufferedWriter` to `buffer`. This change in name does not indicate a bug fix, but rather a change in API. The answer is **NotBuggy**."
commons-io,353.json,56f2a758983611adea7308e90ae9934c973efa72,"@@ -1,17 +1,17 @@
     public int read(final byte[] pBuffer, final int pOffset, final int pLength) throws IOException {
         int result = 0;
         IOException ioe = null;
         try {
             result = super.read(pBuffer, pOffset, pLength);
-        } catch (IOException pException) {
+        } catch (final IOException pException) {
             ioe = pException;
         }
         if (ioe != null) {
             noteError(ioe);
         } else if (result == -1) {
             noteFinished();
         } else if (result > 0) {
             noteDataBytes(pBuffer, pOffset, result);
         }
         return result;
     }",NotBuggy,Consistently use the final modifier with local variables.,NotBuggy,"The commit message indicates a change to consistently use the `final` modifier with local variables. The diff shows the addition of the `final` modifier to the `pException` variable in the catch block. This change doesn't seem to be related to fixing a bug, but rather to improve code style and enforce immutability where possible. The answer is **NotBuggy**."
commons-io,754.json,56f2a758983611adea7308e90ae9934c973efa72,"@@ -1,12 +1,12 @@
     public static void writeChunked(final char[] data, final Writer output) throws IOException {
         if (data != null) {
             int bytes = data.length;
             int offset = 0;
             while (bytes > 0) {
-                int chunk = Math.min(bytes, DEFAULT_BUFFER_SIZE);
+                final int chunk = Math.min(bytes, DEFAULT_BUFFER_SIZE);
                 output.write(data, offset, chunk);
                 bytes -= chunk;
                 offset += chunk;
             }
         }
     }",NotBuggy,Consistently use the final modifier with local variables.,NotBuggy,"The commit message indicates a change to consistently use the `final` modifier with local variables. The diff shows the addition of the `final` modifier to the `chunk` variable within the `writeChunked` method. This change doesn't appear to be related to fixing a bug; rather, it's a code style improvement to enhance code clarity and prevent unintended modification of the `chunk` variable. The answer is **NotBuggy**."
commons-io,1022.json,11f0abe7a3fb6954b2985ca4ab0697b2fb489e84,"@@ -1,4 +1,4 @@
     public static boolean directoryContentEquals(final Path path1, final Path path2, final int maxDepth,
-            LinkOption[] linkOptions, FileVisitOption... fileVisitOptions) throws IOException {
+            LinkOption[] linkOptions, FileVisitOption[] fileVisitOptions) throws IOException {
         return new RelativeSortedPaths(path1, path2, maxDepth, linkOptions, fileVisitOptions).equals;
     }",NotBuggy,"[IO-648] Implement directory content equality.

Integrate feedback from Michael Osipov.",NotBuggy,"The commit message indicates the implementation of directory content equality, and the diff shows a change in the parameter type of the `fileVisitOptions` argument from a varargs to an array. While this change might be related to the implementation, it doesn't inherently indicate a bug fix. It's more likely a change in the API signature or a refactoring of the method. Therefore, it's not necessarily a bug fix. The answer is **NotBuggy**."
commons-io,1020.json,11f0abe7a3fb6954b2985ca4ab0697b2fb489e84,"@@ -1,34 +1,34 @@
     public static boolean directoryAndFileContentEquals(final Path path1, final Path path2,
-            final LinkOption[] linkOptions, final OpenOption[] openOptions, final FileVisitOption... fileVisitOption)
+            final LinkOption[] linkOptions, final OpenOption[] openOptions, final FileVisitOption[] fileVisitOption)
             throws IOException {
         // First walk both file trees and gather normalized paths.
         if (path1 == null && path2 == null) {
             return true;
         }
         if (path1 == null ^ path2 == null) {
             return false;
         }
         if (!Files.exists(path1) && !Files.exists(path2)) {
             return true;
         }
         final RelativeSortedPaths relativeSortedPaths = new RelativeSortedPaths(path1, path2, Integer.MAX_VALUE,
                 linkOptions, fileVisitOption);
         // If the normalized path names and counts are not the same, no need to compare contents.
         if (!relativeSortedPaths.equals) {
             return false;
         }
         // Both visitors contain the same normalized paths, we can compare file contents.
         final List<Path> fileList1 = relativeSortedPaths.relativeFileList1;
         final List<Path> fileList2 = relativeSortedPaths.relativeFileList2;
         for (Path path : fileList1) {
             final int binarySearch = Collections.binarySearch(fileList2, path);
             if (binarySearch > -1) {
                 if (!fileContentEquals(path1.resolve(path), path2.resolve(path), linkOptions, openOptions)) {
                     return false;
                 }
             } else {
                 throw new IllegalStateException(String.format(""Unexpected mismatch.""));
             }
         }
         return true;
     }",NotBuggy,"[IO-648] Implement directory content equality.

Integrate feedback from Michael Osipov.",Buggy,"The commit message indicates the implementation of directory content equality, along with feedback integration. The code diff focuses on the `directoryAndFileContentEquals` method, which compares the contents of two directories. The change in the parameter type `FileVisitOption... fileVisitOption` to `FileVisitOption[] fileVisitOption` suggests a correction in how the variable arguments are handled, which could be related to fixing a bug where the method wasn't correctly processing file visit options. The rest of the code ensures that the file paths and counts are the same before comparing file contents, and throws an `IllegalStateException` if there's an unexpected mismatch. This exception handling, along with the parameter type correction, points towards bug fixing. The answer is **Buggy**.
"
wicket,134.json,2e1812d71cd5be231e57f82f7132f62295ef07c0,"@@ -1,4 +1,14 @@
 		public void detach(final IRequestCycle requestCycle)
 		{
 			delegate.detach(requestCycle);
+
+			if (logData == null)
+			{
+				ILogData delegateData;
+				if (delegate instanceof ILoggableRequestHandler)
+					delegateData = ((ILoggableRequestHandler)delegate).getLogData();
+				else
+					delegateData = new NoLogData();
+				logData = new DelegateLogData(delegateData);
+			}
 		}",NotBuggy,"WICKET-4074 RequestLogger needs a clear separation of concerns, courtesy of Emond Papegaaij


git-svn-id: https://svn.apache.org/repos/asf/wicket/trunk@1183064 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a separation of concerns in the RequestLogger. The diff shows that the `detach` method now handles the case where `logData` is null. It retrieves `logData` from the delegate if the delegate is an `ILoggableRequestHandler`, otherwise it creates a `NoLogData` instance. This ensures that `logData` is always initialized, preventing potential NullPointerExceptions or unexpected behavior. This change addresses a potential issue related to uninitialized data, which can be considered a bug fix. The answer is **Buggy**."
wicket,5717.json,83f3fa8231c877705edf3e7250a11fe18252ac1e,"@@ -1,5 +1,5 @@
-	public NumberTextField setMaximum(final Double maximum)
+	public NumberTextField<N> setMaximum(final N maximum)
 	{
 		this.maximum = maximum;
 		return this;
 	}",NotBuggy,"WICKET-3241 Add support for the new HTML 5 input types

Relax the type of (Number|Range)TextField.
Now it can be any subtype of java.lang.Number

git-svn-id: https://svn.apache.org/repos/asf/wicket/trunk@1053609 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates the addition of support for new HTML 5 input types by relaxing the type of `NumberTextField`. The diff shows a change in the `setMaximum` method's parameter type from `Double` to a generic type `N`, which is a subtype of `java.lang.Number`. This allows the `NumberTextField` to accept any subtype of `java.lang.Number`, aligning with the commit message's intent to relax the type constraint. This change is not directly indicative of a bug fix, but rather an enhancement or feature addition. The answer is **NotBuggy**."
wicket,8450.json,2ec1b5524431393274d459e2da2649dcf4446233,"@@ -1,4 +1,4 @@
 	public String getCharacterEncoding()
 	{
-		return null;
+		return characterEncoding;
 	}",NotBuggy,"WICKET-6063 Add support for WebSocketRequest#getUrl() and other properties which are available in the handshake request
",Buggy,"The commit message indicates the addition of support for `WebSocketRequest#getUrl()` and other properties available in the handshake request. The diff shows a change in the `getCharacterEncoding()` method, where it now returns the `characterEncoding` field instead of `null`. This suggests that the original implementation was incomplete or incorrect, and the change is aimed at providing the correct character encoding information. This can be considered a bug fix, as the previous implementation was not providing the expected or correct value. The answer is **Buggy**.
"
wicket,11326.json,d2be7fd368cdca02dd7481490de587bcc1a6e4cd,"@@ -1,13 +1,15 @@
-	protected OrderByLink newOrderByLink(String id, String property, ISortStateLocator stateLocator)
+	protected OrderByLink newOrderByLink(final String id, final String property,
+		final ISortStateLocator stateLocator)
 	{
-		return new OrderByLink(id, property, stateLocator, OrderByLink.VoidCssProvider.getInstance())
+		return new OrderByLink(id, property, stateLocator,
+			OrderByLink.VoidCssProvider.getInstance())
 		{
 			private static final long serialVersionUID = 1L;
 
 			@Override
 			protected void onSortChanged()
 			{
 				OrderByBorder.this.onSortChanged();
 			}
 		};
 	}",NotBuggy,"let eclipse do a final cleanup. No functional change

git-svn-id: https://svn.apache.org/repos/asf/wicket/trunk@1062370 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,The commit message indicates a cleanup with no functional change. The diff introduces `final` keywords to method parameters. This change does not alter the functionality of the code. The answer is **NotBuggy**.
wicket,2324.json,d1710298c7e371f260299f732c58d0bf4d647161,"@@ -1,9 +1,9 @@
 	public boolean getStatelessHint(Component component)
 	{
-		if (this instanceof IBehaviorListener)
+		if (this instanceof IRequestListener)
 		{
 			// this behavior implements a callback interface, so it cannot be stateless
 			return false;
 		}
 		return true;
 	}",NotBuggy,"WICKET-6137 remove RequestListenerInterface
",Buggy,The commit message indicates the removal of `RequestListenerInterface`. The code diff changes `IBehaviorListener` to `IRequestListener` in the `getStatelessHint` method. This suggests that `IBehaviorListener` was replaced by `IRequestListener`. This change is likely a bug fix or refactoring related to how statelessness is determined for components using request listeners. The change ensures that the correct interface is checked to determine if a component is stateless. The answer is **Buggy**.
wicket,1688.json,9220f60d68e223ff1bf5d0dd5cd0f3112a1a2396,"@@ -1,4 +1,21 @@
 	public IResourceCachingStrategy getCachingStrategy()
 	{
+		if (resourceCachingStrategy == null)
+		{
+			final IResourceVersion resourceVersion;
+
+			if (application.usesDevelopmentConfig())
+			{
+				// use file last modified for resource cache keys
+				resourceVersion = new RequestCycleCachedResourceVersion(new LastModifiedResourceVersion());
+			}
+			else
+			{
+				// use md5 message digest for resource cache keys
+				resourceVersion = new CachingResourceVersion(new MessageDigestResourceVersion());
+			}
+			resourceCachingStrategy =
+				new FilenameWithVersionResourceCachingStrategy(resourceVersion);
+		}
 		return resourceCachingStrategy;
 	}",NotBuggy,"WICKET-3846 rework resource caching so it will by default work in clusters, also made the whole caching stuff pluggable and easier to extend

git-svn-id: https://svn.apache.org/repos/asf/wicket/trunk@1141298 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates a rework of resource caching to improve cluster compatibility and make it more pluggable and extensible. The code diff shows that the `getCachingStrategy` method is being modified to initialize the `resourceCachingStrategy` if it's null. It also introduces different resource versioning strategies based on whether the application is in development or production mode. In development mode, it uses `LastModifiedResourceVersion`, and in production mode, it uses `MessageDigestResourceVersion`. This change doesn't explicitly fix a bug but rather enhances the caching mechanism. The changes are related to improving the caching strategy and making it more robust and configurable. Therefore, the answer is **NotBuggy**."
wicket,3339.json,d06f84d1b87011e5c152c5fb3f05ae50c1c58cda,"@@ -1,4 +1,4 @@
-			public Object getContainerRequest()
-			{
-				return null;
-			}

+				public Object getContainerRequest()
+				{
+					return null;
+				}",NotBuggy,"WICKET-5698 WebApplication#unmount() unmounts the whole compound mapper if some of its inner ones matches
",NotBuggy,The diff consists of a minor formatting change (indentation). The functionality remains the same. The answer is **NotBuggy**.
wicket,11304.json,ed7c99768ab5764a76d77a1282d28b2f048fd1b8,"@@ -1,35 +1,43 @@
 			protected void populateItem(Item<IColumn<T, S>> item)
 			{
 				final IColumn<T, S> column = item.getModelObject();
 
-				WebMarkupContainer header = null;
+				WebMarkupContainer header;
 
 				if (column.isSortable())
 				{
 					header = newSortableHeader(""header"", column.getSortProperty(), stateLocator);
 				}
 				else
 				{
 					header = new WebMarkupContainer(""header"");
 				}
 
 				if (column instanceof IStyledColumn)
 				{
 					CssAttributeBehavior cssAttributeBehavior = new DataTable.CssAttributeBehavior()
 					{
 						private static final long serialVersionUID = 1L;
 
 						@Override
 						protected String getCssClass()
 						{
 							return ((IStyledColumn<?, S>)column).getCssClass();
 						}
 					};
 
 					header.add(cssAttributeBehavior);
 				}
 
+				if (column.getColspan() > 1) {
+					header.add(AttributeModifier.replace(""colspan"", column.getColspan()));
+				}
+
+				if (column.getRowspan() > 1) {
+					header.add(AttributeModifier.replace(""rowspan"", column.getRowspan()));
+				}
+
 				item.add(header);
 				item.setRenderBodyOnly(true);
 				header.add(column.getHeader(""label""));
 			}",NotBuggy,"WICKET-6095 Multiline headers in DataTable
",NotBuggy,"The commit message ""WICKET-6095 Multiline headers in DataTable"" suggests that the changes are related to supporting multiline headers in a DataTable component. The diff introduces `colspan` and `rowspan` attributes to the header based on the values returned by `column.getColspan()` and `column.getRowspan()`. This addition allows a column header to span multiple columns or rows, which is necessary for implementing multiline headers. The changes directly address the feature described in the commit message. The answer is **NotBuggy**."
wicket,3741.json,86fa5a7ac1e5e0aab3a2f47d423ffc53afb9f49c,"@@ -1,23 +1,23 @@
 	public BufferedWebResponse get(Object key)
 	{
 		BufferedWebResponse result = null;
 		Value value;
 		synchronized (this)
 		{
 			value = (Value)super.get(key);
 		}
 		if (value != null)
 		{
-			Duration elapsedTime =  Duration.between(value.creationTime, Instant.now());
-			if (lifetime.compareTo(elapsedTime) > 0)
+			Duration elapsedTime = Time.now().subtract(value.creationTime);
+			if (lifetime.greaterThan(elapsedTime))
 			{
 				result = value.response;
 			}
 			else
 			{
 				// expired, remove it
 				remove(key);
 			}
 		}
 		return result;
 	}",NotBuggy,"Revert ""Removed wicket-util date entities""

This reverts commit a8bb129af777670e6315142945155de3813d7b83.
",Buggy,"The commit message indicates a revert of a previous commit that removed ""wicket-util date entities."" The diff shows changes in how elapsed time is calculated. Specifically, it replaces `Duration.between(value.creationTime, Instant.now())` with `Time.now().subtract(value.creationTime)` and `lifetime.compareTo(elapsedTime) > 0` with `lifetime.greaterThan(elapsedTime)`. These changes suggest a potential issue with the original implementation of time calculation or comparison, which could be related to a bug. The revert aims to restore the previous functionality, implying that the removed code was necessary or that its removal introduced a bug. The answer is **Buggy**."
wicket,6083.json,9e028af34cc432ddbffd6e09e82bf46d6226c0c8,"@@ -1,4 +1,5 @@
 				public boolean getDefaultFormProcessing()
 				{
+					// do not process the whole form
 					return false;
 				}",NotBuggy,"WICKET-6348 minor addition: update raw input when component is not contained in form
this wasn't done in 7.x either, but for consistency the raw input should be updated, so it is available when a validation fails
",Buggy,"The commit message indicates a minor addition to update raw input when a component is not contained in a form, ensuring consistency and availability of raw input during validation failures. The code diff shows a change in the `getDefaultFormProcessing()` method, where `return false;` is used, along with a comment `// do not process the whole form`. This change suggests that the component is intentionally configured not to process the entire form. The commit message and the code change align, indicating a bug fix related to form processing and input handling. The answer is **Buggy**.
"
wicket,4077.json,7c40e4dafa75973bf99b34567d977f310a5f02d0,"@@ -1,9 +1,9 @@
 	protected boolean shouldFilter(AppendingStringBuffer responseBuffer)
 	{
 		// To avoid reading the whole buffer for non-Ajax responses
 		// read just the first N chars. A candidate can start with:
 		// <?xml version=""1.0"" encoding=""UTF-8"" standalone=""yes""?><ajax-response>
 		int min = Math.min(150, responseBuffer.length());
 		String firstNChars = responseBuffer.substring(0, min);
-		return firstNChars.contains(XmlAjaxResponse.START_ROOT_ELEMENT);
+		return firstNChars.contains(XmlPartialPageUpdate.START_ROOT_ELEMENT);
 	}",NotBuggy,"WICKET-5929 renamed *AjaxResponse to *PartialPageUpdate; moved all XML related encoding into XmlPartialPageUpdate
",NotBuggy,"The commit message indicates a renaming and moving of XML-related encoding. The code change reflects this by replacing `XmlAjaxResponse.START_ROOT_ELEMENT` with `XmlPartialPageUpdate.START_ROOT_ELEMENT`. This change is directly related to the renaming mentioned in the commit message. There is no indication of a bug fix in the traditional sense, but rather a refactoring and reorganization of code. The answer is **NotBuggy**."
wicket,39.json,64024788d5122cfe9a9b173b165ac7c069b7f665,"@@ -1,14 +1,14 @@
 		public boolean equals(Object obj)
 		{
 			if (this == obj)
 			{
 				return true;
 			}
 			if (obj instanceof QueryParameter == false)
 			{
 				return false;
 			}
 			QueryParameter rhs = (QueryParameter)obj;
-			return Objects.equal(getName(), rhs.getName()) &&
-				Objects.equal(getValue(), rhs.getValue());
+			return Objects.equal(getName(), rhs.getName())
+					&& Objects.equal(getValue(), rhs.getValue());
 		}",NotBuggy,"wip on decoupling and extracting wicket-request. also the request encoding is now properly propagated to url handling.

git-svn-id: https://svn.apache.org/repos/asf/wicket/trunk@924375 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates work in progress on decoupling and extracting wicket-request, and mentions proper propagation of request encoding to URL handling. The provided diff shows a change in the `equals` method of the `QueryParameter` class. The change involves reformatting the return statement to improve readability by splitting it into two lines. This change does not appear to be related to a bug fix, but rather a code style improvement. The functionality of the `equals` method remains the same. The answer is **NotBuggy**."
wicket,10296.json,83740b9e5f9ca91ae185bc5683b6c94a5349b85b,"@@ -1,19 +1,20 @@
 	private Form<?> getCallbackForm()
 	{
 		Boolean insideModal = form.visitParents(ModalWindow.class,
 			new IVisitor<ModalWindow, Boolean>()
 			{
+				@Override
 				public void component(final ModalWindow object, final IVisit<Boolean> visit)
 				{
 					visit.stop(true);
 				}
 			});
 		if ((insideModal != null) && insideModal)
 		{
 			return form;
 		}
 		else
 		{
 			return form.getRootForm();
 		}
 	}",NotBuggy,"Add @Override to all overridden methods

This includes all places @Override is permitted in Java 6, since Wicket
6 requires Java 6.
",NotBuggy,"The commit message indicates that the changes add `@Override` annotations to overridden methods. The diff shows the addition of `@Override` to the `component` method within an anonymous inner class. This change improves code clarity and maintainability by explicitly marking overridden methods. However, it doesn't fix any functional bugs. The answer is **NotBuggy**."
wicket,2209.json,ca03c252b5745bdccfb7c8cdffcc705846593ec1,"@@ -1,5 +1,6 @@
-	public void removePage(final String sessionId, final int pageId)
+	public void removePage(IPageContext context, IManageablePage page)
 	{
-		pagesCache.removePage(sessionId, pageId);
-		removePageData(sessionId, pageId);
+		getRequestData(context).remove(page);
+
+		getDelegate().removePage(context, page);
 	}",NotBuggy,"WICKET-6563 new IPageStore implementation

this closes #283
",Buggy,"The commit message indicates a new implementation of IPageStore, and the diff shows a change in the `removePage` method's signature and implementation. The old implementation removed a page based on sessionId and pageId from a cache and data store. The new implementation takes an IPageContext and IManageablePage, removes the page from request data, and delegates the removal to another component. This suggests a significant change in how pages are stored and managed, likely addressing limitations or issues in the previous approach. The change in method signature and delegation to another component indicates a bug fix or enhancement related to page management. The answer is **Buggy**."
wicket,4773.json,fe1e621081ebe22c22f6e5b5fa98d51e44d50d76,"@@ -1,4 +1,4 @@
 	public static <Z> CompoundPropertyModel<Z> of(IModel<Z> model)
 	{
-		return new CompoundPropertyModel<Z>(model);
+		return new CompoundPropertyModel<>(model);
 	}",NotBuggy,"Java 7 diamonds
",NotBuggy,"The commit message ""Java 7 diamonds"" suggests the change involves using the diamond operator `<>` for type inference, a feature introduced in Java 7. The diff confirms this by showing the replacement of `new CompoundPropertyModel<Z>(model)` with `new CompoundPropertyModel<>(model)`. This change simplifies the code and leverages Java 7's type inference capabilities. It doesn't fix a bug but rather improves code readability and conciseness. The answer is **NotBuggy**.
"
vraptor4,1191.json,8757c99184f25ee826446209ff419f1120cf6131,"@@ -1,4 +1,4 @@
 	public void showAllMusicsAsHTTP() {
 		result.use(http()).body(""<p class=\""content\"">""+
-			dao.listAll().toString()+""</p>"");
+			musicDao.listAll().toString()+""</p>"");
 	}",NotBuggy,"Improving validation to musicjungle
",Buggy,"The commit message indicates an improvement in validation related to ""musicjungle."" However, the provided diff shows a change in the `showAllMusicsAsHTTP` method, specifically replacing `dao.listAll()` with `musicDao.listAll()`. This suggests a correction in the data access object being used, likely resolving an issue where the incorrect data source was being accessed. This can be seen as a bug fix. The answer is **Buggy**."
vraptor4,142.json,8660dccf28a2ebee23fc51fe61980b0f8ca4639b,"@@ -1,20 +1,20 @@
-		public String toString() {
+		public String getLink() {
 			Method method = null;
 
 			if (getMethodsAmountWithSameName() > 1) {
 				method = new Mirror().on(controller).reflect().method(methodName).withArgs(getClasses(args));
 				if (method == null && args.isEmpty()) {
 					throw new IllegalArgumentException(""Ambiguous method '"" + methodName + ""' on "" + controller + "". Try to add some parameters to resolve ambiguity, or use different method names."");
 				}
 			} else {
 				method = findMethodWithName(controller, methodName);
 			}
 
 			if(method == null) {
 				throw new IllegalArgumentException(
 					String.format(""There are no methods on %s named '%s' that receives args of types %s"",
 							controller, methodName, Arrays.toString(getClasses(args))));
 			}
 
 			return context.getContextPath() + router.urlFor(controller, method, getArgs(method));
 		}",NotBuggy,"Linker does not need to be a Map anymore
",NotBuggy,"The diff renames the `toString` method to `getLink`. The original `toString` method constructs a URL based on the controller, method name, and arguments. The change doesn't seem to be fixing a bug, but rather refactoring or renaming a method to better reflect its purpose. The answer is **NotBuggy**."
vraptor4,1047.json,634361fe0135b236651256d7c97945dec4f2c26d,"@@ -1,8 +1,8 @@
 	public void start() {
 		ControllerMethod method = controllerMethod.get();
-		stackStartingEvent.fire(new StackStarting(method));
+		interceptorsReadyEvent.fire(new InterceptorsReady(method));
 		LinkedList<InterceptorHandler> handlers = cache.getInterceptorHandlers();
 		internalStack.addFirst(handlers.iterator());
 		this.next(method, controllerInstance.get().getController());
 		internalStack.poll();
 	}",NotBuggy,"Renaming all events, to add some naming consistency
",NotBuggy,"The commit message indicates a renaming of events for naming consistency. The diff shows that `stackStartingEvent.fire(new StackStarting(method))` is replaced with `interceptorsReadyEvent.fire(new InterceptorsReady(method))`. This change reflects a renaming of the event and its associated class. There is no indication of a bug fix in the code. The answer is **NotBuggy**.
"
vraptor4,1127.json,d63f531e2548c47d274077d9938be60c100d3caa,"@@ -1,5 +1,3 @@
 	private boolean isWebsocketRequest(HttpServletRequest request) {
-		// according to the Websocket spec (https://tools.ietf.org/html/rfc6455): The WebSocket Protocol 
-		// 5. The request MUST contain an |Upgrade| header field whose value MUST include the ""websocket"" keyword.
-	    return request.getHeader(""Upgrade"") != null && request.getHeader(""Upgrade"").toLowerCase().contains(""websocket"");
+		return request.getHeader(""Upgrade"") != null && request.getHeader(""Upgrade"").toLowerCase().contains(""websocket"");
 	}",NotBuggy,Moving comments within the method to the method's javadoc.,NotBuggy,"The commit message indicates that comments within the method are being moved to the method's Javadoc. The diff shows that the comments are indeed removed from the method body. This change does not indicate a bug fix, but rather an improvement in code documentation and readability. The answer is **NotBuggy**."
vraptor4,800.json,68bc254803a4e712c85d525df1319f1a6d68600c,"@@ -1,7 +1,7 @@
-	private File checkFile(File file) throws IOException {
+	private File checkFile(File file) throws FileNotFoundException {
 		if (!file.exists()) {
 			throw new FileNotFoundException(""File "" + file.getName() + ""doesn't exists"");
 		}
 		
 		return file;
 	}",NotBuggy,"Downgrading exception
",Buggy,"The commit message ""Downgrading exception"" suggests a change in the type of exception being thrown. The diff confirms this by changing the `checkFile` method's `throws` declaration from `IOException` to `FileNotFoundException`. Since `FileNotFoundException` is a subclass of `IOException`, this change represents a narrowing of the exception type. This is often done to provide more specific information about the error condition, which can be considered a bug fix or improvement in error handling. The answer is **Buggy**.
"
vraptor4,601.json,41ba6271a5f50496c4ada07f67c52b27ca15ef33,"@@ -1,3 +1,3 @@
 	public void indented() {
-		builder.setPrettyPrinting();
+		getGsonBuilder().setPrettyPrinting();
 	}",NotBuggy,"Uses protected getter for GsonBuilder to allow for extension
",NotBuggy,The commit message indicates a change to allow for extension by using a protected getter for GsonBuilder. The diff shows that `builder.setPrettyPrinting()` is replaced with `getGsonBuilder().setPrettyPrinting()`. This change doesn't fix a bug but rather improves the extensibility of the class. The answer is **NotBuggy**.
vraptor4,62.json,8660dccf28a2ebee23fc51fe61980b0f8ca4639b,"@@ -1,3 +1,3 @@
-		protected Map<Object, Linker> delegate() {
-			return Collections.emptyMap();
-		}

+	protected Map<Class<?>, Object> delegate() {
+		return Collections.emptyMap();
+	}",NotBuggy,"Linker does not need to be a Map anymore
",NotBuggy,"The commit message indicates a change in the type of the `Linker`. The diff shows that the return type of the `delegate()` method has been changed from `Map<Object, Linker>` to `Map<Class<?>, Object>`. This change suggests a modification to the data structure used for linking, potentially to improve type safety or performance. However, without more context, it's difficult to determine if this change is related to a bug fix. It could be a refactoring or optimization. Since there is no clear indication of a bug fix, the answer is **NotBuggy**."
vraptor4,98.json,45b2bcef87bdd4c342f05b0134a3231749dc380d,"@@ -1,5 +1,5 @@
-	private <T> void includeParametersInFlash(final Class<T> type, Method method, Object[] args) {
+	protected <T> void includeParametersInFlash(final Class<T> type, Method method, Object[] args) {
 		if (args != null && args.length != 0) {
 			flash.includeParameters(DefaultControllerMethod.instanceFor(type, method), args);
 		}
 	}",NotBuggy,"Changing visibility of methods includeParametersInFlash and acceptsHttpGet
",NotBuggy,The commit message indicates a change in the visibility of methods `includeParametersInFlash` and `acceptsHttpGet`. The provided diff only shows a change in the visibility of the `includeParametersInFlash` method from `private` to `protected`. This change doesn't inherently indicate a bug fix. It's likely a refactoring or a change in API design. The answer is **NotBuggy**.
vraptor4,1124.json,68ec20ba9f131586d351821b75a9d3e7c842ef4c,"@@ -1,7 +1,8 @@
 	private URL getResource(String path) throws ServletException {
 		try {
 			return servletContext.getResource(path);
 		} catch (MalformedURLException e) {
-			throw new ServletException(""Something went wrong when trying to locate a beans.xml file"", e);
+			logger.error(""Something went wrong when trying to locate a beans.xml file"", e);
+			return null;
 		}
 	}",NotBuggy,"returning null instead of rethrowing exception
",Buggy,"The commit message indicates a change from throwing an exception to returning null. The code diff confirms this: the `MalformedURLException` is now caught, logged as an error, and null is returned instead of re-throwing a `ServletException`. This suggests a bug fix where the original exception handling was causing issues, possibly interrupting the application's flow unnecessarily. The change to return null allows the application to continue, potentially with a fallback mechanism. The answer is **Buggy**."
vraptor4,853.json,8fbc9e23ee5038a35159f408a961f98de86b562a,"@@ -1,6 +1,6 @@
 	public <V, K> CacheStore<K, V> createCacheWrapper(int capacity) {
 		Cache<K, V> guavaCache = CacheBuilder.newBuilder()
 			.maximumSize(capacity)
 			.build();
-		return new GuavaCacheWrapper<K,V>(guavaCache);
+		return new GuavaCacheWrapper<>(guavaCache);
 	}",NotBuggy,"squid:S1192, squid:S2293 - String literals should not be duplicated, The diamond operator should be used
",NotBuggy,"The commit message indicates improvements related to code style and readability, specifically addressing duplicated string literals and suggesting the use of the diamond operator. The provided diff shows a change where the diamond operator `<>` is used in the `GuavaCacheWrapper` constructor, replacing the explicit type arguments `<K,V>`. This aligns with the commit message's suggestion to use the diamond operator. This change improves code conciseness and readability without altering the functionality. The answer is **NotBuggy**.
"
vraptor4,645.json,fc1de5e5d28c1b21ea5c9e87120c1cfec3b1e1ee,"@@ -1,4 +1,4 @@
 	public Serializer recursive() {
-		this.serializee.setRecursive(true);
+		builder.getSerializee().setRecursive(true);
 		return this;
 	}",NotBuggy,"Reusing Serializee from builder
",NotBuggy,"The commit message ""Reusing Serializee from builder"" suggests a change in how the `Serializee` object is accessed or managed. The diff shows that instead of directly modifying `this.serializee`, the code now uses `builder.getSerializee()` to obtain and modify the `Serializee` instance. This indicates a change in object ownership or access patterns, likely to improve encapsulation or manage object lifecycles more effectively. It doesn't inherently suggest a bug fix, but rather a refactoring or architectural change. The answer is **NotBuggy**."
vraptor4,113.json,62fe19ddb7501c4371a8646e2f4316cb70ee0eaf,"@@ -1,7 +1,7 @@
 	public Object[] consumeParameters(ControllerMethod method) {
-		Object[] args = (Object[]) session.getAttribute(nameFor(method));
-		if (args != null) {
-			session.removeAttribute(nameFor(method));
-		}
+		String param = nameFor(method);
+		Object[] args = (Object[]) session.getAttribute(param);
+		session.removeAttribute(param);
+
 		return args;
 	}",NotBuggy,"Removing unnecessary statement
",NotBuggy,"The commit message indicates the removal of an unnecessary statement. The diff shows that the code was refactored to remove a conditional check (`if (args != null)`) before removing an attribute from the session. The attribute is now unconditionally removed. If `args` is null, the `getAttribute` method will return null, and the `removeAttribute` method will still execute without error. The removal of the conditional statement suggests that it was deemed unnecessary, likely because the `removeAttribute` method handles the case where the attribute doesn't exist gracefully. This change simplifies the code and removes a potentially redundant check. The change doesn't seem to fix a bug, but rather to simplify the code. The answer is **NotBuggy**."
vraptor4,881.json,743c3c430bf1b46974df6ef038892a42ef97c667,"@@ -1,6 +1,6 @@
 	public Map<String, Collection<Message>> getGrouped() {
 		if (grouped == null) {
-			grouped = Multimaps.index(delegate, new GroupByCategory()).asMap();
+			grouped = index(delegate, groupByCategory).asMap();
 		}
 		return grouped;
 	}",NotBuggy,"Static instances and static import
",NotBuggy,"The commit message ""Static instances and static import"" suggests changes related to static members and imports. The diff replaces `Multimaps.index` with `index`, which implies a static import of the `index` method. This change doesn't inherently indicate a bug fix. It's more likely a refactoring to improve code readability or style by using static imports. The answer is **NotBuggy**.
"
vraptor4,935.json,7f19b84c6bed93f11e11501f35c21330780bd540,"@@ -1,8 +1,8 @@
-	private void addAll(Set<ConstraintViolation<Object>> errors) {
-		for (ConstraintViolation<Object> v : errors) {
+	public <T> void addAll(Set<ConstraintViolation<T>>  errors) {
+		for (ConstraintViolation<T> v : errors) {
 			String msg = interpolator.interpolate(v.getMessageTemplate(), new BeanValidatorContext(v), locale);
 			String category = v.getPropertyPath().toString();
 			add(new SimpleMessage(category, msg));
 			logger.debug(""added message {}={} for contraint violation"", category, msg);
 		}
 	}",NotBuggy,"Relaxing visibility
",NotBuggy,"The commit message ""Relaxing visibility"" suggests a change in the access level of a method or class, making it more accessible from other parts of the code. The diff shows that the `addAll` method's visibility has been changed from `private` to `public`. Additionally, the generic type of the `ConstraintViolation` has been changed from `<Object>` to `<T>`. This change doesn't inherently indicate a bug fix. It's more likely a refactoring or enhancement to allow broader use of the method. Therefore, the answer is **NotBuggy**.
"
vraptor4,1163.json,b92a72ec752eea09e6e59da5b42aafae87e198c0,"@@ -1,3 +1,3 @@
 	public void add(Music music) {
-		session.save(music);
+		getSession().save(music);
 	}",NotBuggy,"Configures JPA and uses EntityManager instead of Hibernate Session;
",NotBuggy,"The commit message indicates a configuration change related to JPA and the use of EntityManager instead of Hibernate Session. The code diff shows a change from `session.save(music)` to `getSession().save(music)`. While this change might be part of the larger effort described in the commit message, it doesn't inherently indicate a bug fix. It seems to be related to adapting the code to use the new JPA configuration. The answer is **NotBuggy**.
"
flink,31570.json,1ab79f65fabdb47d7d041557132b4978db4bc9e2,"@@ -1,4 +1,9 @@
 	public CatalogBaseTable copy() {
-		return new HiveCatalogView(
-			this.getOriginalQuery(), this.getExpandedQuery(), this.getSchema().copy(), new HashMap<>(this.getProperties()), getComment());
+		return new CatalogViewImpl(
+			getOriginalQuery(),
+			getExpandedQuery(),
+			getSchema().copy(),
+			new HashMap<>(getProperties()),
+			getComment()
+		);
 	}",NotBuggy,"[FLINK-12965][table][hive] unify catalog view implementations

This PR unified implementations of CatalogView.

This closes #8882.
",NotBuggy,"The commit message indicates a unification of `CatalogView` implementations. The code change replaces `HiveCatalogView` with `CatalogViewImpl` in the `copy()` method. This suggests a refactoring or consolidation of code, rather than a bug fix. There's no explicit mention of fixing incorrect behavior or handling errors. The change seems to be about simplifying the codebase by using a single implementation. The answer is **NotBuggy**.
"
flink,32529.json,891075aefcfd702142c90cab51759b40765c5a5d,"@@ -1,3 +1,3 @@
 	public <T> T accept(QueryOperationVisitor<T> visitor) {
-		return visitor.visitSort(this);
+		return visitor.visit(this);
 	}",NotBuggy,"[hotfix][table-api] Renamed methods of TableOperationsVisitor
",Buggy,"The commit message indicates a hotfix related to renaming methods in `TableOperationsVisitor`. The diff shows a method name change from `visitSort` to `visit`. This suggests a bug fix or refactoring related to method naming conventions or API consistency. The change directly addresses the method name, aligning with the commit message's intention to rename methods. The answer is **Buggy**.
"
flink,25786.json,8d67aa55f1c11e167a936f1f2a9d954db5960673,"@@ -1,13 +1,12 @@
 	public O returns(Class<OUT> typeClass) {
-		if (typeClass == null) {
-			throw new IllegalArgumentException(""Type class must not be null."");
-		}
+		requireNonNull(typeClass, ""type class must not be null"");
 		
 		try {
-			TypeInformation<OUT> ti = (TypeInformation<OUT>) TypeExtractor.createTypeInfo(typeClass);
-			return returns(ti);
+			return returns(TypeInformation.of(typeClass));
 		}
 		catch (InvalidTypesException e) {
-			throw new InvalidTypesException(""The given class is not suited for providing necessary type information."", e);
+			throw new InvalidTypesException(""Cannot infer the type information from the class alone."" +
+					""This is most likely because the class represents a generic type. In that case,"" +
+					""please use the 'returns(TypeHint)' method instead."", e);
 		}
 	}",NotBuggy,"[FLINK-2788] [apis] Add TypeHint class to allow type-safe generic type parsing

This closes #1744
",Buggy,"The commit message indicates the addition of a `TypeHint` class to allow type-safe generic type parsing. The diff shows changes to the `returns(Class<OUT> typeClass)` method. Specifically, it replaces a null check with `requireNonNull`, and it changes the way `TypeInformation` is created, using `TypeInformation.of(typeClass)` instead of `TypeExtractor.createTypeInfo(typeClass)`. Also, the exception message is updated to provide more specific guidance when type information cannot be inferred from the class alone. These changes suggest improvements in type handling and error reporting, which can be considered bug fixes or enhancements related to type safety. The changes address potential issues with generic type parsing, indicating a bug fix. The answer is **Buggy**."
flink,3845.json,f728129bdb8c3176fba03c3e74c65ed254146061,"@@ -1,5 +1,5 @@
 	public int releaseMemory() {
-		// The pipelined subpartition does not react to memory release requests. The buffers will be
-		// recycled by the consuming task.
+		// The pipelined subpartition does not react to memory release requests.
+		// The buffers will be recycled by the consuming task.
 		return 0;
 	}",NotBuggy,"[FLINK-5169] [network] Make consumption of InputChannels fair
",NotBuggy,"The commit message indicates a change to make the consumption of InputChannels fair. The code change consists of a minor comment update. The comment clarifies that the pipelined subpartition does not react to memory release requests and that the buffers will be recycled by the consuming task. This change does not seem to be related to a bug fix, but rather to documentation. The answer is **NotBuggy**."
flink,12245.json,c401da36dfb5ff05527ed769babe16cbfdfc80be,"@@ -1,3 +1,3 @@
-	Optional<String> getPyExecutable() {
-		return Optional.ofNullable(pyExecutable);
+	Optional<String> getEntryPointScript() {
+		return Optional.ofNullable(entryPointScript);
 	}",NotBuggy,"[FLINK-16668][python] Improve PythonDriver to parse python dependency info from configuration

This closes #11682.
",NotBuggy,"The commit message indicates an improvement to the PythonDriver, specifically related to parsing Python dependency information from the configuration. The diff shows a change from `getPyExecutable()` to `getEntryPointScript()`. This suggests a renaming or refactoring of a method related to retrieving the Python executable or entry point script. This change doesn't inherently indicate a bug fix, but rather an enhancement or refactoring. The answer is **NotBuggy**."
flink,291.json,4452be3a6c1c165b7d808cb4b779e6c2ecd69f01,"@@ -1,3 +1,3 @@
-	public long getMin() {
-		return (long) descriptiveStatistics.getMin();
-	}

+		public double getMin() {
+			return min;
+		}",NotBuggy,"[FLINK-12982][metrics] improve DescriptiveStatisticsHistogramStatistics performance

Instead of redirecting DescriptiveStatisticsHistogramStatistics calls to
DescriptiveStatistics, it takes a point-in-time snapshot using an own
UnivariateStatistic implementation that
a) calculates min, max, mean, and standard deviation in one go (as opposed to
   four iterations over the values array!)
b) caches pivots for the percentile calculation to speed up retrieval of
   multiple percentiles/quartiles

As a result, this roughly increases value retrieval performance by 120% when
accessing typical statistics in a metrics reporter, e.g. the InfluxDB reporter:
count, min, max, mean, stddev, p50, p75, p95, p98, p99, p999.
",Buggy,"The commit message indicates a performance improvement in `DescriptiveStatisticsHistogramStatistics` by avoiding multiple iterations over the values array and caching pivots for percentile calculations. The diff shows a change in the `getMin()` method, where the return type is changed from `long` to `double` and the value is directly returned from a `min` field, instead of calling `descriptiveStatistics.getMin()`. This suggests that the `min` value is now pre-calculated and stored, aligning with the commit message's claim of taking a point-in-time snapshot and caching values. While the commit focuses on performance, the change in return type and the caching mechanism could be addressing a potential bug or inaccuracy in the original implementation. Therefore, the answer is **Buggy**.
"
flink,15309.json,d01d369701c8f79b06d60ee0f731392d3da9b912,"@@ -1,4 +1,4 @@
 	public GraphCsvReader fieldDelimiterEdges(String delimiter) {
-		this.EdgeReader.fieldDelimiter(delimiter);
+		this.edgeReader.fieldDelimiter(delimiter);
 		return this;
 	}",NotBuggy,"[FLINK-1520] [gelly] add types methods and make formatting changes to the graph csv reader

This squashes the following commits:

[FLINK-1520] [gelly] add named types methods for reading a Graph from CSV input,
with and without vertex/edge values. Change the examples and the tests accordingly.

[FLINK-1520] [gelly] corrections in Javadocs; updated documentation

This closes #1149
",Buggy,"The commit message indicates the addition of new methods for reading a Graph from CSV input, along with documentation updates and corrections. The diff shows a change in the `fieldDelimiterEdges` method, where `EdgeReader` is corrected to `edgeReader`. This suggests a typo or incorrect variable name was present, which is now being fixed. This correction is likely addressing a bug where the field delimiter for edges was not being set correctly due to the incorrect reference. The answer is **Buggy**.
"
flink,16953.json,1f7150fc6037406f0656de1babdbaa498c514bb5,"@@ -1,14 +1,15 @@
 	public <L, R> SingleOutputStreamOperator<R> select(
 			final OutputTag<L> timeoutOutputTag,
 			final PatternTimeoutFunction<T, L> patternTimeoutFunction,
 			final TypeInformation<R> outTypeInfo,
 			final PatternSelectFunction<T, R> patternSelectFunction) {
 		return CEPOperatorUtils.createTimeoutPatternStream(
 			inputStream,
 			pattern,
 			comparator,
 			clean(patternSelectFunction),
 			outTypeInfo,
 			timeoutOutputTag,
-			clean(patternTimeoutFunction));
+			clean(patternTimeoutFunction),
+			lateDataOutputTag);
 	}",NotBuggy,"[FLINK-9476][cep] Emit late elements in CEP as sideOutput

This closes #6104
",Buggy,"The commit message indicates that late elements in CEP (Complex Event Processing) are emitted as side output. The code diff adds `lateDataOutputTag` as an argument to `CEPOperatorUtils.createTimeoutPatternStream`. This suggests that the change is related to handling late data, which is a common issue in stream processing. By emitting late elements as side output, the system can handle out-of-order or delayed data more gracefully. This is a bug fix because it addresses a deficiency in how late data was handled previously. The answer is **Buggy**."
flink,19917.json,24eb47ac86299920aa137d5b6394a248ff58d19e,"@@ -1,3 +1,3 @@
-	protected int getLength() {
-		return 2;
+	public int getLength() {
+		return 4;
 	}",NotBuggy,"Renaming part 2 (core and java api)
",Buggy,"The commit message indicates a renaming operation, which typically doesn't involve bug fixes. However, the diff shows a change in the return value of the `getLength()` method from 2 to 4. This change in value suggests a modification in the logic or behavior of the code, which could be related to correcting a previously incorrect length calculation. Therefore, it is likely a bug fix. The answer is **Buggy**."
flink,1138.json,033c69f9477c6352865e7e0da01296dd778ffe59,"@@ -1,5 +1,5 @@
-	public Class<FlatCombineFunction<IN, OUT>> getStubType() {
+	public Class<GroupCombineFunction<IN, OUT>> getStubType() {
 		@SuppressWarnings(""unchecked"")
-		final Class<FlatCombineFunction<IN, OUT>> clazz = (Class<FlatCombineFunction<IN, OUT>>) (Class<?>) FlatCombineFunction.class;
+		final Class<GroupCombineFunction<IN, OUT>> clazz = (Class<GroupCombineFunction<IN, OUT>>) (Class<?>) GroupCombineFunction.class;
 		return clazz;
 	}",NotBuggy,"[Flink-1780] Rename FlatCombineFunction to GroupCombineFunction

This closes #530
",NotBuggy,"The commit message indicates a renaming of `FlatCombineFunction` to `GroupCombineFunction`. The code diff confirms this renaming by changing the return type and casting target from `FlatCombineFunction` to `GroupCombineFunction`. This change is a refactoring/renaming and does not indicate a bug fix. The answer is **NotBuggy**.
"
flink,39338.json,5cebfb76c717568ede8f7e8a82e9bbdf774c19ed,"@@ -1,12 +1,22 @@
 	private void finishAndReportAsync(Map<OperatorID, OperatorSnapshotFutures> snapshotFutures, CheckpointMetaData metadata, CheckpointMetrics metrics) {
+		final Future<?> channelWrittenFuture;
+		if (unalignedCheckpointEnabled) {
+			ChannelStateWriteResult writeResult = channelStateWriter.getWriteResult(metadata.getCheckpointId());
+			channelWrittenFuture = CompletableFuture.allOf(
+				writeResult.getInputChannelStateHandles(),
+				writeResult.getResultSubpartitionStateHandles());
+		} else {
+			channelWrittenFuture = FutureUtils.completedVoidFuture();
+		}
 		// we are transferring ownership over snapshotInProgressList for cleanup to the thread, active on submit
 		executorService.execute(new AsyncCheckpointRunnable(
 			snapshotFutures,
 			metadata,
 			metrics,
+			channelWrittenFuture,
 			System.nanoTime(),
 			taskName,
 			closeableRegistry,
 			env,
 			asyncExceptionHandler));
 	}",NotBuggy,"[FLINK-16587][checkpointing] Spill the in-flight input and output buffers during checkpointing.
",Buggy,"The commit message indicates a change related to checkpointing in Flink, specifically spilling in-flight input and output buffers during checkpointing. The code diff introduces a `channelWrittenFuture` which is used to wait for the completion of writing channel state handles (input and output buffers) when unaligned checkpoints are enabled. If unaligned checkpoints are disabled, a completed future is used, meaning no waiting is needed. This change ensures that the in-flight buffers are properly handled during checkpointing, which is essential for consistency and correctness. This looks like a bug fix. The answer is **Buggy**."
flink,35096.json,9a6ca547d6bd261730c46519f6bffa0b699ec218,"@@ -1,3 +1,3 @@
-		public Iterable<UV> values() throws Exception {
-			return internalMap.values();
+		public Iterable<MV> values() throws Exception {
+			return new ValuesIterable(this.iterator());
 		}",NotBuggy,"[FLINK-13266][table] Relocate blink runtime classes to avoid class clashes
",Buggy,"The commit message indicates a relocation of classes to avoid class clashes, which is a refactoring task rather than a bug fix. However, the diff shows a change in the return type of the `values()` method from `Iterable<UV>` to `Iterable<MV>` and a change in the return statement. This suggests a potential type mismatch or an attempt to correct the return type, which could be related to a bug. The change from `internalMap.values()` to `new ValuesIterable(this.iterator())` also indicates a change in implementation, possibly to address an issue with the original implementation. Therefore, the answer is **Buggy**."
flink,20306.json,bb76fda2444969c4e3a05e05485c43b533d8a499,"@@ -1,3 +1,3 @@
 	public boolean isImmutableType() {
-		return true;
+		return false;
 	}",NotBuggy,"Added direct array serializers for all primitives
",NotBuggy,"The commit message indicates the addition of direct array serializers for all primitives. The diff changes the return value of `isImmutableType()` from `true` to `false`. This change likely indicates that the type is no longer considered immutable due to the new serialization logic for primitive arrays. This doesn't directly point to a bug fix, but rather an adjustment to reflect the new capabilities of the class. The answer is **NotBuggy**.
"
flink,22520.json,1a10fbef644ad32a3358711bfa5a167118186482,"@@ -1,3 +1,3 @@
-				public URL nextElement() {
-					return iter.next();
-				}

+			public URL nextElement() {
+				return iter.next();
+			}",NotBuggy,"[FLINK-11952][1/3] Make ChildFirstClassLoader a top-level class in flink-core
",NotBuggy,"The commit message indicates a structural change: moving `ChildFirstClassLoader` to a top-level class. The diff shows a minor formatting change (indentation). This change does not appear to be related to fixing a bug. The answer is **NotBuggy**.
"
flink,41571.json,8cdf2ff7e5817acc0c239ce31c098daf33d326b7,"@@ -1,3 +1,3 @@
-		public void setPendingTransaction(TXN pendingTransaction) {
+		public void setPendingTransaction(TransactionHolder<TXN> pendingTransaction) {
 			this.pendingTransaction = pendingTransaction;
 		}",NotBuggy,"[FLINK-7784] [kafka011-producer] Make TwoPhaseCommitSinkFunction aware of transaction timeouts.

TwoPhaseCommitSinkFunction allows to configure a transaction timeout. The
timeout can be used to log warnings if the transaction's age is appraoching
the timeout, and it can be used to swallow exceptions that are likely
irrecoverable. This commit also integrates these changes to the
FlinkKafkaProducer011.

This closes #4910.
",Buggy,"The commit message indicates that the `TwoPhaseCommitSinkFunction` is being updated to be aware of transaction timeouts. The provided diff shows a change in the type of the `pendingTransaction` field from `TXN` to `TransactionHolder<TXN>`. This change suggests that the transaction is now being wrapped in a `TransactionHolder`, which likely contains additional information about the transaction, such as its start time, allowing the system to track the transaction's age and implement the timeout functionality described in the commit message. This change is related to the feature described in the commit message, and it is likely part of a bug fix or improvement related to transaction management. The answer is **Buggy**.
"
eclipseJdt,25888.json,bd6803034b95b7e0dd8c0cbcd0aead0a5c726f65,"@@ -1,4 +1,4 @@
 public void goForHeaders(){
 	super.goForHeaders();
-	isFirst = true;
+	this.isFirst = true;
 }",NotBuggy,"HEAD - Clean-up pass 1: add 'this.' to fields +remove 'this.' to msg send + remove trailing white spaces
",NotBuggy,"The commit message indicates a cleanup pass, focusing on code style improvements such as adding 'this.' to fields, removing 'this.' from message sends, and removing trailing whitespace. The provided diff shows the addition of 'this.' to the 'isFirst' field assignment. This change doesn't seem to fix any bug but rather improves code readability and consistency. The answer is **NotBuggy**.
"
eclipseJdt,20964.json,8245bd73c0aef7bf5f6de5def8941104b56db6f7,"@@ -1,3 +1,3 @@
 public int getNumberOfParameters() {
-	return fParameterTypes == null ? 0 : fParameterTypes.length;
+	return this.parameterTypes == null ? 0 : this.parameterTypes.length;
 }",NotBuggy,"*** empty log message ***
",NotBuggy,"The commit message is empty, which is unhelpful. The code change replaces `fParameterTypes` with `this.parameterTypes`. This suggests a possible renaming or refactoring of the field. Without more context, it's impossible to determine if this change fixes a bug or is simply a refactoring. However, given the lack of a meaningful commit message and the nature of the change, it's less likely to be a bug fix.

The answer is **NotBuggy**."
eclipseJdt,15535.json,d8d3d5200f27e422421ae0303798b8a7f8e75502,"@@ -1,2 +1,3 @@
 	public void endVisit(SingleVariableDeclaration node) {
+		// default implementation: do nothing
 	}",NotBuggy,"Add comment to empty blocks
",NotBuggy,"The commit message indicates adding a comment to empty blocks. The diff shows a comment being added to an empty `endVisit` method. This change doesn't fix any bug, but rather adds documentation. The answer is **NotBuggy**.
"
eclipseJdt,18964.json,527f8171257d427ef8c7c37dd3ea4724f920d7c5,"@@ -1,3 +1,3 @@
-boolean isKnownType(String qualifiedTypeName) {
+public boolean isKnownType(String qualifiedTypeName) {
 	return typeLocators.containsKey(qualifiedTypeName);
 }",NotBuggy,"Changes for API tools
",NotBuggy,"The commit message ""Changes for API tools"" is vague and doesn't explicitly mention a bug fix. The code change modifies the visibility of the `isKnownType` method from package-private to public. This change is related to API tools and doesn't seem to be related to fixing a bug. The answer is **NotBuggy**."
eclipseJdt,5067.json,3c02a6af2c4b681d96ee9d070d231fb04b85b5fd,"@@ -1,3 +1,3 @@
 	public boolean isVarArgs() {
-		return (this.type.bits & IsVarArgs) != 0;
+		return this.type != null &&  (this.type.bits & IsVarArgs) != 0;
 	}",NotBuggy,"*** empty log message ***
",Buggy,The diff adds a null check for `this.type` before accessing its `bits` field. This prevents a potential NullPointerException if `this.type` is null. This indicates a bug fix. The answer is **Buggy**.
eclipseJdt,19335.json,f69c200469412ca3b93e80844c6c1f4e077169ac,"@@ -1,3 +1,3 @@
-protected OpenableElementInfo createElementInfo() {
+protected Object createElementInfo() {
 	return new OpenableElementInfo();
 }",NotBuggy,"*** empty log message ***
",Buggy,"The commit message is empty, which is not helpful. The code change modifies the return type of the `createElementInfo` method from `OpenableElementInfo` to `Object`. While it's hard to say definitively without more context, changing a return type like this suggests a potential issue with type safety or compatibility that might be related to a bug fix or a broader refactoring effort to address an underlying problem. The change itself doesn't scream ""bug fix"", but the lack of a commit message and the nature of the change lean towards a bug fix. The answer is **Buggy**."
eclipseJdt,11399.json,f389464fbe74a4e5c3adfde8847a7200db37d80c,"@@ -1,3 +1,3 @@
 public int hash(int key) {
-	return (key & 0x7FFFFFFF) % keyTable.length;
+	return (key & 0x7FFFFFFF) % this.keyTable.length;
 }",NotBuggy,"HEAD - Use classfile pool to create class files
",Buggy,"The commit message indicates a change related to class file creation, specifically using a ""classfile pool."" The diff shows a change in the `hash` method where `keyTable.length` is replaced with `this.keyTable.length`. This change ensures that the `keyTable` used for calculating the hash is the instance variable of the current object, rather than a potentially different `keyTable` accessible in the same scope. While this might seem like a minor change, it's possible that the original code was unintentionally accessing a different `keyTable`, leading to incorrect hash calculations and potential issues with class file creation or management. This correction addresses a potential logical error. The answer is **Buggy**."
eclipseJdt,20530.json,046b7e91db887d0087d68280c71a1c85744c622c,"@@ -1,13 +1,13 @@
 	public char[][] fullInclusionPatternChars() {
 
-		if (this.fullCharInclusionPatterns == UNINIT_PATTERNS) {
+		if (this.fullInclusionPatternChars == UNINIT_PATTERNS) {
 			int length = this.inclusionPatterns.length;
-			this.fullCharInclusionPatterns = new char[length][];
+			this.fullInclusionPatternChars = new char[length][];
 			IPath prefixPath = this.path.removeTrailingSeparator();
 			for (int i = 0; i < length; i++) {
-				this.fullCharInclusionPatterns[i] = 
+				this.fullInclusionPatternChars[i] = 
 					prefixPath.append(this.inclusionPatterns[i]).toString().toCharArray();
 			}
 		}
-		return this.fullCharInclusionPatterns;
+		return this.fullInclusionPatternChars;
 	}",NotBuggy,"*** empty log message ***
",NotBuggy,"The commit message is empty, which makes it difficult to assess the purpose of the change. However, the diff shows that `this.fullCharInclusionPatterns` is replaced with `this.fullInclusionPatternChars`. This suggests a variable rename, which could be part of a larger refactoring or bug fix. Without more context, it's impossible to determine if this change fixes a bug. However, the variable name change itself doesn't obviously indicate a bug fix.

The code change involves initializing and returning a 2D char array. The condition checks if the array is uninitialized before proceeding. The loop iterates through `this.inclusionPatterns` and populates the `fullInclusionPatternChars` array. The functionality seems to be related to path manipulation and pattern matching.

Given the limited information, it's difficult to definitively say whether this change is a bug fix. It could be a refactoring, a performance improvement, or a correction of a subtle error. Without a meaningful commit message, I will assume it is not a bug fix.

**NotBuggy**
"
eclipseJdt,21417.json,f69aa90ed015a30d4053155fb28339fdbd4fbd07,"@@ -1,8 +1,5 @@
 	public void _ldiv(int pc) {
-		writeTabs();
-		buffer
-			.append(pc)
-			.append(Util.bind(""disassembler.tab"")) //$NON-NLS-1$
-			.append(OpcodeStringValues.BYTECODE_NAMES[IOpcodeMnemonics.LDIV]);
+		dumpPcNumber(pc);
+		buffer.append(OpcodeStringValues.BYTECODE_NAMES[IOpcodeMnemonics.LDIV]);
 		writeNewLine();
 	}",NotBuggy,"Improvement for disassembled string
",NotBuggy,"The commit message indicates an ""improvement"" for disassembled strings. The diff shows that the code was changed to use `dumpPcNumber(pc)` instead of `writeTabs(); buffer.append(pc).append(Util.bind(""disassembler.tab""))`. It seems like the change is to improve the formatting of the disassembled string by using a dedicated function for printing the program counter (pc) number. This could be for better readability or consistency in the output. There is no indication of a bug fix in the code or the commit message. The answer is **NotBuggy**."
eclipseJdt,26771.json,f7a77c5e8210599982f60934db59bb901b30cb8c,"@@ -1,9 +1,9 @@
-	public void buildStarting(ICompilationParticipantResult[] files, boolean isBatch){
+	public void buildStarting(BuildContext[] files, boolean isBatch){
 		// this gets called multiple times during a build.
 		// This gets called:
 		// 1) after ""aboutToBuild"" is called.
         // 2) everytime an incremental build occur because of newly generated files
         // this gets called.
 		if( _buildRound == 0 )
 			_isBatch = isBatch;
 	}",NotBuggy,"HEAD - improve compilation participant API
",NotBuggy,"The commit message indicates an improvement to the compilation participant API. The diff shows a change in the parameter type of the `buildStarting` method from `ICompilationParticipantResult[]` to `BuildContext[]`. This suggests a modification to the API's structure or data representation. While it's an improvement, it doesn't inherently imply a bug fix. It seems to be an enhancement or refactoring of the API. The answer is **NotBuggy**."
eclipseJdt,15494.json,d8d3d5200f27e422421ae0303798b8a7f8e75502,"@@ -1,2 +1,3 @@
 	public void endVisit(FieldDeclaration node) {
+		// default implementation: do nothing
 	}",NotBuggy,"Add comment to empty blocks
",NotBuggy,"The commit message indicates adding a comment to an empty block. The diff shows a comment being added to an empty `endVisit` method. This change doesn't fix a bug; it adds documentation. The answer is **NotBuggy**.
"
eclipseJdt,16095.json,24c9e84b2ec4fea87b1bf2f444e5d541a0e3f479,"@@ -1,4 +1,4 @@
-	public boolean subtreeMatch(ASTMatcher matcher, Object other) {
+	final boolean subtreeMatch0(ASTMatcher matcher, Object other) {
 		// dispatch to correct overloaded match method
 		return matcher.match(this, other);
 	}",NotBuggy,"Reduce API footprint
",NotBuggy,"The commit message indicates a reduction in API footprint. The code change renames a public method `subtreeMatch` to `subtreeMatch0` and makes it final. This change reduces the API footprint by making the method inaccessible from outside the class, thus aligning with the commit message. This change does not indicate a bug fix. The answer is **NotBuggy**.
"
eclipseJdt,15705.json,24c9e84b2ec4fea87b1bf2f444e5d541a0e3f479,"@@ -1,4 +1,4 @@
-	public boolean subtreeMatch(ASTMatcher matcher, Object other) {
+	final boolean subtreeMatch0(ASTMatcher matcher, Object other) {
 		// dispatch to correct overloaded match method
 		return matcher.match(this, other);
 	}",NotBuggy,"Reduce API footprint
",NotBuggy,"The commit message states ""Reduce API footprint"". The diff renames a public method `subtreeMatch` to `subtreeMatch0` and makes it final. This change reduces the API footprint by making a public method non-public, which aligns with the commit message. This change is not related to fixing a bug. The answer is **NotBuggy**.
"
eclipseJdt,19189.json,bd6803034b95b7e0dd8c0cbcd0aead0a5c726f65,"@@ -1,11 +1,11 @@
 private void rehash() {
-	QualifiedNameSet newSet = new QualifiedNameSet(elementSize * 2); // double the number of expected elements
+	QualifiedNameSet newSet = new QualifiedNameSet(this.elementSize * 2); // double the number of expected elements
 	char[][] current;
-	for (int i = qualifiedNames.length; --i >= 0;)
-		if ((current = qualifiedNames[i]) != null)
+	for (int i = this.qualifiedNames.length; --i >= 0;)
+		if ((current = this.qualifiedNames[i]) != null)
 			newSet.add(current);
 
 	this.qualifiedNames = newSet.qualifiedNames;
 	this.elementSize = newSet.elementSize;
 	this.threshold = newSet.threshold;
 }",NotBuggy,"HEAD - Clean-up pass 1: add 'this.' to fields +remove 'this.' to msg send + remove trailing white spaces
",NotBuggy,"The commit message indicates a cleanup pass, focusing on adding 'this.' to fields, removing 'this.' from message sends, and removing trailing whitespace. The provided diff shows the addition of 'this.' to field accesses within the `rehash` method. This change improves code clarity and consistency but doesn't directly address a bug or incorrect behavior. It's a refactoring change. The answer is **NotBuggy**."
eclipseJdt,21045.json,3b79f63e00cbebb1fe326313cd6905bd51bae92d,"@@ -1,3 +1,3 @@
 	public int getNameSourceEnd() {
-		return this.nameEnd;
+		return -1;
 	}",NotBuggy,"HEAD - 260257
",Buggy,"The commit message ""HEAD - 260257"" is not descriptive. The code change modifies the `getNameSourceEnd()` method to always return -1. Without more context, it's difficult to definitively say if this is a bug fix. However, the change from returning `this.nameEnd` to a constant `-1` suggests a potential issue with the original logic or a deliberate disabling of the functionality. It's possible that `this.nameEnd` was sometimes returning an incorrect value, leading to a bug. Therefore, the answer is **Buggy**.
"
commons-lang,2261.json,5bd622dab027ef37001a630a7c825c5b8c19d1db,"@@ -1,6 +1,6 @@
-    public static Short[] nullToEmpty(Short[] array) {
+    public static Short[] nullToEmpty(final Short[] array) {
         if (array == null || array.length == 0) {
             return EMPTY_SHORT_OBJECT_ARRAY;
         }
         return array;
     }",NotBuggy,"Add final modifier to method parameters.

git-svn-id: https://svn.apache.org/repos/asf/commons/proper/lang/trunk@1436768 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates that the changes involve adding the `final` modifier to method parameters. The diff confirms this by showing the addition of `final` to the `array` parameter in the `nullToEmpty` method. Adding `final` to method parameters prevents modification of the parameter within the method, which can improve code clarity and prevent accidental modifications. This change does not indicate a bug fix, but rather a code style improvement. The answer is **NotBuggy**.
"
commons-lang,852.json,93a3b6b251429030ee0393a8483d2a225a7b0db2,"@@ -1,4 +1,4 @@
-    public ToStringBuilder<T> append(double value) {
+    public ToStringBuilder append(double value) {
         style.append(buffer, null, value);
         return this;
     }",NotBuggy,"Remove generics as they provide little value and get in the way

git-svn-id: https://svn.apache.org/repos/asf/commons/proper/lang/trunk@829635 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates the removal of generics from the code. The diff shows that the generic type parameter `<T>` has been removed from the `ToStringBuilder` class's `append` method. This change aligns with the commit message and suggests a refactoring rather than a bug fix. The answer is **NotBuggy**.
"
commons-lang,2074.json,3a818ed6a833f083a2db9bb6804c1bdb43b9b0ec,"@@ -1,3 +1,3 @@
-    public static final Processor getProcessor() {
+    public static Processor getProcessor() {
         return getProcessor(SystemUtils.OS_ARCH);
     }",NotBuggy,"Prevent redundant modifiers
",NotBuggy,"The commit message indicates the removal of redundant modifiers. The diff shows the removal of the `final` modifier from the `getProcessor()` method. This change aligns with the commit message and suggests a code cleanup rather than a bug fix. The answer is **NotBuggy**.
"
commons-lang,1759.json,5bd622dab027ef37001a630a7c825c5b8c19d1db,"@@ -1,10 +1,10 @@
-    public static String reverseDelimited(String str, char separatorChar) {
+    public static String reverseDelimited(final String str, final char separatorChar) {
         if (str == null) {
             return null;
         }
         // could implement manually, but simple way is to reuse other,
         // probably slower, methods.
         String[] strs = split(str, separatorChar);
         ArrayUtils.reverse(strs);
         return join(strs, separatorChar);
     }",NotBuggy,"Add final modifier to method parameters.

git-svn-id: https://svn.apache.org/repos/asf/commons/proper/lang/trunk@1436768 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates that the change is to add the `final` modifier to method parameters. The diff shows that the `final` modifier has been added to the `str` and `separatorChar` parameters of the `reverseDelimited` method. This change does not indicate a bug fix, but rather an improvement in code style and potentially preventing accidental modification of the parameters within the method. The answer is **NotBuggy**."
commons-lang,772.json,5bd622dab027ef37001a630a7c825c5b8c19d1db,"@@ -1,3 +1,3 @@
-    public static boolean isPrimitiveWrapper(Class<?> type) {
+    public static boolean isPrimitiveWrapper(final Class<?> type) {
         return wrapperPrimitiveMap.containsKey(type);
     }",NotBuggy,"Add final modifier to method parameters.

git-svn-id: https://svn.apache.org/repos/asf/commons/proper/lang/trunk@1436768 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates the addition of the `final` modifier to a method parameter. The diff confirms this change. Adding `final` to a method parameter prevents the parameter from being reassigned within the method, which can improve code clarity and prevent accidental modification of the parameter. This change does not directly address a bug, but rather enhances code quality. The answer is **NotBuggy**.
"
commons-lang,1787.json,5bd622dab027ef37001a630a7c825c5b8c19d1db,"@@ -1,3 +1,3 @@
-    public static String strip(String str) {
+    public static String strip(final String str) {
         return strip(str, null);
     }",NotBuggy,"Add final modifier to method parameters.

git-svn-id: https://svn.apache.org/repos/asf/commons/proper/lang/trunk@1436768 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates the addition of the `final` modifier to method parameters. The diff confirms this change, adding `final` to the `str` parameter in the `strip` method. This change does not fix a bug; it's a code style improvement that prevents accidental modification of the parameter within the method. The answer is **NotBuggy**.
"
commons-lang,1773.json,5bd622dab027ef37001a630a7c825c5b8c19d1db,"@@ -1,3 +1,3 @@
-    public static String[] splitByWholeSeparator( String str, String separator, int max ) {
+    public static String[] splitByWholeSeparator( final String str, final String separator, final int max ) {
         return splitByWholeSeparatorWorker(str, separator, max, false);
     }",NotBuggy,"Add final modifier to method parameters.

git-svn-id: https://svn.apache.org/repos/asf/commons/proper/lang/trunk@1436768 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates that the change is to add the `final` modifier to method parameters. The diff confirms this by showing the addition of `final` to the `str`, `separator`, and `max` parameters of the `splitByWholeSeparator` method. Adding `final` to method parameters prevents them from being reassigned within the method, which can improve code clarity and prevent accidental modification of the parameters. This change does not indicate a bug fix. The answer is **NotBuggy**.
"
commons-lang,1672.json,96c30e248d6d27a1df8e26be14aa10c633168cdb,"@@ -1,3 +1,3 @@
-    public static boolean isNoneBlank(CharSequence... css) {
+    public static boolean isNoneBlank(final CharSequence... css) {
       return !isAnyBlank(css);
     }",NotBuggy,"Use final consistently. 

git-svn-id: https://svn.apache.org/repos/asf/commons/proper/lang/trunk@1606051 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates a change to use the `final` keyword consistently. The diff shows the addition of the `final` keyword to the `css` parameter in the `isNoneBlank` method signature. This change does not indicate a bug fix; it's a code style improvement. The answer is **NotBuggy**.
"
commons-lang,186.json,dd74cf1da57ed432ce38631baebe6882419b01e6,"@@ -1,19 +1,18 @@
     public static int distance(final Class<?> child, final Class<?> parent) {
         if (child == null || parent == null) {
             return -1;
         }
 
         if (child.equals(parent)) {
             return 0;
         }
 
         final Class<?> cParent = child.getSuperclass();
         int d = BooleanUtils.toInteger(parent.equals(cParent));
 
         if (d == 1) {
             return d;
-        } else {
-            d += distance(cParent, parent);
-            return d > 0 ? d + 1 : -1;
         }
+        d += distance(cParent, parent);
+        return d > 0 ? d + 1 : -1;
     }",NotBuggy,"Statement unnecessarily nested within else clause.

git-svn-id: https://svn.apache.org/repos/asf/commons/proper/lang/trunk@1606088 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates a simplification of the code by removing unnecessary nesting. The diff shows that the `else` block has been removed, and the code within it is now executed unconditionally. This change doesn't introduce any bug fixes, but rather improves code readability by removing unnecessary nesting. The answer is **NotBuggy**."
commons-lang,2630.json,5292526e476ffbb19c6613a98464054236c86ace,"@@ -1,8 +1,8 @@
         public Character next() {
             if (hasNext == false) {
                 throw new NoSuchElementException();
             }
-            char cur = current;
+            final char cur = current;
             prepareNext();
             return Character.valueOf(cur);
         }",NotBuggy,"Add final modifier to local variables.

git-svn-id: https://svn.apache.org/repos/asf/commons/proper/lang/trunk@1436770 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,The commit message indicates the addition of the `final` modifier to local variables. The diff confirms this by showing the addition of `final` to the `cur` variable declaration within the `next()` method. This change doesn't inherently fix a bug but rather improves code clarity and prevents unintended modification of the variable. The answer is **NotBuggy**.
commons-lang,136.json,c79f82491a8bae9c3b9895f296a8519cd0dfe7ff,"@@ -1,15 +1,19 @@
     public static void removeFinalModifier(Field field) {
         Validate.isTrue(field != null, ""The field must not be null"");
 
         try {
             if (Modifier.isFinal(field.getModifiers())) {
                 Field modifiersField = Field.class.getDeclaredField(""modifiers"");
                 modifiersField.setAccessible(true);
-                modifiersField.setInt(field, field.getModifiers() & ~Modifier.FINAL);
+                try {
+                    modifiersField.setInt(field, field.getModifiers() & ~Modifier.FINAL);
+                } finally {
+                    modifiersField.setAccessible(false);
+                }
             }
         } catch (NoSuchFieldException ignored) {
             // The field class contains always a modifiers field
         } catch (IllegalAccessException ignored) {
              // The modifiers field is made accessible
         }
     }",NotBuggy,"removeFinalModifier does not clean up after itself.

git-svn-id: https://svn.apache.org/repos/asf/commons/proper/lang/trunk@1562985 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates that the `removeFinalModifier` method does not clean up after itself. The diff shows that a `finally` block has been added to ensure that `modifiersField.setAccessible(false)` is always called, even if an exception occurs within the `try` block. This prevents potential security issues or unexpected behavior in subsequent code that relies on the field's accessibility. This addresses a resource leak/cleanup issue, which qualifies as a bug fix. The answer is **Buggy**."
commons-lang,1942.json,3728344459e91509c4b5b80d67a50c98063e08bd,"@@ -1,14 +1,14 @@
-    public static short max(final short[] array) {
+    public static short max(final short... array) {
         // Validates input
         validateArray(array);
     
         // Finds and returns max
         short max = array[0];
         for (int i = 1; i < array.length; i++) {
             if (array[i] > max) {
                 max = array[i];
             }
         }
     
         return max;
     }",NotBuggy,"LANG-1008: Change min/max methods in NumberUtils/IEEE754rUtils from array input parameters to varargs. This also closes #23 from github. Thanks to Thiago Andrade.

git-svn-id: https://svn.apache.org/repos/asf/commons/proper/lang/trunk@1593118 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates a change from array input parameters to varargs for min/max methods in NumberUtils/IEEE754rUtils. The diff shows that the `max` method for short arrays is modified to accept varargs (`short... array`) instead of a fixed array (`short[] array`). This change allows the method to be called with a variable number of short arguments, which is consistent with the commit message. This change doesn't fix a bug, but rather enhances the API. The answer is **NotBuggy**."
commons-lang,2067.json,3a818ed6a833f083a2db9bb6804c1bdb43b9b0ec,"@@ -1,4 +1,4 @@
-    private static final void init_X86_64Bit() {
+    private static void init_X86_64Bit() {
         Processor processor = new Processor(Processor.Arch.BIT_64, Processor.Type.X86);
         addProcessors(processor, ""x86_64"", ""amd64"", ""em64t"", ""universal"");
     }",NotBuggy,"Prevent redundant modifiers
",NotBuggy,"The commit message indicates the removal of redundant modifiers. The diff shows the removal of `final` from the `init_X86_64Bit()` method declaration. This suggests a cleanup of the code by removing unnecessary modifiers, which doesn't inherently indicate a bug fix.
The answer is **NotBuggy**."
commons-lang,1223.json,9072a7615a54d5e753cf088326d9e14f1a5a6751,"@@ -1,7 +1,7 @@
     public EqualsBuilder append(final int lhs, final int rhs) {
         if (isEquals == false) {
             return this;
         }
-        isEquals = (lhs == rhs);
+        isEquals = lhs == rhs;
         return this;
     }",NotBuggy,"PMD: Useless parentheses

git-svn-id: https://svn.apache.org/repos/asf/commons/proper/lang/trunk@1671669 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates the removal of useless parentheses, suggesting a code cleanup rather than a bug fix. The diff confirms this by showing the removal of parentheses around the expression `lhs == rhs`. The change doesn't alter the logic or functionality of the code, only its readability. The answer is **NotBuggy**."
commons-lang,1772.json,5bd622dab027ef37001a630a7c825c5b8c19d1db,"@@ -1,3 +1,3 @@
-    public static String[] splitByWholeSeparator(String str, String separator) {
+    public static String[] splitByWholeSeparator(final String str, final String separator) {
         return splitByWholeSeparatorWorker( str, separator, -1, false ) ;
     }",NotBuggy,"Add final modifier to method parameters.

git-svn-id: https://svn.apache.org/repos/asf/commons/proper/lang/trunk@1436768 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates the addition of the `final` modifier to method parameters. The diff confirms this change, adding `final` to the `str` and `separator` parameters of the `splitByWholeSeparator` method. This change does not fix a bug but rather improves code clarity and prevents accidental modification of the parameters within the method. The answer is **NotBuggy**.
"
elasticsearch,45328.json,bd7efa908ab69831757d45ca7519c286097777dc,"@@ -1,20 +1,20 @@
-    static int generateShardId(IndexMetaData indexMetaData, @Nullable String id, @Nullable String routing) {
+    public static int generateShardId(IndexMetaData indexMetaData, @Nullable String id, @Nullable String routing) {
         final String effectiveRouting;
         final int partitionOffset;
 
         if (routing == null) {
             assert(indexMetaData.isRoutingPartitionedIndex() == false) : ""A routing value is required for gets from a partitioned index"";
             effectiveRouting = id;
         } else {
             effectiveRouting = routing;
         }
 
         if (indexMetaData.isRoutingPartitionedIndex()) {
             partitionOffset = Math.floorMod(Murmur3HashFunction.hash(id), indexMetaData.getRoutingPartitionSize());
         } else {
             // we would have still got 0 above but this check just saves us an unnecessary hash calculation
             partitionOffset = 0;
         }
 
         return calculateScaledShardId(indexMetaData, effectiveRouting, partitionOffset);
     }",NotBuggy,"Add ability to split shards (#26931)

This change adds a new `_split` API that allows to split indices into a new
index with a power of two more shards that the source index.  This API works
alongside the `_shrink` API but doesn't require any shard relocation before
indices can be split.

The split operation is conceptually an inverse `_shrink` operation since we
initialize the index with a _syntetic_ number of routing shards that are used
for the consistent hashing at index time. Compared to indices created with
earlier versions this might produce slightly different shard distributions but
has no impact on the per-index backwards compatibility.  For now, the user is
required to prepare an index to be splittable by setting the
`index.number_of_routing_shards` at index creation time.  The setting allows the
user to prepare the index to be splittable in factors of
`index.number_of_routing_shards` ie. if the index is created with
`index.number_of_routing_shards: 16` and `index.number_of_shards: 2` it can be
split into `4, 8, 16` shards. This is an intermediate step until we can make
this the default. This also allows us to safely backport this change to 6.x.

The `_split` operation is implemented internally as a DeleteByQuery on the
lucene level that is executed while the primary shards execute their initial
recovery. Subsequent merges that are triggered due to this operation will not be
executed immediately. All merges will be deferred unti the shards are started
and will then be throttled accordingly.

This change is intended for the 6.1 feature release but will not support pre-6.1
indices to be split unless these indices have been shrunk before. In that case
these indices can be split backwards into their original number of shards.
",NotBuggy,"The commit message describes the addition of a new `_split` API for splitting indices into more shards. The provided code snippet modifies the `generateShardId` method, making it public and adjusting the calculation of `partitionOffset` based on routing. While the commit message describes a new feature, the code change itself doesn't obviously indicate a bug fix. The change to `generateShardId` seems related to the new splitting functionality, potentially ensuring correct shard ID generation during the split operation. There's no clear indication of error handling, logical corrections, or exception handling improvements in the provided diff. The change appears to be part of the new feature implementation. The answer is **NotBuggy**."
elasticsearch,38113.json,85c26d682a5a6fbaf345d66b9a783b78f4b1cc63,"@@ -1,19 +1,19 @@
             public boolean advanceExact(int doc) throws IOException {
                 if (bytesValues.advanceExact(doc)) {
                     count = bytesValues.docValueCount();
                     grow();
                     for (int i = 0; i < count; ++i) {
                         final BytesRef value = bytesValues.nextValue();
                         script.setNextAggregationValue(value.utf8ToString());
                         Object run = script.run();
-                        CollectionUtils.ensureNoSelfReferences(run);
+                        CollectionUtils.ensureNoSelfReferences(run, ""ValuesSource.BytesValues script"");
                         values[i].copyChars(run.toString());
                     }
                     sort();
                     return true;
                 } else {
                     count = 0;
                     grow();
                     return false;
                 }
             }",NotBuggy,"Call ensureNoSelfReferences() on _agg state variable after scripted metric agg script executions (#31044)

Previously this was called for the combine script only. This change checks for self references for
init, map, and reduce scripts as well, and adds unit test coverage for the init, map, and combine cases.

",Buggy,"The commit message indicates that the `ensureNoSelfReferences()` method, which was previously only called for the combine script in scripted metric aggregations, is now being called for the init, map, and reduce scripts as well. The diff shows that `ensureNoSelfReferences()` is now called after `script.run()` in the `advanceExact` method. This suggests a bug fix where self-references were not being properly checked in all script execution contexts, potentially leading to issues like infinite loops or stack overflows. The addition of unit tests for init, map, and combine cases further supports the idea that this change addresses a bug related to self-references in scripted metric aggregations. The answer is **Buggy**."
elasticsearch,38632.json,48b0908fc62a4047c8b254a3c3b52c973019561a,"@@ -1,3 +1,3 @@
-    Comparable<?>[] values() {
+    Comparable[] values() {
         return values;
     }",NotBuggy,"Make InternalComposite key comparable

Keys are compared in BucketSortPipelineAggregation so making key type (ArrayMap) implement Comparable. Maps are compared using the entry set's iterator so ordered maps order is maintain. For each entry first comparing key then value. Assuming all keys are strings. When comparing entries' values if type is not identical and\or type not implementing Comparable, throwing exception. Not implementing equals() and hashCode() functions as parent's ones are sufficient. Tests included.",Buggy,"The commit message indicates a change to make `InternalComposite` keys comparable, specifically for use in `BucketSortPipelineAggregation`. The provided diff changes the return type of the `values()` method from `Comparable<?>[]` to `Comparable[]`. This change makes the array of values explicitly `Comparable`, which aligns with the commit message's goal of enabling key comparison. This suggests that the previous type was not specific enough, potentially leading to issues during comparison. The change ensures that the values are treated as `Comparable`, which is necessary for the intended use case. The answer is **Buggy**."
elasticsearch,12197.json,9e8cfbb40d7596eb56151fc8a30234fb336c0a43,"@@ -1,3 +1,3 @@
     public String getName() {
-        return ""xpack_ml_get_overall_buckets_action"";
+        return ""ml_get_overall_buckets_action"";
     }",NotBuggy,"[ML] Deprecate X-Pack centric ML endpoints (#36315)

This commit is part of our plan to deprecate and
ultimately remove the use of _xpack in the REST APIs.

Relates #35958",NotBuggy,"The commit message indicates a deprecation of X-Pack centric ML endpoints, aiming to remove ""_xpack"" from REST APIs. The code change replaces ""xpack_ml_get_overall_buckets_action"" with ""ml_get_overall_buckets_action"", which aligns with the commit message's intention to remove the ""xpack"" prefix. This change is not related to fixing a bug but rather to deprecating and renaming endpoints. The answer is **NotBuggy**.
"
elasticsearch,39219.json,7bd2abe48af5a651b54bcd5bcb41c88e29390be0,"@@ -1,3 +1,3 @@
-    protected List<Bucket> getBucketsInternal() {
+    public List<Bucket> getBuckets() {
         return emptyList();
     }",NotBuggy,"Change Terms.Bucket to an interface (#24492)

This commit changes the Terms.Bucket abstract class to an interface, so
that it's easier for the Java High Level Rest Client to provide its own
implementation.

In its current state, the Terms.Bucket abstract class inherits from
InternalMultiBucketAggregation.InternalBucket which forces subclasses to
implement Writeable and exposes a public getProperty() method that relies
on InternalAggregation. This two points make it difficult for the Java
High Level Rest Client to implement the Terms and Terms.Bucket correctly.
This is also different from other MultiBucketsAggregation like Range
which are pure interfaces.

Changing Terms.Bucket to an interface causes a method clashes for the
`getBuckets()` method in InternalTerms. This is because:
 - InternalTerms implements Terms which declared a
 `List<Terms.Bucket> getBuckets()` method
 - InternalTerms extends InternalMultiBucketAggregation which declares a
 `List<? extends InternalBucket> getBuckets()` method
 - both overrides the MultiBucketsAggregation
 `List<? extends Bucket> getBuckets()` method

 There was no clashes before this change because Terms.Bucket extends
 InternalBucket and conformed to both declaration. With Terms.Bucket now
 an interface, the getBuckets() method in the Terms interface is changed
 to avoid method clash. This is a breaking change in the Java API but
 it's a straightforward change and the Terms multi bucket aggregation
 interface is also more coherent with the other Range, Histogram,
 Filters, AdjacencyMatrix etc that all return a `List<? extends Bucket>`.",NotBuggy,"The commit message describes a change from an abstract class to an interface to improve the Java High Level Rest Client's ability to provide its own implementation. The diff shows a change in the `getBucketsInternal()` method to `getBuckets()`. This change seems to be related to resolving method clashes after the interface change. This is not a bug fix, but rather a refactoring and API change. The answer is **NotBuggy**."
elasticsearch,61546.json,b70f925ccb735dc84d59598de06df6bf35bd4bdc,"@@ -1,3 +1,3 @@
-    Configuration getConfiguration() {
-        return configuration;
+    public String getMajor() {
+        return major;
     }",NotBuggy,"Support downloading JDKs with legacy version format (#51587)

",Buggy,"The commit message indicates support for downloading JDKs with a legacy version format. The diff shows a change from `getConfiguration()` to `getMajor()`. This suggests that the configuration retrieval logic has been modified to specifically extract the major version, likely to accommodate the legacy version format. This could be in response to a bug or limitation in handling older version formats. The answer is **Buggy**."
elasticsearch,39443.json,a0b004acb5965508782f5430c240f71580478832,"@@ -1,4 +1,4 @@
-    public IPv4RangeAggregatorFactory addRange(String key, String from, String to) {
+    public IPv4RangeAggregatorBuilder addRange(String key, String from, String to) {
         addRange(new Range(key, from, to));
         return this;
     }",NotBuggy,"Split AggregatorFactory into AggregatorBuilder and AggregatorFactory
",NotBuggy,"The commit message indicates a refactoring by splitting `AggregatorFactory` into `AggregatorBuilder` and `AggregatorFactory`. The diff shows a change in the return type of the `addRange` method from `IPv4RangeAggregatorFactory` to `IPv4RangeAggregatorBuilder`. This change aligns with the commit message, suggesting a structural modification rather than a bug fix. The answer is **NotBuggy**."
elasticsearch,8046.json,2879e6717eedd6caad60e4fca3991ed4a9faad38,"@@ -1,11 +1,15 @@
     private static BytesReference filterSource(FetchSourceContext fetchSourceContext, BytesReference source) throws IOException {
+        if (fetchSourceContext.includes().length == 0 && fetchSourceContext.excludes().length == 0) {
+            return source;
+        }
+
         Set<String> includes = Set.of(fetchSourceContext.includes());
         Set<String> excludes = Set.of(fetchSourceContext.excludes());
 
         XContentBuilder builder =
             new XContentBuilder(XContentType.SMILE.xContent(), new BytesStreamOutput(source.length()), includes, excludes);
         XContentParser sourceParser = XContentHelper.createParser(NamedXContentRegistry.EMPTY,
             DeprecationHandler.THROW_UNSUPPORTED_OPERATION, source, XContentType.SMILE);
         builder.copyCurrentStructure(sourceParser);
         return BytesReference.bytes(builder);
     }",NotBuggy," Enrich processor configuration changes (#45466)

Enrich processor configuration changes:
* Renamed `enrich_key` option to `field` option.
* Replaced `set_from` and `targets` options with `target_field`.

The `target_field` option behaves different to how `set_from` and
`targets` worked. The `target_field` is the field that will contain
the looked up document.

Relates to #32789",NotBuggy,"The commit message describes renaming and replacing options in processor configuration changes. The code diff introduces a check for empty includes and excludes in `fetchSourceContext`. If both are empty, the original source is returned directly. This avoids unnecessary processing with `XContentBuilder` and `XContentParser`. This change optimizes the case where no filtering is needed, improving efficiency. There is no indication of bug fixing. The answer is **NotBuggy**."
elasticsearch,63479.json,df2acb3d9d81118e5dcf64979e0fa716beec8164,"@@ -1,10 +1,10 @@
     protected boolean token(String aggregationName, String currentFieldName, XContentParser.Token token, XContentParser parser,
                             ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        if (parseFieldMatcher.match(currentFieldName, MULTIVALUE_MODE_FIELD)) {
+        if (MULTIVALUE_MODE_FIELD.match(currentFieldName)) {
             if (token == XContentParser.Token.VALUE_STRING) {
                 otherOptions.put(MULTIVALUE_MODE_FIELD, parser.text());
                 return true;
             }
         }
         return false;
     }",NotBuggy,"Remove some more usages of ParseFieldMatcher in favour of using ParseField directly

Relates to #19552
Relates to #22130
",NotBuggy,"The commit message indicates a shift from using `ParseFieldMatcher` to using `ParseField` directly. The diff shows that `parseFieldMatcher.match` is replaced with `MULTIVALUE_MODE_FIELD.match`. This change suggests a refactoring to simplify the code and potentially improve performance by avoiding the use of a matcher. There is no indication of a bug fix in the traditional sense, such as correcting a logical error or handling an exception. The change is more about code style and optimization. The answer is **NotBuggy**.
"
elasticsearch,26493.json,78f9af19c668886c682d5d1d74f5fb6f2a4c5e77,"@@ -1,3 +1,3 @@
     public int hashCode() {
-        return Objects.hash(accessToken, refreshToken);
+        return Objects.hash(accessToken, refreshToken, realmName, username);
     }",NotBuggy,"Invalidate Token API enhancements - HLRC (#36362)

* Adds Invalidate Token API enhancements to HLRC

Relates: #35388
",Buggy,"The commit message indicates enhancements to the Invalidate Token API, specifically within the High-Level REST Client (HLRC). The diff shows a change to the `hashCode()` method, adding `realmName` and `username` to the calculation. This suggests that the previous implementation of `hashCode()` was incomplete, potentially leading to collisions and incorrect behavior when used in hash-based collections or comparisons. While not explicitly a ""bug fix"" in the sense of correcting a functional error, the change addresses a potential issue with object identity and equality, which can be considered a bug fix in a broader sense. The answer is **Buggy**."
elasticsearch,25327.json,92eb324776f26e0edf8b7957c81950890b8ce4d9,"@@ -1,9 +1,9 @@
     public CancelTasksResponse cancel(CancelTasksRequest cancelTasksRequest, RequestOptions options ) throws IOException {
         return restHighLevelClient.performRequestAndParseEntity(
             cancelTasksRequest,
             RequestConverters::cancelTasks,
             options,
-            parser -> CancelTasksResponse.fromXContent(parser),
+            CancelTasksResponse::fromXContent,
             emptySet()
         );
     }",NotBuggy,"REST high-level Client: remove deprecated API methods (#31200)

This commit removes all the API methods that accept a `Header` varargs
argument, in favour of the newly introduced API methods that accept a
`RequestOptions` argument.

Relates to #31069",NotBuggy,"The commit message indicates the removal of deprecated API methods in favor of new ones using `RequestOptions`. The diff shows a change in the `cancel` method, specifically in how the `CancelTasksResponse` is parsed from the XContent. It replaces a lambda expression `parser -> CancelTasksResponse.fromXContent(parser)` with a method reference `CancelTasksResponse::fromXContent`. This change doesn't seem to be fixing a bug but rather refactoring the code to use a more concise syntax (method reference) which aligns with the commit message's intention of removing deprecated APIs and using the new `RequestOptions`. Therefore, it's not a bug fix. The answer is **NotBuggy**."
elasticsearch,44847.json,690136327fe2b08c14dcf7bcd140ceb6f10ae96f,"@@ -1,7 +1,6 @@
         public synchronized void reset() {
             super.reset();
-            version = UNKNOWN;
             fileDetails.clear();
             sourceThrottlingInNanos = UNKNOWN;
             targetThrottleTimeInNanos = UNKNOWN;
         }",NotBuggy,"Cleanup Deadcode o.e.indices (#44931)

* none of this is used anywhere",NotBuggy,"The commit message indicates the removal of dead code within the `o.e.indices` package. The diff shows the removal of the `version = UNKNOWN;` line within the `reset()` method. Since the commit message explicitly states the removal of unused code, this change does not indicate a bug fix. The answer is **NotBuggy**.
"
elasticsearch,24653.json,28bb1a3ac0c529beb38cd3553b50e095a07df13b,"@@ -1,5 +1,5 @@
     public final GraphExploreResponse explore(GraphExploreRequest graphExploreRequest,
                                                              RequestOptions options) throws IOException {
         return restHighLevelClient.performRequestAndParseEntity(graphExploreRequest, GraphRequestConverters::explore,
-                options, GraphExploreResponse::fromXContext, emptySet());
+                options, GraphExploreResponse::fromXContent, emptySet());
     }",NotBuggy,"XPack/HLRC request/response compatibility tests (#34547)


Relates #34451 , #29827
",Buggy,"The commit message indicates compatibility tests for XPack/HLRC. The code change replaces `fromXContext` with `fromXContent` in the `explore` method. This suggests an update to align with changes in how the response is parsed, likely due to a change in the underlying data structure or format. This could be related to a bug fix or an API change. The change in method name suggests a potential bug fix related to incorrect context handling. The answer is **Buggy**.
"
elasticsearch,55784.json,7e3cd6a01931a95bfad785490f3319ed79f258f4,"@@ -1,5 +1,17 @@
-    public Status getStatus() {
-        return new Status(sliceId, total.get(), updated.get(), created.get(), deleted.get(), batch.get(), versionConflicts.get(),
-                noops.get(), bulkRetries.get(), searchRetries.get(), timeValueNanos(throttledNanos.get()), getRequestsPerSecond(),
-                getReasonCancelled(), throttledUntil());
+    public BulkByScrollTask.Status getStatus() {
+        return new BulkByScrollTask.Status(
+            sliceId,
+            total.get(),
+            updated.get(),
+            created.get(),
+            deleted.get(),
+            batch.get(),
+            versionConflicts.get(),
+            noops.get(),
+            bulkRetries.get(),
+            searchRetries.get(),
+            timeValueNanos(throttledNanos.get()),
+            getRequestsPerSecond(),
+            task.getReasonCancelled(),
+            throttledUntil());
     }",NotBuggy,"reindex: automatically choose the number of slices (#26030)

In reindex APIs, when using the `slices` parameter to choose the number of slices, adds the option to specify `slices` as ""auto"" which will choose a reasonable number of slices. It uses the number of shards in the source index, up to a ceiling. If there is more than one source index, it uses the smallest number of shards among them.

This gives users an easy way to use slicing in these APIs without having to make decisions about how to configure it, as it provides a good-enough configuration for them out of the box. This may become the default behavior for these APIs in the future.
",NotBuggy,"The commit message describes a new feature: automatically determining the number of slices for reindex operations. The diff changes the return type of `getStatus()` from `Status` to `BulkByScrollTask.Status` and updates the constructor call to match the new type, passing `task.getReasonCancelled()` instead of `getReasonCancelled()`. This change seems to be related to adapting the code to the new slicing feature, possibly to integrate task-related information into the status. There's no indication of a bug fix in the traditional sense (correcting erroneous behavior). The change is more about feature enhancement and code adaptation. The answer is **NotBuggy**."
elasticsearch,33161.json,d8b4556e2dfa48b38baf614e3ac99196be07055c,"@@ -1,39 +1,12 @@
             public Scorer scorer(LeafReaderContext context) throws IOException {
                 Scorer subQueryScorer = subQueryWeight.scorer(context);
                 if (subQueryScorer == null) {
                     return null;
                 }
-                final LeafScoreFunction leafFunction = function.getLeafScoreFunction(context);
-                Scorer scriptScorer = new Scorer(this) {
-                    @Override
-                    public float score() throws IOException {
-                        int docId = docID();
-                        float subQueryScore = subQueryScoreMode == ScoreMode.COMPLETE ? subQueryScorer.score() : 0f;
-                        float score = (float) leafFunction.score(docId, subQueryScore);
-                        if (score == Float.NEGATIVE_INFINITY || Float.isNaN(score)) {
-                            throw new ElasticsearchException(
-                                ""script score query returned an invalid score: "" + score + "" for doc: "" + docId);
-                        }
-                        return score;
-                    }
-                    @Override
-                    public int docID() {
-                        return subQueryScorer.docID();
-                    }
-
-                    @Override
-                    public DocIdSetIterator iterator() {
-                        return subQueryScorer.iterator();
-                    }
-
-                    @Override
-                    public float getMaxScore(int upTo) {
-                        return Float.MAX_VALUE; // TODO: what would be a good upper bound?
-                    }
-                };
+                Scorer scriptScorer = makeScriptScorer(subQueryScorer, context, null);
 
                 if (minScore != null) {
                     scriptScorer = new MinScoreScorer(this, scriptScorer, minScore);
                 }
                 return scriptScorer;
             }",NotBuggy,"Add explanations to script score queries (#46693)

While function scores using scripts do allow explanations, they are only
creatable with an expert plugin. This commit improves the situation for
the newer script score query by adding the ability to set the
explanation from the script itself.

To set the explanation, a user would check for `explanation != null` to
indicate an explanation is needed, and then call
`explanation.set(""some description"")`.
",NotBuggy,"The commit message describes adding the ability to set explanations from within a script used in a script score query. The diff shows that a `makeScriptScorer` method is now being used, which likely encapsulates the logic for creating the scorer and handling the explanation. This change introduces new functionality rather than fixing a bug. The answer is **NotBuggy**."
checkstyle,1745.json,bb1b29851f14bf7eaf5656e01c285c0f5f3b9000,"@@ -1,11 +1,11 @@
-    private static String getInstanceName(DetailAST aMethodCallAst)
+    private static String getInstanceName(DetailAST methodCallAst)
     {
         final String methodCallName =
-                FullIdent.createFullIdentBelow(aMethodCallAst).getText();
+                FullIdent.createFullIdentBelow(methodCallAst).getText();
         final int lastDotIndex = methodCallName.lastIndexOf('.');
         String instanceName = """";
         if (lastDotIndex != -1) {
             instanceName = methodCallName.substring(0, lastDotIndex);
         }
         return instanceName;
     }",NotBuggy,"Prefixes, coding, #512
",NotBuggy,"The commit message ""Prefixes, coding, #512"" is vague. The code change renames `aMethodCallAst` to `methodCallAst`. This looks like a simple renaming refactoring, which does not indicate a bug fix. The answer is **NotBuggy**."
checkstyle,1737.json,bb1b29851f14bf7eaf5656e01c285c0f5f3b9000,"@@ -1,4 +1,4 @@
-    public void setAllowedDistance(int aAllowedDistance)
+    public void setAllowedDistance(int allowedDistance)
     {
-        this.mAllowedDistance = aAllowedDistance;
+        this.allowedDistance = allowedDistance;
     }",NotBuggy,"Prefixes, coding, #512
",NotBuggy,"The commit message ""Prefixes, coding, #512"" is vague. The diff shows a change in variable naming from `aAllowedDistance` to `allowedDistance` and `mAllowedDistance` to `allowedDistance`. This change suggests a coding style update, possibly related to code prefixes. There is no indication of a bug fix. The answer is **NotBuggy**."
checkstyle,2819.json,0d8e93ce79ea33b72b3e904d8796984271ee3d8d,"@@ -1,10 +1,10 @@
     private static DetailAST findTokenWhichBeginsTheLine(DetailAST root) {
-        DetailAST tokenWhichBeginsTheLine;
+        final DetailAST tokenWhichBeginsTheLine;
         if (isUsingOfObjectReferenceToInvokeMethod(root)) {
             tokenWhichBeginsTheLine = findStartTokenOfMethodCallChain(root);
         }
         else {
             tokenWhichBeginsTheLine = root.getFirstChild().findFirstToken(TokenTypes.IDENT);
         }
         return tokenWhichBeginsTheLine;
     }",NotBuggy,"minor: enforce FinalLocalVariableCheck on checkstyle code
",NotBuggy,"The commit message indicates enforcement of `FinalLocalVariableCheck` on checkstyle code. The diff introduces the `final` keyword to the local variable `tokenWhichBeginsTheLine`. This change aligns with the commit message and suggests an improvement in code style and potentially preventing unintended modifications of the variable. However, it doesn't directly indicate a bug fix. The answer is **NotBuggy**.
"
checkstyle,627.json,b8ca6a585b824e91b3b8c72dd5cc53c0eb0ab0f1,"@@ -1,11 +1,11 @@
     private void tagSuppressions(Collection<TextBlock> comments)
     {
         for (TextBlock comment : comments) {
             final int startLineNo = comment.getStartLineNo();
             final String[] text = comment.getText();
-            tagCoontLine(text[0], startLineNo, comment.getStartColNo());
+            tagCommentLine(text[0], startLineNo, comment.getStartColNo());
             for (int i = 1; i < text.length; i++) {
-                tagCoontLine(text[i], startLineNo + i, 0);
+                tagCommentLine(text[i], startLineNo + i, 0);
             }
         }
     }",NotBuggy,"Prefixes, checkstyle, #512

Conflicts:
	src/main/java/com/puppycrawl/tools/checkstyle/TreeWalker.java
",Buggy,"The commit message mentions ""checkstyle"", suggesting code style improvements. The diff shows a change from `tagCoontLine` to `tagCommentLine`. This looks like a typo correction, which can be considered a bug fix. The answer is **Buggy**."
checkstyle,2956.json,58219b9bd31c8f7fbc4f4bba0eeb811851f276b3,"@@ -1,12 +1,12 @@
-    private boolean isIgnoredParam(DetailAST aParamDef)
+    private boolean isIgnoredParam(DetailAST paramDef)
     {
         boolean result = false;
-        if (mIgnorePrimitiveTypes) {
-            final DetailAST parameterType = aParamDef.
+        if (ignorePrimitiveTypes) {
+            final DetailAST parameterType = paramDef.
                     findFirstToken(TokenTypes.TYPE).getFirstChild();
-            if (mPrimitiveDataTypes.contains(parameterType.getType())) {
+            if (primitiveDataTypes.contains(parameterType.getType())) {
                 result = true;
             }
         }
         return result;
     }",NotBuggy,"Prefixes, checks, #512

Conflicts:
	src/main/java/com/puppycrawl/tools/checkstyle/checks/NewlineAtEndOfFileCheck.java
",NotBuggy,"The commit message ""Prefixes, checks, #512"" suggests changes related to prefixes and checks, likely in the context of issue #512. The code diff shows a change in the `isIgnoredParam` method within the `NewlineAtEndOfFileCheck`. Specifically, it replaces `mIgnorePrimitiveTypes` with `ignorePrimitiveTypes`, `aParamDef` with `paramDef`, and `mPrimitiveDataTypes` with `primitiveDataTypes`. These changes suggest that the code is being updated to use more conventional naming conventions (removing the 'm' prefix for member variables) and potentially to align with the requirements of issue #512, which might involve ignoring certain parameter types. The change itself doesn't scream bug fix but rather a refactoring or style update. Therefore, the answer is **NotBuggy**."
checkstyle,1180.json,d97a2df674716efb7deda50180ae933bdc46d68a,"@@ -1,9 +1,9 @@
-    private boolean isEmptyLoop(DetailAST aAST, int aParentType)
+    private boolean isEmptyLoop(DetailAST ast, int parentType)
     {
-        return mAllowEmptyLoops
-            && (isEmptyBlock(aAST, aParentType, TokenTypes.LITERAL_FOR)
-                    || isEmptyBlock(aAST,
-                            aParentType, TokenTypes.LITERAL_WHILE)
-                            || isEmptyBlock(aAST,
-                                    aParentType, TokenTypes.LITERAL_DO));
+        return allowEmptyLoops
+            && (isEmptyBlock(ast, parentType, TokenTypes.LITERAL_FOR)
+                    || isEmptyBlock(ast,
+                            parentType, TokenTypes.LITERAL_WHILE)
+                            || isEmptyBlock(ast,
+                                    parentType, TokenTypes.LITERAL_DO));
     }",NotBuggy,"Prefixes, whitespace, #512
",NotBuggy,"The commit message ""Prefixes, whitespace, #512"" suggests changes related to code style, possibly including variable naming and whitespace adjustments. The diff shows renaming `aAST` to `ast` and `aParentType` to `parentType`, and `mAllowEmptyLoops` to `allowEmptyLoops` which is likely a field name change to remove the m prefix. These changes are consistent with code style improvements and do not indicate a bug fix. The answer is **NotBuggy**."
checkstyle,2157.json,750b733bd124eaea34d4692a1bb8d0fd0fe90976,"@@ -1,4 +1,4 @@
-    public void setIgnoreName(boolean aValue)
+    public void setIgnoreName(boolean value)
     {
-        mIgnoreName = aValue;
+        ignoreName = value;
     }",NotBuggy,"Prefixes, naming, #512
",NotBuggy,"The commit message ""Prefixes, naming, #512"" suggests changes related to prefixes and naming conventions, possibly addressing issue #512. The diff shows a change in the setter method `setIgnoreName`. The variable name `aValue` is changed to `value`, and `mIgnoreName` is changed to `ignoreName`. These changes are likely refactoring to improve code readability and consistency. There is no indication of a bug fix in this diff. The answer is **NotBuggy**.
"
checkstyle,2400.json,50c9bb69bcd93e6fbf272670e8b9d95f2f451c5f,"@@ -1,4 +1,4 @@
-    public void setAllowMarkerInterfaces(boolean aFlag)
+    public void setAllowMarkerInterfaces(boolean flag)
     {
-        mAllowMarkerInterfaces = aFlag;
+        allowMarkerInterfaces = flag;
     }",NotBuggy,"Prefixes, design, #512
",NotBuggy,"The commit message ""Prefixes, design, #512"" is vague and doesn't explicitly mention a bug fix. The code change renames the parameter `aFlag` to `flag` and the member variable `mAllowMarkerInterfaces` to `allowMarkerInterfaces`. This looks like a refactoring change to improve code style or naming conventions. There's no indication of error handling, logical corrections, or exception handling improvements. The answer is **NotBuggy**."
checkstyle,1279.json,d97a2df674716efb7deda50180ae933bdc46d68a,"@@ -1,4 +1,4 @@
-    public void setEachLine(boolean aEachLine)
+    public void setEachLine(boolean eachLine)
     {
-        mEachLine = aEachLine;
+        this.eachLine = eachLine;
     }",NotBuggy,"Prefixes, whitespace, #512
",NotBuggy,"The commit message ""Prefixes, whitespace, #512"" suggests some minor changes related to prefixes, whitespace, or issue #512. The diff shows a change in the variable name from `aEachLine` to `eachLine` and `mEachLine` to `this.eachLine`. This looks like a simple renaming and code style change, not a bug fix. The answer is **NotBuggy**."
checkstyle,2496.json,16dac636b246c658cad31ee9b2e45c53b851d4f2,"@@ -1,4 +1,4 @@
-    public final void setSortImportsInGroupAlphabetically(boolean aValue)
+    public final void setSortImportsInGroupAlphabetically(boolean value)
     {
-        mSortImportsInGroupAlphabetically = aValue;
+        sortImportsInGroupAlphabetically = value;
     }",NotBuggy,"Prefixes, imports, #512
",NotBuggy,"The commit message ""Prefixes, imports, #512"" suggests changes related to prefixes, imports, and issue #512. The diff shows a change in a setter method where the parameter name and the field name are updated from `aValue` and `mSortImportsInGroupAlphabetically` to `value` and `sortImportsInGroupAlphabetically` respectively. This looks like a simple renaming and doesn't indicate a bug fix.
The answer is **NotBuggy**."
checkstyle,2285.json,5f68bb50fd191fec409f5edb90755a18fce3e0ac,"@@ -1,6 +1,12 @@
     public void beginTree(DetailAST rootAst)
     {
         immutableClassShortNames.clear();
-        final List<String> shortNames = getClassShortNames(immutableClassCanonicalNames);
-        immutableClassShortNames.addAll(shortNames);
+        final List<String> classShortNames =
+                getClassShortNames(immutableClassCanonicalNames);
+        immutableClassShortNames.addAll(classShortNames);
+
+        ignoreAnnotationShortNames.clear();
+        final List<String> annotationShortNames =
+                getClassShortNames(ignoreAnnotationCanonicalNames);
+        ignoreAnnotationShortNames.addAll(annotationShortNames);
     }",NotBuggy,"Visibility Modifier Check, ignore annotated fields
",NotBuggy,"The commit message indicates changes related to visibility modifier checks and ignoring annotated fields. The diff shows the addition of logic to handle `ignoreAnnotationCanonicalNames`, similar to how `immutableClassCanonicalNames` are processed. This suggests that the change introduces a mechanism to exclude fields annotated with specific annotations from visibility checks. This is likely a feature enhancement rather than a bug fix. The answer is **NotBuggy**.
"
checkstyle,1285.json,0d8e93ce79ea33b72b3e904d8796984271ee3d8d,"@@ -1,17 +1,17 @@
     private static DetailAST getWhitespaceFollowedNode(DetailAST ast) {
-        DetailAST whitespaceFollowedAst;
+        final DetailAST whitespaceFollowedAst;
         switch (ast.getType()) {
             case TokenTypes.TYPECAST:
                 whitespaceFollowedAst = ast.findFirstToken(TokenTypes.RPAREN);
                 break;
             case TokenTypes.ARRAY_DECLARATOR:
                 whitespaceFollowedAst = getArrayDeclaratorPreviousElement(ast);
                 break;
             case TokenTypes.INDEX_OP:
                 whitespaceFollowedAst = getIndexOpPreviousElement(ast);
                 break;
             default:
                 whitespaceFollowedAst = ast;
         }
         return whitespaceFollowedAst;
     }",NotBuggy,"minor: enforce FinalLocalVariableCheck on checkstyle code
",NotBuggy,"The commit message indicates enforcement of `FinalLocalVariableCheck` on checkstyle code. The diff introduces the `final` keyword to the local variable `whitespaceFollowedAst`. This change suggests an update to adhere to code style guidelines by marking a local variable as final, which doesn't inherently fix a bug but rather improves code quality and readability. The answer is **NotBuggy**."
checkstyle,2860.json,1bd7f9bdf3dc678b70e79481971aebc50db69dc7,"@@ -1,4 +1,4 @@
-    public final void setAllowSamelineMultipleAnnotations(boolean aAllow)
+    public final void setAllowSamelineMultipleAnnotations(boolean allow)
     {
-        mAllowSamelineMultipleAnnotations = aAllow;
+        allowSamelineMultipleAnnotations = allow;
     }",NotBuggy,"Prefixes, annotation, #512
",NotBuggy,"The commit message ""Prefixes, annotation, #512"" is vague. The diff shows a change in a setter method where the parameter name and the field name are changed from `aAllow` and `mAllowSamelineMultipleAnnotations` to `allow` and `allowSamelineMultipleAnnotations` respectively. This looks like a simple renaming or refactoring. There's no indication of a bug fix. The answer is **NotBuggy**."
checkstyle,1965.json,eb5b977a475c2240db5efc83aec8973d734895eb,"@@ -1,13 +1,13 @@
         public Optional<FinalVariableCandidate> findFinalVariableCandidateForAst(DetailAST ast) {
-            Optional<FinalVariableCandidate> result = Optional.absent();
+            Optional<FinalVariableCandidate> result = Optional.empty();
             DetailAST storedVariable = null;
             final Optional<FinalVariableCandidate> candidate =
-                Optional.fromNullable(scope.get(ast.getText()));
+                Optional.ofNullable(scope.get(ast.getText()));
             if (candidate.isPresent()) {
                 storedVariable = candidate.get().variableIdent;
             }
             if (storedVariable != null && isSameVariables(storedVariable, ast)) {
                 result = candidate;
             }
             return result;
         }",NotBuggy,"Pull #3293: Replace Guava's Optional with Java's native (#3293)

",NotBuggy,The commit message indicates a replacement of Guava's `Optional` with Java's native `Optional`. The diff shows changes from `Optional.absent()` to `Optional.empty()` and `Optional.fromNullable()` to `Optional.ofNullable()`. These changes are part of the migration to the native Java `Optional` and do not indicate a bug fix. The answer is **NotBuggy**.
checkstyle,3762.json,b94bac0719851b65b59b4aafc5bd3ef029321ed7,"@@ -1,4 +1,4 @@
     public Charset getCharset()
     {
-        return mCharset;
+        return charset;
     }",NotBuggy,"Prefixes, api, #512
",NotBuggy,"The commit message ""Prefixes, api, #512"" is vague and doesn't clearly indicate a bug fix. The code change replaces `mCharset` with `charset`. Without more context, it's difficult to determine if this change is related to fixing a bug or simply renaming a variable. It could be a refactoring change to align with coding standards or to improve readability. Therefore, based on the available information, it's not possible to confirm a bug fix.
The answer is **NotBuggy**."
lombok,2687.json,a05360a8eaba0de61f16f75816daf5a5af0a4567,"@@ -1,5 +1,4 @@
 	@Override public boolean visit(QualifiedNameReference node, ClassScope scope) {
-		setGeneratedBy(node, source);
-		applyOffsetExpression(node);
+		fixPositions(setGeneratedBy(node, source));
 		return super.visit(node, scope);
 	}",NotBuggy,ecj @Builder @Singular support for j.u. sets and maps.,NotBuggy,"The commit message indicates the addition of `@Builder` and `@Singular` support for sets and maps. The diff modifies the `visit` method by replacing two separate calls (`setGeneratedBy` and `applyOffsetExpression`) with a single call `fixPositions(setGeneratedBy(node, source))`. This change seems to consolidate operations related to node processing, potentially improving efficiency or ensuring correct execution order. The change doesn't directly indicate a bug fix, but rather a modification to how the compiler handles qualified name references, likely to accommodate the new features. The answer is **NotBuggy**.
"
lombok,2499.json,fb401d4887895d1ebb8529d6323797f1bc8072a2,"@@ -1,3 +1,3 @@
-	public static EclipseSingularsRecipes get() {
-		return INSTANCE;
-	}

+				@Override public ReturnStatement get() {
+					return chain ? new ReturnStatement(new ThisReference(0, 0), 0, 0) : null;
+				}",NotBuggy,ecj: setter methods now return self(),NotBuggy,"The commit message ""ecj: setter methods now return self()"" doesn't align with the provided diff. The diff shows a change in a `get()` method, which now returns a `ReturnStatement` containing a `ThisReference` or `null` based on the `chain` boolean. This suggests a modification in how a return statement is generated, potentially for method chaining or a similar pattern. The original code simply returned a singleton instance. This change doesn't seem to be directly related to setter methods returning `self()`. It's more likely related to a builder pattern or fluent interface where methods are chained together. The change doesn't obviously indicate a bug fix, but it's a significant functional change. The answer is **NotBuggy**.
"
lombok,1375.json,5b4ab9dae8940973ce8b87c8c2a0388b768dac89,"@@ -1,4 +1,7 @@
-	@Override public java.util.List<Name> listFieldsToBeGenerated(JavacNode builderType, SingularData data) {
-		String p = data.getPluralName().toString();
-		return Arrays.asList(builderType.toName(p + ""$key""), builderType.toName(p + ""$value""));
+	@Override public java.util.List<Name> listMethodsToBeGenerated(SingularData data, JavacNode builderType) {
+		if (useGuavaInstead(builderType)) {
+			return guavaMapSingularizer.listMethodsToBeGenerated(data, builderType);
+		}
+		
+		return super.listMethodsToBeGenerated(data, builderType);
 	}",NotBuggy,added the always use guava lombok.config option for @Singular @Builder.,NotBuggy,"The commit message indicates the addition of a configuration option to always use Guava for `@Singular` `@Builder` with Lombok. The diff shows that the `listMethodsToBeGenerated` method is overridden to conditionally use `guavaMapSingularizer` based on the `useGuavaInstead` configuration. This change introduces new functionality based on a configuration option, rather than fixing a bug. The answer is **NotBuggy**."
lombok,2745.json,35691e83edffdadd5ef438793eec9c968e8bfd35,"@@ -1,9 +1,9 @@
-	public static TypeReference[] copyTypes(TypeReference[] refs) {
+	public static TypeReference[] copyTypes(TypeReference[] refs, ASTNode source) {
 		if ( refs == null ) return null;
 		TypeReference[] outs = new TypeReference[refs.length];
 		int idx = 0;
 		for ( TypeReference ref : refs ) {
-			outs[idx++] = copyType(ref);
+			outs[idx++] = copyType(ref, source);
 		}
 		return outs;
 	}",NotBuggy,"Massive change to the eclipse handlers: They now set the 'generatedBy' flag which we can use to patch eclipse in specific places to ignore generated nodes.
",NotBuggy,"The commit message indicates a significant change to Eclipse handlers, focusing on setting a 'generatedBy' flag to allow targeted patching of Eclipse to ignore generated nodes. The provided diff shows a modification to the `copyTypes` method, where a new parameter `ASTNode source` is introduced. This suggests that the change is related to providing context (the source AST node) during the copying of type references, which could be used to determine if a node is generated and thus needs the 'generatedBy' flag set. This doesn't directly indicate a bug fix but rather an enhancement or feature addition. The answer is **NotBuggy**."
lombok,2695.json,0234039babafa1098c337136a97f4e1f94e35855,"@@ -1,5 +1,5 @@
 	@Override public boolean visit(SingleMemberAnnotation node, BlockScope scope) {
 		setGeneratedBy(node, source);
-		applyOffset(node, newSourceStart, newSourceEnd);
+		applyOffset(node);
 		return super.visit(node, scope);
 	}",NotBuggy,"Removed statics and lots of parameters, reimplemented recalcSourcePosition
",NotBuggy,"The commit message indicates a refactoring effort focused on removing static elements and simplifying parameter passing, alongside reimplementing the `recalcSourcePosition` functionality. The diff shows a change in the `visit` method of the `SingleMemberAnnotation` class, where the `applyOffset` method call is modified. The original call `applyOffset(node, newSourceStart, newSourceEnd)` is replaced with `applyOffset(node)`. This suggests that the `applyOffset` method's implementation has been altered to no longer require the `newSourceStart` and `newSourceEnd` parameters, likely as part of the refactoring described in the commit message. This change doesn't inherently indicate a bug fix but rather a structural improvement. The answer is **NotBuggy**.
"
lombok,962.json,4be46113e81292a88cd5fdb3a5ce18fbcffd570d,"@@ -1,7 +1,7 @@
-	public static LombokOptions replaceWithDelombokOptions(Context context) {
+	public static Javac6BasedLombokOptions replaceWithDelombokOptions(Context context) {
 		Options options = Options.instance(context);
 		context.put(optionsKey, (Options)null);
-		LombokOptions result = new LombokOptions(context);
+		Javac6BasedLombokOptions result = new Javac6BasedLombokOptions(context);
 		result.putAll(options);
 		return result;
 	}",NotBuggy,"Compiler options can now be specified for JDK 8 or JDK 6/7 or lower.

After this was finalized I realize that we might consider moving entirely
to String based options, instead of inferring the options from the
provided enum. This setup does have the benefit of throwing exceptions when
options are not present.
",NotBuggy,"The commit message indicates a change in compiler options for different JDK versions. The diff shows a change in the return type of the `replaceWithDelombokOptions` method from `LombokOptions` to `Javac6BasedLombokOptions`. This suggests a modification to accommodate different compiler options based on the JDK version. The change doesn't directly indicate a bug fix, but rather an enhancement or refactoring to support different JDK versions. The answer is **NotBuggy**."
lombok,244.json,eca219ee6433cd964f0549a114a791ca4eb9f0fa,"@@ -1,21 +1,20 @@
 	private <T> T readObject(JCTree tree, String fieldName, T defaultValue) {
 		Class<?> tClass = tree.getClass();
 		Map<String, Field> c = reflectionCache.get(tClass);
 		if (c == null) reflectionCache.put(tClass, c = new HashMap<String, Field>());
 		Field f = c.get(fieldName);
 		if (f == null) {
 			try {
-				f = tClass.getDeclaredField(fieldName);
+				f = Permit.getField(tClass, fieldName);
 			} catch (Exception e) {
 				return defaultValue;
 			}
-			f.setAccessible(true);
 			c.put(fieldName, f);
 		}
 		
 		try {
 			return (T) f.get(tree);
 		} catch (Exception e) {
 			return defaultValue;
 		}
 	}",NotBuggy,"eliminate you are using private API warnings by streamlining all reflective access via a class that uses sun.misc.Unsafe to arrange access. From the nqzero permit-reflect library.
",NotBuggy,"The commit message indicates a change to eliminate ""private API"" warnings by streamlining reflective access using `sun.misc.Unsafe`. The diff shows that `f.setAccessible(true)` is removed and `tClass.getDeclaredField(fieldName)` is replaced with `Permit.getField(tClass, fieldName)`. This suggests that the code is now using a different mechanism (likely involving `sun.misc.Unsafe` as the commit message states) to access the field, instead of directly setting it accessible. This change is aimed at avoiding warnings related to accessing private APIs, which can be considered a bug or at least an undesirable behavior. The change is not directly fixing a functional bug, but rather addressing a warning/issue related to API usage.

The answer is **NotBuggy**."
lombok,817.json,f07948d4959cfdb81bb0c540ebcbe879df0109fb,"@@ -1,3 +1,3 @@
 	@Override public int hashCode() {
-		return path.hashCode();
+		return eclipseIniPath.hashCode();
 	}",NotBuggy,"The installer is now waaay smarter on linuxes, and should support, out of the box and automatically, debian and debianesques like ubuntu.
",Buggy,"The commit message describes improvements to the installer on Linux, specifically targeting Debian and Ubuntu. The diff shows a change in the `hashCode()` method where `path.hashCode()` is replaced with `eclipseIniPath.hashCode()`. This change suggests that the hash code calculation was previously based on an incorrect path, potentially leading to issues with data structures that rely on hash codes (e.g., hash tables, hash sets). This correction directly relates to fixing a bug where the hash code was not calculated correctly, impacting the installer's functionality. The answer is **Buggy**.
"
lombok,2906.json,bb66465751b8f0b129e53e639854f95fcdb3dca8,"@@ -1,20 +1,12 @@
 	public static TypeReference createTypeReference(String typeName, Annotation source) {
 		int pS = source.sourceStart, pE = source.sourceEnd;
 		long p = (long)pS << 32 | pE;
 		
-		TypeReference typeReference;
-		if (typeName.contains(""."")) {
-			
-			char[][] typeNameTokens = fromQualifiedName(typeName);
-			long[] pos = new long[typeNameTokens.length];
-			Arrays.fill(pos, p);
-			
-			typeReference = new QualifiedTypeReference(typeNameTokens, pos);
-		}
-		else {
-			typeReference = null;
-		}
+		char[][] typeNameTokens = fromQualifiedName(typeName);
+		long[] pos = new long[typeNameTokens.length];
+		Arrays.fill(pos, p);
 		
+		TypeReference typeReference = new QualifiedTypeReference(typeNameTokens, pos);
 		setGeneratedBy(typeReference, source);
 		return typeReference;
 	}",NotBuggy,"Custom log declaration
",NotBuggy,"The commit message ""Custom log declaration"" doesn't suggest a bug fix. The code changes involve creating a `TypeReference` object. The original code had a conditional block that created either a `QualifiedTypeReference` or set `typeReference` to null based on whether the `typeName` contained a ""."". The modified code removes this conditional block and always creates a `QualifiedTypeReference`. This change suggests a modification to how type references are created, but it doesn't inherently indicate a bug fix. It's possible the original conditional logic was flawed or unnecessary, but without more context, it's impossible to determine if this change is correcting a bug. The change could be part of a larger feature implementation or refactoring.
**NotBuggy**
"
lombok,2557.json,519f95c2410c2e43c60e81233cad105552975606,"@@ -1,4 +1,6 @@
 	private static final boolean toBoolean(Object expr, boolean defaultValue) {
 		if (expr == null) return defaultValue;
+		if (expr instanceof FalseLiteral) return false;
+		if (expr instanceof TrueLiteral) return true;
 		return ((Boolean) expr).booleanValue();
 	}",NotBuggy,builder @Singleton now works fully in eclipse as well! feature done!,Buggy,"The commit message indicates a feature completion related to the `@Singleton` builder, and its improved Eclipse compatibility. The code diff introduces checks for `FalseLiteral` and `TrueLiteral` instances within the `toBoolean` method. This suggests that the previous implementation might have failed to correctly interpret these literal types, potentially leading to incorrect boolean conversions. The added checks ensure that `FalseLiteral` and `TrueLiteral` are explicitly handled, returning `false` and `true` respectively. This correction addresses a specific scenario where the original code produced incorrect results, indicating a bug fix. The answer is **Buggy**.
"
lombok,2948.json,a31e9ffdac96737f5a441efa6c80a2394bba63d1,"@@ -1,3 +1,3 @@
-	public A getAst() {
+	public EclipseAST getAst() {
 		return ast;
 	}",NotBuggy,"use delared ast Type instead of generic one saves a lot of ""checkcasts"" in the byte code
",NotBuggy,"The commit message indicates a performance optimization by using a more specific type (`EclipseAST`) instead of a generic one (`A`). This change avoids unnecessary type checks (`checkcasts`) in the bytecode, which can improve performance. While this is an optimization, it doesn't directly address a bug. The code functions correctly before and after the change, but the change improves efficiency. The answer is **NotBuggy**."
lombok,824.json,df84dffa62fc51231e9215d04b9751dba6c6cf4d,"@@ -1,17 +1,17 @@
 	private static String generateWriteErrorMessage() {
 		String osSpecificError;
 		
-		switch (EclipseFinder.getOS()) {
+		switch (IdeFinder.getOS()) {
 		default:
 		case MAC_OS_X:
 		case UNIX:
 			osSpecificError = "":\nStart terminal, go to the directory with lombok.jar, and run: sudo java -jar lombok.jar"";
 			break;
 		case WINDOWS:
 			osSpecificError = "":\nStart a new cmd (dos box) with admin privileges, go to the directory with lombok.jar, and run: java -jar lombok.jar"";
 			break;
 		}
 		
 		return "", probably because this installer does not have the access rights.\n"" +
 		""Try re-running the installer with administrative privileges"" + osSpecificError;
 	}",NotBuggy,"Massive update to how the installer works, and added netbeans to the installer.
",NotBuggy,"The commit message indicates a large update to the installer and the addition of Netbeans support. The diff shows a modification to the error message generation, specifically in the instructions for running the installer with administrative privileges. The change involves updating the class name from `EclipseFinder` to `IdeFinder` when determining the operating system. This suggests a broader scope for IDE detection, aligning with the addition of Netbeans to the installer. While the change itself doesn't scream ""bug fix"", it's part of a larger feature update. Therefore, it's not directly fixing a bug. The answer is **NotBuggy**."
lombok,1913.json,143143276da646bcc9a195d827364f8499187a6a,"@@ -1,12 +1,14 @@
 	private void printValue(ConfigurationKey<?> key, Object value, boolean verbose, Collection<String> history) {
-		if (verbose) out.printf(""# %s\n"", key.getDescription());
+		if (verbose) out.printf(""# %s%n"", key.getDescription());
 		if (value == null) {
-			out.printf(""clear %s\n"", key.getKeyName());
+			out.printf(""clear %s%n"", key.getKeyName());
 		} else if (value instanceof List<?>) {
-			for (Object element : (List<?>)value) out.printf(""%s += %s\n"", key.getKeyName(), element);
+			List<?> list = (List<?>)value;
+			if (list.isEmpty()) out.printf(""clear %s%n"", key.getKeyName());
+			for (Object element : list) out.printf(""%s += %s%n"", key.getKeyName(), element);
 		} else {
-			out.printf(""%s = %s\n"", key.getKeyName(), value);
+			out.printf(""%s = %s%n"", key.getKeyName(), value);
 		}
 		if (!verbose) return;
-		for (String modification : history) out.printf(""# %s\n"", modification);
+		for (String modification : history) out.printf(""# %s%n"", modification);
 	}",NotBuggy,"[configuration] Have the ConfigurationApp print platform-specific newLine characters, and print clear for empty lists.
",Buggy,"The commit message indicates changes to the ConfigurationApp to print platform-specific newline characters and handle empty lists clearly. The diff confirms these changes. Specifically, `\n` is replaced with `%n` in the `printf` statements, which ensures platform-specific newline characters are used. Additionally, the code now explicitly checks if a list is empty and prints ""clear"" if it is, improving clarity. These changes enhance the output format and readability, addressing a potential issue of unclear representation of empty lists. The answer is **Buggy**."
lombok,1884.json,713822db9894cf99184aff57b3387f99846aa870,"@@ -1,7 +1,13 @@
 			@Override public Object parse(String value) {
 				try {
 					return Enum.valueOf(rawType, value);
 				} catch (Exception e) {
-					return Enum.valueOf(rawType, value.toUpperCase());
+					StringBuilder sb = new StringBuilder();
+					for (int i = 0; i < value.length(); i++) {
+						char c = value.charAt(i);
+						if (Character.isUpperCase(c) && i > 0) sb.append(""_"");
+						sb.append(Character.toUpperCase(c));
+					}
+					return Enum.valueOf(rawType, sb.toString());
 				}
 			}",NotBuggy,"Changed the lombok.nonNull.exceptionType configkey to be an enum, partly to enforce only sensible exceptions and partly to make it possible to have the message be x is null when throwing IAEs.",Buggy,"The code modifies the parsing logic for enum values from a string. It attempts to convert the input string to an enum value. If the initial attempt fails, it transforms the input string to uppercase with underscores before each uppercase character (except the first) and tries the conversion again. This suggests the original parsing logic was not robust enough to handle various input formats, indicating a potential bug or limitation in the previous implementation. The change aims to make the parsing more flexible and resilient to different naming conventions. The answer is **Buggy**."
lombok,2541.json,09ea02e4f5752e615be2ff5177be1fb328702a5b,"@@ -1,11 +1,11 @@
-	private LocalDeclaration createLocalDeclaration(ASTNode source, char[] dollarFieldName, TypeReference type, Expression initializer) {
+	public LocalDeclaration createLocalDeclaration(ASTNode source, char[] dollarFieldName, TypeReference type, Expression initializer) {
 		int pS = source.sourceStart, pE = source.sourceEnd;
 		LocalDeclaration tempVar = new LocalDeclaration(dollarFieldName, pS, pE);
 		setGeneratedBy(tempVar, source);
 		tempVar.initialization = initializer;
 		tempVar.type = type;
 		tempVar.type.sourceStart = pS; tempVar.type.sourceEnd = pE;
 		setGeneratedBy(tempVar.type, source);
 		tempVar.modifiers = Modifier.FINAL;
 		return tempVar;
 	}",NotBuggy,"added package-infos to lots of packages, updated the javadoc in these, and made lots of private/package private methods 'public' in packages that aren't actually public API. Tumi asked for this, and I can imagine this is useful for others who are hacking add-ons to lombok. The package-info files clarify that we aren't committing to long-term stability of anything except the stuff directly in the lombok package, lombok.experimental, and lombok.extern.
",NotBuggy,"The commit message indicates changes related to package information, Javadoc updates, and making private/package-private methods public. The diff shows a change in the visibility of the `createLocalDeclaration` method from `private` to `public`. This aligns with the commit message's description of making methods public for add-on development. There's no indication of a bug fix in either the commit message or the code change. The answer is **NotBuggy**.
"
hibernate-search,521.json,1741d8fb1dd68d1fbe72e5650c3815e39e9426fa,"@@ -1,20 +1,19 @@
 	private TypeMetadata doGetTypeMetadataFor(Class<?> clazz, XClass xClass, ParseContext parseContext) {
-		TypeMetadata.Builder typeMetadataBuilder = new TypeMetadata.Builder( clazz, configContext )
+		TypeMetadata.Builder typeMetadataBuilder = new TypeMetadata.Builder( clazz, parseContext.getIndexManagerType(), configContext )
 				.boost( getBoost( xClass ) )
-				.boostStrategy( AnnotationProcessingHelper.getDynamicBoost( xClass ) )
-				.analyzerReference( configContext.getDefaultLuceneAnalyzerReference() );
+				.boostStrategy( AnnotationProcessingHelper.getDynamicBoost( xClass ) );
 
 		initializePackageLevelAnnotations( packageInfo( clazz ), configContext );
 
 		initializeClass(
 				typeMetadataBuilder,
 				true,
 				EMPTY_PREFIX,
 				parseContext,
 				configContext,
 				false,
 				null
 		);
 
 		return typeMetadataBuilder.build();
 	}",NotBuggy,"HSEARCH-2222 Change the way ScopedAnalyzer are managed

Also provide sensible default analyzer in the remote case.
",Buggy,"The commit message indicates a change in how ScopedAnalyzer are managed and the provision of a default analyzer in the remote case. The code diff modifies the TypeMetadata.Builder constructor to include `parseContext.getIndexManagerType()` and removes the explicit setting of `analyzerReference` using `configContext.getDefaultLuceneAnalyzerReference()`. This suggests a shift in how the analyzer is determined, potentially to provide a more context-aware or dynamic analyzer selection mechanism. It's difficult to determine if this is a bug fix without more context, but the change in analyzer management could be addressing an issue where the default analyzer was not appropriate in certain scenarios. It's possible that the old way of setting the analyzer was causing problems, and this change is intended to resolve them. The answer is **Buggy**."
hibernate-search,5198.json,db76c736dbdcd54cfc605472cadcbb5f3a4051b1,"@@ -1,3 +1,3 @@
-	public JestClient getClient() {
+	public RestClient getClient() {
 		return client;
 	}",NotBuggy,"HSEARCH-2581 Use the official Elasticsearch REST client instead of Jest
",NotBuggy,"The commit message indicates a change from using the Jest client to the official Elasticsearch REST client. The code change reflects this by changing the return type of the `getClient()` method from `JestClient` to `RestClient`. This is a library upgrade/migration rather than a bug fix. The answer is **NotBuggy**.
"
hibernate-search,7079.json,6524d5233928c0ca691e79cc0f28a6b6116d3b1b,"@@ -1,3 +1,3 @@
 	public Address getAddress() {
-		return channel.getAddress();
+		return channelContainer.getChannel().getAddress();
 	}",NotBuggy,"HSEARCH-1459 Refactor ServiceManager and ServiceProvider
",NotBuggy,"The commit message indicates a refactoring of `ServiceManager` and `ServiceProvider`. The diff shows a change in how the `Address` is retrieved, switching from `channel.getAddress()` to `channelContainer.getChannel().getAddress()`. This suggests a change in the internal structure or dependencies of the classes involved. While refactoring can sometimes address underlying issues, this specific change doesn't inherently indicate a bug fix. It's more likely a structural modification to improve code organization or maintainability. The answer is **NotBuggy**.
"
hibernate-search,2895.json,8c9a35568e74f8c4d21eda2a6f517a51c7fad55b,"@@ -1,4 +1,4 @@
-	public PropertySpatialMapping bottomGridLevel(int bottomGridLevel) {
-		spatial.put( ""bottomQuadTreeLevel"", bottomGridLevel );
+	public PropertySpatialMapping bottomSpatialHashLevel(int bottomSpatialHashLevel) {
+		spatial.put( ""bottomSpatialHashLevel"", bottomSpatialHashLevel );
 		return this;
 	}",NotBuggy,"HSEARCH-1327 Reword reference to Quad Tree for Spatial Hash in spatial module
",NotBuggy,"The commit message indicates a simple rewording related to spatial hashing, specifically changing ""Quad Tree"" to ""Spatial Hash"". The code diff confirms this, as it replaces `bottomQuadTreeLevel` with `bottomSpatialHashLevel` in the `PropertySpatialMapping` class. This change doesn't involve any bug fixes, error handling, or logical corrections. It's purely a cosmetic change to align the code with the intended terminology. The answer is **NotBuggy**."
hibernate-search,5149.json,ee9e8d7bcbb74216bde5b521e9d7275cc944bbe7,"@@ -1,7 +1,7 @@
 	public void add(BulkableElasticsearchWork<?> work) {
 		currentBulkWorks.add( work );
-		if ( currentBulkWorks.size() >= MAX_BULK_SIZE ) {
+		if ( currentBulkWorks.size() >= maxBulkSize ) {
 			flushBulked();
 			flushBulk();
 		}
 	}",NotBuggy,"HSEARCH-2764 Set the minim bulk size to 1 for stream work orchestration

For stream works, we only submit single-work changesets, which means
the decision on whether to bulk the work or not will always happen
immediately after each work, when we only have one work to bulk.
Thus if we set the minimum to a value higher than 1, we would always
decide not to start a bulk (because there would always be only one
work to bulk), which would result in terrible performance.
",Buggy,"The commit message indicates a performance issue due to incorrect bulk sizing for stream works. The diff modifies the condition for flushing bulked works, changing it from `MAX_BULK_SIZE` to `maxBulkSize`. This suggests that the `MAX_BULK_SIZE` was not appropriate for stream works, and `maxBulkSize` is now being used, likely configured to a more suitable value (possibly 1, as suggested by the commit message). This resolves a performance bug. The answer is **Buggy**.
"
hibernate-search,6283.json,359d8d7e1ab9ba2d9abc0aa8c4a16c7f61526996,"@@ -1,6 +1,6 @@
 	public void addDelete(String entityClassName, byte[] id) {
-		GenericRecord delete = new GenericData.Record( schemas.get( ""Delete"" ) );
+		GenericRecord delete = new GenericData.Record( protocol.getType( ""Delete"" ) );
 		delete.put( ""class"", entityClassName );
 		delete.put( ""id"", ByteBuffer.wrap( id ) );
 		operations.add( delete );
 	}",NotBuggy,"HSEARCH-757 Use Avro Protocol to implement proper schema relations

The previous solution was fragile and protocols accept several types with cross references
",Buggy,"The commit message indicates a change to use Avro Protocol for schema relations, replacing a fragile solution. The diff shows a modification in the `addDelete` method where `schemas.get(""Delete"")` is replaced with `protocol.getType(""Delete"")`. This change suggests a shift from a simple schema lookup to using the Avro protocol to retrieve the schema type, likely to improve robustness and handle cross-references as mentioned in the commit message. This indicates a bug fix or improvement related to schema handling. The answer is **Buggy**."
hibernate-search,1714.json,afb48fce1e1b306db8ec9f5f14fd5e9eba2afb0b,"@@ -1,3 +1,3 @@
 			public Function<HashSet<IndexedTypeIdentifier>, IndexedTypeSet> finisher() {
-				return IndexedTypesSets::fromSafeHashSet;
+				return IndexedTypeSets::fromSafeHashSet;
 			}",NotBuggy,"HSEARCH-1404 Rename IndexedTypesSets to IndexedTypeSets
",NotBuggy,"The commit message indicates a renaming of `IndexedTypesSets` to `IndexedTypeSets`. The diff shows a change in a method reference from `IndexedTypesSets::fromSafeHashSet` to `IndexedTypeSets::fromSafeHashSet`. This change directly reflects the renaming described in the commit message. There's no indication of a bug fix, error handling, or logical correction. It's a simple renaming refactoring. The answer is **NotBuggy**.
"
hibernate-search,6888.json,2bcf160fa7b80fa94734705d593e3cbb50ee7aa5,"@@ -1,50 +1,50 @@
 	private FetchingStrategy createCriteriaFetchingStrategy(
 			CacheMode cacheMode, int entityFetchSize, Integer maxResults) throws Exception {
 		Class<?> entityType = jobData.getEntityType( entityName );
 		Object upperBound = SerializationUtil.deserialize( serializedUpperBound );
 		Object lowerBound = SerializationUtil.deserialize( serializedLowerBound );
 		Set<Criterion> customQueryCriteria = jobData.getCustomQueryCriteria();
 
 		EntityTypeDescriptor typeDescriptor = jobData.getEntityTypeDescriptor( entityType );
 		IdOrder idOrder = typeDescriptor.getIdOrder();
 
 		return (session, lastCheckpointInfo) -> {
-			Criteria criteria = session.createCriteria( entityType );
+			Criteria criteria = new CriteriaImpl( entityType.getName(), session.unwrap( SessionImplementor.class ) );
 
 			// build orders for this entity
 			idOrder.addAscOrder( criteria );
 
 			// build criteria using job context data
 			customQueryCriteria.forEach( c -> criteria.add( c ) );
 
 			// build criteria using bounds
 			if ( upperBound != null ) {
 				criteria.add( idOrder.idLesser( upperBound ) );
 			}
 			if ( lastCheckpointInfo != null ) {
 				criteria.add( idOrder.idGreater( lastCheckpointInfo.getLastProcessedEntityId() ) );
 			}
 			else if ( lowerBound != null ) {
 				criteria.add( idOrder.idGreaterOrEqual( lowerBound ) );
 			}
 
 			if ( maxResults != null ) {
 				int remaining;
 				if ( lastCheckpointInfo != null ) {
 					remaining = maxResults - lastCheckpointInfo.getProcessedEntityCount();
 				}
 				else {
 					remaining = maxResults;
 				}
 				criteria.setMaxResults( remaining );
 			}
 
 			return criteria.setReadOnly( true )
 					.setCacheable( false )
 					.setLockMode( LockMode.NONE )
 					.setFlushMode( FlushMode.MANUAL )
 					.setCacheMode( cacheMode )
 					.setFetchSize( entityFetchSize )
 					.scroll( ScrollMode.FORWARD_ONLY );
 		};
 	}",NotBuggy,"HSEARCH-2910 JSR-352: Avoid logging warnings caused by internal usage of the legacy Criteria API
",NotBuggy,"The commit message indicates that the change is to avoid logging warnings caused by internal usage of the legacy Criteria API. The diff shows that `session.createCriteria( entityType )` is replaced with `new CriteriaImpl( entityType.getName(), session.unwrap( SessionImplementor.class ) )`. This change likely suppresses the warnings that are generated when using the legacy Criteria API directly. This is done by using the `CriteriaImpl` directly, which is an internal implementation detail. This change is not directly fixing a bug, but rather suppressing warnings caused by the usage of a deprecated API. The answer is **NotBuggy**."
hibernate-search,75.json,bb3f94df96e6e487ba231e4e201064cbb850aa02,"@@ -1,3 +1,3 @@
-	protected final void applyToLuceneOptions(LuceneOptions luceneOptions, String name, Number value, Document document) {
+	protected void applyToLuceneOptions(LuceneOptions luceneOptions, String name, Number value, Document document) {
 		luceneOptions.addNumericFieldToDocument( name, value, document );
 	}",NotBuggy,"HSEARCH-997 Adding short numeric bridge
",NotBuggy,"The commit message indicates the addition of a short numeric bridge. The code change modifies the `applyToLuceneOptions` method by removing the `final` keyword. This change doesn't seem to be related to fixing a bug. It's likely related to allowing subclasses to override this method, which is a feature addition or refactoring rather than a bug fix. The answer is **NotBuggy**."
hibernate-search,5090.json,3bf6cd975dc1182ff31acb0eb5da642003664708,"@@ -1,3 +1,3 @@
-		public GetIndexMappingWork build() {
-			return new GetIndexMappingWork( this );
+		public GetIndexTypeMappingsWork build() {
+			return new GetIndexTypeMappingsWork( this );
 		}",NotBuggy,"HSEARCH-2581 Make GsonService part of the ElasticsearchWorkExecutionContext

And make JestAPIFormatter a utils class taking a GsonService as a parameter,
since it's only ever used where an execution context is available.
",NotBuggy,"The commit message indicates a refactoring of how `GsonService` is used within the `ElasticsearchWorkExecutionContext`. The diff shows a change from `GetIndexMappingWork` to `GetIndexTypeMappingsWork`. This change, while potentially related to the refactoring described in the commit message, doesn't inherently indicate a bug fix. It seems more like a change in the type of work being built, possibly to align with the new context. Therefore, it's unlikely that this change is directly addressing a bug. The answer is **NotBuggy**.
"
hibernate-search,2698.json,45debe412916f6c6b4b4fbc84270dca378f0153d,"@@ -1,7 +1,7 @@
-	public Map<String, AnalyzerDef> build() {
-		Map<String, AnalyzerDef> result = new LinkedHashMap<>();
+	public SimpleLuceneAnalysisDefinitionRegistry build() {
+		SimpleLuceneAnalysisDefinitionRegistry registry = new SimpleLuceneAnalysisDefinitionRegistry();
 		for ( Map.Entry<String, LuceneAnalyzerDefinitionContextImpl> entry : definitions.entrySet() ) {
-			result.put( entry.getKey(), entry.getValue().build() );
+			registry.register( entry.getKey(), entry.getValue().build() );
 		}
-		return result;
+		return registry;
 	}",NotBuggy,"HSEARCH-2726 Store Lucene analyzer definitions in a dedicated class

This doesn't change much for now, but it will allow future commits
to add more information to the ""analysis definition registry"",
in particular normalizer definitions.
",NotBuggy,"The commit message indicates a change in how Lucene analyzer definitions are stored, moving them to a dedicated class. The diff shows that the `build()` method now returns a `SimpleLuceneAnalysisDefinitionRegistry` instead of a `Map<String, AnalyzerDef>`. It also registers the analyzer definitions using `registry.register()` instead of putting them into a map. This change refactors the code to use a dedicated registry class, which is intended to allow future enhancements. There is no indication of a bug fix in this change. The answer is **NotBuggy**."
hibernate-search,4788.json,9451d18a3cf1f82dc134dcb6ddbd19ff771c1e03,"@@ -1,7 +1,7 @@
-		public void set(String name, Object value, Document document, LuceneOptions luceneOptions) {
-			if ( value == null ) {
-				return;
-			}
+	public void set(String name, Object value, Document document, LuceneOptions luceneOptions) {
+		if ( value == null ) {
+			return;
+		}
 
-			luceneOptions.addFieldToDocument( name, convertToString( (Date) value ), document );
-		}

+		luceneOptions.addFieldToDocument( name, convertToString( (Date) value ), document );
+	}",NotBuggy,"HSEARCH-2245 Handle Calendar and resolution in the Elasticsearch specific BridgeProvider

No tests yet as tests are blocked by HSEARCH-2246
",NotBuggy,"The code change itself doesn't provide enough context to determine if it's a bug fix. It seems to be related to handling `Date` objects and converting them to strings for indexing in Elasticsearch. Without knowing the previous behavior or the intended outcome, it's impossible to say if this change corrects a bug or introduces new functionality. The commit message mentions ""Handle Calendar and resolution"", which suggests a new feature or enhancement rather than a bug fix. The absence of tests further supports the idea that this is not a bug fix, as bug fixes are typically accompanied by tests to verify the fix. The answer is **NotBuggy**."
hibernate-search,3749.json,0049cce1f0e0f6588e4f65bfa8f941267839a22e,"@@ -1,3 +1,3 @@
-		protected int size() {
-			return values.length;
-		}

+			public int size(int[] array) {
+				return array.length;
+			}",NotBuggy,"HSEARCH-2510 Avoid the extra iterable instantiation in CollectionHelper.iteratorFromArray

Because there will be someone who's not happy with that extra
instantiation, I'm sure.

Calls to array accessors seem to be inlined, so there shouldn't be much
execution overhead.
",Buggy,"The commit message indicates an optimization to avoid an extra iterable instantiation in `CollectionHelper.iteratorFromArray`. The diff shows the removal of a `size()` method that was likely part of an anonymous class or iterable implementation, and the introduction of a static `size(int[] array)` method. This change avoids creating an object just to get the size of the array, which aligns with the commit message's intention to reduce object instantiation. While this is an optimization, it's driven by a desire to avoid unnecessary object creation, which can be considered a performance-related bug fix in certain contexts. The answer is **Buggy**."
hibernate-search,5464.json,ab4312a79532524ca5cf2bceaa1e173d4bd10630,"@@ -1,10 +1,8 @@
 		SearchResult scroll(String scrollId) {
-			try ( ServiceReference<ElasticsearchDialectProvider> dialectProvider =
-							getExtendedSearchIntegrator().getServiceManager().requestReference( ElasticsearchDialectProvider.class );
-					ServiceReference<ElasticsearchWorkProcessor> processor =
-							getExtendedSearchIntegrator().getServiceManager().requestReference( ElasticsearchWorkProcessor.class ) ) {
-				ElasticsearchWork<SearchResult> work = dialectProvider.get().getDialect().getWorkFactory()
+			try ( ServiceReference<ElasticsearchService> elasticsearchService =
+					getExtendedSearchIntegrator().getServiceManager().requestReference( ElasticsearchService.class ) ) {
+				ElasticsearchWork<SearchResult> work = elasticsearchService.get().getWorkFactory()
 						.scroll( scrollId, getScrollTimeout() ).build();
-				return processor.get().executeSyncUnsafe( work );
+				return elasticsearchService.get().getWorkProcessor().executeSyncUnsafe( work );
 			}
 		}",NotBuggy,"HSEARCH-2581 Make ElasticsearchService the single entry point for operations on the Elasticsearch server

This means in particular that the Elasticsearch service will manage the
bootstrapping of every component involved, which will allow us to avoid
circular dependencies in the next few commits.

Also, it seems cleaner, and it brings us one step closer to supporting
simulatenous access to multiple Elasticsearch clusters.
",NotBuggy,The commit message indicates a refactoring effort to centralize Elasticsearch operations through the `ElasticsearchService`. The diff shows that the code is being modified to use `ElasticsearchService` instead of directly accessing `ElasticsearchDialectProvider` and `ElasticsearchWorkProcessor`. This change aims to avoid circular dependencies and support multiple Elasticsearch clusters. There is no indication of a bug fix in the commit message or the code changes. The answer is **NotBuggy**.
hibernate-search,1557.json,5cd16e6bac5161f228af80752581fd3bb0baa733,"@@ -1,4 +1,4 @@
 	public void initialize(Properties props, WorkerBuildContext context) {
 		// no-op
-		log.warn( ""initialized \""blackhole\"" backend. Index changes will be prepared but discarded!"" );
+		log.initializedBlackholeBackend();
 	}",NotBuggy,"HSEARCH-675 - Replace slf4j with JBoss Logging (except for archetype)
",NotBuggy,"The commit message indicates a change from slf4j to JBoss Logging. The diff shows a change in how a log message is emitted, replacing `log.warn(...)` with `log.initializedBlackholeBackend()`. This suggests a change in the logging framework used, but not necessarily a bug fix. The change seems to be related to adapting to the new logging framework. The answer is **NotBuggy**."
wildfly,13917.json,874e6032075a06b60e01bd96fe82d32a564bdc04,"@@ -1,2 +1,3 @@
-    protected void writeAttributes(final XMLExtendedStreamWriter writer, final SubsystemMarshallingContext context) throws XMLStreamException {
+    public void undeploy(final DeploymentUnit context) {
+
     }",NotBuggy,"AS7-4386 Add the option to configure a default distinct name for remote EJB's
",NotBuggy,"The provided diff shows an empty method body, which doesn't seem related to the commit message about configuring a default distinct name for remote EJB's. The commit message suggests adding configuration options, but the code change is an empty method. This looks like a stubbed method or incomplete implementation. Without more context, it's difficult to determine if this is a bug fix. However, given the mismatch between the commit message and the code, it's unlikely to be a bug fix. The answer is **NotBuggy**.
"
wildfly,7993.json,315a9c2129eb1ed4921ed4ea128e94c5014ff010,"@@ -1,8 +1,8 @@
-    static void addConnectorServiceConfigs(final OperationContext context, final Configuration configuration, final ModelNode model)  throws OperationFailedException {
+    static void addConnectorServiceConfigurations(final OperationContext context, final Configuration configuration, final ModelNode model)  throws OperationFailedException {
         if (model.hasDefined(CommonAttributes.CONNECTOR_SERVICE)) {
             final List<ConnectorServiceConfiguration> configs = configuration.getConnectorServiceConfigurations();
             for (Property prop : model.get(CommonAttributes.CONNECTOR_SERVICE).asPropertyList()) {
                 configs.add(createConnectorServiceConfiguration(context, prop.getName(), prop.getValue()));
             }
         }
     }",NotBuggy," [WFLY-12859]:  Refactoring code to centralize the configuration of the broker.
",NotBuggy,"The commit message indicates a refactoring to centralize broker configuration. The code change renames the method `addConnectorServiceConfigs` to `addConnectorServiceConfigurations`. This change is purely a refactoring, as it only affects the name of the method and not its functionality. There's no indication of a bug fix. The answer is **NotBuggy**."
wildfly,21431.json,8da4de218c105bcf583f886e35a107b43708cdee,"@@ -1,38 +1,37 @@
     public void readElement(final XMLExtendedStreamReader reader, final ParseResult<JBossServiceXmlDescriptor> value) throws XMLStreamException {
         final JBossServiceXmlDescriptor serviceXmlDescriptor = new JBossServiceXmlDescriptor();
         final List<JBossServiceConfig> serviceConfigs = new ArrayList<JBossServiceConfig>();
         serviceXmlDescriptor.setServiceConfigs(serviceConfigs);
         value.setResult(serviceXmlDescriptor);
 
         final int count = reader.getAttributeCount();
         for(int i = 0; i < count; i++) {
-            final Attribute attribute = Attribute.of(reader.getAttributeName(i));
+            final QName attributeName = reader.getAttributeName(i);
+            final Attribute attribute = Attribute.of(attributeName);
             final String attributeValue = reader.getAttributeValue(i);
             switch(attribute) {
                 case MODE:
                     serviceXmlDescriptor.setControllerMode(JBossServiceXmlDescriptor.ControllerMode.of(attributeValue));
                     break;
-                default:
-                    throw unexpectedContent(reader);
             }
         }
 
         while (reader.hasNext()) {
             switch (reader.nextTag()) {
                 case COMMENT:
                     break;
                 case END_ELEMENT:
                     return;
                 case START_ELEMENT:
                     switch(Element.of(reader.getName())) {
                         case MBEAN:
                             serviceConfigs.add(parseMBean(reader));
                             break;
                         case UNKNOWN:
                             unexpectedContent(reader);
                             break;
                     }
                     break;
             }
         }
     }",NotBuggy,"Start to finish up parsed service deployment.  Testing, etc...
",Buggy,"The commit message indicates the completion of parsing service deployment, including testing. The code diff focuses on the `readElement` method, which parses XML to create a `JBossServiceXmlDescriptor`. The changes involve:

1.  Replacing `Attribute attribute = Attribute.of(reader.getAttributeName(i));` with `final QName attributeName = reader.getAttributeName(i); final Attribute attribute = Attribute.of(attributeName);` to get the attribute name as a QName before creating the Attribute enum.
2.  Removing the `default` case in the attribute processing switch statement that throws an `unexpectedContent` exception.
3.  Adding a `UNKNOWN` case in the element processing switch statement that calls `unexpectedContent(reader)`.

The removal of the `default` case in the attribute processing switch statement suggests that unexpected attributes are now silently ignored, which could mask errors or unexpected configurations. The addition of the `UNKNOWN` case in the element processing switch statement suggests that unexpected elements are now handled by throwing an exception. Overall, the changes seem to be related to handling unexpected content during XML parsing. The removal of the default case for attributes could be problematic if it was intended to catch errors. The addition of the `UNKNOWN` case for elements suggests a more robust error handling for unexpected elements. It's difficult to say definitively if these changes are bug fixes without more context, but the changes in error handling suggest that there might have been issues with how unexpected content was handled previously.

The changes suggest that the code is being updated to handle unexpected XML elements more explicitly, which could be in response to a bug or unexpected behavior. The removal of the `default` case in the attribute processing switch statement is concerning, as it could mask errors. However, the addition of the `UNKNOWN` case in the element processing switch statement suggests a more robust error handling for unexpected elements.

Given the changes in error handling and the commit message indicating testing, it is likely that the changes are related to fixing bugs or unexpected behavior in the XML parsing logic.

The answer is **Buggy**."
wildfly,9094.json,98592f3855adb79a6b9af100072db28f290929e5,"@@ -1,3 +1,3 @@
-    public Context getValue() throws IllegalStateException {
-        return context;
+    public ComponentView getValue() throws IllegalStateException, IllegalArgumentException {
+        return instance;
     }",NotBuggy,"Rework EE Component model for extensibility into EJBs and beyond
",NotBuggy,"The commit message indicates a rework of the EE Component model for extensibility. The diff shows a change in the return type of the `getValue()` method from `Context` to `ComponentView`, and the addition of `IllegalArgumentException` to the throws clause. This suggests a significant change in the component model, potentially to accommodate different types of components (EJBs and beyond). The change in return type and the addition of an exception indicate a change in functionality, but do not necessarily indicate a bug fix. The answer is **NotBuggy**.
"
wildfly,1756.json,546d5ace217411e91be9573c4ddf0c4e88fc6654,"@@ -1,17 +1,19 @@
     private void parseBackups(XMLExtendedStreamReader reader, PathAddress cacheAddress, Map<PathAddress, ModelNode> operations) throws XMLStreamException {
 
-        PathAddress backupsAddress = cacheAddress.append(BackupsResourceDefinition.PATH);
+        PathAddress address = cacheAddress.append(BackupsResourceDefinition.PATH);
+        ModelNode operation = Util.createAddOperation(address);
+        operations.put(address, operation);
 
         while (reader.hasNext() && (reader.nextTag() != XMLStreamConstants.END_ELEMENT)) {
             XMLElement element = XMLElement.forName(reader.getLocalName());
             switch (element) {
                 case BACKUP: {
-                    this.parseBackup(reader, backupsAddress, operations);
+                    this.parseBackup(reader, address, operations);
                     break;
                 }
                 default: {
                     throw ParseUtils.unexpectedElement(reader);
                 }
             }
         }
     }",NotBuggy,"WFLY-5672 Implicitly added resources should be added by parent add operation instead of the parser
",Buggy,"The commit message indicates that resources implicitly added should be added by the parent add operation instead of the parser. The code diff shows that the `parseBackups` method now creates an add operation for the backups resource and adds it to the `operations` map. The `parseBackup` method is then called with the address of the backups resource. This change aligns with the commit message, suggesting a modification in how resources are added, likely to correct a previous issue where the parser was incorrectly handling the addition of these resources. The answer is **Buggy**."
wildfly,14902.json,d9e37a5a768a6ba4f4a3fdd692cab4d5d6064667,"@@ -1,8 +1,8 @@
             public ManagedReference getReference() {
                 try {
-                    final Object value = objectFactoryClassInstance.getObjectInstance(name, null, null, environment);
+                    final Object value = new InitialContext().lookup(lookup);
                     return new ValueManagedReference(new ImmediateValue<Object>(value));
                 } catch (Exception e) {
                     throw new RuntimeException(e);
                 }
             }",NotBuggy,"Revert ""AS7-6121: re-implements subsystem 'lookup' binding service with a linkref, allowing Context#lookupLink() to return the link name""

This reverts commit 9ce9d0010c78fe957fc92b4180e53ea60702994d.
",Buggy,"The commit message indicates a reversion of a previous commit related to the 'lookup' subsystem binding service. The diff shows the removal of code that used `new InitialContext().lookup(lookup)` to obtain a value, suggesting that the previous implementation was problematic. Reverting to a prior state implies that the changes introduced in the reverted commit were likely causing issues or bugs. The answer is **Buggy**.
"
wildfly,12438.json,92d027aaed28b83a6057d45690c0bbf8b5cecbaa,"@@ -1,4 +1,4 @@
         void close() {
-            this.clientMappingRegistry.removeListener(this);
+            this.listenerRegistration.close();
             this.clusterTopologyListeners.clear();
         }",NotBuggy,"Update usage of deprecated Group listener API.
",NotBuggy,"The commit message indicates an update to a deprecated API. The code change replaces `this.clientMappingRegistry.removeListener(this)` with `this.listenerRegistration.close()`. This suggests a change in how listeners are managed, likely due to the deprecation of the old method. This doesn't inherently indicate a bug fix, but rather an adaptation to a new API. The answer is **NotBuggy**.
"
wildfly,13536.json,b63c847db3fc4c885c809d837aee4361711166f8,"@@ -1,11 +1,11 @@
         void executeRuntime(final OperationContext context, final ModelNode operation) throws OperationFailedException {
             final String opName = operation.require(ModelDescriptionConstants.OP).asString();
             if (!opName.equals(ModelDescriptionConstants.READ_ATTRIBUTE_OPERATION)) {
-                throw MESSAGES.unknownOperations(opName);
+                throw EjbLogger.ROOT_LOGGER.unknownOperations(opName);
             }
 
             final TimerImpl timer = getTimer(context, operation);
 
             readAttribute(timer, context.getResult());
             context.completeStep(OperationContext.RollbackHandler.NOOP_ROLLBACK_HANDLER);
         }",NotBuggy,"[WFLY-2864] WildFly EJB module
",Buggy,"The commit message indicates a change related to the WildFly EJB module. The diff shows a modification in the `executeRuntime` method where an exception is thrown for unknown operations. The original code used `MESSAGES.unknownOperations(opName)` to throw the exception, while the modified code uses `EjbLogger.ROOT_LOGGER.unknownOperations(opName)`. This change suggests that the logging and exception handling have been improved or standardized within the EJB module. It is likely a bug fix related to logging or exception handling. The answer is **Buggy**."
wildfly,9080.json,c4dceafe3f52e61a8bb318f10722eae193da8c62,"@@ -1,3 +1,3 @@
-    protected AtomicReference<ManagedReference> getInstanceReference() {
-        return instanceReference;
+    public Object getInstanceData(Object key) {
+        return instanceData.get(key);
     }",NotBuggy,"WFLY-2021 Change EE interceptors to be stateless

All state is now stored on the ComponentInstance, and can be setup during the post construct
phase. This largely removes the need for InterceptorFactory. At most a single instance of
each interceptor chain will be created per component, this should result in a massive decrease
in the amount of memory a component instance, and an instance reference allocates.
",NotBuggy,"The commit message indicates a significant change in how EE interceptors are managed, focusing on reducing memory usage by making them stateless. The diff shows a change from `AtomicReference<ManagedReference>` to `Object getInstanceData(Object key)`. This suggests that the interceptor instance is no longer directly holding a reference to a managed instance. Instead, it's retrieving data associated with a key, implying that the state is now managed externally, likely on the `ComponentInstance` as the commit message describes. This change is architectural and aimed at performance improvement by reducing memory footprint, not directly fixing a bug. The change is about refactoring the interceptor management to be stateless. The answer is **NotBuggy**.
"
wildfly,14713.json,45fe4a432cf42b096be038ece2ff80b69c9d930c,"@@ -1,3 +1,3 @@
-    public void rebind(final String name, final Object obj) throws NamingException {
-        throw MESSAGES.readOnlyNamingContext();
+    public void rebind(final String name, final Object object) throws NamingException {
+        rebind(parseName(name), object);
     }",NotBuggy,"Introduce writable naming context
",NotBuggy,"The commit message indicates the introduction of a writable naming context. The diff shows the implementation of the `rebind` method, which previously threw an exception indicating a read-only context. The new implementation delegates to `rebind(parseName(name), object)`, suggesting the context is now writable. This change introduces new functionality rather than fixing a bug. The answer is **NotBuggy**.
"
wildfly,13160.json,8fec6d40a157018646ee131584fe5b88e31ef230,"@@ -1,53 +1,56 @@
     protected void parseRemote(final XMLExtendedStreamReader reader, List<ModelNode> operations) throws XMLStreamException {
         final int count = reader.getAttributeCount();
         final PathAddress ejb3RemoteServiceAddress = SUBSYSTEM_PATH.append(SERVICE, REMOTE);
         ModelNode operation = Util.createAddOperation(ejb3RemoteServiceAddress);
         final EnumSet<EJB3SubsystemXMLAttribute> required = EnumSet.of(EJB3SubsystemXMLAttribute.CONNECTOR_REF,
                 EJB3SubsystemXMLAttribute.THREAD_POOL_NAME);
         for (int i = 0; i < count; i++) {
             requireNoNamespaceAttribute(reader, i);
             final String value = reader.getAttributeValue(i);
             final EJB3SubsystemXMLAttribute attribute = EJB3SubsystemXMLAttribute.forName(reader.getAttributeLocalName(i));
             required.remove(attribute);
             switch (attribute) {
                 case CLIENT_MAPPINGS_CLUSTER_NAME:
                     EJB3RemoteResourceDefinition.CLIENT_MAPPINGS_CLUSTER_NAME.parseAndSetParameter(value, operation, reader);
                     break;
                 case CONNECTOR_REF:
                     EJB3RemoteResourceDefinition.CONNECTOR_REF.parseAndSetParameter(value, operation, reader);
                     break;
                 case THREAD_POOL_NAME:
                     EJB3RemoteResourceDefinition.THREAD_POOL_NAME.parseAndSetParameter(value, operation, reader);
                     break;
+                case EXECUTE_IN_WORKER:
+                    EJB3RemoteResourceDefinition.EXECUTE_IN_WORKER.parseAndSetParameter(value, operation, reader);
+                    break;
                 default:
                     throw unexpectedAttribute(reader, i);
             }
         }
         if (!required.isEmpty()) {
             throw missingRequired(reader, required);
         }
         // each profile adds it's own operation
         operations.add(operation);
 
         final Set<EJB3SubsystemXMLElement> parsedElements = new HashSet<EJB3SubsystemXMLElement>();
         while (reader.hasNext() && reader.nextTag() != XMLStreamConstants.END_ELEMENT) {
             EJB3SubsystemXMLElement element = EJB3SubsystemXMLElement.forName(reader.getLocalName());
             switch (element) {
                 case CHANNEL_CREATION_OPTIONS: {
                     if (parsedElements.contains(EJB3SubsystemXMLElement.CHANNEL_CREATION_OPTIONS)) {
                         throw unexpectedElement(reader);
                     }
                     parsedElements.add(EJB3SubsystemXMLElement.CHANNEL_CREATION_OPTIONS);
                     this.parseChannelCreationOptions(reader, ejb3RemoteServiceAddress, operations);
                     break;
                 }
                 case PROFILES: {
                     parseProfiles(reader, operations);
                     break;
                 }
                 default: {
                     throw unexpectedElement(reader);
                 }
             }
         }
     }",NotBuggy,"WFLY-5886 Remote EJB dispatches to thread pools twice
",Buggy,"The commit message ""WFLY-5886 Remote EJB dispatches to thread pools twice"" indicates a bug where remote EJB calls are being dispatched to thread pools more than once. The diff adds support for a new attribute `EXECUTE_IN_WORKER` to the `remote` element in the EJB3 subsystem configuration. This attribute likely controls whether the EJB invocation should be dispatched to a worker thread, which would address the double dispatching issue. The addition of this attribute suggests a fix for the bug described in the commit message. The answer is **Buggy**."
wildfly,777.json,59f6d84cb689c50e44f68aeacd5a41d7e0056fd6,"@@ -1,6 +1,8 @@
     public void schedule(I id) {
-        BeanEntry<I> entry = this.factory.findValue(id);
-        if (entry != null) {
-            this.schedule(id, entry);
+        try (Batch batch = this.batcher.createBatch()) {
+            BeanEntry<I> entry = this.factory.findValue(id);
+            if (entry != null) {
+                this.schedule(id, entry);
+            }
         }
     }",NotBuggy,"WFLY-12587 Distributed web/SFSB expiration after failover schedules premature expiration
",Buggy,"The commit message indicates a fix for premature expiration in distributed web/SFSB scenarios after failover. The code change adds a try-with-resources block using `Batch` from `this.batcher.createBatch()` around the existing logic. This suggests that the `batcher` might be related to managing distributed operations or transactions. The added `Batch` ensures proper resource management, which could prevent issues like premature expiration by ensuring that operations related to the bean entry are completed within the batch. This change likely addresses a bug related to resource management or transactional behavior that caused premature expiration. The answer is **Buggy**."
wildfly,16565.json,01a8fd7e3132ffed1bf87443991c7560051b2156,"@@ -1,3 +1,3 @@
             protected void handleNode(OperationContext context, ModClusterStatus.Node ctx, ModelNode operation) throws OperationFailedException {
-                context.getResult().set(new ModelNode(ctx.getStatus().name()));
+                context.getResult().set(new ModelNode(ctx.isQueueNewRequests()));
             }",NotBuggy,"Add some more mod_cluster information
",NotBuggy,"The commit message ""Add some more mod_cluster information"" suggests an enhancement or addition of data related to mod_cluster. The diff replaces `ctx.getStatus().name()` with `ctx.isQueueNewRequests()`. This change replaces the node's status with a boolean indicating whether the node is queuing new requests. This is a change in the information being exposed, but it doesn't inherently indicate a bug fix. It's more likely a modification to provide different or more relevant information. The answer is **NotBuggy**.
"
wildfly,8286.json,cc78bb76673be1d963bb01205cd4e5dcce1c3949,"@@ -1,41 +1,41 @@
     private ValueConfig parseValueValue(final XMLExtendedStreamReader reader) throws XMLStreamException {
         ValueConfig value = null;
         while (reader.hasNext()) {
             switch (reader.next()) {
                 case END_ELEMENT:
                     if (value == null)
                         throw new IllegalArgumentException(""Null value: "" + reader.getLocation());
                     return value;
                 case START_ELEMENT:
-                    switch (Element.of(reader.getName())) {
+                    switch (Element.of(reader.getLocalName())) {
                         case VALUE:
                             value = parseValue(reader);
                             break;
                         case INJECT:
                             value = parseInject(reader);
                             break;
                         case VALUE_FACTORY:
                             value = parseValueFactory(reader);
                             break;
                         case LIST:
                             value = parseList(reader);
                             break;
                         case SET:
                             value = parseSet(reader);
                             break;
                         case MAP:
                             value = parseMap(reader);
                             break;
                         default:
                             throw unexpectedElement(reader);
                     }
                     break;
                 case CHARACTERS:
                     StringValueConfig svc = new StringValueConfig();
                     svc.setValue(reader.getText());
                     value = svc;
                     break;
             }
         }
         throw unexpectedElement(reader);
     }",NotBuggy,"Allow for legacy MC bean deployment to be parsed as well.
",Buggy,"The code changes the way the element name is extracted from the XML stream reader. Instead of using `reader.getName()`, it now uses `reader.getLocalName()`. This suggests a change in how XML elements are processed, possibly to accommodate different XML structures or namespaces. The commit message indicates that this change is to allow for legacy MC bean deployment to be parsed as well. This suggests that the previous implementation was not compatible with the XML structure used in legacy MC bean deployments. Therefore, the change is likely a bug fix or an enhancement to support a wider range of XML structures. The answer is **Buggy**."
wildfly,16563.json,dee5c658c668a3752e7273dc0b271b84b16aa753,"@@ -1,3 +1,7 @@
             protected void handleNode(OperationContext context, ModClusterStatus.Node ctx, ModelNode operation) throws OperationFailedException {
-                context.getResult().set(new ModelNode(ctx.getUri().toString()));
+                ModelNode list = new ModelNode(ModelType.LIST);
+                for(String alias : ctx.getAliases()) {
+                    list.add(alias);
+                }
+                context.getResult().set(list);
             }",NotBuggy,"WFLY-4992 add nodes alises
",NotBuggy,"The commit message ""WFLY-4992 add nodes alises"" suggests the addition of node aliases. The code diff modifies the `handleNode` method to retrieve node aliases using `ctx.getAliases()` and adds them to a list, which is then set as the result. This aligns with the commit message's intention to add node aliases. The original code only retrieved the URI, while the modified code retrieves and returns a list of aliases. This change introduces new functionality rather than fixing a bug. The answer is **NotBuggy**."
javaparser,4978.json,ee156d8b09ad78f5eb75e0cc1b8d74999f16cafe,"@@ -1,3 +1,3 @@
-		public Pos transform(Pos pos) {
+		public Position transform(Position pos) {
 			return lookup(pos).transform(pos);
 		}",NotBuggy,Apply recorded position mapping in a parser post-processing phase.,NotBuggy,"The commit message indicates a change related to position mapping in a parser. The diff shows a change in the method signature from `Pos transform(Pos pos)` to `Position transform(Position pos)`. This suggests a type change from `Pos` to `Position`. While this could be part of a larger bug fix, the provided diff alone doesn't provide enough context to confirm if a bug was fixed. It's likely a refactoring or enhancement related to position tracking in the parser.

**NotBuggy**
"
javaparser,6046.json,dcf4b43e5c2f05d22bad9567dcd98d9cdf707c82,"@@ -1,4 +1,4 @@
     public SymbolReference<ResolvedMethodDeclaration> solveMethod(String name, List<ResolvedType> argumentsTypes,
-                                                                  boolean staticOnly, TypeSolver typeSolver) {
-        return getContext().solveMethod(name, argumentsTypes, staticOnly, typeSolver);
+                                                                  boolean staticOnly) {
+        return getContext().solveMethod(name, argumentsTypes, staticOnly);
     }",NotBuggy,"#1910 Removed TypeSolver parameter from various methods.
",NotBuggy,"The commit message indicates the removal of the `TypeSolver` parameter from several methods. The diff confirms this change in the `solveMethod` function, where the `TypeSolver` parameter is removed from the method signature and the corresponding call to `getContext().solveMethod`. This change suggests a refactoring or simplification of the code, likely to reduce dependencies or improve code clarity. There is no indication of a bug fix. The answer is **NotBuggy**.
"
javaparser,166.json,adb78b277512189cc5f8ecb076d9bf8106f9e1e0,"@@ -1,3 +1,3 @@
-    public boolean isWhiteSpaceOrComment() {
-        return child instanceof Comment;
+    public final boolean isWhiteSpaceOrComment() {
+        return isWhiteSpace() || isComment();
     }",NotBuggy,"issue823: distinguish between spaces and comments
",Buggy,"The commit message indicates a change to distinguish between spaces and comments. The code change replaces the check `child instanceof Comment` with `isWhiteSpace() || isComment()`. This suggests that the original code incorrectly treated whitespace as comments, or vice versa, which is now corrected by explicitly checking for both. This is a bug fix. The answer is **Buggy**."
javaparser,5042.json,26b23bccac8c67aed1023f0b2c142c3a7f3d5f08,"@@ -1,23 +1,22 @@
     public List<ParseResult<CompilationUnit>> tryToParseParallelized(String startPackage) throws IOException {
         assertNotNull(startPackage);
         logPackage(startPackage);
         final Path path = packageAbsolutePath(root, startPackage);
-        ParallelParse parse = new ParallelParse(path, new ParallelParse.VisitFileCallback() {
-            @Override
-            public FileVisitResult process(Path file, BasicFileAttributes attrs) {
-                if (!attrs.isDirectory() && file.toString().endsWith("".java"")) {
-                    Path relative = root.relativize(file.getParent());
-                    try {
-                        tryToParse(relative.toString(), file.getFileName().toString(), new JavaParser(
-                                SourceRoot.this.javaParser.getParserConfiguration()));
-                    } catch (IOException e) {
-                        Log.error(e);
-                    }
+        ParallelParse parse = new ParallelParse(path, (file, attrs) -> {
+            if (!attrs.isDirectory() && file.toString().endsWith("".java"")) {
+                Path relative = root.relativize(file.getParent());
+                try {
+                    tryToParse(
+                            relative.toString(),
+                            file.getFileName().toString(),
+                            new JavaParser(parserConfiguration));
+                } catch (IOException e) {
+                    Log.error(e);
                 }
-                return FileVisitResult.CONTINUE;
             }
+            return FileVisitResult.CONTINUE;
         });
         ForkJoinPool pool = new ForkJoinPool();
         pool.invoke(parse);
         return getCache();
     }",NotBuggy,"Redo SourceRoot again
",NotBuggy,"The commit message ""Redo SourceRoot again"" is vague and doesn't provide specific details about the changes. The diff replaces an anonymous inner class with a lambda expression. This change improves code readability and conciseness but doesn't inherently indicate a bug fix. The functionality remains the same. Therefore, the changes do not indicate a bug fix. The answer is **NotBuggy**."
javaparser,3272.json,6d06ae94c521628da4bcd76675f3660fee8ebf76,"@@ -1,3 +1,3 @@
-    public ModuleProvidesStmtMetaModel getMetaModel() {
-        return JavaParserMetaModel.moduleProvidesStmtMetaModel;
+    public ModuleProvidesDirectiveMetaModel getMetaModel() {
+        return JavaParserMetaModel.moduleProvidesDirectiveMetaModel;
     }",NotBuggy,"Module statements -> directives
",NotBuggy,"The commit message indicates a change from ""statements"" to ""directives"" in the context of modules. The code diff confirms this by renaming `ModuleProvidesStmtMetaModel` to `ModuleProvidesDirectiveMetaModel`. This change suggests a correction in terminology or a refactoring to align with a more accurate representation of the underlying concept. It's not directly indicative of a bug fix in the sense of correcting faulty logic or error handling, but rather a change in nomenclature. The answer is **NotBuggy**.
"
javaparser,225.json,c806b3355d14040857d754094f07595d7f84e02c,"@@ -1,3 +1,3 @@
-    boolean isProcessed() {
+    final boolean isProcessed() {
         return isProcessed;
     }",NotBuggy,"JavaDoc, newline at end of files and made methods final
",NotBuggy,"The commit message indicates changes related to JavaDoc updates, newline additions at the end of files, and marking methods as final. The diff shows a method being made final. This change doesn't directly address a bug but rather improves code maintainability and prevents overriding. The answer is **NotBuggy**."
javaparser,6917.json,37f742068fc5f198eef8c019d7f9673423c0b36a,"@@ -1,13 +1,13 @@
-    static String extractParameterName(CtBehavior method, int paramNumber) {
+    static Optional<String> extractParameterName(CtBehavior method, int paramNumber) {
         MethodInfo methodInfo = method.getMethodInfo();
         CodeAttribute codeAttribute = methodInfo.getCodeAttribute();
         if (codeAttribute != null) {
             LocalVariableAttribute attr = (LocalVariableAttribute) codeAttribute.getAttribute(LocalVariableAttribute
                     .tag);
             if (attr != null) {
                 int pos = Modifier.isStatic(method.getModifiers()) ? 0 : 1;
-                return attr.variableName(paramNumber + pos);
+                return Optional.ofNullable(attr.variableName(paramNumber + pos));
             }
         }
-        return null;
+        return Optional.empty();
     }",NotBuggy,"Adding javadoc comment to explain why the name might not be available.

Changed return value from String to Optional to avoid null. Renamed tests to conform to code style.
",Buggy,"The commit message indicates a change from returning `String` to `Optional<String>` to avoid returning `null`. The code diff confirms this change, replacing `return null;` with `return Optional.empty();` and wrapping the original return value in `Optional.ofNullable()`. This change improves null-safety and clarifies that a parameter name might not be available. This is a bug fix because returning null can lead to NullPointerExceptions. The answer is **Buggy**."
javaparser,535.json,621f1f76216a62bf1631fb337ba6c126c326a0cc,"@@ -1,5 +1,5 @@
     public String print(Node node) {
-        final PrettyPrintVisitor visitor = configuration.getVisitorFactory().apply(configuration);
+        final VoidVisitor<Void> visitor = configuration.getVisitorFactory().apply(configuration);
         node.accept(visitor, null);
-        return visitor.getSource();
+        return visitor.toString();
     }",NotBuggy,"Make toString's printer configuration accessible
",NotBuggy,"The commit message indicates a change to make the printer configuration accessible, and the diff modifies the `print` method. The change replaces `PrettyPrintVisitor` with `VoidVisitor<Void>` and calls `visitor.toString()` instead of `visitor.getSource()`. This suggests a change in how the output is generated, potentially to expose or configure the printing process. It's not immediately clear if this is a bug fix, but rather a change in functionality or API. The answer is **NotBuggy**."
javaparser,6134.json,5d8d0cb7947efcf2282f7d056302dd905f348532,"@@ -1,3 +1,3 @@
-    public ClassDeclaration declaringType() {
+    public ResolvedClassDeclaration declaringType() {
         return classDeclaration;
     }",NotBuggy,"more work on converting to moved classes
",NotBuggy,"The commit message ""more work on converting to moved classes"" suggests a refactoring or code movement activity. The diff shows a change in the return type of the `declaringType()` method from `ClassDeclaration` to `ResolvedClassDeclaration`. This indicates a change in the class hierarchy or a move of classes, aligning with the commit message. This change doesn't inherently indicate a bug fix, but rather an adaptation to the new class structure. The answer is **NotBuggy**.
"
javaparser,5700.json,e15d5a8aa84a34917e4ef47135dfc62e58ea7650,"@@ -1,3 +1,3 @@
-    public List<Type> getFormalArgumentTypes() {
+    public List<ResolvedType> getFormalArgumentTypes() {
         return formalArgumentTypes;
     }",NotBuggy,"correcting module core to use new classes
",NotBuggy,"The commit message indicates a correction in the core module to use new classes. The diff shows a change in the return type of the `getFormalArgumentTypes` method from `Type` to `ResolvedType`. This suggests an update to use a new or more specific class (`ResolvedType`) instead of a general one (`Type`). This could be due to a change in the API or data model, but it doesn't inherently indicate a bug fix. It's more likely a refactoring or an adaptation to new requirements. The answer is **NotBuggy**.
"
javaparser,1394.json,9a62aa11f05a3eb80b8d11a756ff63be0b259b95,"@@ -1,3 +1,3 @@
     public Integer visit(MethodCallExpr n, Void arg) {
-        return (n.getArguments().accept(this, arg)) * 31 + (n.getName().accept(this, arg)) * 31 + (n.getScope().isPresent() ? n.getScope().get().accept(this, arg) : 0) * 31 + (n.getTypeArguments().isPresent() ? n.getTypeArguments().get().accept(this, arg) : 0);
+        return (n.getArguments().accept(this, arg)) * 31 + (n.getName().accept(this, arg)) * 31 + (n.getScope().isPresent() ? n.getScope().get().accept(this, arg) : 0) * 31 + (n.getTypeArguments().isPresent() ? n.getTypeArguments().get().accept(this, arg) : 0) * 31 + (n.getComment().accept(this, arg));
     }",NotBuggy,"Use superclass fields for hashcode generation too
",Buggy,"The commit message indicates the change is to include superclass fields in hashcode generation. The diff modifies the `visit` method by adding `(n.getComment().accept(this, arg))` to the hashcode calculation. While the commit message doesn't explicitly mention a bug fix, the inclusion of superclass fields (or in this case, the comment field) in hashcode generation is often done to correct hashcode collisions or ensure proper object equality, which can be considered a bug fix in certain contexts. The addition of the comment field to the hashcode calculation suggests that the previous hashcode implementation was incomplete, potentially leading to incorrect behavior when comparing objects based on their hashcodes. The answer is **Buggy**."
javaparser,640.json,ca2adb511d3e8c8b73c63a5da6cb3b8e49fe5564,"@@ -1,3 +1,3 @@
     public ResolvedIntersectionType resolve() {
-        return getSymbolResolver().resolve(this, ResolvedIntersectionType.class);
+        return getSymbolResolver().toResolvedType(this, ResolvedIntersectionType.class);
     }",NotBuggy,"divide SymbolResolver.resolve in two methods
",NotBuggy,"The commit message indicates a refactoring of the `SymbolResolver.resolve` method into two methods. The diff shows that the call to `getSymbolResolver().resolve` is replaced with a call to `getSymbolResolver().toResolvedType`. This suggests that the original `resolve` method was likely split into `toResolvedType` and potentially another method (though not shown in this diff). This change doesn't inherently indicate a bug fix, but rather a restructuring of the code. The answer is **NotBuggy**."
javaparser,3650.json,54abb2027d67c0efc1c82712175e06d7eed2593b,"@@ -1,5 +1,2 @@
     public void ifSynchronizedStmt(Consumer<SynchronizedStmt> action) {
-        if (isSynchronizedStmt()) {
-            action.accept(asSynchronizedStmt());
-        }
     }",NotBuggy,"Be more like Java Symbol Solver (for ifType)
",Buggy,"The commit message ""Be more like Java Symbol Solver (for ifType)"" suggests aligning the current code with the Java Symbol Solver's approach for handling `ifType`. The diff removes a conditional check (`if (isSynchronizedStmt())`) before executing the action. This implies the original code might have been unnecessarily cautious or incorrect in its handling of `SynchronizedStmt`. Removing the check suggests a bug fix or an optimization based on the Java Symbol Solver's behavior. The answer is **Buggy**."
javaparser,5313.json,356ae8dcc930e7ff6e418c30f49c922c08279786,"@@ -1,3 +1,3 @@
     public static CompilationUnit parseResource(final ClassLoader classLoader, final String path, Charset encoding) throws IOException {
-        return handleResult(parser.parseResource(classLoader, path, encoding));
+        return handleResult(newParser().parseResource(classLoader, path, encoding));
     }",NotBuggy,"Make StaticJavaParser thread safe
",Buggy,"The commit message indicates an effort to make `StaticJavaParser` thread-safe. The diff shows a change in the `parseResource` method where `parser.parseResource` is replaced with `newParser().parseResource`. This suggests that instead of using a single, potentially shared `parser` instance, a new parser instance is created for each call to `parseResource`. This would indeed contribute to thread safety, as each thread would be operating on its own parser instance, avoiding potential race conditions or shared state issues. The change directly addresses the goal stated in the commit message. The answer is **Buggy**.
"
javaparser,946.json,9a03bc6291c871b2d705a17b4cad923bc954ba84,"@@ -1,3 +1,3 @@
     public Integer visit(final AnnotationMemberDeclaration n, final Void arg) {
-        return (n.getDefaultValue().isPresent() ? n.getDefaultValue().get().accept(this, arg) : 0) * 31 + (n.getModifiers().accept(this, arg)) * 31 + (n.getName().accept(this, arg)) * 31 + (n.getType().accept(this, arg)) * 31 + (n.getAnnotations().accept(this, arg));
+        return (n.getDefaultValue().isPresent() ? n.getDefaultValue().get().accept(this, arg) : 0) * 31 + (n.getModifiers().hashCode()) * 31 + (n.getName().accept(this, arg)) * 31 + (n.getType().accept(this, arg)) * 31 + (n.getAnnotations().accept(this, arg));
     }",NotBuggy,"Revert ""Create a Modifier node""
",Buggy,"The commit message indicates a revert, meaning the changes undo a previous commit. The diff shows the reversal of a change related to how modifiers are handled within the `visit` method for `AnnotationMemberDeclaration`. Specifically, it changes from `n.getModifiers().accept(this, arg)` back to `n.getModifiers().hashCode()`. Reverting a change suggests that the original change introduced a bug or was otherwise undesirable. The answer is **Buggy**."
intellij-community,24004.json,9c7a04d5af8b287ea37ac662bb86d527e0253acd,"@@ -1,3 +1,9 @@
   protected ActionToolbar createToolbar() {
-    return null;
+    ActionGroup actionGroup = createLeftToolbarActionGroup();
+    if (actionGroup != null) {
+      return ActionManager.getInstance().createActionToolbar(""TextEditorWithPreview"", actionGroup, true);
+    }
+    else {
+      return null;
+    }
   }",NotBuggy,"IDEA-CR-48269: PY-34819 TextEditorWithPreview and Markdown SplitFileEditor changed according to UX/UI team recommendations

* Right action group customization added in TextEditorWithPreview
* Gutter alignment turned off for the left toolbar
* Reserved spaces removed from the toolbars

GitOrigin-RevId: bb53affaa3ace7e1191757631ed9756fa2c22b6c",NotBuggy,"The commit message indicates changes related to UI/UX improvements, specifically mentioning toolbar customization and alignment adjustments. The code diff shows the implementation of a toolbar creation mechanism using ActionGroup and ActionManager. This aligns with the commit message's description of adding right action group customization. There is no indication of bug fixing. The answer is **NotBuggy**."
intellij-community,33570.json,62eeca9c70c26cc115590b8ddfdab4d4cfa83eb7,"@@ -1,20 +1,19 @@
   public ProjectTemplate[] createTemplates(String group, WizardContext context) {
     Collection<Pair<URL, ClassLoader>> urls = myGroups.getValue().get(group);
     List<ProjectTemplate> templates = new ArrayList<ProjectTemplate>();
     for (Pair<URL, ClassLoader> url : urls) {
       try {
         List<String> children = UrlUtil.getChildrenRelativePaths(url.first);
         for (String child : children) {
           if (child.endsWith(ZIP)) {
             URL templateUrl = new URL(url.first.toExternalForm() + ""/"" + child);
-            String name = getTemplateName(child);
-            templates.add(new LocalArchivedTemplate(name, templateUrl, url.second));
+            templates.add(new LocalArchivedTemplate(templateUrl, url.second));
           }
         }
       }
       catch (IOException e) {
         LOG.error(e);
       }
     }
     return templates.toArray(new ProjectTemplate[templates.size()]);
   }",NotBuggy,"new project wizard: adding project templates
",NotBuggy,"The commit message indicates the addition of project templates to the new project wizard. The code diff shows changes within the `createTemplates` method, specifically how template URLs are processed and added to the list of templates. The change involves removing the explicit extraction of the template name and instead directly passing the URL to the `LocalArchivedTemplate` constructor. This suggests a change in how template names are handled, potentially relying on the `LocalArchivedTemplate` class to derive the name from the URL. There is no explicit error handling or logical correction, so it's unlikely a bug fix. The answer is **NotBuggy**."
intellij-community,15453.json,ccb245867bc3b8f973f3fe9db7fd9a02a1169f49,"@@ -1,3 +1,3 @@
   public VirtualFile refreshAndFindFileByPath(@NotNull String path) {
-    return VfsImplUtil.refreshAndFindFileByPath(this, path);
+    return isValid(path) ? VfsImplUtil.refreshAndFindFileByPath(this, path) : null;
   }",NotBuggy,"[vfs] early .jar path validation (EA-95201)
",Buggy,"The commit message indicates a validation fix related to file paths. The code change adds a check `isValid(path)` before calling `VfsImplUtil.refreshAndFindFileByPath(this, path)`. If the path is invalid, the method returns null, preventing further processing with an invalid path. This suggests a bug where invalid paths were previously causing issues, and this change is intended to address that. The answer is **Buggy**."
intellij-community,27721.json,3fee046d2007f647e44c0a3744adfba605aad47d,"@@ -1,3 +1,3 @@
           public Dimension getPreferredSize() {
-            return new Dimension(super.getPreferredSize().width, JBUI.scale(44));
+            return new Dimension(super.getPreferredSize().width, JBUIScale.scale(44));
           }",NotBuggy,"[vcs-log] cleanup RootCellRenderer

GitOrigin-RevId: bba216626193ac756cd133b8ae9bc4e577abea42
",NotBuggy,"The commit message indicates a cleanup of the RootCellRenderer. The diff replaces `JBUI.scale` with `JBUIScale.scale`. This change likely involves refactoring or code modernization, possibly to align with updated UI scaling practices. It doesn't appear to be addressing a specific bug, but rather improving the code's structure or maintainability. The answer is **NotBuggy**.
"
intellij-community,25643.json,3b06cfe23b19153bd54c129cb9eba8cae7b4654e,"@@ -1,10 +1,9 @@
-          public Builder setModuleName(
-              int index, String value) {
-            if (value == null) {
+            public Builder setTypeId(String value) {
+              if (value == null) {
     throw new NullPointerException();
   }
-  ensureModuleNameIsMutable();
-            moduleName_.set(index, value);
-            
-            return this;
-          }

+  bitField0_ |= 0x00000001;
+              typeId_ = value;
+              
+              return this;
+            }",NotBuggy,"external compiler: compile scope based on build targets
",NotBuggy,"The commit message ""external compiler: compile scope based on build targets"" doesn't seem directly related to the code diff provided. The code diff appears to be related to setting a `typeId` in a builder, including a null check and setting a bit field. This looks like auto-generated code, possibly from a protocol buffer or similar mechanism. There's no clear indication of a bug fix in the provided diff. The change adds a null check and sets a bitfield, which could be part of a new feature or a refactoring, but not necessarily a bug fix. The answer is **NotBuggy**."
intellij-community,21829.json,8f877cd64f6e456fa8b4bb6cc67a65a473358191,"@@ -1,3 +1,3 @@
   public boolean isDeclarationWriteAccess(PsiElement element) {
-    return element instanceof PyTargetExpression;
+    return element instanceof PyTargetExpression || element.getParent() instanceof PyDelStatement;
   }",NotBuggy,"detect 'del x' as write access (PY-9784)
",Buggy,"The commit message indicates a fix related to detecting write access in Python code, specifically for 'del x' statements. The code change modifies the `isDeclarationWriteAccess` method to include `PyDelStatement` as a condition for write access. This suggests that the original code incorrectly identified 'del x' statements as not having write access, which is a bug. The fix ensures that deletion operations are correctly recognized as write access. The answer is **Buggy**."
intellij-community,20356.json,9f970709265d9f147b917bd74bd19667f5b8536d,"@@ -1,5 +1,5 @@
-  public final void editNewSchemeName(@NotNull String preferredName, boolean isProjectScheme, @NotNull Consumer<String> nameConsumer) {
+  public final void editNewSchemeName(@NotNull String preferredName, boolean isProjectScheme, @NotNull Consumer<? super String> nameConsumer) {
     String name =
       SchemeNameGenerator.getUniqueName(preferredName, schemeName -> getModel().containsScheme(schemeName, isProjectScheme));
     mySchemesCombo.startEdit(name, isProjectScheme, nameConsumer);
   }",NotBuggy,"bounded wildcards in API
",NotBuggy,"The commit message ""bounded wildcards in API"" suggests the changes involve the use of bounded wildcards in the API. The diff shows a change in the `editNewSchemeName` method signature. Specifically, the `Consumer<String>` parameter has been changed to `Consumer<? super String>`. This change introduces a bounded wildcard, allowing the `nameConsumer` to accept not only `String` objects but also any superclass of `String`. This is a common practice to increase the flexibility of an API. This change doesn't seem to fix a bug, but rather broadens the type of objects the consumer can accept. The answer is **NotBuggy**."
intellij-community,9028.json,571f34ea65d8637b1c3b3f8022ba8956a9eaaa59,"@@ -1,6 +1,6 @@
       private void process(@NotNull CaretEvent event) {
         Caret caret = event.getCaret();
-        if (caret == myCaretModel.getPrimaryCaret()) {
+        if (caret == caretModel.getPrimaryCaret()) {
           applyFocusMode(caret);
         }
       }",NotBuggy,"extract FocusModeModel
",NotBuggy,"The commit message ""extract FocusModeModel"" suggests a refactoring or restructuring of the code related to focus mode. The diff shows a change from `myCaretModel` to `caretModel`. This looks like a simple variable renaming or a change in the scope of the variable. There's no indication of a bug fix in the traditional sense (no error handling, no logical correction, no exception handling).
The answer is **NotBuggy**."
intellij-community,5202.json,32ebeeb487dc2bb7d5327285e8cc7930f92a22ff,"@@ -1,10 +1,9 @@
-  private Presentation update(AnAction action) {
+  private Presentation update(AnAction action, UpdateStrategy strategy) {
     if (myUpdatedPresentations.containsKey(action)) {
       return myUpdatedPresentations.get(action);
     }
 
-    AnActionEvent event = createActionEvent(action);
-    Presentation presentation = doUpdate(myModalContext, action, event) ? event.getPresentation(): null;
+    Presentation presentation = strategy.update.fun(action);
     myUpdatedPresentations.put(action, presentation);
     return presentation;
   }",NotBuggy,"update menu and toolbar actions with timeout, display non-updated actions using their default/previous presentation
",Buggy,"The commit message indicates updates to menu and toolbar actions, specifically addressing timeout issues and the display of non-updated actions. The diff shows a change in the `update` method, which now accepts an `UpdateStrategy` parameter. This suggests a modification in how actions are updated and presented, potentially related to handling timeouts or default presentations as mentioned in the commit message. The introduction of a strategy pattern hints at a more flexible and controlled update process, which could be a fix for issues related to action updates. The answer is **Buggy**.
"
intellij-community,15839.json,3d7fcfef434c97bb682789e4d4aabbd56e856816,"@@ -1,14 +1,14 @@
-  private void setSelectedEditor(final FileEditor editor) {
+  private void setSelectedEditor(@NotNull FileEditor editor) {
     final EditorWithProviderComposite composite = getEditorComposite(editor);
     if (composite == null) return;
 
     final FileEditor[] editors = composite.getEditors();
     for (int i = 0; i < editors.length; i++) {
       final FileEditor each = editors[i];
       if (editor == each) {
         composite.setSelectedEditor(i);
         composite.getSelectedEditor().selectNotify();
         break;
       }
     }
   }",NotBuggy,"cleanup
",NotBuggy,"The commit message ""cleanup"" is vague. The diff adds a `@NotNull` annotation to the `editor` parameter of the `setSelectedEditor` method. This change indicates an improvement in code clarity and potentially prevents null pointer exceptions by explicitly stating that the `editor` parameter should not be null. While this change improves code quality, it doesn't necessarily fix a specific, pre-existing bug. It's more of a preventative measure or a code contract enforcement. The answer is **NotBuggy**.
"
intellij-community,27442.json,e253d6a0dc44dfd8cea8853fbd6c2bb6de9c0038,"@@ -1,5 +1,5 @@
-  public void applyChanges(@NotNull JavaModuleExtensionImpl modified) {
+  public void applyChanges(@NotNull JpsJavaModuleExtensionImpl modified) {
     setLanguageLevel(modified.myLanguageLevel);
     setOutputUrl(modified.myOutputUrl);
     setTestOutputUrl(modified.myTestOutputUrl);
   }",NotBuggy,"new project model: more java-specific properties added to the model
",NotBuggy,"The commit message indicates the addition of more Java-specific properties to the project model. The code change involves renaming `JavaModuleExtensionImpl` to `JpsJavaModuleExtensionImpl`. This suggests a structural change or refactoring related to the project model, aligning with the commit message's description of adding Java-specific properties. The change itself doesn't obviously fix a bug. The answer is **NotBuggy**.
"
intellij-community,19676.json,8440df0fda18a651251e13ac1b5a693c85d1a617,"@@ -1,26 +1,26 @@
     private static void appendNode(Node node, StringBuilder sb) {
       if (node.target == ERROR_NODE) {
         sb.append(node.method);
         return;
       }
       else if (node.target != null) {
         appendNode(node.target, sb);
         sb.append('.');
       }
       sb.append(node.method).append('(');
       boolean first = true;
-      for (Object arg : (node.args == null ? ArrayUtil.EMPTY_OBJECT_ARRAY : node.args)) {
+      for (Object arg : (node.args == null ? ArrayUtilRt.EMPTY_OBJECT_ARRAY : node.args)) {
         if (first) first = false;
         else sb.append(',').append(' ');
         if (arg instanceof Node) {
           appendNode((Node)arg, sb);
         }
         else if (arg instanceof String) {
           sb.append('\""').append(StringUtil.escapeStringCharacters((String)arg)).append('\""');
         }
         else if (arg instanceof Number) {
           sb.append(arg);
         }
       }
       sb.append(')');
     }",NotBuggy,"prefer ArrayUtilRt

GitOrigin-RevId: f4b11d48949267d778f6cdab4ef8ac60eb85aa2e
",NotBuggy,"The commit message ""prefer ArrayUtilRt"" suggests a preference for `ArrayUtilRt` over `ArrayUtil`. The diff replaces `ArrayUtil.EMPTY_OBJECT_ARRAY` with `ArrayUtilRt.EMPTY_OBJECT_ARRAY`. This change is likely a refactoring or optimization, not a bug fix, as it involves switching to a preferred utility class without altering the core logic. The answer is **NotBuggy**."
intellij-community,21985.json,b5c92ae0ba1ee1964a0e18b02eb49ceafaf8679e,"@@ -1,16 +1,16 @@
-  public PyType getCallableType(@NotNull Callable callable, @NotNull TypeEvalContext context) {
+  public PyType getCallableType(@NotNull PyCallable callable, @NotNull TypeEvalContext context) {
     if (callable instanceof PyFunction) {
       final String qualifiedName = callable.getQualifiedName();
       if (qualifiedName != null && qualifiedName.startsWith(""PyQt"")){
         final QualifiedName name = QualifiedName.fromDottedString(qualifiedName);
         final String qtVersion = name.getComponents().get(0);
         final String docstring = ((PyFunction)callable).getDocStringValue();
         if (docstring != null && docstring.contains(""[signal]"")) {
           final PyClass aClass = PyClassNameIndex.findClass(qtVersion + ""."" + ourQtBoundSignal, callable.getProject());
           if (aClass != null)
             return new PyClassTypeImpl(aClass, false);
         }
       }
     }
     return null;
   }",NotBuggy,"Rename com.jetbrains.python.psi.Callable to PyCallable so as not to clash with java.util.concurrent.Callable
",Buggy,"The commit message indicates a renaming of a class to avoid naming conflicts. The diff shows the renaming of the parameter type from `Callable` to `PyCallable` in the `getCallableType` method. This change directly reflects the renaming described in the commit message and resolves a potential naming conflict, which can be considered a bug fix in terms of code maintainability and clarity. The answer is **Buggy**."
intellij-community,13487.json,47f3db3350684a82b8dead8a0b4f7c961e0ed685,"@@ -1,3 +1,3 @@
           public boolean hasLine() {
-            return ((bitField0_ & 0x00000040) == 0x00000040);
+            return instance.hasLine();
           }",NotBuggy,"protobuf version advanced to 3.0.0
",Buggy,"The commit message indicates a version update of protobuf. The code change replaces a direct bitwise check with a call to `instance.hasLine()`. This suggests the change is related to adapting to the new protobuf version, likely due to changes in how fields are accessed or checked for presence. While it's not a direct bug fix in the traditional sense, it's a necessary adaptation to a new version of a dependency, which could be considered fixing a compatibility issue. The answer is **Buggy**.
"
intellij-community,26056.json,4f5720f30aaff736459a6dc2d23912bf00f0d745,"@@ -1,18 +1,18 @@
   private void configureStartingScriptPanel(final Project project, final PyConsoleOptionsProvider.PyConsoleSettings optionsProvider) {
-    myEditorTextField = new EditorTextField(createDocument(project, optionsProvider.myStartScript), project, PythonFileType.INSTANCE) {
+    myEditorTextField = new EditorTextField(createDocument(project, optionsProvider.myCustomStartScript), project, PythonFileType.INSTANCE) {
       @Override
       protected EditorEx createEditor() {
         final EditorEx editor = super.createEditor();
         editor.setVerticalScrollbarVisible(true);
         return editor;
       }
 
       @Override
       protected boolean isOneLineMode() {
         return false;
       }
     };
     myStartingScriptPanel.setLayout(new BorderLayout());
     myStartingScriptPanel.add(myEditorTextField, BorderLayout.CENTER);
     myConsoleSettings = optionsProvider;
   }",NotBuggy,"Setting to remove separator line in console (PY-2573).
",NotBuggy,"The commit message indicates a change related to removing a separator line in the console. The diff replaces `optionsProvider.myStartScript` with `optionsProvider.myCustomStartScript` when creating the `EditorTextField`. This suggests the change aims to allow users to customize the starting script without a separator, which is related to the feature request mentioned in the commit message. This change doesn't seem to be directly fixing a bug, but rather implementing a feature or preference setting. The answer is **NotBuggy**."
hector,1607.json,8751ec479084eca0622c2f26f6c0b02577ad2a15,"@@ -1,4 +1,4 @@
   public String system_update_keyspace(KsDef ks_def)
-      throws InvalidRequestException, TException {
+      throws InvalidRequestException, TException, SchemaDisagreementException {
     return client.system_update_keyspace(ks_def);
   }",NotBuggy,"v0.8 specific changes for bringing up to date with latest cassandra trunk
",Buggy,"The commit message indicates changes to bring the code up to date with the Cassandra trunk. The diff shows that the `system_update_keyspace` method now throws a `SchemaDisagreementException` in addition to `InvalidRequestException` and `TException`. This suggests that the updated code now handles a new type of exception that was not previously handled, which could be due to a bug fix or a new feature. Since the commit message mentions bringing the code up to date, it is likely that this change is related to a bug fix or a change in the Cassandra trunk that required this exception to be handled. The answer is **Buggy**."
hector,2275.json,747116cab7950e8466a5b7b89c223866472dab49,"@@ -1,21 +1,21 @@
       public Column execute(Cassandra.Client cassandra) throws HectorException {
         try {          
           if ( queryLogger.isDebugEnabled() ) {
-            queryLogger.debug(""---------\nColumnFamily: {} slicePredicate: {}"", columnFamilyName, activeSlicePredicate.toString());
+            queryLogger.debug(""---------\nColumnFamily multiget: {} slicePredicate: {}"", columnFamilyName, activeSlicePredicate.toString());
           }
           long startTime = System.nanoTime();
-          Map<ByteBuffer, List<ColumnOrSuperColumn>> rows = cassandra.multiget_slice(keySerializer.toBytesList(_keys), columnParent, activeSlicePredicate.toThrift(), 
+          rows = cassandra.multiget_slice(keySerializer.toBytesList(_keys), columnParent, activeSlicePredicate.toThrift(), 
               ThriftConverter.consistencyLevel(consistencyLevelPolicy.get(operationType)));
           long duration = System.nanoTime() - startTime;
 
-
+          
           if ( queryLogger.isDebugEnabled() ) {
             queryLogger.debug(""Execution took {} microseconds on host {}\n----------"", duration/1000, getCassandraHost());
           }
         } catch (Exception e) {
           throw exceptionsTranslator.translate(e);
         }
         hasValues = true;
 
         return null;
       }",NotBuggy,"initial cut at automatic multiget
",NotBuggy,"The commit message indicates the introduction of ""automatic multiget"". The code diff shows changes related to `cassandra.multiget_slice`. Specifically, the debug log message is updated to reflect ""multiget"", and the result of the `multiget_slice` call is assigned to a variable `rows`. These changes suggest the implementation of a new feature, ""multiget"", rather than fixing a bug. The answer is **NotBuggy**."
hector,1823.json,a54d8e467f611b1e3467ad27145004668efc0d6d,"@@ -1,5 +1,5 @@
   public HColumn<N, V> setValue(V value) {
     notNull(value, ""value is null"");
-    this.value = value;
+    this.column.value = valueSerializer.toByteBuffer(value);
     return this;
   }",NotBuggy,"huge efficiency improvement in hcolumimpl. tied a lot closer to thrift Column, but the efficiency gains are worth it and we are still behind an interface
",NotBuggy,"The commit message indicates a significant efficiency improvement by tightly integrating with Thrift's Column structure. The code change in `setValue` modifies how the value is stored. Instead of directly assigning the value, it now uses `valueSerializer.toByteBuffer(value)` to convert the value into a ByteBuffer before assigning it to `this.column.value`. This change suggests an optimization related to how values are handled and stored, likely to improve performance when interacting with Thrift. The change is not directly indicative of a bug fix, but rather an optimization. The answer is **NotBuggy**.
"
hector,2272.json,229041fa14667b7d0ea1064511055e96713b230e,"@@ -1,13 +1,13 @@
   private void maybeExecuteSlice(N columnName) {
     if ( columnNames == null ) {
       columnNames = new HashSet<N>();
     }
     if ( columns == null ) {
-      columns = new HashMap<N, Column>();
+      columns = new HashMap<N, HColumn<N,ByteBuffer>>();
     }
     if ( columns.get(columnName) == null ) {
       columnNames.add(columnName);
       activeSlicePredicate.setColumnNames(columnNames);
       doExecuteSlice();
     }    
   }",NotBuggy,"added more innards to hcolumnfamily, test coverage for such
",NotBuggy,"The commit message indicates the addition of functionality and test coverage to `hcolumnfamily`. The diff shows a change in the type of the `columns` HashMap from `HashMap<N, Column>` to `HashMap<N, HColumn<N,ByteBuffer>>`. This suggests a modification to accommodate a more specific type of column, likely related to the new functionality being added. While it's not a direct bug fix, it's part of a larger change that introduces new features and improves the code's structure. The change itself doesn't fix a bug, but it's related to the addition of new functionality. The answer is **NotBuggy**."
hector,516.json,50daddbde6d764dd15c3c1fc6b74da2ea8da240d,"@@ -1,12 +1,12 @@
   public <T> AbstractComposite setComponent(int index, T value,
       Serializer<T> s, String comparator, boolean inclusive) {
     serialized = null;
 
     while (components.size() <= index) {
       components.add(null);
     }
-    components.set(index, new Component(value, s, comparator, inclusive));
+    components.set(index, new Component(value, null, s, comparator, inclusive));
 
     return this;
 
   }",NotBuggy,"Made deserialization of components lazy and occur at get() time
",Buggy,"The commit message indicates a change in how components are deserialized, making it lazy and occurring at `get()` time. The diff shows a modification in the `setComponent` method where the `Component` object is created. Specifically, the `value` parameter is passed directly, but the serializer is passed as null during the `setComponent` call. This suggests that the actual serialization/deserialization process is deferred until the `get()` method is called, aligning with the commit message. This change is likely an optimization or a fix for issues related to immediate deserialization, such as performance problems or incorrect state during initial setup. The change in the diff aligns with the lazy deserialization mentioned in the commit message. The answer is **Buggy**."
hector,1926.json,12bad5fac77e564df933c1893fd3210e16152326,"@@ -1,27 +1,27 @@
 	public void copy() throws HectorException {
 		if (this.cf == null) {
 			throw new HectorException(""Unable to clone row with null column family"");
 		}
 		if (this.rowKey == null) {
 			throw new HectorException(""Unable to clone row with null row key"");
 		}
 		if (this.destinationKey == null) {
 			throw new HectorException(""Unable to clone row with null clone key"");
 		}
 
 		ColumnFamilyTemplate<K, ByteBuffer> template = new ThriftColumnFamilyTemplate<K, ByteBuffer>(this.keyspace, this.cf, this.keySerializer, this.bs);
 		Mutator<K> mutator = HFactory.createMutator(this.keyspace, this.keySerializer, new BatchSizeHint(1, this.mutateInterval));
 		ColumnFamilyUpdater<K, ByteBuffer> updater = template.createUpdater(this.destinationKey, mutator);
 
-		SliceQuery<K, ByteBuffer, V> query = HFactory.createSliceQuery(this.keyspace, this.keySerializer, this.bs, this.valueSerializer).
+		SliceQuery<K, ByteBuffer, ByteBuffer> query = HFactory.createSliceQuery(this.keyspace, this.keySerializer, this.bs, this.bs).
 						setColumnFamily(this.cf).
 						setKey(this.rowKey);
 
-		ColumnSliceIterator<K, ByteBuffer, V> iterator = new ColumnSliceIterator<K, ByteBuffer, V>(query, this.bs.fromBytes(new byte[0]), this.bs.fromBytes(new byte[0]), false);
+		ColumnSliceIterator<K, ByteBuffer, ByteBuffer> iterator = new ColumnSliceIterator<K, ByteBuffer, ByteBuffer>(query, this.bs.fromBytes(new byte[0]), this.bs.fromBytes(new byte[0]), false);
 		while (iterator.hasNext()) {
-			HColumn<ByteBuffer, V> column = iterator.next();
-			updater.setValue(column.getName(), column.getValue(), this.valueSerializer);
+			HColumn<ByteBuffer, ByteBuffer> column = iterator.next();
+			updater.setValue(column.getName(), column.getValue(), this.bs);
 		}
 
 		template.update(updater);
 	}",NotBuggy,"Changed class to use ByteBuffer as the column value serializer to accomodate CF with different value types
",Buggy,"The commit message indicates a change to use `ByteBuffer` as the column value serializer to accommodate column families with different value types. The diff shows modifications to the `copy` method, specifically changing the `SliceQuery` and `ColumnSliceIterator` to use `ByteBuffer` for value serialization instead of a generic type `V`. The `setValue` method in the `ColumnFamilyUpdater` is also updated to use `ByteBuffer` serializer (`this.bs`). These changes suggest that the original code might have had issues handling different value types in column families, which is now addressed by using `ByteBuffer` as a common serializer. This indicates a bug fix related to handling different value types. The answer is **Buggy**."
hector,2776.json,6f80d0e241cea8f85ccab3514c6b45fdd9239856,"@@ -0,0 +1,3 @@
+  public void setUseThriftFramedTransport(boolean useThriftFramedTransport) {
+    this.useThriftFramedTransport = useThriftFramedTransport;
+  }",NotBuggy,"added framed transport to CHC
",NotBuggy,"The commit message indicates the addition of framed transport to CHC. The diff shows a setter method for a boolean flag `useThriftFramedTransport`. This change introduces a new feature rather than fixing a bug. The answer is **NotBuggy**.
"
hector,22.json,1af2d5ee84680a2abfe86f1710cce6569d2d5b7a,"@@ -1,3 +1,3 @@
-  public void setCfBaseMapDef(CFMappingDef<? super T, I> cfSuperMapDef) {
-    this.cfBaseMapDef = cfSuperMapDef;
+  public void setCfBaseMapDef(CFMappingDef<? super T, I> cfBaseMapDef) {
+    this.cfBaseMapDef = cfBaseMapDef;
   }",NotBuggy,"refactored ClassCacheMgr to make a leaner more readable class and prevent bloat, and make parallel work a bit easier
",NotBuggy,"The commit message indicates a refactoring for improved readability and prevention of bloat, along with facilitating parallel work. The diff shows a simple variable name change from `cfSuperMapDef` to `cfBaseMapDef`. This change does not indicate a bug fix, but rather aligns with the refactoring goal of improving code clarity. The answer is **NotBuggy**.
"
hector,927.json,8996eec24a06cfc68a8ef691674d2c187af07b60,"@@ -1,14 +1,11 @@
   public static Subject loginService(String serviceName) throws LoginException {
-    LoginContext loginCtx = new LoginContext(serviceName,
-        new CallbackHandler() {
-          // as we use .keytab file there is no need to specify any options in
-          // callback
+    LoginContext loginCtx = new LoginContext(serviceName, new CallbackHandler() {
+          // as we use .keytab file there is no need to specify any options in callback
           public void handle(Callback[] callbacks) throws IOException,
               UnsupportedCallbackException {
           }
         });
 
     loginCtx.login();
-
     return loginCtx.getSubject();
   }",NotBuggy,"Edit sample files and better jadavocs
",NotBuggy,The commit message indicates edits to sample files and improvements to Javadoc. The diff removes an unnecessary newline character. This change does not indicate a bug fix. The answer is **NotBuggy**.
hector,1382.json,6c42b8d05ffc0aef7de1c5e5e67c1ded7e4f45d4,"@@ -1,11 +1,5 @@
   public <N> MutationResult deleteCounter(final K key, final String cf, final N counterColumnName, 
       final Serializer<N> nameSerializer) {
-    return new MutationResultImpl(keyspace.doExecute(new KeyspaceOperationCallback<Void>() {
-        @Override
-        public Void doInKeyspace(KeyspaceService ks) throws HectorException {
-          ks.removeCounter(keySerializer.toByteBuffer(key), ThriftFactory.createColumnPath(cf, counterColumnName, 
-              nameSerializer));
-          return null;
-        }
-    }));
+    addCounterDeletion(key,cf,counterColumnName,nameSerializer);
+    return execute();
   }",NotBuggy,"initial cleanup to support #354
",NotBuggy,"The commit message indicates an initial cleanup to support a feature request. The diff replaces an anonymous class with a call to `addCounterDeletion` and `execute`. This change suggests a refactoring to improve code structure or prepare for new functionality, rather than fixing a bug. The answer is **NotBuggy**."
hector,2389.json,da2edd4944bc071a11859fb61f76df5112b06a80,"@@ -1,14 +1,11 @@
       public Map<ByteBuffer,List<ColumnOrSuperColumn>> execute(Cassandra.Client cassandra) throws HectorException {
-        Map<ByteBuffer,List<ColumnOrSuperColumn>> cosc = new LinkedHashMap<ByteBuffer, List<ColumnOrSuperColumn>>();
         try {          
           List<K> keyList = new ArrayList<K>();
           Iterators.addAll(keyList, keys.iterator());
-          cosc = cassandra.multiget_slice(keySerializer.toBytesList(keyList), columnParent,
+          return cassandra.multiget_slice(keySerializer.toBytesList(keyList), columnParent,
               (workingSlicePredicate == null ? activeSlicePredicate.setColumnNames(columnValueSerializers.keySet()).toThrift() : workingSlicePredicate.toThrift()),              
             ThriftConverter.consistencyLevel(consistencyLevelPolicy.get(operationType)));
         } catch (Exception e) {
           throw exceptionsTranslator.translate(e);
         }        
-
-        return cosc;
       }",NotBuggy,"remove unnecessary allocation
",NotBuggy,"The commit message indicates the removal of an unnecessary allocation. The diff shows that the `cosc` variable, which was a `LinkedHashMap`, is no longer being explicitly created and returned. Instead, the result of the `cassandra.multiget_slice` method is directly returned. This eliminates the unnecessary allocation of the `cosc` map, aligning with the commit message. This is an optimization, not a bug fix. The answer is **NotBuggy**."
hector,1732.json,f8ad7a5aa29b297d3f6281a4d061f8dd9da0da4e,"@@ -1,5 +1,5 @@
-  public  RangeSubSlicesQuery<SN,N,V> setSuperColumn(SN sc) {
+  public RangeSubSlicesQuery<SN, N, V> setSuperColumn(SN sc) {
     Assert.notNull(sc, ""sc can't be null"");
     superColumn = sc;
     return this;
   }",NotBuggy,"Refactor: Extract interface from RangeSlicesQuery, RangeSubSlicesQuery, SuperSlicesQuery, SubSliceQuery and SuperSliceQuery and move them to the api package
",NotBuggy,"The commit message indicates a refactoring effort, specifically the extraction of an interface from several query classes and moving them to the `api` package. The provided diff snippet only shows a change in spacing and does not reflect any interface extraction or movement of classes. Therefore, based on the provided diff, there is no indication of a bug fix. The answer is **NotBuggy**.
"
hector,587.json,ea13a02750c3c4dba496c54ee459a0f2e749ee43,"@@ -1,19 +1,6 @@
   public String toString() {
-    StringBuilder builder = new StringBuilder();
-    builder.append(""HLockManagerConfigurator [keyspaceName="");
-    builder.append(keyspaceName);
-    builder.append("", lockManagerCF="");
-    builder.append(lockManagerCF);
-    builder.append("", rowsCacheEnabled="");
-    builder.append(rowsCacheEnabled);
-    builder.append("", locksTTLInMillis="");
-    builder.append(locksTTLInMillis);
-    builder.append("", backOffRetryDelayInMillis="");
-    builder.append(backOffRetryDelayInMillis);
-    builder.append("", replicationFactor="");
-    builder.append(replicationFactor);
-    builder.append("", numberOfLockObserverThreads="");
-    builder.append(numberOfLockObserverThreads);
-    builder.append(""]"");
-    return builder.toString();
+    return ""HLockManagerConfigurator [keyspaceName="" + keyspaceName + "", lockManagerCF="" + lockManagerCF
+        + "", rowsCacheEnabled="" + rowsCacheEnabled + "", locksTTLInMillis="" + locksTTLInMillis
+        + "", backOffRetryDelayInMillis="" + backOffRetryDelayInMillis + "", replicationFactor="" + replicationFactor
+        + "", numberOfLockObserverThreads="" + numberOfLockObserverThreads + "", maxSelectSize="" + maxSelectSize + ""]"";
   }",NotBuggy,"Made ack size configurable.  The first 10 elements is more than
sufficient to guarantee ordered locking.  ",NotBuggy,"The commit message indicates a configuration change related to the size of acknowledgements. The diff modifies the `toString()` method to include `maxSelectSize` in the output, which suggests that this new parameter is now part of the configuration. While the commit message explains the rationale behind the size, the code change itself doesn't directly fix a bug. It's an enhancement to make the system more configurable. The answer is **NotBuggy**."
hector,24.json,c6fd6f3fa896a6f616cb2332dacbf9491316dfb0,"@@ -1,3 +1,3 @@
-  public Map<Object, CFMappingDef<? extends T, I>> getDerivedClassMap() {
+  public Map<Object, CFMappingDef<? extends T>> getDerivedClassMap() {
     return derivedClassMap;
   }",NotBuggy,"a POJO's ID was genericized in most places as <I> but this doesn't make sense when starting to use complex types as the key - so it has been removed
",NotBuggy,"The commit message indicates a change in the generic type of a POJO's ID, specifically removing the `<I>` generic type. The diff shows a change in the return type of the `getDerivedClassMap()` method, where the generic type `I` has been removed from the `CFMappingDef` class. This suggests a modification to accommodate complex types as keys, aligning with the commit message. While this change might address a limitation or improve flexibility, it doesn't explicitly point to a bug fix in the sense of correcting erroneous behavior. It's more of a design adjustment. The answer is **NotBuggy**."
hector,2693.json,b0dc2661d56bb80142771050c0124ea013fe4e31,"@@ -1,7 +1,9 @@
-      public List<TokenRange> execute(Cassandra.Client cassandra) throws HectorException {
+      public String execute(Cassandra.Client cassandra) throws HectorException {
         try {
-          return cassandra.describe_ring(keyspace);
+          log.info(""in execute with client {}"", cassandra);
+          return cassandra.describe_cluster_name();
         } catch (Exception e) {
           throw xtrans.translate(e);
         }
+
       }",NotBuggy,"Refactor: Seperate the thrift classes from the Cluster interface so it doesn't import any thrift specific classes and create a ThriftCluster implementation
",NotBuggy,"The commit message indicates a refactoring to separate Thrift-specific classes from the `Cluster` interface. The diff shows a change in the `execute` method's return type from `List<TokenRange>` to `String` and a call to `cassandra.describe_cluster_name()` instead of `cassandra.describe_ring(keyspace)`. The code now retrieves the cluster name instead of token ranges. While this is a significant change, it aligns with the refactoring goal of decoupling Thrift dependencies. There's no explicit error handling improvement or logical correction that suggests a bug fix. The change seems to be part of the refactoring process. The answer is **NotBuggy**.
"
cucumber-jvm,1199.json,669dd85aad21b1c030d21bf3def4d770ca4f891e,"@@ -1,3 +1,3 @@
-    void execute(Scenario scenario) throws Throwable {
+    void execute(TestCaseState scenario) {
         delegate.execute(scenario);
     }",NotBuggy,"[Core] Rename Scenario to TestCaseState
",NotBuggy,"The commit message indicates a renaming of `Scenario` to `TestCaseState`. The diff shows that the parameter type in the `execute` method has been changed from `Scenario` to `TestCaseState`. This change directly reflects the renaming described in the commit message. There's no indication of a bug fix in this change; it's purely a renaming refactoring. The answer is **NotBuggy**.
"
cucumber-jvm,565.json,fe092af8acad11799b24f396a50f870d3f2792a0,"@@ -1,7 +1,7 @@
     public boolean equals(Object o) {
         if (this == o) return true;
         if (o == null || getClass() != o.getClass()) return false;
-        CucumberLocation that = (CucumberLocation) o;
+        GherkinMessagesLocation that = (GherkinMessagesLocation) o;
         return line == that.line &&
             column == that.column;
     }",NotBuggy,"[Core] Support Gherkin Rule keyword (#1840)

* [Core] Support Gherkin Rule keyword

Introduces the `gherkin-messages` module which will add support for the Gherkin
`Rule` keyword. Because the rule keyword is not supported by the `json` or
`html` formatter it will be an opt-in module.
",NotBuggy,"The commit message indicates the introduction of a new module (`gherkin-messages`) to support the Gherkin `Rule` keyword. The provided diff shows a change in the `equals` method, specifically casting `o` to `GherkinMessagesLocation` instead of `CucumberLocation`. This suggests a type change related to the new module. This change is related to the new functionality and not a bug fix. The answer is **NotBuggy**.
"
cucumber-jvm,978.json,365f45a2df0bff71567bd457d663185f5f3cc788,"@@ -1,11 +1,11 @@
     private List<Node> reversedPoppedNodes(List<Node> currentStack, List<Node> newStack) {
         for (int i = 0; i < currentStack.size() && i < newStack.size(); i++) {
             if (!currentStack.get(i).equals(newStack.get(i))) {
                 return currentStack.subList(i, currentStack.size());
             }
         }
         if (newStack.size() < currentStack.size()) {
             return currentStack.subList(newStack.size(), currentStack.size());
         }
-        return Collections.emptyList();
+        return emptyList();
     }",NotBuggy,"[Plugin] Add TestSourceParsed event (#1895)

Cucumber scenarios and individual examples in a scenario outline are compiled
into pickles. These pickles are wrapped by a `TestCase`. As such
Cucumbers internal representation lacks any hierarchy. I.e. once compiled
into a a pickle a scenario is no longer associated with a feature file.

However consumers of Cucumbers output generally expect results to be reported
in hierarchical fashion. This event allows test cases to be associated with
 with a `Node` in the hierarchy. The `TestSourceParsed` event provides this information. 

A node is an element in a tree. It has a name, keyword and location. A node can optionally be container of yet more nodes. 

```
 TestSourceParsed
       Node
       Container
        Node
        Node
       Container
         Container
            Node
            Node
```

Note that this information is intentionally abstract. To recreate a a facsimile of the original feature file it is still recommended to use the AST produced by a Gherkin parser. And to avoid conflict with other tools is recommended that this parser is shaded.

Closes: #1870",NotBuggy,"The commit message describes the addition of a `TestSourceParsed` event to provide hierarchical information about test cases, associating them with nodes in a tree structure. The diff replaces `Collections.emptyList()` with `emptyList()`. This change suggests using a static method or constant for an empty list instead of creating a new empty list instance each time. While this could be considered a performance improvement or code cleanup, it doesn't directly address a bug. The functionality remains the same. The answer is **NotBuggy**.
"
cucumber-jvm,700.json,4d2aa2b173a286d766da8ada1546733fd26d5664,"@@ -1,7 +1,7 @@
-    private List<Class<?>> findClassesForUris(List<URI> baseUris, String basePackageName, Predicate<Class<?>> classFilter) {
+    private List<Class<?>> findClassesForUris(List<URI> baseUris, String packageName, Predicate<Class<?>> classFilter) {
         return baseUris.stream()
-            .map(baseUri -> findClassesForUri(baseUri, basePackageName, classFilter))
+            .map(baseUri -> findClassesForUri(baseUri, packageName, classFilter))
             .flatMap(Collection::stream)
             .distinct()
             .collect(toList());
     }",NotBuggy,"[Core] Use consistent naming in scanners

From a consumer the package being scanned is the `packageName` rather
then the `basePackageName` which only makes sense ocne the package is
compared to some other package.
",NotBuggy,"The commit message indicates a change in naming convention for a variable related to package scanning, from `basePackageName` to `packageName`, to improve clarity for consumers. The code diff confirms this change, with `basePackageName` being replaced by `packageName` in the `findClassesForUris` method. This change improves code readability and maintainability by using a more intuitive name for the package being scanned. There is no indication of a bug fix. The answer is **NotBuggy**.
"
cucumber-jvm,608.json,4f2ded527763a56e55bc6713e56ba6fa6ef9b359,"@@ -1,3 +1,3 @@
-    static InjectionProvider<?>[] setUpInjectionProviders(final String resourceName) {
-        return new CucumberNeedleConfiguration(resourceName).getInjectionProviders();
+    static InjectionProvider<?>[] setUpInjectionProviders() {
+        return new CucumberNeedleConfiguration().getInjectionProviders();
     }",NotBuggy,"[Needle] Move ObjectFactory to io.cucumber.needle
",NotBuggy,"The commit message indicates a package move, but the diff shows a change in the method signature of `setUpInjectionProviders`. It seems the method no longer requires a `resourceName` parameter. This could be due to a change in how the `CucumberNeedleConfiguration` is initialized or how it obtains the resource name. Without more context, it's difficult to determine if this change is related to a bug fix. However, the change in method signature suggests a potential refactoring or simplification of the code. The answer is **NotBuggy**."
cucumber-jvm,468.json,f2eeac155f0893001bda9bf3a0161d151e2eff38,"@@ -1,3 +1,3 @@
-        Type getType() {
-            return type;
-        }

+    public Type getType() {
+        return type;
+    }",NotBuggy,"[Core] Extract CoreStepDefinitions from Java and Java8 implementation

Removes the `CucumberExpression` creation from the backend modules and
will allow a type registry to be created for each pickle.
",NotBuggy,"The commit message indicates a refactoring and extraction of `CoreStepDefinitions`. The diff shows a minor change in the `getType()` method, specifically adding the `public` access modifier. This change doesn't seem to address any specific bug or error in the original code. It's more likely related to making the method accessible from other parts of the codebase after the refactoring. The answer is **NotBuggy**."
cucumber-jvm,744.json,e029121744ad9f3736894a8e751d4b50fe59bf79,"@@ -1,3 +1,3 @@
-        public void configureTypeRegistry(cucumber.api.TypeRegistry typeRegistry) {
+        public void configureTypeRegistry(io.cucumber.core.api.TypeRegistry typeRegistry) {
             //noop
         }",NotBuggy,"[Core] Move core api packages to io.cucumber.core.api
",NotBuggy,The commit message indicates a package move for core API packages. The diff shows a change in the import statement from `cucumber.api.TypeRegistry` to `io.cucumber.core.api.TypeRegistry`. This change reflects the package move described in the commit message and doesn't indicate a bug fix. The answer is **NotBuggy**.
cucumber-jvm,830.json,a5499df8f8ed5c534ffe43a6fe7825e560f222a4,"@@ -1,4 +1,4 @@
-    public static void main(String[] argv) {
+    public static void main(String... argv) {
         byte exitStatus = run(argv, Thread.currentThread().getContextClassLoader());
         System.exit(exitStatus);
     }",NotBuggy,"Print supported languages details (#1290)

Prints code, name, and native name of supported languages
in a three column list.

```
af              Afrikaans                 Afrikaans
am              Armenian                  
ar              Arabic                    
ast             Asturian                  asturianu
az              Azerbaijani               Azrbaycanca
...

",NotBuggy,"The commit message indicates a change to print details of supported languages. The diff modifies the main method signature from `String[] argv` to `String... argv`. This change allows the main method to accept a variable number of string arguments, which is a minor change and doesn't directly relate to fixing a bug. The change seems related to the feature described in the commit message, but it doesn't indicate a bug fix. The answer is **NotBuggy**.
"
cucumber-jvm,1183.json,f72d25ce870c7bd40c1e8b65ef4ca32633ba9c50,"@@ -1,3 +1,3 @@
-    List<DefaultParameterTransformerDefinition> getDefaultParameterTransformers() {
+    Collection<DefaultParameterTransformerDefinition> getDefaultParameterTransformers() {
         return defaultParameterTransformers;
     }",NotBuggy,"[Core] Use sorted set to improve performance
",NotBuggy,"The commit message indicates a performance improvement by using a sorted set. The diff changes the return type of `getDefaultParameterTransformers` from `List` to `Collection`. While this change might be related to using a `SortedSet` internally (which implements `Collection`), the diff itself doesn't reveal whether a `SortedSet` is actually being used or if any bug is being fixed. It's possible the internal implementation was changed to use a `SortedSet` and this is just reflecting the change in the return type. Without more context, it's impossible to determine if this change is related to a bug fix. The answer is **NotBuggy**.
"
cucumber-jvm,1070.json,3e4dae2c35f99d424f7a1a5f447ba36aced51aa1,"@@ -1,3 +1,3 @@
-    public int getTotal() {
-        return passed + failed + skipped + pending + undefined;
-    }

+        public int getTotal() {
+            return passed + failed + skipped + pending + undefined;
+        }",NotBuggy,"Report summaries and  in the same way as the CLI
",NotBuggy,"The diff involves a minor change in indentation. There is no functional change in the code. The commit message ""Report summaries and in the same way as the CLI"" does not seem to be related to the indentation change. Therefore, it is not a bug fix. The answer is **NotBuggy**."
cucumber-jvm,208.json,fe092af8acad11799b24f396a50f870d3f2792a0,"@@ -1,3 +1,3 @@
     public int hashCode() {
-        return Objects.hash(line, column);
+        return Objects.hash(uri);
     }",NotBuggy,"[Core] Support Gherkin Rule keyword (#1840)

* [Core] Support Gherkin Rule keyword

Introduces the `gherkin-messages` module which will add support for the Gherkin
`Rule` keyword. Because the rule keyword is not supported by the `json` or
`html` formatter it will be an opt-in module.
",NotBuggy,"The commit message indicates the introduction of a new module to support the Gherkin `Rule` keyword. However, the provided diff only shows a change in the `hashCode` method, specifically changing the field used for hashing from `line` and `column` to `uri`. This change in `hashCode` suggests a modification to how objects are identified or compared, potentially related to the new `Rule` keyword's implementation or how it interacts with existing data structures. It's difficult to determine if this is a bug fix without more context, but the change in `hashCode` hints at a potential issue with object identity or comparison that needed correction. The answer is **NotBuggy**."
cucumber-jvm,5.json,e63b108cc53f6773d21c4c8fabec86e0c4909a95,"@@ -1,16 +1,18 @@
     public <T> T getInstance(final Class<T> type) {
         final Object instance = standaloneInstances.get(type);
         if (instance != null) {
             return type.cast(instance);
         }
         final Instance<T> selected = container.select(type);
         if (selected.isUnsatisfied()) {
-            final Unmanaged.UnmanagedInstance<T> value = new Unmanaged<>(container.getBeanManager(), type).newInstance();
+            BeanManager beanManager = container.getBeanManager();
+            Unmanaged<T> unmanaged = new Unmanaged<>(beanManager, type);
+            Unmanaged.UnmanagedInstance<T> value = unmanaged.newInstance();
             value.produce();
             value.inject();
             value.postConstruct();
             standaloneInstances.put(type, value);
             return value.get();
         }
         return selected.get();
     }",NotBuggy,"[CDI2] Apply spotless formatting
",NotBuggy,"The commit message indicates a formatting change with ""spotless formatting."" The diff introduces local variables `beanManager` and `unmanaged` to break up a long line. This change does not alter the functionality of the code, only its formatting. Therefore, it's not a bug fix. The answer is **NotBuggy**."
cucumber-jvm,793.json,f64093d5dde6b8d9f28cea614f759cd166876fa6,"@@ -1,12 +1,12 @@
         private <T> T cacheNewInstance(Class<T> type) {
             try {
                 Constructor<T> constructor = type.getConstructor();
                 T instance = constructor.newInstance();
                 instances.put(type, instance);
                 return instance;
             } catch (NoSuchMethodException e) {
-                throw new CucumberException(String.format(""%s doesn't have an empty constructor. If you need DI, put cucumber-picocontainer on the classpath"", type), e);
+                throw new CucumberException(String.format(""%s doesn't have an empty constructor. If you need dependency injection, put cucumber-picocontainer on the classpath"", type), e);
             } catch (Exception e) {
                 throw new CucumberException(String.format(""Failed to instantiate %s"", type), e);
             }
         }",NotBuggy,"[Core] Throw exception when multiple object factories are found (#1832)

When multiple object factories are available on the classpath Cucumber
would print out a warning and fall back to the default object factory.

With the introduction of `cucumber.object-factory` the user can specify
exactly which object factory should be used. There should be no need for
a soft failure anymore.",NotBuggy,"The commit message indicates a change in how Cucumber handles multiple object factories. Instead of a warning and fallback, the code now throws an exception when multiple object factories are found. The diff shows a change in the exception message thrown when a class doesn't have an empty constructor, clarifying the suggestion to use dependency injection. This change doesn't directly fix a bug but enhances error handling and provides clearer guidance to the user. The change in exception message is a minor improvement, not a bug fix. The answer is **NotBuggy**."
cucumber-jvm,1330.json,312039f90cbaf3a63b1e5f259f558575a2c060fc,"@@ -1,3 +1,3 @@
-    public boolean isMultiThreaded() {
-        return threads > 1;
+    public int getCount() {
+        return count;
     }",NotBuggy,"[Core] Refactor runtime options parsing (#1670)

Re-factoring of the runtime options parsing. Tried to achieve several goals:
 * Use a builder rather then CLI arguments to represents options in parsing
 * Pull default behaviour setting up into the runners
 * Pull the different runtime option sources into the runners
 * Run JUnit and TestNG with zero output by default

## Motivation and Context

Cucumber was build with hexagonal architecture in mind. Unfortunately one of the ports that was frequently reused was the command line interface. This resulted in a complex and complicated chain of methods that construct and then parse CLI arguments. This system was hard to understand and change.

On top of this Cucumber accepts commandline options from both the CLI and the environment. This however happened in the bowels of option parsing. Making it non obvious and again hard to follow.

Because the command line interface was used as a port it also imposed the command line defaults on all runners. However for JUnit and TesstNG the default progress formatter nor summary plug-in are particularly useful. Ideal tests are quite.

Finally this structure also forces us to pull in implementation defaults from different runners into the core (e.g. `junitOptions`). This re factoring will allow JUnit and TestNG to define their own `@CucumberOptions` annotation. 

## Related stuff

 * #1476
 * #1537
 * #1135
 * #1029",NotBuggy,"The commit message indicates a refactoring of runtime options parsing in Cucumber. The goal is to simplify the option parsing logic, decouple runners from command-line defaults, and allow runners like JUnit and TestNG to define their own options. The diff shows a change from `isMultiThreaded()` to `getCount()`. This change does not indicate a bug fix, but rather a change in the API. The answer is **NotBuggy**."
cucumber-jvm,889.json,1bd057452353bdb2c343b9460d7b4e36f05f8f75,"@@ -1,6 +1,4 @@
-    public void addPlugin(Object plugin) {
+    public void addPlugin(Formatter plugin) {
         plugins.add(plugin);
-        if (plugin instanceof Formatter) {
-            setEventBusFormatterPlugins(plugin);
-        }
+        setEventBusFormatterPlugins(plugin);
     }",NotBuggy,"Improve plugin and formatter structure

The following improvements have been made

1. All plugin interfaces (Formatter, StepDefinitionReporter,
   SummaryPrinter) now extend the Plugin interface.

2. Updated documentation to make it clear what each plugin does.

3. All plugins have been made final. They are not designed for
   extension.

4. Moved android formatters into formatter package to limit visibility
   of TestSourcesModel.

5. Classes in runtime/android have been made final and have had their
   visibility reduced. They are not designed for extension.
",NotBuggy,"The commit message indicates improvements to the plugin and formatter structure, including making plugin interfaces extend the `Plugin` interface. The code change reflects this by changing the parameter type of the `addPlugin` method from `Object` to `Formatter`. This ensures that only `Formatter` plugins are added, aligning with the stated goal of improving the plugin structure. The removal of the `instanceof` check further supports this structural improvement. There is no indication of a bug fix in this change. The answer is **NotBuggy**.
"
hazelcast,26584.json,67630aeab5cc4398c3f84a394807269a890bc2e0,"@@ -1,20 +1,22 @@
-    private static int calculateCleanupOperationCount(HazelcastProperties properties, int partitionCount, int partitionThreadCount) {
-        String stringValue = properties.getString(CLEANUP_OPERATION_COUNT);
+    private static int calculateCleanupOperationCount(HazelcastProperties properties,
+                                                      final HazelcastProperty cleanupOpCountProperty,
+                                                      int partitionCount, int partitionThreadCount) {
+        String stringValue = properties.getString(cleanupOpCountProperty);
         if (stringValue != null) {
             return parseInt(stringValue);
         }
 
         // calculate operation count to be sent by using partition-count.
         final double scanPercentage = 0.1D;
         final int opCountFromPartitionCount = (int) (partitionCount * scanPercentage);
 
         // calculate operation count to be sent by using partition-thread-count.
         final int inflationFactor = 3;
         int opCountFromThreadCount = partitionThreadCount * inflationFactor;
 
         if (opCountFromPartitionCount == 0) {
             return opCountFromThreadCount;
         }
 
         return min(opCountFromPartitionCount, opCountFromThreadCount);
     }",NotBuggy,"JCache automatic expiration

JCache expiration mechanism only worked for entries that were touched after their expiration time. Therefore, an entry and its backups could stay in the record store forever. This PR introduces a periodic cache cleanup task for removing expired entries. JCache expired entries are removed from the record stores and backups eventually.
",Buggy,"The commit message describes a bug where JCache entries weren't being properly expired and removed from record stores and backups, leading to them persisting indefinitely. The provided diff modifies the `calculateCleanupOperationCount` method, introducing a `HazelcastProperty cleanupOpCountProperty` parameter. This suggests a change in how the cleanup operation count is configured, potentially influencing the frequency or scope of the cleanup task. While the diff itself doesn't directly show the bug fix, it's related to the cleanup process mentioned in the commit message, which addresses the expiration bug. Therefore, the changes are likely part of the bug fix. The answer is **Buggy**."
hazelcast,27157.json,58c2cdbbe75d9a9855f727db2f0d2f7304099098,"@@ -1,3 +1,3 @@
-    public ItemListenerConfig setImplementation(ItemListener implementation) {
-        throw new UnsupportedOperationException(""This config is read-only"");
+    public PNCounterConfig setReplicaCount(int replicaCount) {
+        throw new UnsupportedOperationException(""This config is read-only PN counter: "" + getName());
     }",NotBuggy,"Move read-only config to internal package (#15569)

Move read-only config to internal package

Moves all read-only config classes to internal package, removes the
deprecation annotations as these classes will stay in 4.0 and marks the
getAsReadOnly methods as PrivateApi.",NotBuggy,"The commit message indicates a reorganization of code by moving read-only configuration classes to an internal package. The diff shows that setting the replica count is not allowed for read-only PN counter configurations. This enforces the read-only nature of the configuration. This change is related to code structure and access control, not a bug fix. The answer is **NotBuggy**."
hazelcast,36355.json,244d65a80c4d14c1f76dd185faeee689fd2422a2,"@@ -1,3 +1,3 @@
-    public boolean send(PreVoteResponse response, RaftEndpoint target) {
-        return send(new PreVoteResponseOp(raftGroupId, response), target);
+    public boolean send(PreVoteResponse response, RaftMember target) {
+        return send(new PreVoteResponseOp(groupId, response), target);
     }",NotBuggy,"Raft session management implementation & lock improvements

Co-authored-by: Ensar Basri Kahveci <ebkahveci@gmail.com>
Co-authored-by: Mehmet Dogan <mehmet@dogan.io>
",Buggy,"The commit message indicates improvements to Raft session management and lock mechanisms. The code diff changes the `send` method's parameter type from `RaftEndpoint` to `RaftMember` and replaces `raftGroupId` with `groupId`. While the commit message doesn't explicitly mention a bug fix, the change in parameter type suggests a potential correction in how messages are routed or handled within the Raft implementation. It's possible that `RaftEndpoint` was not the correct abstraction, and using `RaftMember` provides more accurate or complete information for message delivery, thus fixing a bug or preventing one. The change from `raftGroupId` to `groupId` may also indicate a correction in how the group identifier is accessed or used.

Given these observations, it's reasonable to infer that the changes address a bug or potential issue in the Raft implementation.

**Buggy**
"
hazelcast,33686.json,ae4e6e8a8211f695a851c2fe8d9f6f53ad042751,"@@ -1,5 +1,5 @@
     public long tailSequence() {
         GenericOperation op = new GenericOperation(name, OPERATION_TAIL);
         InternalCompletableFuture f = invoke(op);
-        return (Long) f.getSafely();
+        return (Long) f.join();
     }",NotBuggy,"InvocationFuture method renaming to align with CompletableFuture

Renamed InvocationFuture.set to complete
Renamed InternalCompletableFuture.getSafely to join
",NotBuggy,"The commit message indicates a renaming of methods to align with `CompletableFuture`. The diff shows that `InternalCompletableFuture.getSafely` is renamed to `join`. This change is purely a renaming refactoring and does not indicate a bug fix. The answer is **NotBuggy**.
"
hazelcast,38624.json,43243bf3287025ae21ad8ecd8c962b986d921c17,"@@ -1,4 +1,4 @@
-    public DistributedClassloadingConfig setProviderFilter(String providerFilter) {
+    public UserCodeDeploymentConfig setProviderFilter(String providerFilter) {
         this.providerFilter = providerFilter;
         return this;
     }",NotBuggy,"Distributed Classloading renamed to User Code Deployment
",NotBuggy,"The commit message indicates a renaming of ""Distributed Classloading"" to ""User Code Deployment"". The code diff confirms this by renaming `DistributedClassloadingConfig` to `UserCodeDeploymentConfig`. This is a refactoring change, not a bug fix. The answer is **NotBuggy**.
"
hazelcast,32184.json,4307a6aa07fbae5d93d5b5b8c50b598332183f8e,"@@ -1,22 +1,22 @@
     public Object invoke(Data key, EntryProcessor entryProcessor, Object[] arguments, int completionId) {
         long now = Clock.currentTimeMillis();
-        long start = isStatisticsEnabled() ? System.nanoTime() : 0;
+        long startNanos = isStatisticsEnabled() ? Timer.nanos() : 0;
         R record = records.get(key);
         boolean isExpired = processExpiredEntry(key, record, now);
         if (isExpired) {
             record = null;
         }
         if (isStatisticsEnabled()) {
             if (recordNotExistOrExpired(record, isExpired)) {
                 statistics.increaseCacheMisses(1);
             } else {
                 statistics.increaseCacheHits(1);
             }
-            statistics.addGetTimeNanos(System.nanoTime() - start);
+            statistics.addGetTimeNanos(Timer.nanosElapsed(startNanos));
         }
         CacheEntryProcessorEntry entry = createCacheEntryProcessorEntry(key, record, now, completionId);
         injectDependencies(entryProcessor);
         Object result = entryProcessor.process(entry, arguments);
         entry.applyChanges();
         return result;
     }",NotBuggy,"use Timer to measure elapsed time (#16780)

",Buggy,"The commit message indicates a change to use a `Timer` for measuring elapsed time. The diff shows that `System.nanoTime()` is replaced with `Timer.nanos()` and the elapsed time calculation is changed to use `Timer.nanosElapsed(startNanos)`. This change aims to improve the accuracy or efficiency of time measurement. While it doesn't explicitly state a bug fix, the change suggests a potential issue with the previous method of time measurement, possibly related to performance or accuracy. Therefore, it can be inferred that the change addresses a bug or an inefficiency. The answer is **Buggy**."
hazelcast,5483.json,02825e67b1bf9fac68116e5504e605a70cbf2182,"@@ -1,5 +1,5 @@
     void validate() {
         if (className == null && listenerImplementation == null) {
-            throw new IllegalArgumentException(""One of class name or listener implementation must be not null"");
+            throw new IllegalArgumentException(""Either class name or listener implementation must be not null"");
         }
     }",NotBuggy,"Improve javadocs & typos (#11716)

",NotBuggy,"The commit message indicates improvements to Javadocs and typo fixes. The diff shows a change in an exception message, correcting ""must be not null"" to ""must be not null"". This is a typo fix. The answer is **NotBuggy**."
hazelcast,28495.json,e7657e3c867ba1ac879355b5c8751e772ee0cadd,"@@ -1,8 +1,18 @@
     protected ExceptionAction onException(Throwable t) {
         raftInvocationContext.updateKnownLeaderOnFailure(groupId, t);
 
-        if (shouldFailOnIndeterminateOperationState() && (t instanceof MemberLeftException)) {
-            return THROW_EXCEPTION;
+        if (t instanceof IndeterminateOperationState) {
+            if (isRetryableOnIndeterminateOperationState()) {
+                if (indeterminateException == null) {
+                    indeterminateException = t;
+                }
+                return RETRY_INVOCATION;
+            } else if (shouldFailOnIndeterminateOperationState()) {
+                return THROW_EXCEPTION;
+            } else if (indeterminateException == null) {
+                indeterminateException = t;
+            }
         }
+
         return isRetryable(t) ? RETRY_INVOCATION : op.onInvocationException(t);
     }",NotBuggy,"Implement Raft based IAtomicReference, ISemaphore and ICountdownLatch

- Implement auto-removal of missing Raft members:
If a Raft member is missing for some time, it will be automatically
removed from the CP sub-system if enabled.

Co-authored-by: Ensar Basri Kahveci <ebkahveci@gmail.com>
Co-authored-by: Mehmet Dogan <mehmet@dogan.io>
",Buggy,"The commit message indicates the implementation of Raft-based data structures and auto-removal of missing Raft members. The diff focuses on exception handling within the Raft invocation context. Specifically, it addresses the `IndeterminateOperationState` exception. The code now retries the invocation if `isRetryableOnIndeterminateOperationState()` returns true, otherwise, it throws the exception if `shouldFailOnIndeterminateOperationState()` returns true. The original code only handled `MemberLeftException`. This change introduces more robust handling of indeterminate operation states, which could arise due to various issues in a distributed system. This suggests a bug fix or at least an improvement in error handling related to the Raft implementation. The answer is **Buggy**."
hazelcast,31303.json,faa6428c28eba0b8147e38196fe3711c46773ac8,"@@ -1,3 +1,3 @@
     public String toString() {
-        return attribute + ""="" + value;
+        return attributeName + ""="" + value;
     }",NotBuggy,"1. Introducing predicates on arrays and collection.

Let's have following classes:

class Body {
  Collection<Limb> limbs;
  Map<String, String> pocket;
}

class Limb {
  String name;
  String[] nails;
}

You can can use predicates like this:

IMap<Integer, Body> map = getMap();
Predicate p = Predicates.equals('limb[0].name', 'hand'); // <- this matches all bodies where 1st limb has name 'hand'
Collection<Body> result = map.values(p);

You can also use SQLPredicate:
Predicate p = new SQLPredicate('limb[0].name = hand');
Collection<Body> result = map.values(p);

You can use a wildcard instead of the exact position parameters:
Predicate p = Predicates.equals('limb[any].name', 'hand'); // <- this matches all bodies where any limb has name 'hand'

This is another possible query:
Predicate p = Predicates.equals('limb[0].nails[any]', = 'red'); // <- this matchinges all bodies where 1st limb has any nail with red color

You can create indexes on collections/arrays. But bear in mind the attribute used in index definition has to be the same as the one used in the query:

<indexes>
  <index ordered=""false"">limb[any].name</index>
</indexes>

The this predicate will use the index:
Predicate p = Predicates.equals('limb[any].name', 'hand');

but this one will not:
Predicates.equals('limb[0].name', 'hand')

You would have to create another index:
<indexes>
  <index ordered=""false"">limb[0].name</index>
</indexes>

2. Introducing custom value extractors
You can also define a custom attribute using a ValueExtractor that encompasses an extraction logic.
The extraction logic may just extract the underlying value or group, reduce or transform it.
Reflection-based extraction is the default mechanism - ValueExtractor(s) are an alternative way of getting values from objects.

How to use a ValueExtractor?
First, extend the com.hazelcast.query.extractor.ValueExtractor class and implement the extract() method.
Then define a new attribute with registering the above-mentioned extractor in the configuration of the map.

Please have a look at some code samples:

* How to define a new custom attribute?
<code>
MapAttributeConfig attributeConfig = new MapAttributeConfig();
extractorConfig.setName(""currency"");
extractorConfig.setExtractor(""com.bank.CurrencyExtractor"");
</code>

* How to register the newly-defined attribute in a configuration of a Map?
<code>
MapConfig mapConfig = (...);
mapConfig.addMapAttributeConfig(attributeConfig);
</code>

Extractors may be also defined in the XML configuration.
Please, bear in mind that an extractor may not be added while the map has been instantiated. All extractor have to be defined upfront in the map's initial configuration.
",NotBuggy,"The commit message describes the introduction of predicates on arrays and collections, along with custom value extractors for Hazelcast's querying capabilities. The diff modifies the `toString()` method of an unspecified class, changing `attribute` to `attributeName`. This change is likely related to the new features, possibly to provide a more descriptive representation of the attribute being used in predicates or value extraction. It's unlikely to be a bug fix, but rather a change to improve the clarity of the output when debugging or logging. The change aligns with the overall goal of enhancing the querying functionality. The answer is **NotBuggy**.
"
hazelcast,10974.json,ce1b80bdee78c9a39a7567ac835772242b11715c,"@@ -1,3 +1,3 @@
     ManagedService createManagedService() {
-        return new MapManagedService(getMapServiceContext());
+        return new MapManagedService(mapServiceContext);
     }",NotBuggy,"Renamed MapServiceContextAwareFactory to MapServiceFactory
",Buggy,"The commit message indicates a renaming, which is a refactoring operation. However, the diff shows a change in how `MapManagedService` is instantiated, specifically changing `getMapServiceContext()` to `mapServiceContext`. This suggests that the method call `getMapServiceContext()` might have been problematic or unnecessary, and directly using the `mapServiceContext` field resolves an issue. This could be related to incorrect context retrieval or potential null pointer exceptions if the getter was not properly handling the context. Therefore, this change is likely a bug fix. The answer is **Buggy**.
"
hazelcast,3827.json,2fcead2b9c8264248187f203e7739970b7230d16,"@@ -1,4 +1,3 @@
-    protected static ClientMessage prepareResponse(MessageHandlerContext context, Object... objects) {
-
+    public String getMethodName() {
         return null;
     }",NotBuggy,"Message handling task impl
ClientMessage improvement and Parameters base
Authentication impls
MessageTask Factory
",NotBuggy,"The commit message indicates improvements and implementations related to message handling, client messages, authentication, and a message task factory. The diff removes a `prepareResponse` method and adds a `getMethodName` method. Without more context, it's difficult to definitively say if this is a bug fix. It looks like a refactoring or a change in the API. The answer is **NotBuggy**."
hazelcast,14416.json,253b515b1b443a2035cec75d0b576fa3ee7f8667,"@@ -1,3 +1,3 @@
-        private RowBatch getBatch() {
-            return batch;
-        }

+    public int getBatchSize() {
+        return batchSize;
+    }",NotBuggy,"SQL send/receive operators (#16848)

",NotBuggy,"The commit message indicates changes related to SQL send/receive operators. However, the provided diff only shows a change from a `getBatch()` method returning a `RowBatch` to a `getBatchSize()` method returning an integer `batchSize`. There is no indication of error handling, logical corrections, or exception handling improvements. This change appears to be a modification of the API, likely related to how batch information is accessed or represented, rather than a bug fix. The answer is **NotBuggy**.
"
hazelcast,37315.json,98e86ecc1e48b6b3b442235a7a0a783346b89d59,"@@ -1,5 +1,5 @@
     public boolean eval(Object arg) {
-        final QueryEntry entry = (QueryEntry) arg;
+        final QueryableEntry entry = (QueryableEntry) arg;
         final Data keyData = entry.getKeyData();
         return (key == null || key.equals(keyData)) && predicate.apply((Map.Entry) arg);
     }",NotBuggy,"Cached QueryEntry implementation
",NotBuggy,"The commit message ""Cached QueryEntry implementation"" suggests the introduction or modification of a caching mechanism related to `QueryEntry`. The diff shows a change from `QueryEntry` to `QueryableEntry`. This change alone doesn't indicate a bug fix. It's likely part of the implementation of the caching mechanism, possibly introducing a new interface or class that provides queryable capabilities for entries. Without more context, it's difficult to determine if this change is directly related to fixing a bug.

The change from `QueryEntry` to `QueryableEntry` suggests a refactoring or redesign to support querying capabilities, which is consistent with the commit message.

The answer is **NotBuggy**."
hazelcast,35549.json,cab58ba9e0a6b904b0c0ddb659fc339aaee9c86c,"@@ -1,5 +1,4 @@
     protected void toString(StringBuilder sb) {
         super.toString(sb);
-        sb.append("", expectedRound="").append(expectedRound)
-          .append("", invocationUid="").append(invocationUid);
+        sb.append("", invocationUid="").append(invocationUid).append("", timeoutMillis="").append(timeoutMillis);
     }",NotBuggy,"Prepare CP Subsystem for release

- Move RaftGroupId and RaftMember to core module
- Rename packages to cp.internal...
- Move Raft packages to core module & remove Raft modules
- Make use of Hazelcast's proxy layer for Raft proxies
- Implement XML configuration for the CP subsystem
- Prepare REST API for CP subsystem management
- Prepare CP subsystem management script

Co-authored-by: Ensar Basri Kahveci <ebkahveci@gmail.com>
Co-authored-by: Mehmet Dogan <mehmet@dogan.io>
",NotBuggy,"The commit message indicates a preparation for the CP subsystem release, involving moving components, renaming packages, implementing XML configuration, and preparing REST API and management scripts. The diff modifies the `toString` method, removing `expectedRound` and adding `timeoutMillis`. This change doesn't obviously indicate a bug fix. It seems more related to restructuring or adding information useful for debugging or logging in the context of the CP subsystem. The answer is **NotBuggy**."
hazelcast,35758.json,cab58ba9e0a6b904b0c0ddb659fc339aaee9c86c,"@@ -1,3 +1,3 @@
-        private static AcquireResult notAcquired(Collection<LockInvocationKey> cancelled) {
-            return new AcquireResult(NOT_LOCKED, cancelled);
-        }

+    static AcquireResult failed(Collection<LockInvocationKey> cancelled) {
+        return new AcquireResult(FAILED, INVALID_FENCE, cancelled);
+    }",NotBuggy,"Prepare CP Subsystem for release

- Move RaftGroupId and RaftMember to core module
- Rename packages to cp.internal...
- Move Raft packages to core module & remove Raft modules
- Make use of Hazelcast's proxy layer for Raft proxies
- Implement XML configuration for the CP subsystem
- Prepare REST API for CP subsystem management
- Prepare CP subsystem management script

Co-authored-by: Ensar Basri Kahveci <ebkahveci@gmail.com>
Co-authored-by: Mehmet Dogan <mehmet@dogan.io>
",Buggy,"The commit message indicates a preparation for the CP subsystem release, involving moving and renaming packages, implementing XML configuration, and preparing REST API and management scripts. The diff shows a change in the `AcquireResult` method, replacing `notAcquired` with `failed` and introducing `FAILED` and `INVALID_FENCE`. This change suggests an update in the locking mechanism, possibly to handle failure scenarios more explicitly with an invalid fence. This could be a bug fix or an enhancement related to handling locking failures. The change from `NOT_LOCKED` to `FAILED` and the addition of `INVALID_FENCE` suggest a more robust error handling mechanism. Therefore, the answer is **Buggy**."
jetty,11818.json,33c11dffaa86d09b33fd350f07cd9c548b7d3b7c,"@@ -1,9 +1,28 @@
-    public DecoderWrapper getWrapperFor(Class<?> type)
+    public Wrapper getWrapperFor(Class<?> type)
     {
-        DecoderMetadata metadata = getMetadataFor(type);
-        if (metadata != null)
+        synchronized (activeWrappers)
         {
-            return newWrapper(metadata);
+            Wrapper wrapper = activeWrappers.get(type);
+
+            // Try parent (if needed)
+            if ((wrapper == null) && (parentFactory != null))
+            {
+                wrapper = parentFactory.getWrapperFor(type);
+            }
+
+            if (wrapper == null)
+            {
+                // Attempt to create Wrapper on demand
+                DecoderMetadata metadata = metadatas.getMetadataByType(type);
+                if (metadata == null)
+                {
+                    return null;
+                }
+                wrapper = newWrapper(metadata);
+                // track wrapper
+                activeWrappers.put(type,wrapper);
+            }
+
+            return wrapper;
         }
-        return null;
     }",NotBuggy,"JSR-356 - cleaning up Decoder and Encoder lifecycle.
",NotBuggy,"The code changes involve synchronizing access to `activeWrappers`, checking for a parent factory, and creating/caching wrappers on demand. The commit message mentions cleaning up Decoder and Encoder lifecycle. The synchronization and caching suggest improvements in resource management and thread safety related to decoder/encoder instances. The changes prevent race conditions when multiple threads access the same decoder/encoder, and improve performance by reusing existing instances. These changes do not directly fix a bug, but rather improve the overall design and thread safety of the decoder/encoder lifecycle. The answer is **NotBuggy**."
jetty,6323.json,314f94decc0e814baf31f500502020e05fcb73a2,"@@ -1,4 +1,4 @@
-        public XmlParser.Node getRoot ()
-        {
-            return _root;
-        }

+    public int getMinorVersion()
+    {
+        return _minorVersion;
+    }",NotBuggy,"320073 Reconcile jetty-8 and jetty-7 webapp configuration mechanisms

git-svn-id: svn+ssh://dev.eclipse.org/svnroot/rt/org.eclipse.jetty/jetty/trunk@2145 7e9141cc-0065-0410-87d8-b60c137991c4
",Buggy,"The commit message indicates a reconciliation of webapp configuration mechanisms between Jetty 8 and Jetty 7. The diff shows a change from `getRoot()` to `getMinorVersion()`. Without more context, it's difficult to definitively say if this is a bug fix. However, the change from getting the root node to getting the minor version suggests a shift in how the configuration is being accessed or processed. This could be due to a bug or incompatibility between the Jetty versions. Therefore, it's more likely a bug fix. The answer is **Buggy**.
"
jetty,11565.json,e69b41e9cdb3c1d10a426adc3a6d474aeaeb2bab,"@@ -1,4 +1,5 @@
     public void customizeParamsOnClose(LinkedList<IJsrParamId> params)
     {
-        params.addFirst(JsrParamPath.INSTANCE);
+        super.customizeParamsOnClose(params);
+        params.addFirst(JsrPathParamId.INSTANCE);
     }",NotBuggy,"JSR-356 Adding suppport for @PathParam
",NotBuggy,The commit message indicates the addition of support for `@PathParam` in JSR-356. The code adds `JsrPathParamId.INSTANCE` to the list of parameters. The addition of new functionality is not a bug fix. The answer is **NotBuggy**.
jetty,5631.json,0301de099d876124b5a0b26da171105c63ab0632,"@@ -1,10 +1,6 @@
     private static void addListeners(ContainerLifeCycle container)
     {
-        WebSocketCdiListener listener = new WebSocketCdiListener(container);
-
-        if (listener != null)
-        {
-            container.addLifeCycleListener(listener);
-            container.addEventListener(listener);
-        }
+        WebSocketCdiListener listener = new WebSocketCdiListener();
+        container.addLifeCycleListener(listener);
+        container.addEventListener(listener);
     }",NotBuggy,"Delaying decoration of websocket endpoints
",Buggy,"The commit message ""Delaying decoration of websocket endpoints"" suggests a change in how websocket endpoints are handled, possibly related to initialization or setup. The code diff removes a null check on the `listener` object before adding it as a lifecycle listener and event listener to the container. This implies that the `WebSocketCdiListener` is now always added, regardless of some previous condition. The removal of the null check suggests that the listener creation might have been conditional before, and now it's unconditional. This could be to ensure that the decoration of websocket endpoints is consistently applied, even if it's delayed.

The removal of the null check suggests a potential bug fix. If the listener was not being added under certain conditions, websocket endpoints might not have been decorated correctly. By ensuring the listener is always added, the change addresses this potential issue.

**Buggy**
"
jetty,10058.json,2b43ae931cdbadcc2db001aed740ed91be1a75e8,"@@ -1,9 +1,9 @@
     public void bindToENC(String localName)
     throws NamingException
     {
         //TODO - check on the whole overriding/non-overriding thing
         InitialContext ic = new InitialContext();
         Context env = (Context)ic.lookup(""java:comp/env"");
-        Log.debug(""Binding java:comp/env/""+localName+"" to ""+objectNameString);
+        __log.debug(""Binding java:comp/env/""+localName+"" to ""+objectNameString);
         NamingUtil.bind(env, localName, new LinkRef(objectNameString));
     }",NotBuggy,"improved debugging of JNDI. own logger

git-svn-id: svn+ssh://dev.eclipse.org/svnroot/rt/org.eclipse.jetty/jetty/trunk@2608 7e9141cc-0065-0410-87d8-b60c137991c4
",NotBuggy,"The commit message indicates improved debugging of JNDI using its own logger. The code diff replaces `Log.debug` with `__log.debug`. This change suggests that the logging mechanism has been updated to use a dedicated logger instance (`__log`) instead of a general `Log` class. This change is related to debugging and logging, and it's likely done to improve the clarity and control over JNDI-related logs. This is not a bug fix. The answer is **NotBuggy**."
jetty,12021.json,745f757552698139934a6f403d9c7396cb06044a,"@@ -1,17 +1,20 @@
     public void init(EndpointConfig config)
     {
-        LOG.debug(""init({})"",config);
+        if (LOG.isDebugEnabled())
+        {
+            LOG.debug(""init({})"",config);
+        }
 
         // Instantiate all declared encoders
         for (EncoderMetadata metadata : metadatas)
         {
             Wrapper wrapper = newWrapper(metadata);
             activeWrappers.put(metadata.getObjectType(),wrapper);
         }
 
         // Initialize all encoders
         for (Wrapper wrapper : activeWrappers.values())
         {
             wrapper.encoder.init(config);
         }
     }",NotBuggy,"Guarded calls to LOG.debug() with if (LOG.isDebugEnabled()) to reduce allocation of varargs Object[].
",Buggy,"The commit message indicates that calls to `LOG.debug()` are now guarded by a check `if (LOG.isDebugEnabled())`. This is done to prevent unnecessary allocation of varargs `Object[]` when debug logging is disabled, which improves performance. The diff confirms this change by adding the conditional check before the `LOG.debug()` call. This change is an optimization to avoid unnecessary object creation, which can be considered a bug fix in terms of performance. The answer is **Buggy**.
"
jetty,14333.json,c9f4513a89474e86da004c5e4ac2cba23c7f7a83,"@@ -1,5 +1,5 @@
-    public Request idleTimeout(long timeout)
+    public Request timeout(long timeout, TimeUnit unit)
     {
-        this.idleTimeout = timeout;
+        this.timeout = unit.toMillis(timeout);
         return this;
     }",NotBuggy,"Reworked HTTP client API, removing usage of Future.
",NotBuggy,"The commit message indicates a rework of the HTTP client API, specifically removing the usage of `Future`. The diff shows a change in the `idleTimeout` method, renaming it to `timeout` and adding a `TimeUnit` parameter. The original method took a `long timeout` directly, while the new method takes a `long timeout` and a `TimeUnit`, converting the timeout to milliseconds. This change doesn't appear to be directly related to fixing a bug but rather to provide more flexibility and control over the timeout value. It's an API change, not a bug fix. The answer is **NotBuggy**.
"
jetty,10063.json,e10c14c77aee8c4c0aaf1b7cd0def390239dbf2b,"@@ -1,20 +1,20 @@
-    protected void save (Object scope)
+    protected void save (Object object)
     throws NamingException
     {
-        __log.debug(""SAVE {} in {}"",this,scope);
+        __log.debug(""SAVE {} in {}"",this,_scope);
         InitialContext ic = new InitialContext();
         NameParser parser = ic.getNameParser("""");
-        Name prefix = NamingEntryUtil.getNameForScope(scope);
+        Name prefix = NamingEntryUtil.getNameForScope(_scope);
       
         //bind the NamingEntry into the context
         Name namingEntryName = NamingEntryUtil.makeNamingEntryName(parser, getJndiName());
         namingEntryName.addAll(0, prefix);
-        namingEntryNameString = namingEntryName.toString();
-        NamingUtil.bind(ic, namingEntryNameString, this);
+        _namingEntryNameString = namingEntryName.toString();
+        NamingUtil.bind(ic, _namingEntryNameString, this);
                 
         //bind the object as well
         Name objectName = parser.parse(getJndiName());
         objectName.addAll(0, prefix);
-        objectNameString = objectName.toString();
-        NamingUtil.bind(ic, objectNameString, objectToBind);
+        _objectNameString = objectName.toString();
+        NamingUtil.bind(ic, _objectNameString, object);
     }",NotBuggy,"332937 Added Destroyable interface and reworked dependent lifecycles, specially of JNDI
Also added Dumpable interface to improve and standardize the output of nested structures while debugging

git-svn-id: svn+ssh://dev.eclipse.org/svnroot/rt/org.eclipse.jetty/jetty/trunk@2615 7e9141cc-0065-0410-87d8-b60c137991c4
",Buggy,"The commit message indicates a rework of lifecycles, especially of JNDI, and the introduction of `Destroyable` and `Dumpable` interfaces. The diff shows changes within the `save` method. Specifically, the `scope` parameter is renamed to `object`. Also, `scope` is replaced with `_scope`, `namingEntryNameString` with `_namingEntryNameString`, and `objectNameString` with `_objectNameString`. These changes suggest that the code was modified to use instance variables instead of the passed in `scope` parameter. This could be to fix a bug related to incorrect scope management or to improve the lifecycle handling of JNDI resources. The change in variable names and the use of instance variables suggest a correction or improvement in how the JNDI context is managed, which could be related to fixing a bug.
The answer is **Buggy**."
jetty,6021.json,722f39080072328301279dceb1c74b4c83b5035f,"@@ -1,4 +1,4 @@
     public void appendJSON(final StringBuffer buffer, Convertible converter)
     {
-        appendJSON((StringBuffer)buffer,converter);
+        appendJSON((Appendable)buffer,converter);
     }",NotBuggy,"numerous findbugs inspired cleanups
",NotBuggy,"The commit message indicates cleanups inspired by FindBugs. The code change modifies the `appendJSON` method signature to accept an `Appendable` instead of a `StringBuffer`. This change increases the flexibility of the method, as `Appendable` is a more general interface than `StringBuffer`. This is likely a cleanup/refactoring change to improve code quality and doesn't directly address a specific bug. The answer is **NotBuggy**."
jetty,5349.json,fe773d22e1473b230538558b6992a157a62bc350,"@@ -1,4 +1,4 @@
     public List<File> getWebInfLib()
     {
-        return webInfJars;
+        return _webInfJars;
     }",NotBuggy,"JETTY-846 Support maven-war-plugin overlay configuration with jetty:run
",NotBuggy,"The commit message indicates support for maven-war-plugin overlay configuration with jetty:run, which suggests a new feature or enhancement rather than a bug fix. The code change itself involves returning `_webInfJars` instead of `webInfJars`. Without more context, it's difficult to determine if this change is related to a bug fix or simply a correction of a variable name. However, given the commit message and the limited information available, it's more likely that this change is related to the new feature or enhancement. The answer is **NotBuggy**."
jetty,7053.json,314f94decc0e814baf31f500502020e05fcb73a2,"@@ -1,42 +1,37 @@
         public void service(ServletRequest req, ServletResponse res) throws ServletException, IOException
         {
             Servlet s;
             synchronized(this)
             {
                 if(_stack.size()>0)
                     s=(Servlet)_stack.pop();
                 else
                 {
                     try
                     {
-                        s = (Servlet) newInstance();
-                        s = getServletHandler().customizeServlet(s);
+                        s = newInstance();
                         s.init(_config);
                     }
                     catch (ServletException e)
                     {
                         throw e;
                     }
-                    catch (IOException e)
-                    {
-                        throw e;
-                    }
                     catch (Exception e)
                     {
                         throw new ServletException(e);
                     }
                 }
             }
             
             try
             {
                 s.service(req,res);
             }
             finally
             {
                 synchronized(this)
                 {
                     _stack.push(s);
                 }
             }
         }",NotBuggy,"320073 Reconcile jetty-8 and jetty-7 webapp configuration mechanisms

git-svn-id: svn+ssh://dev.eclipse.org/svnroot/rt/org.eclipse.jetty/jetty/trunk@2145 7e9141cc-0065-0410-87d8-b60c137991c4
",Buggy,"The commit message indicates a reconciliation of webapp configuration mechanisms between Jetty 8 and Jetty 7. The code diff focuses on the `service` method within a servlet handling mechanism. Specifically, the changes involve servlet instance creation and initialization.

The original code attempted to handle `IOException` separately from other exceptions during servlet instantiation. The modified code removes the separate `IOException` catch block and consolidates exception handling into a single catch block for all `Exception` types, wrapping them in a `ServletException`.

This change suggests a simplification and unification of exception handling, potentially addressing inconsistencies or redundancies in how servlet instantiation errors were managed across different Jetty versions. It's possible that the original separate handling of `IOException` was either unnecessary or led to issues in certain scenarios. The consolidation of exception handling suggests a bug fix or at least a more robust approach to error management during servlet creation.

Therefore, the answer is **Buggy**."
jetty,13676.json,aeb27cd461d2faa4b77413e029f04f4b13e79777,"@@ -1,14 +1,15 @@
     public void forwardFailure(List<Response.ResponseListener> listeners, Response response, Throwable failure)
     {
         notifyBegin(listeners, response);
         for (Iterator<HttpField> iterator = response.getHeaders().iterator(); iterator.hasNext();)
         {
             HttpField field = iterator.next();
             if (!notifyHeader(listeners, response, field))
                 iterator.remove();
         }
         notifyHeaders(listeners, response);
         if (response instanceof ContentResponse)
-            notifyContent(listeners, response, ByteBuffer.wrap(((ContentResponse)response).getContent()));
+            // TODO: handle callback
+            notifyContent(listeners, response, ByteBuffer.wrap(((ContentResponse)response).getContent()), new Callback.Adapter());
         notifyFailure(listeners, response, failure);
     }",NotBuggy,"431642 - Implement ProxyServlet using Servlet 3.1 async I/O.

Intermediate commit that implements asynchronous content in HttpClient,
for the HTTP protocol, passing the tests.

This work needs to be extended to FCGI and SPDY and finally implement
the asynchronous proxy servlet.
",NotBuggy,"The commit message indicates an implementation of asynchronous content handling in HttpClient, specifically for the HTTP protocol. The diff shows a modification in the `forwardFailure` method where `notifyContent` is called. The change introduces a `Callback.Adapter()` as an argument to `notifyContent`, and there's a `// TODO: handle callback` comment, suggesting that the callback handling is not yet fully implemented. This change is part of the larger effort to implement the asynchronous proxy servlet, as mentioned in the commit message. While the change introduces a new callback, it doesn't seem to be fixing a bug but rather adding functionality. The presence of the `TODO` comment indicates that the implementation is incomplete, but it doesn't necessarily mean that the current code is fixing a bug. The answer is **NotBuggy**.
"
jetty,10705.json,c2f1d23f2baff2587e261cf4034be9dc86eb616b,"@@ -1,4 +1,5 @@
-    public void setMasked(boolean mask)
+    public WebSocketFrame setMasked(boolean mask)
     {
         this.masked = mask;
+        return this;
     }",NotBuggy,"Making WebSocketFrame use builder pattern
",NotBuggy,"The commit message indicates a change to use the builder pattern. The diff shows that the `setMasked` method is modified to return the `WebSocketFrame` object itself (`return this;`). This allows for method chaining, which is a characteristic of the builder pattern. This change doesn't fix a bug, but rather enhances the API to be more fluent. The answer is **NotBuggy**."
jetty,3380.json,fd099aa77d54cd06463ccba3da3c9425c0c71437,"@@ -1,38 +1,37 @@
-    private V getBest(int t,byte[] b,int offset,int len)
+    private V getBest(int t,String s,int offset,int len)
     {
         int node=0;
         for(int i=0; t!=0 && i<len; i++)
         {
-            char c=StringUtil.lowercases[b[offset+i]&0x7f];
+            char c=s.charAt(offset+i);
+            if(isCaseInsensitive() && c<128)
+                c=StringUtil.lowercases[c];
 
             while (t!=0)
             {
                 int row = ROW_SIZE*t;
                 char n=_tree[row];
                 int diff=n-c;
                 
                 if (diff==0)
                 {
                     node=t;
                     t=_tree[row+EQ];
                     
                     // if this node is a match, recurse to remember 
                     if (_key[node]!=null)
                     {
-                        V best=getBest(t,b,offset+i+1,len-i-1);
+                        V best=getBest(t,s,offset+i+1,len-i-1);
                         if (best!=null)
                             return best;
                         return (V)_value[node];
                     }
                     
                     break;
                 }
 
-                if (diff<0)
-                    t=_tree[row+LO];
-                else
-                    t=_tree[row+HI];
+                t=_tree[row+((diff<0)?LO:HI)];
             }
         }
         return null;
     }",NotBuggy,"404511 Replaced all StringMap usage with Tries
",Buggy,"The commit message indicates a replacement of StringMap usage with Tries. The diff shows a change in the `getBest` method signature from accepting a byte array (`byte[] b`) to a String (`String s`). Additionally, the code now retrieves characters directly from the String using `s.charAt(offset+i)` instead of accessing a byte array. It also includes case-insensitive handling using `StringUtil.lowercases` if the `isCaseInsensitive()` flag is set. The logic for traversing the Trie structure remains similar, but it's adapted to work with characters from the String. The change from byte array to String suggests a potential fix related to character encoding or handling, or an improvement in efficiency. The addition of case-insensitive handling also indicates a potential bug fix or feature enhancement.

The change from byte[] to String and the addition of case-insensitive handling suggests a bug fix or feature enhancement related to character encoding or handling.

The answer is **Buggy**."
jetty,4526.json,720263151596d37014013c1a0033a8dc0fab9d5e,"@@ -1,39 +1,4 @@
     public static String unquoteOnly(String s)
     {
-        if (s==null)
-            return null;
-        if (s.length()<2)
-            return s;
-
-        char first=s.charAt(0);
-        char last=s.charAt(s.length()-1);
-        if (first!=last || (first!='""' && first!='\''))
-            return s;
-
-        StringBuilder b = new StringBuilder(s.length() - 2);
-        boolean escape=false;
-        for (int i=1;i<s.length()-1;i++)
-        {
-            char c = s.charAt(i);
-
-            if (escape)
-            {
-                escape=false;
-                if (!isValidEscaping(c))
-                {
-                    b.append('\\');
-                }
-                b.append(c);
-            }
-            else if (c=='\\')
-            {
-                escape=true;
-            }
-            else
-            {
-                b.append(c);
-            }
-        }
-
-        return b.toString(); 
+        return unquoteOnly(s, false);
     }",NotBuggy,"396886 MultiPartFilter strips bad escaping on filename=""...""
",Buggy,"The commit message indicates a fix related to escaping in filenames within the MultiPartFilter. The diff removes the original `unquoteOnly` method and replaces it with a call to `unquoteOnly(s, false)`. This suggests that the original method had incorrect or insufficient handling of escaping, and the new implementation (presumably within the `unquoteOnly(s, boolean)` method, which is not shown) corrects this. The change directly addresses the issue described in the commit message, suggesting a bug fix. The answer is **Buggy**."
jclouds,27006.json,1fc9b0e2597ee0e11d8ae09db478c2dbf516912b,"@@ -1,6 +1,7 @@
       public UriBuilder query(Multimap<String, ?> parameters) {
-         checkNotNull(parameters, ""parameters"");
+         Multimap<String, QueryValue> queryValueMultimap = Multimaps.transformValues(
+               checkNotNull(parameters, ""parameters""), QUERY_VALUE_TRANSFORMER);
          query.clear();
-         query.putAll(parameters);
+         query.putAll(queryValueMultimap);
          return this;
       }",NotBuggy,"Move QueryParam encoding to a separate class.

The patch implements a QueryValue class, which encodes the underlying
value based on whether the ""encoded"" flag is set. This class is used
by the RestAnnotationProcessor to propagate the @Encoded value set on
any parameters.

Since the encoding is now handled by the QueryValue instances, we
should no longer call encodeQueryLine() in the URI builder and instead
call buildQueryLine(). The caveat is that we need to make sure all of
the parameters that may need to be encoded are converted to QueryValue
objects. This is done by converting Object instances to QueryValue by
an instance of the TransformObjectToQueryValue when adding any query
parameters to the URI.
",NotBuggy,"The commit message indicates a refactoring of query parameter encoding, moving it to a separate class (QueryValue). The diff shows that the `query` method in `UriBuilder` is modified to use `Multimaps.transformValues` with `QUERY_VALUE_TRANSFORMER` to convert the input parameters to `QueryValue` objects before adding them to the `query` multimap. This change aligns with the commit message's description of converting Object instances to QueryValue. There's no explicit mention or indication of a bug fix in the commit message or the diff. The change appears to be a structural improvement for better handling of encoded query parameters. The answer is **NotBuggy**."
jclouds,18683.json,359b6b88d04d232f6f01d3f42429062e6c795b83,"@@ -1,10 +1,10 @@
    public String apply(byte[] from) {
       return Joiner.on(':').join(transform(partition(asList(from), 1), new Function<List<Byte>, String>() {
 
          @Override
          public String apply(List<Byte> from) {
-            return CryptoStreams.hex(toArray(from));
+            return base16().lowerCase().encode(toArray(from));
          }
 
       }));
    }",NotBuggy,"Move to Guava 14 BaseEncoding and Hashing
",NotBuggy,"The commit message indicates a move to Guava 14's BaseEncoding and Hashing. The diff replaces `CryptoStreams.hex(toArray(from))` with `base16().lowerCase().encode(toArray(from))`. This change suggests an update to use Guava's BaseEncoding for hexadecimal encoding, which aligns with the commit message. This is a library update, not a bug fix. The answer is **NotBuggy**."
jclouds,16909.json,308911162109cd540f9246f8fcf6f6f7478bbd8d,"@@ -1,4 +1,4 @@
    public ParseObjectFromHeadersAndHttpContent setContext(HttpRequest request) {
       this.uri = request.getEndpoint();
-      return setPath(GeneratedHttpRequest.class.cast(request).getArgs().get(0).toString());
+      return setPath(GeneratedHttpRequest.class.cast(request).getInvocation().getArgs().get(0).toString());
    }",NotBuggy,"refactored internal code and tests to use FunctionalReflection
",NotBuggy,"The commit message indicates a refactoring using FunctionalReflection. The code change replaces `getArgs()` with `getInvocation().getArgs()`. This suggests a change in the internal structure or API of `GeneratedHttpRequest`, likely to align with the new FunctionalReflection approach. While refactoring can sometimes expose or resolve subtle issues, the primary intent here is code restructuring rather than directly fixing a known bug. The change seems to be about adapting to a new way of accessing arguments, not correcting a faulty calculation or logic. The answer is **NotBuggy**.
"
jclouds,10478.json,3ac6f475e7d3baa61c3547a4f9693f2e130ea09d,"@@ -1,4 +1,4 @@
-      public AdminAccess.Builder adminFullName(String adminFullName) {
+      public Builder adminFullName(String adminFullName) {
          this.adminFullName = adminFullName;
          return this;
       }",NotBuggy,"unwound dependencies relating to ssh keys and crypt
",NotBuggy,"The commit message indicates a change in dependencies, specifically related to SSH keys and cryptography. The code diff shows a change in the return type of the `adminFullName` method in the `AdminAccess.Builder` class (or a class named `Builder` within `AdminAccess`). The return type is changed from `AdminAccess.Builder` to `Builder`. This suggests that the `Builder` class might have been moved or refactored, potentially as part of the dependency unwinding mentioned in the commit message. This change doesn't directly indicate a bug fix, but rather a restructuring of the code related to dependencies. The answer is **NotBuggy**.
"
jclouds,26927.json,a625127fd2cf05c5c33c03d1649be558f76cbc2d,"@@ -1,10 +1,3 @@
                         public void close() throws IOException {
-                           try {
-                              method.invoke(injectee);
-                           } catch (InvocationTargetException ie) {
-                              Throwable e = ie.getTargetException();
-                              throw new IOException(e.getMessage());
-                           } catch (IllegalAccessException e) {
-                              throw new IOException(e.getMessage());
-                           }
+                           invokeOnInjectee(method, injectee);
                         }",NotBuggy,"added ability to look up constructors as Invokables
",NotBuggy,"The commit message indicates the addition of a feature to look up constructors as Invokables. The diff shows a change in the `close()` method where the original code used a try-catch block to handle `InvocationTargetException` and `IllegalAccessException` during method invocation. The modified code replaces the try-catch block with a call to `invokeOnInjectee(method, injectee)`. This suggests that the exception handling logic has been moved into the `invokeOnInjectee` method. While this change might be related to the new feature, it doesn't explicitly indicate a bug fix in the traditional sense. It's more of a refactoring or enhancement to handle exceptions in a centralized manner. The answer is **NotBuggy**."
jclouds,26813.json,d5f7f8b07e6a2be7631bd82d04fb1d21406307ec,"@@ -1,3 +1,3 @@
-      public int hashCode() {
-         return Objects.hashCode(value, someOtherValue);
-      }

+         public int hashCode() {
+            return delegate.hashCode();
+         }",NotBuggy,"refactored json internals to use Reflections2
",NotBuggy,"The commit message indicates a refactoring to use Reflections2. The diff shows a change in the `hashCode()` method implementation. Instead of using `Objects.hashCode(value, someOtherValue)`, it now uses `delegate.hashCode()`. This change suggests a shift in how the hash code is generated, likely as part of the refactoring effort. There is no indication of a bug fix in this change. The answer is **NotBuggy**."
jclouds,14460.json,3af4d3a5bb345253c58d8ff8fbdbaff0307653f0,"@@ -1,7 +1,7 @@
-      public static Protocol fromValue(String value) {
+      public static State fromValue(String value) {
          try {
             return valueOf(value.toUpperCase());
          } catch(IllegalArgumentException e) {
             return UNKNOWN;
          }
       }",NotBuggy,"Use enum for *Rule.state
",NotBuggy,"The commit message indicates a change in the representation of `*Rule.state` to an enum. The provided diff snippet shows a method `fromValue` that converts a string to an enum value. The change involves renaming `Protocol` to `State`, which suggests that the `state` field was previously represented as a string and is now being represented by an enum called `State`. This change improves type safety and code clarity, and reduces the likelihood of errors caused by invalid string values. While this is a good refactoring, it doesn't directly fix a bug. The answer is **NotBuggy**.
"
jclouds,13261.json,d9c65046d0a46839b4d05570817c43a7df5b5bd6,"@@ -1,3 +1,3 @@
    public String getAccount() {
-      return account;
+      return this.account;
    }",NotBuggy,"cloudstack: adjusting beans to use newer de/serialization annotations
",NotBuggy,"The commit message indicates an adjustment of beans to use newer de/serialization annotations. The diff shows a change from `return account;` to `return this.account;`. This change is likely related to how the `account` field is accessed within the class, possibly to ensure correct behavior during serialization or deserialization. It's a minor adjustment that doesn't inherently indicate a bug fix but rather a refinement in how the bean's properties are accessed. The answer is **NotBuggy**.
"
jclouds,6344.json,40d9f8bfe75f48f2a317aecdd5439fd6a9840d62,"@@ -1,5 +1,5 @@
-      public ListenableFuture<Boolean> create(Throwable t) {
+      public Boolean createOrPropagate(Throwable t) throws Exception {
          if (returnValueOnCodeOrNull(t, false, equalTo(400)) != null)
-            return immediateFuture(false);
+            return false;
          throw propagate(t);
       }",NotBuggy,"changes that facilitate synchronous fallbacks and http invocation
",Buggy,"The commit message mentions changes to facilitate synchronous fallbacks and HTTP invocation. The code diff modifies a method `create` to `createOrPropagate`. The return type changes from `ListenableFuture<Boolean>` to `Boolean`, and it now throws an `Exception`. The original method returned an immediate future with a boolean value or propagated the exception. The modified method directly returns a boolean value or throws the exception. This change facilitates synchronous fallbacks by directly returning a boolean value instead of wrapping it in a `ListenableFuture`. The exception handling is also modified to propagate exceptions directly. This indicates a functional change related to error handling and control flow, which could be in response to a bug or unexpected behavior. The answer is **Buggy**."
jclouds,13530.json,bff49f8311cce1e45fe57e7e1fb43419b3af00ba,"@@ -1,3 +1,3 @@
    public int hashCode() {
-      return Objects.hashCode(id, allocationState, averageLoad, capabilities, clusterId, clusterName, clusterType, cpuAllocated, cpuNumber, cpuSpeed, cpuUsed, cpuWithOverProvisioning, created, disconnected, diskSizeAllocated, diskSizeTotal, events, hasEnoughCapacity, hostTags, hypervisor, ipAddress, localStorageActive, jobId, jobStatus, lastPinged, managementServerId, memoryAllocated, memoryTotal, memoryUsed, name, networkKbsRead, networkKbsWrite, osCategoryId, osCategoryName, podId, podName, removed, state, type, version, zoneId, zoneName);
+      return Objects.hashCode(id, allocationState, averageLoad, capabilities, clusterId, clusterName, clusterType, cpuAllocated, cpuNumber, cpuSpeed, cpuUsed, cpuWithOverProvisioning, created, disconnected, diskSizeAllocated, diskSizeTotal, events, hasEnoughCapacity, tags, hypervisor, ipAddress, localStorageActive, jobId, jobStatus, lastPinged, managementServerId, memoryAllocated, memoryTotal, memoryUsed, name, networkKbsRead, networkKbsWrite, osCategoryId, osCategoryName, podId, podName, removed, state, type, version, zoneId, zoneName);
    }",NotBuggy,"compatible tags across 2.0 and 4.0
",Buggy,"The commit message ""compatible tags across 2.0 and 4.0"" suggests a change related to tag compatibility between different versions. The diff replaces `hostTags` with `tags` in the `hashCode` method. This change likely aims to use a unified `tags` field instead of the version-specific `hostTags`, which aligns with the commit message's intent to improve compatibility. This could be due to a bug where tags were not being handled correctly across versions. The answer is **Buggy**."
jclouds,5234.json,b1877bbee57afc4886627c4ad69cda50aca622ab,"@@ -1,3 +1,3 @@
-      public Builder archiveSizes(Integer... archiveSizes) {
-         return archiveSizes(Arrays.asList(archiveSizes));
+      public T archiveSizes(Integer... in) {
+         return archiveSizes(ImmutableList.copyOf(in));
       }",NotBuggy,"glesys: bean cleaning, using ConstructorProperties for deserialization (note no serialization annotations as we don't currently serialize any of these beans)
",NotBuggy,"The commit message indicates bean cleaning and the use of ConstructorProperties for deserialization. The diff shows a change in the `archiveSizes` method, specifically how it handles the input `archiveSizes`. It changes from `Arrays.asList(archiveSizes)` to `ImmutableList.copyOf(in)`. This change suggests a move towards using immutable collections, which is a common practice for improving code reliability and preventing unintended modifications. While this change improves code quality and maintainability, it doesn't directly address a bug. It's more of a refactoring or enhancement. The answer is **NotBuggy**."
jclouds,5905.json,602002660abb287d684e6a68d0ea5f908279e13a,"@@ -1,4 +1,8 @@
    public DomainOptions responsiblePerson(String responsiblePerson) {
+      responsiblePerson = responsiblePerson.replaceAll(""@"", ""."");
+      if (!responsiblePerson.endsWith(""."")) {
+         responsiblePerson = responsiblePerson + ""."";
+      }
       formParameters.put(""resp_person"", responsiblePerson);
       return this;
    }",NotBuggy,"Improving javadocs for Archive, Domain, Email and Server GleSYS clients
",Buggy,"The commit message indicates improvements to Javadocs, but the diff shows code modifications within the `responsiblePerson` method. Specifically, the code replaces ""@"" with ""."" and ensures the `responsiblePerson` string ends with a ""."". This suggests a functional change related to email or domain handling, rather than just documentation updates. These changes are likely intended to fix a bug or enforce a specific format for the responsible person string. The answer is **Buggy**.
"
jclouds,27602.json,edde721937115beeee01ea4076d60b4ecf381807,"@@ -1,6 +1,6 @@
-   protected Supplier<Map<String, Supplier<URI>>> regionIdToURISupplier(
+   protected final Supplier<Map<String, Supplier<URI>>> regionIdToURISupplier(
             AtomicReference<AuthorizationException> authException, @Named(PROPERTY_SESSION_INTERVAL) long seconds,
             RegionIdToURISupplier uncached) {
       return MemoizedRetryOnTimeOutButNotOnAuthorizationExceptionSupplier.create(authException, uncached, seconds,
                TimeUnit.SECONDS);
    }",NotBuggy,"JCLOUDS-962: Make all @Provider methods final
",NotBuggy,"The commit message indicates a change to make all `@Provider` methods final. The diff shows that the method `regionIdToURISupplier` is being made final. This change prevents subclasses from overriding this method, which could be important for maintaining the intended behavior of the provider. While this change might be related to ensuring the correct behavior of the provider, it doesn't directly indicate a bug fix in the sense of correcting an error. It's more of a preventative measure or a design decision. The answer is **NotBuggy**.
"
jclouds,11802.json,5ae1ba44d243824d6a33c9547876b71fc4a272ec,"@@ -1,3 +1,3 @@
-   public OperatingSystem build() {
-      return new OperatingSystem(family, name, version, arch, description, is64Bit);
-   }

+      public OperatingSystem build() {
+         return new OperatingSystem(family, name, version, arch, description, is64Bit);
+      }",NotBuggy,"added default operating system types to savvis
",NotBuggy,"The diff shows a minor change in indentation. The functionality of the code remains the same. The commit message indicates adding default operating system types, but the diff doesn't reflect that. The change is not related to a bug fix. The answer is **NotBuggy**."
jclouds,27976.json,04fe89b75a90ae197fca1a5128c52986e5beb100,"@@ -1,5 +1,4 @@
    public void exit() {
       checkState(caller.get() != null, ""No scoping block in progress"");
-      callerEnclosingType.remove();
       caller.remove();
    }",NotBuggy,"update to guava 14.0-rc2
",Buggy,"The commit message indicates an update to Guava version 14.0-rc2. The code diff removes `callerEnclosingType.remove()`. Without more context, it's difficult to definitively say if this is a bug fix. It could be a cleanup, a removal of redundant code, or a correction of an error related to thread-local management. However, the removal of a `remove()` call on a thread-local variable suggests a potential memory leak or incorrect state management issue that was resolved by this change. Therefore, it's plausible that this change addresses a bug. The answer is **Buggy**.
"
okhttp,755.json,c9a89876de476983f273edbf108c365127c18c5e,"@@ -1,3 +1,3 @@
-  public String name(int index) {
-    return percentDecode(encodedName(index), true);
+  public String value(int index) {
+    return percentDecode(encodedValue(index), true);
   }",NotBuggy,"Rename com.squareup.okhttp to okhttp3

Maven group changes will come in follow up.
",NotBuggy,"The commit message indicates a renaming operation, specifically changing the package name from `com.squareup.okhttp` to `okhttp3`. The provided diff, however, shows a change in a method name from `name` to `value`. This change doesn't seem directly related to the package renaming described in the commit message. It's likely a separate refactoring or bug fix that was included in the same commit, but the provided information doesn't allow me to confirm that it's a bug fix. Therefore, based on the information provided, it's not possible to determine if the change indicates a bug fix.

**NotBuggy**
"
okhttp,1728.json,d3c57bd4f38d5d3944da88266dd9b474a84d802b,"@@ -1,3 +1,3 @@
   @Override public String toString() {
-    throw new UnsupportedOperationException();
+    return url;
   }",NotBuggy,"HttpURL parsing.

This includes everything but character encoding. The implementation
is built around one oversized method that runs top to bottom; I found
this was easier to follow than the switch statement recommended by the
URL standard.
",NotBuggy,"The commit message indicates the implementation of HTTP URL parsing. The diff shows a change in the `toString()` method, replacing an `UnsupportedOperationException` with a return of the `url` field. This suggests that the `toString()` method was previously unimplemented and is now being implemented as part of the HTTP URL parsing functionality. This is not a bug fix, but rather a new feature implementation. The answer is **NotBuggy**.
"
okhttp,131.json,1a1cc18f26c381f405d62f5b9262b3113736d014,"@@ -1,3 +1,5 @@
   public static void main(String[] args) throws IOException {
-    new GetExample().run();
+    GetExample example = new GetExample();
+    String response = example.run(""https://raw.github.com/square/okhttp/master/README.md"");
+    System.out.println(response);
   }",NotBuggy,"Update samples and marketing site.
",NotBuggy,"The commit message indicates updates to samples and the marketing site. The diff modifies the `main` method in `GetExample` to fetch and print the content of a README file from GitHub. This change seems to be part of updating or demonstrating the usage of the `GetExample` class, which aligns with the commit message. There's no indication of a bug fix in this change. The answer is **NotBuggy**."
okhttp,2091.json,22a03b59fac9fa58cb59a6f240c8280a2a1eb801,"@@ -1,3 +1,3 @@
-  public void trackConditionalCacheHit() {
-    // This method is treated as optional.
+  @Override public void trackConditionalCacheHit() {
+    // This method is optional.
   }",NotBuggy,"Drop ResponseSource.
",NotBuggy,"The commit message ""Drop ResponseSource"" suggests the removal of a class or functionality related to `ResponseSource`. However, the provided diff only shows a change in the `trackConditionalCacheHit` method, specifically adding an `@Override` annotation. This change doesn't seem to be directly related to dropping a `ResponseSource`. It's possible that the `ResponseSource` was dropped elsewhere, and this change is a consequence of that, or the commit message is misleading. The addition of `@Override` suggests the method is implementing or overriding a method from a superclass or interface, which is a standard practice and doesn't inherently indicate a bug fix. Therefore, based on the provided diff, it does not appear to be a bug fix. The answer is **NotBuggy**."
okhttp,1794.json,7028195274f67d2c72f4c5ee472d0dcd6a97c1e4,"@@ -1,12 +1,13 @@
   public boolean isRedirect() {
     switch (code) {
+      case HTTP_PERM_REDIRECT:
       case HTTP_TEMP_REDIRECT:
       case HTTP_MULT_CHOICE:
       case HTTP_MOVED_PERM:
       case HTTP_MOVED_TEMP:
       case HTTP_SEE_OTHER:
         return true;
       default:
         return false;
     }
   }",NotBuggy,"Handle response code 308 - Permanent Redirect
",Buggy,"The commit message indicates handling a new response code, 308 - Permanent Redirect. The code adds `case HTTP_PERM_REDIRECT:` to the `isRedirect()` method's switch statement. This ensures that a 308 response code is correctly identified as a redirect, which was likely not the case before. This is a bug fix because the previous code did not correctly handle the 308 status code. The answer is **Buggy**.
"
okhttp,1402.json,e74e3f3bf744ef7f4d8ee724a7cf2347e486cfab,"@@ -1,11 +1,22 @@
-  public static boolean skipAll(Source in, int timeoutMillis) throws IOException {
-    // TODO: Implement deadlines everywhere so they can do this work.
-    long startNanos = System.nanoTime();
-    Buffer skipBuffer = new Buffer();
-    while (NANOSECONDS.toMillis(System.nanoTime() - startNanos) < timeoutMillis) {
-      long read = in.read(skipBuffer, 2048);
-      if (read == -1) return true; // Successfully exhausted the stream.
-      skipBuffer.clear();
+  public static boolean skipAll(Source source, int duration, TimeUnit timeUnit) throws IOException {
+    long now = System.nanoTime();
+    long originalDuration = source.timeout().hasDeadline()
+        ? source.timeout().deadlineNanoTime() - now
+        : Long.MAX_VALUE;
+    source.timeout().deadlineNanoTime(now + Math.min(originalDuration, timeUnit.toNanos(duration)));
+    try {
+      Buffer skipBuffer = new Buffer();
+      while (source.read(skipBuffer, 2048) != -1) {
+        skipBuffer.clear();
+      }
+      return true; // Success! The source has been exhausted.
+    } catch (InterruptedIOException e) {
+      return false; // We ran out of time before exhausting the source.
+    } finally {
+      if (originalDuration == Long.MAX_VALUE) {
+        source.timeout().clearDeadline();
+      } else {
+        source.timeout().deadlineNanoTime(now + originalDuration);
+      }
     }
-    return false; // Ran out of time.
   }",NotBuggy,"Move cache writing out of the transport.

This is necessary to unblock network interceptors, where the interceptor
may elect to rewrite the response body. If we've already cached the
original response body, we're too late.
",Buggy,"The commit message indicates a change to accommodate network interceptors that rewrite response bodies, implying a potential issue with caching the original response body too early. The code diff modifies the `skipAll` method to use `Source.timeout()` for managing timeouts, introduces a try-catch block to handle `InterruptedIOException`, and ensures the original deadline is restored in a finally block. This suggests a fix for a timing-related issue or a problem with handling interruptions during the skipping process. The changes improve error handling and resource management, which are indicative of addressing a bug or potential issue. The answer is **Buggy**.
"
okhttp,649.json,8e201297fe9e6b5c908848c3cef633966e49c0d9,"@@ -1,3 +1,3 @@
-  public List<String> getHeaders() {
+  public Headers getHeaders() {
     return headers;
   }",NotBuggy,"Update MockWebServer to use Headers.
",NotBuggy,"The commit message indicates an update to use `Headers` instead of a list of strings for headers in MockWebServer. The diff confirms this by changing the return type of the `getHeaders()` method from `List<String>` to `Headers`. This change suggests a modification to the API or data structure used for handling headers, but it doesn't inherently indicate a bug fix. It's more likely a feature enhancement or a change to align with a new API. The answer is **NotBuggy**.
"
okhttp,303.json,40dddb9dcf8ba654e16f6497020043b38b6d22a1,"@@ -1,7 +1,6 @@
   public void open(String accessToken) throws IOException {
-    if (webSocketCall != null) throw new IllegalStateException();
+    if (webSocket != null) throw new IllegalStateException();
 
     RtmStartResponse rtmStartResponse = slackApi.rtmStart(accessToken);
-    webSocketCall = slackApi.rtm(rtmStartResponse.url);
-    webSocketCall.enqueue(this);
+    webSocket = slackApi.rtm(rtmStartResponse.url, this);
   }",NotBuggy,"Migrate MockWebServer to the async web sockets API.
",NotBuggy,"The commit message indicates a migration to an asynchronous web sockets API. The code diff shows a change from `webSocketCall.enqueue(this)` to `webSocket = slackApi.rtm(rtmStartResponse.url, this)`. Also, the check `if (webSocketCall != null)` is changed to `if (webSocket != null)`. This suggests that the original code used a synchronous approach (`enqueue`) and the updated code uses an asynchronous approach, likely returning a `WebSocket` object directly. The change in the null check also reflects this. While this is a migration, it doesn't inherently indicate a bug fix. It's a change in implementation. The answer is **NotBuggy**."
okhttp,852.json,7131fb250c34882c32f084c5b01a28ad0b6de25c,"@@ -1,3 +1,3 @@
       @Override public Call newWebSocketCall(OkHttpClient client, Request originalRequest) {
-        return new RealCall(client, originalRequest, true);
+        return RealCall.newRealCall(client, originalRequest, true);
       }",NotBuggy,"Make EventListener public and begin implementing Dns events.
",NotBuggy,"The commit message indicates changes related to making `EventListener` public and starting the implementation of DNS events. The provided diff, however, shows a change in how `RealCall` is instantiated within the `newWebSocketCall` method. Specifically, it replaces `new RealCall(...)` with `RealCall.newRealCall(...)`. This suggests a change in the instantiation pattern of `RealCall`, possibly to use a factory method. This change doesn't seem directly related to the commit message's primary focus on `EventListener` and DNS events. It is likely a refactoring or a minor adjustment made in conjunction with the other changes. The change itself doesn't obviously indicate a bug fix.

**NotBuggy**
"
okhttp,1412.json,e786a376426b938738d3333f52891c68339ae187,"@@ -1,6 +1,6 @@
-  public static <T> int indexOf(T[] array, T value) {
+  public static int indexOf(Comparator<String> comparator, String[] array, String value) {
     for (int i = 0, size = array.length; i < size; i++) {
-      if (equal(array[i], value)) return i;
+      if (comparator.compare(array[i], value) == 0) return i;
     }
     return -1;
   }",NotBuggy,"Handle either TLS_ or SSL_ prefixes for cipher suite names.

Closes: https://github.com/square/okhttp/issues/3173
",Buggy,"The commit message indicates a change to handle TLS_ or SSL_ prefixes for cipher suite names, suggesting a bug fix or enhancement related to cipher suite handling. The diff introduces a comparator to the indexOf method, which implies a change in how elements are compared within the array. This could be related to handling different prefixes for cipher suite names. The change from `equal` to `comparator.compare` suggests a more flexible and potentially bug-fixing approach to comparing cipher suite names. The answer is **Buggy**.
"
okhttp,185.json,c9a89876de476983f273edbf108c365127c18c5e,"@@ -1,3 +1,3 @@
   public static void main(String... args) throws Exception {
-    new PerCallSettings().run();
+    new CertificatePinning().run();
   }",NotBuggy,"Rename com.squareup.okhttp to okhttp3

Maven group changes will come in follow up.
",NotBuggy,"The commit message indicates a renaming operation from `com.squareup.okhttp` to `okhttp3`. The diff shows a change in the main method, replacing `PerCallSettings` with `CertificatePinning`. This change doesn't seem related to bug fixing but rather to reflect the renaming or a change in the example being run. The change in the main method is likely due to the package rename. The answer is **NotBuggy**."
okhttp,1274.json,40dddb9dcf8ba654e16f6497020043b38b6d22a1,"@@ -1,25 +1,26 @@
   private void checkResponse(Response response) throws ProtocolException {
     if (response.code() != 101) {
       throw new ProtocolException(""Expected HTTP 101 response but was '""
           + response.code() + "" "" + response.message() + ""'"");
     }
 
     String headerConnection = response.header(""Connection"");
     if (!""Upgrade"".equalsIgnoreCase(headerConnection)) {
       throw new ProtocolException(""Expected 'Connection' header value 'Upgrade' but was '""
           + headerConnection + ""'"");
     }
 
     String headerUpgrade = response.header(""Upgrade"");
     if (!""websocket"".equalsIgnoreCase(headerUpgrade)) {
       throw new ProtocolException(
           ""Expected 'Upgrade' header value 'websocket' but was '"" + headerUpgrade + ""'"");
     }
 
     String headerAccept = response.header(""Sec-WebSocket-Accept"");
-    String acceptExpected = Util.shaBase64(key + WebSocketProtocol.ACCEPT_MAGIC);
+    String acceptExpected = ByteString.encodeUtf8(key + WebSocketProtocol.ACCEPT_MAGIC)
+        .sha1().base64();
     if (!acceptExpected.equals(headerAccept)) {
       throw new ProtocolException(""Expected 'Sec-WebSocket-Accept' header value '""
           + acceptExpected + ""' but was '"" + headerAccept + ""'"");
     }
   }",NotBuggy,"Migrate MockWebServer to the async web sockets API.
",Buggy,"The commit message indicates a migration to an asynchronous web sockets API. The code diff modifies the `checkResponse` method, specifically how the `Sec-WebSocket-Accept` header is validated. It changes from `Util.shaBase64` to `ByteString.encodeUtf8(key + WebSocketProtocol.ACCEPT_MAGIC).sha1().base64()`. This change suggests an update to use a different method for calculating the expected `Sec-WebSocket-Accept` value, likely due to changes in the underlying web sockets API or to improve security/correctness. The change in the calculation of the expected value suggests a potential bug fix related to web socket handshake validation.
The answer is **Buggy**."
okhttp,1474.json,64df3922164136ac4587df235222d564b016ce25,"@@ -1,11 +1,11 @@
   public void setSettings(Settings settings) throws IOException {
     synchronized (writer) {
       synchronized (this) {
         if (shutdown) {
           throw new ConnectionShutdownException();
         }
         okHttpSettings.merge(settings);
-        writer.settings(settings);
       }
+      writer.settings(settings);
     }
   }",NotBuggy,"Don't hold a lock on the connection when writing settings. (#3867)

This breaks our concurrency rules. Fortunately it's only academic; only
MockWebServer ever writes settings, and it only does so in very specialized
tests.",Buggy,"The commit message indicates a concurrency issue where a lock was held unnecessarily during settings writing. The diff moves `writer.settings(settings);` outside the inner `synchronized (this)` block, which aligns with the commit message's intention to avoid holding a lock on the connection while writing settings. This change addresses a potential concurrency bug, even if it's currently only academic. The answer is **Buggy**."
okhttp,262.json,3699d5c9fd0ad78fc52e3ea317951f9d485f656f,"@@ -1,33 +1,34 @@
   public void run() throws Exception {
     RequestBody requestBody = new RequestBody() {
       @Override public MediaType contentType() {
         return MEDIA_TYPE_MARKDOWN;
       }
 
       @Override public void writeTo(BufferedSink sink) throws IOException {
         sink.writeUtf8(""Numbers\n"");
         sink.writeUtf8(""-------\n"");
         for (int i = 2; i <= 997; i++) {
           sink.writeUtf8(String.format("" * %s = %s\n"", i, factor(i)));
         }
       }
 
       private String factor(int n) {
         for (int i = 2; i < n; i++) {
           int x = n / i;
           if (x * i == n) return factor(x) + ""  "" + i;
         }
         return Integer.toString(n);
       }
     };
 
     Request request = new Request.Builder()
         .url(""https://api.github.com/markdown/raw"")
         .post(requestBody)
         .build();
 
-    Response response = client.newCall(request).execute();
-    if (!response.isSuccessful()) throw new IOException(""Unexpected code "" + response);
+    try (Response response = client.newCall(request).execute()) {
+      if (!response.isSuccessful()) throw new IOException(""Unexpected code "" + response);
 
-    System.out.println(response.body().string());
+      System.out.println(response.body().string());
+    }
   }",NotBuggy,"Better documentation and examples on closing response bodies.

Also implement Closeable on Response to make it easier to do the right thing.
",Buggy,"The commit message indicates improved documentation and examples, along with implementing `Closeable` on `Response`. The diff shows the usage of try-with-resources statement with `Response` object. This ensures that the response body is closed properly after usage, preventing potential resource leaks. This addresses a potential issue where developers might forget to close the response body, leading to resource exhaustion. The answer is **Buggy**.
"
okhttp,1692.json,c07f55ac92d1e931260711edae987b51bee0be3d,"@@ -1,3 +1,3 @@
-  public String getScheme() {
+  public String scheme() {
     return scheme;
   }",NotBuggy,"Drop 'get' prefix on Challenge value type.
",NotBuggy,"The commit message indicates a change in naming convention, specifically dropping the ""get"" prefix from a method name. The diff confirms this, showing the renaming of the `getScheme()` method to `scheme()`. This change doesn't involve any bug fixes, error handling, or logical corrections. It's purely a refactoring change to improve code readability or adhere to a specific style guide. The answer is **NotBuggy**.
"
openmrs-core,6817.json,2f888dceb5bdb71589beaaddf47d66a20fda689d,"@@ -1,4 +1,3 @@
 	public void updateConceptIndex(Concept concept) throws APIException {
-		checkIfLocked();
-		dao.updateConceptWord(concept);
+		Context.updateSearchIndexForObject(concept);
 	}",NotBuggy,"TRUNK-2999: Investigate replacing concept word with full text search
",Buggy,"The commit message ""TRUNK-2999: Investigate replacing concept word with full text search"" indicates a change related to search functionality. The diff replaces `dao.updateConceptWord(concept)` with `Context.updateSearchIndexForObject(concept)`. This suggests a shift from updating a concept word index to using a full-text search index. The removal of `checkIfLocked()` might be related to the new indexing approach or a change in how locking is handled. This change is likely part of an effort to improve search capabilities, and the original approach might have had limitations or bugs that the full-text search aims to address. The change in indexing strategy can be considered a bug fix or improvement. The answer is **Buggy**.
"
openmrs-core,7534.json,fac6dd5b510edc40e5e8a1a96b2da9c6da01ac5c,"@@ -1,3 +1,3 @@
 	public static String drugToString(Drug drug) {
-		return drug.getDrugId() + ""^"" + drug.getName() + ""^"" + FormConstants.HL7_LOCAL_DRUG;
+		return drug.getDrugId() + ""^"" + drug.getName() + ""^"" + HL7Constants.HL7_LOCAL_DRUG;
 	}",NotBuggy,"move HL7-related constants from FormConstants to HL7Constants - #2338
Author: PKirwa

git-svn-id: http://svn.openmrs.org/openmrs/trunk@14004 5bac5841-c719-aa4e-b3fe-cce5062f897a
",NotBuggy,The commit message indicates a refactoring task: moving HL7-related constants from `FormConstants` to `HL7Constants`. The diff confirms this by showing that `FormConstants.HL7_LOCAL_DRUG` is replaced with `HL7Constants.HL7_LOCAL_DRUG`. This change does not fix a bug but rather improves code organization and maintainability. The answer is **NotBuggy**.
openmrs-core,7488.json,c1b7d42d30069758be06afdbe4083759622cfd66,"@@ -1,5 +1,5 @@
 	public void globalPropertyDeleted(String propertyName) {
 		// reset the value
-		defaultLocaleCache = null;
-		localesAllowedListCache = null;
+		setDefaultLocaleCache(null);
+		setLocalesAllowedListCache(null);
 	}",NotBuggy,"TRUNK-4232: SonarQube - Write to static field from instance method
",Buggy,"The commit message indicates a SonarQube issue where a static field is being written to from an instance method. The diff shows that the code was modified to use setter methods (setDefaultLocaleCache, setLocalesAllowedListCache) instead of directly assigning null to the static fields (defaultLocaleCache, localesAllowedListCache). This change likely addresses a design flaw or potential concurrency issue identified by SonarQube. The original code, where instance methods directly modified static fields, could lead to unexpected behavior or data corruption in a multi-threaded environment. The change to use setters allows for better control and encapsulation of the static fields, potentially including synchronization or other logic to ensure thread safety. This indicates a bug fix. The answer is **Buggy**."
openmrs-core,1419.json,fb1e831dc7929e004b486c06545a4cb4517914cb,"@@ -1,8 +1,8 @@
 	public static File getDevelopmentDirectory(String moduleId) {
-		String directory = System.getProperty(""uiFramework.development."" + moduleId);
+		String directory = System.getProperty(moduleId + "".development.directory"");
 		if (StringUtils.isNotBlank(directory)) {
 			return new File(directory);
 		}
 		
 		return null;
 	}",NotBuggy,"Renaming VM argument from uiFramework.development.moduleId to
moduleId.development.directory - TRUNK-4739",NotBuggy,"The commit message indicates a renaming of a VM argument. The code change reflects this renaming, where the `System.getProperty` call now uses the new property name (`moduleId + "".development.directory""`) instead of the old one (`""uiFramework.development."" + moduleId`). This change directly corresponds to the renaming described in the commit message. While renamings can sometimes be related to bug fixes (e.g., renaming a misleading variable), in this case, it seems to be a straightforward configuration change. Therefore, it's unlikely to be a bug fix. The answer is **NotBuggy**.
"
openmrs-core,4987.json,3a3f703f9373ebb6d76917406d7fb6f006c1434e,"@@ -1,4 +1,5 @@
 	public ConceptMapType getConceptMapTypeByName(String name) throws DAOException {
-		return (ConceptMapType) sessionFactory.getCurrentSession().createQuery(
-		    ""from ConceptMapType cmt where cmt.name = :name"").setString(""name"", name).uniqueResult();
+		Criteria criteria = sessionFactory.getCurrentSession().createCriteria(ConceptMapType.class);
+		criteria.add(Restrictions.ilike(""name"", name, MatchMode.EXACT));
+		return (ConceptMapType) criteria.uniqueResult();
 	}",NotBuggy,"Fetching concept reference terms and map types by name should be case insensitive - 3073

git-svn-id: http://svn.openmrs.org/openmrs/trunk@26058 5bac5841-c719-aa4e-b3fe-cce5062f897a
",Buggy,"The commit message indicates that fetching concept reference terms and map types by name should be case-insensitive. The code diff modifies the `getConceptMapTypeByName` method to use a case-insensitive query using `Restrictions.ilike` instead of a case-sensitive equality check. This change directly addresses the issue described in the commit message, indicating a bug fix or enhancement related to case-insensitive searching. The answer is **Buggy**."
openmrs-core,3457.json,225d8cd669d2e31ba80b0f39bd381b1ebc8c11d7,"@@ -1,13 +1,9 @@
 	public void validate(Object obj, Errors errors) {
-		if (!ValidateUtil.isValidationOn()) {
-			return;
-		}
-		
 		PatientIdentifier pi = (PatientIdentifier) obj;
 		try {
 			validateIdentifier(pi);
 		}
 		catch (Exception e) {
 			errors.reject(e.getMessage());
 		}
 	}",NotBuggy,"Revert ""Provide configuration setting that can allow turning off validation in""

This reverts commit 0d0ace1911fd780b7ae8dfb2bec3fc4cb4e279bc.
",NotBuggy,"The commit message indicates a reversion of a previous commit related to disabling validation. The diff removes a conditional statement that checked if validation was enabled (`ValidateUtil.isValidationOn()`). This means the validation logic within the `validate` method will now always be executed, regardless of the configuration setting introduced in the reverted commit. Since the commit reverts a previous change, it's not directly fixing a bug but undoing a feature or configuration option. The answer is **NotBuggy**."
openmrs-core,7636.json,2fefe9576c8a5fb60fa92e1ad36f9d6aeafe3333,"@@ -1,42 +1,52 @@
 	private void migrateFrequenciesToCodedValue(JdbcConnection connection, Set<String> uniqueFrequencies)
 	        throws CustomChangeException, SQLException, DatabaseException {
 		PreparedStatement updateDrugOrderStatement = null;
 		Boolean autoCommit = null;
 		try {
 			autoCommit = connection.getAutoCommit();
 			connection.setAutoCommit(false);
 			updateDrugOrderStatement = connection
 			        .prepareStatement(""update drug_order set frequency = ? where frequency_text = ?"");
+			
+			updateDrugOrderStatement.setNull(1, Types.INTEGER);
+			updateDrugOrderStatement.setNull(2, Types.VARCHAR);
+			updateDrugOrderStatement.executeUpdate();
+			updateDrugOrderStatement.clearParameters();
+			
 			for (String frequency : uniqueFrequencies) {
-				Integer conceptIdForFrequency = UpgradeUtil.getConceptIdForUnits(frequency);
-				if (conceptIdForFrequency == null) {
-					throw new CustomChangeException(""No concept mapping found for frequency: "" + frequency);
+				if (StringUtils.isBlank(frequency)) {
+					updateDrugOrderStatement.setNull(1, Types.INTEGER);
+				} else {
+					Integer conceptIdForFrequency = UpgradeUtil.getConceptIdForUnits(frequency);
+					if (conceptIdForFrequency == null) {
+						throw new CustomChangeException(""No concept mapping found for frequency: "" + frequency);
+					}
+					Integer orderFrequencyId = UpgradeUtil.getOrderFrequencyIdForConceptId(connection
+					        .getUnderlyingConnection(), conceptIdForFrequency);
+					if (orderFrequencyId == null) {
+						throw new CustomChangeException(""No order frequency found for concept "" + conceptIdForFrequency);
+					}
+					
+					updateDrugOrderStatement.setInt(1, orderFrequencyId);
 				}
-				Integer orderFrequencyId = UpgradeUtil.getOrderFrequencyIdForConceptId(connection.getUnderlyingConnection(),
-				    conceptIdForFrequency);
-				if (orderFrequencyId == null) {
-					throw new CustomChangeException(""No order frequency found for concept "" + conceptIdForFrequency);
-				}
-				
-				updateDrugOrderStatement.setInt(1, orderFrequencyId);
 				updateDrugOrderStatement.setString(2, frequency);
 				updateDrugOrderStatement.executeUpdate();
 				updateDrugOrderStatement.clearParameters();
 			}
 			connection.commit();
 		}
 		catch (DatabaseException e) {
 			handleError(connection, e);
 		}
 		catch (SQLException e) {
 			handleError(connection, e);
 		}
 		finally {
 			if (autoCommit != null) {
 				connection.setAutoCommit(autoCommit);
 			}
 			if (updateDrugOrderStatement != null) {
 				updateDrugOrderStatement.close();
 			}
 		}
 	}",NotBuggy,"TRUNK-4403 Blank drug order frequencies and dosing units should be converted to null during upgrade
",Buggy,"The commit message indicates that blank drug order frequencies should be converted to null during the upgrade process. The code changes include a check for blank frequencies using `StringUtils.isBlank(frequency)`. If a frequency is blank, the code sets the corresponding `frequency` column in the `drug_order` table to `NULL`. This aligns with the commit message, indicating a bug fix where blank values were not being handled correctly during the upgrade. The initial setting of null values seems incorrect, but the loop handles the intended logic. The answer is **Buggy**."
openmrs-core,339.json,b54304f90aaa12e7ffc6b51b808d95e500b3fce4,"@@ -1,5 +1,6 @@
 	public Set<PersonAddress> getAddresses() {
-		if (addresses == null)
+		if (addresses == null) {
 			addresses = new TreeSet<PersonAddress>();
+		}
 		return this.addresses;
 	}",NotBuggy,"TRUNK-4246: if/else/for/while/do statements should always use curly braces
",NotBuggy,"The commit message indicates a style fix: adding curly braces to single-line if statements. The diff shows exactly that - curly braces added to an if statement. This doesn't fix a bug, but enforces a coding style. The answer is **NotBuggy**.
"
openmrs-core,6956.json,823d9d0ddb38979543da23aa9e4ed2cb54bca064,"@@ -1,15 +1,3 @@
 	public Field saveField(Field field) throws APIException {
-		Date now = new Date();
-		
-		if (field.getCreator() == null)
-			field.setCreator(Context.getAuthenticatedUser());
-		if (field.getDateCreated() == null)
-			field.setDateCreated(now);
-		
-		if (field.getFieldId() != null) {
-			field.setChangedBy(Context.getAuthenticatedUser());
-			field.setDateChanged(now);
-		}
-		
 		return dao.saveField(field);
 	}",NotBuggy,"Merged uuids branch to trunk

git-svn-id: http://svn.openmrs.org/openmrs/trunk@7606 5bac5841-c719-aa4e-b3fe-cce5062f897a
",NotBuggy,"The commit message indicates a merge of the ""uuids"" branch to the trunk. The diff removes code related to setting creator, date created, changed by, and date changed for a Field object. This suggests that the logic for handling these fields has been moved or is now handled elsewhere, possibly within the ""uuids"" branch. Since the commit message is a merge and the diff doesn't inherently indicate a bug fix, the answer is **NotBuggy**."
openmrs-core,3668.json,2da55993f40c43555f00ca4660242241bf285bc6,"@@ -1,3 +1,3 @@
-	public boolean isDirty() {
+	public boolean getDirty() {
 		return dirty;
 	}",NotBuggy,"TRUNK-2205: deprecated isXyz() getter methods having correspoding getXyz()

changed deprecated methods to call expected methods
",NotBuggy,"The commit message indicates that deprecated `isXyz()` getter methods are being replaced with corresponding `getXyz()` methods. The diff shows that the `isDirty()` method is being renamed to `getDirty()`. This change is related to deprecation and method renaming, not a bug fix. The answer is **NotBuggy**.
"
openmrs-core,3687.json,2da55993f40c43555f00ca4660242241bf285bc6,"@@ -1,3 +1,3 @@
 	public boolean isHidden() {
-		return isHidden;
+		return getIsHidden();
 	}",NotBuggy,"TRUNK-2205: deprecated isXyz() getter methods having correspoding getXyz()

changed deprecated methods to call expected methods
",NotBuggy,"The commit message indicates that deprecated `isXyz()` getter methods are being changed to call their corresponding `getXyz()` methods. The diff shows that `isHidden()` is changed to call `getIsHidden()`. This change is part of deprecating the `isXyz()` methods and ensuring that they delegate to the new `getXyz()` methods. This is a refactoring change to support deprecation and doesn't indicate a bug fix. The answer is **NotBuggy**.
"
openmrs-core,5512.json,95075392b5c295d7ca5e94244ea5338dcaae7ae3,"@@ -1,9 +1,11 @@
 	public boolean isSecretAnswer(User u, String answer) throws DAOException {
 		
 		if (answer == null || answer.equals("""")) {
 			return false;
 		}
 		
-		String answerOnRecord = getLoginCredential(u).getSecretAnswer();
-		return (answer.equals(answerOnRecord));
+		LoginCredential credentials = getLoginCredential(u);
+		String answerOnRecord = credentials.getSecretAnswer();
+		String hashedAnswer = Security.encodeString(answer.toLowerCase() + credentials.getSalt());
+		return (hashedAnswer.equals(answerOnRecord));
 	}",NotBuggy,"TRUNK-3709 - Secret answer should be encrypted
",Buggy,"The commit message indicates that the secret answer should be encrypted. The diff shows that the code now retrieves the user's login credentials, salts and hashes the provided answer, and then compares it to the stored secret answer. This change directly addresses the encryption requirement described in the commit message, and it fixes the bug where the secret answer was not encrypted. The answer is **Buggy**."
openmrs-core,7457.json,da12f21c545d8c4994dccbd2d08bc4c962811d9d,"@@ -1,16 +1,16 @@
 	public static byte[] generateNewSecretKey() {
 		// Get the KeyGenerator
 		KeyGenerator kgen = null;
 		try {
 			kgen = KeyGenerator.getInstance(OpenmrsConstants.ENCRYPTION_KEY_SPEC);
 		}
 		catch (NoSuchAlgorithmException e) {
-			throw new APIException(""Could not generate cipher key"", e);
+			throw new APIException(""could.not.generate.cipher.key"", null, e);
 		}
 		kgen.init(128); // 192 and 256 bits may not be available
 		
 		// Generate the secret key specs.
 		SecretKey skey = kgen.generateKey();
 		
 		return skey.getEncoded();
 	}",NotBuggy,"TRUNK-4500: Use of APIException not localized (mostly)
",Buggy,"The commit message indicates a change to localize APIException messages. The diff shows that the APIException constructor is being called with a key that represents a localized message (""could.not.generate.cipher.key"") instead of a hardcoded English string. This change allows the message to be translated based on the user's locale. This is a bug fix because the original code was not properly localized, which is a type of bug. The answer is **Buggy**."
openmrs-core,6524.json,e374a4c37825bbe530cdf9a449906f6d8a0c18a9,"@@ -1,3 +1,3 @@
 	public Integer getCountOfEncounters(String query, boolean includeVoided) {
-		return OpenmrsUtil.convertToInteger(dao.getCountOfEncounters(query, includeVoided));
+		return OpenmrsUtil.convertToInteger(dao.getCountOfEncounters(query, null, includeVoided));
 	}",NotBuggy,"Encounter autocomplete should filter results by patient - TRUNK-3231

Encounter autocomplete should filter results by patient - TRUNK-3231
",Buggy,"The commit message indicates a feature enhancement related to encounter autocomplete, specifically filtering results by patient. The code diff shows a change in the `getCountOfEncounters` method signature, where a `null` value is now passed as the patient parameter to the DAO layer. This suggests that the original method was not filtering by patient, and the fix involves passing a `null` patient parameter, which might be later used in the DAO layer to filter by patient. This change aligns with the commit message and indicates a bug fix. The answer is **Buggy**."
openmrs-core,6208.json,48386f10175bf557801706f76c79e1413067673b,"@@ -1,15 +1,15 @@
 	public List<Obs> getObservations(List<Person> whom, List<Encounter> encounters, List<Concept> questions,
 	                                 List<Concept> answers, List<PERSON_TYPE> personTypes, List<Location> locations,
 	                                 List<String> sort, Integer mostRecentN, Integer obsGroupId, Date fromDate, Date toDate,
 	                                 boolean includeVoidedObs, String accessionNumber) throws APIException {
 		
 		if (sort == null) {
-			sort = new Vector<>();
+			sort = new ArrayList<>();
 		}
 		if (sort.isEmpty()) {
 			sort.add(""obsDatetime"");
 		}
 		
 		return dao.getObservations(whom, encounters, questions, answers, personTypes, locations, sort, mostRecentN,
 		    obsGroupId, fromDate, toDate, includeVoidedObs, accessionNumber);
 	}",NotBuggy,"TRUNK-5245 Replace Vector collections with ArrayList (#2445)

",NotBuggy,"The commit message indicates a change from `Vector` to `ArrayList`. The diff shows that a `Vector` is being replaced with an `ArrayList`. This is a performance improvement, not a bug fix. The answer is **NotBuggy**."
hadoop,65931.json,a36274d69947648dbe82721220cc5240ec5d396d,"@@ -1,7 +1,8 @@
   public void prettyPrint(StringBuilder sb) {
-    sb.append(String.format(""%-5s %-20s %-7d %-8s %-6s"",
+    sb.append(String.format(""%-5s %-20s %-7d %-8s %-6s %-20s %-20s"",
         fileStatus.isDirectory() ? ""dir"" : ""file"",
         fileStatus.getPath().toString(), fileStatus.getLen(),
-        isEmptyDirectory.name(), isDeleted));
+        isEmptyDirectory.name(), isDeleted,
+        fileStatus.getETag(), fileStatus.getVersionId()));
     sb.append(fileStatus);
   }",NotBuggy,"HADOOP-16085. S3Guard: use object version or etags to protect against inconsistent read after replace/overwrite.

Contributed by Ben Roling.

S3Guard will now track the etag of uploaded files and, if an S3
bucket is versioned, the object version.

You can then control how to react to a mismatch between the data
in the DynamoDB table and that in the store: warn, fail, or, when
using versions, return the original value.

This adds two new columns to the table: etag and version.
This is transparent to older S3A clients -but when such clients
add/update data to the S3Guard table, they will not add these values.
As a result, the etag/version checks will not work with files uploaded by older clients.

For a consistent experience, upgrade all clients to use the latest hadoop version.
",Buggy,"The commit message indicates a significant enhancement to S3Guard, focusing on data consistency after file replacements or overwrites. The changes involve tracking etags and object versions to detect mismatches between the DynamoDB table and the S3 store. The diff adds etag and version information to the output of the `prettyPrint` method, suggesting that these values are now part of the metadata being managed. This change directly supports the feature described in the commit message, which aims to improve data consistency and handle potential inconsistencies arising from S3's eventual consistency model. The addition of etag and version tracking seems to be a direct fix for potential data inconsistency issues, which can be considered a bug. The answer is **Buggy**.
"
hadoop,4963.json,1096917649fd951be633e5619518764f23cca645,"@@ -1,16 +1,16 @@
-  public static FileDiffList loadFileDiffList(DataInputStream in,
+  public static FileDiffList loadFileDiffList(DataInput in,
       FSImageFormat.Loader loader) throws IOException {
     final int size = in.readInt();
     if (size == -1) {
       return null;
     } else {
       final FileDiffList diffs = new FileDiffList();
       FileDiff posterior = null;
       for(int i = 0; i < size; i++) {
         final FileDiff d = loadFileDiff(posterior, in, loader);
         diffs.addFirst(d);
         posterior = d;
       }
       return diffs;
     }
   }",NotBuggy,"HDFS-4611. Update FSImage for INodeReference.


git-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1463332 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates an update to the FSImage for INodeReference, suggesting changes related to how file system images are handled, specifically concerning INodeReference objects. The diff shows a change in the parameter type of the `loadFileDiffList` method from `DataInputStream` to `DataInput`. This change likely broadens the types of input streams that can be used, potentially to accommodate different sources or formats of FSImage data. This could be part of a larger effort to improve the flexibility or robustness of the FSImage loading process, possibly addressing a bug or limitation in the original implementation. The change from `DataInputStream` to the more general `DataInput` suggests a move towards supporting a wider range of input sources, which can be related to bug fixing. The answer is **Buggy**."
hadoop,44452.json,89e5c44f9e891a3579384c3fa3766937cd4970f1,"@@ -1,28 +1,27 @@
   void startTimelineClient(final Configuration conf)
       throws YarnException, IOException, InterruptedException {
     try {
       appSubmitterUgi.doAs(new PrivilegedExceptionAction<Void>() {
         @Override
         public Void run() throws Exception {
-          if (conf.getBoolean(YarnConfiguration.TIMELINE_SERVICE_ENABLED,
-              YarnConfiguration.DEFAULT_TIMELINE_SERVICE_ENABLED)) {
+          if (YarnConfiguration.timelineServiceEnabled(conf)) {
             // Creating the Timeline Client
-            if (newTimelineService) {
+            if (timelineServiceV2) {
               timelineClient = TimelineClient.createTimelineClient(
                   appAttemptID.getApplicationId());
             } else {
               timelineClient = TimelineClient.createTimelineClient();
             }
             timelineClient.init(conf);
             timelineClient.start();
           } else {
             timelineClient = null;
             LOG.warn(""Timeline service is not enabled"");
           }
           return null;
         }
       });
     } catch (UndeclaredThrowableException e) {
       throw new YarnException(e.getCause());
     }
   }",NotBuggy,"YARN-4356. Ensure the timeline service v.2 is disabled cleanly and has no
impact when it's turned off. Contributed by Sangjin Lee.
",NotBuggy,"The commit message indicates a change to ensure the timeline service v.2 is disabled cleanly when turned off. The code diff replaces `conf.getBoolean(YarnConfiguration.TIMELINE_SERVICE_ENABLED, YarnConfiguration.DEFAULT_TIMELINE_SERVICE_ENABLED)` with `YarnConfiguration.timelineServiceEnabled(conf)`. This suggests that the configuration check for enabling the timeline service has been refactored, possibly to provide a cleaner or more robust way to determine if the service should be enabled. The change ensures that the timeline client is properly initialized and started only when the timeline service is enabled, and sets the timelineClient to null and logs a warning when it's disabled. This aligns with the commit message's intention to ensure the timeline service has no impact when turned off. The change doesn't appear to fix a bug, but rather improves the handling of the timeline service's disabled state. The answer is **NotBuggy**."
hadoop,81927.json,75dd866bfb8b63cb9f13179d4365b05c48e0907d,"@@ -1,11 +1,12 @@
-  private void createRootDirRecursively(String path) throws Exception {
-    String pathParts[] = path.split(""/"");
-    Preconditions.checkArgument(pathParts.length >= 1 && pathParts[0].isEmpty(),
+  public void createRootDirRecursively(String path) throws Exception {
+    String[] pathParts = path.split(""/"");
+    Preconditions.checkArgument(
+        pathParts.length >= 1 && pathParts[0].isEmpty(),
         ""Invalid path: %s"", path);
     StringBuilder sb = new StringBuilder();
 
     for (int i = 1; i < pathParts.length; i++) {
       sb.append(""/"").append(pathParts[i]);
       create(sb.toString());
     }
   }",NotBuggy,"HADOOP-14773. Extend ZKCuratorManager API for more reusability. (igo Goiri via Subru).
",NotBuggy,"The commit message indicates an extension of the ZKCuratorManager API for reusability. The code diff changes the visibility of the `createRootDirRecursively` method from private to public. This change makes the method accessible from other classes, which aligns with the commit message's intention of increasing reusability. There's no indication of a bug fix in the traditional sense (correcting erroneous behavior). The change is about making an existing functionality more accessible. The answer is **NotBuggy**."
hadoop,79614.json,5e7cfdca7b73a88bf3c3f1e5eb794a24218cce52,"@@ -1,4 +1,4 @@
-  public FSDataOutputStreamBuilder setReplication(short replica) {
+  public B replication(short replica) {
     replication = replica;
-    return this;
+    return getThisBuilder();
   }",NotBuggy,"HADOOP-14394. Provide Builder pattern for DistributedFileSystem.create. (lei)
",NotBuggy,"The commit message indicates the introduction of a Builder pattern for `DistributedFileSystem.create`. The diff shows a method `setReplication` (renamed to `replication`) being modified to fit the builder pattern. Specifically, it changes the return type to `B` (likely a generic type representing the builder itself) and calls `getThisBuilder()` to return the builder instance. This change is directly related to implementing the builder pattern, as it allows chaining of method calls. There is no indication of a bug fix in this change. The answer is **NotBuggy**.
"
hadoop,6690.json,69e5f90e9febf37d2cdd69c485729c448ac3cabc,"@@ -1,6 +1,8 @@
     public String toString() {
       StringBuilder builder = new StringBuilder();
       builder.append(""RemoveCachePoolOp ["");
-      builder.append(""poolName="" + poolName + ""]"");
+      builder.append(""poolName="" + poolName);
+      appendRpcIdsToString(builder, rpcClientId, rpcCallId);
+      builder.append(""]"");
       return builder.toString();
     }",NotBuggy,"HDFS-5385. Caching RPCs are AtMostOnce, but do not persist client ID and call ID to edit log.  (Chris Nauroth via Colin Patrick McCabe)

git-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1534345 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix related to caching RPCs and their persistence in the edit log, specifically mentioning client ID and call ID. The diff adds `appendRpcIdsToString(builder, rpcClientId, rpcCallId)` to the `toString()` method of `RemoveCachePoolOp`. This addition likely aims to include the RPC client and call IDs in the string representation of the operation, which could be useful for debugging or logging purposes when replaying the edit log. This change directly relates to the issue described in the commit message, suggesting a bug fix. The answer is **Buggy**."
hadoop,14378.json,8dd1eeb94fef59feaf19182dd8f1fcf1389c7f34,"@@ -1,3 +1,4 @@
-  public Long getJournalCTime(String journalId) throws IOException {
-    return jn.getJournalCTime(journalId);
+  public Long getJournalCTime(String journalId,
+                              String nameServiceId) throws IOException {
+    return jn.getJournalCTime(journalId, nameServiceId);
   }",NotBuggy,"HDFS-12553. Add nameServiceId to QJournalProtocol. Contributed by Bharat Viswanadham
",NotBuggy,The commit message indicates the addition of `nameServiceId` to `QJournalProtocol`. The diff shows that the `getJournalCTime` method now takes `nameServiceId` as a parameter. This change doesn't seem to fix a bug but rather adds functionality by including the `nameServiceId`. The answer is **NotBuggy**.
hadoop,34508.json,ab2bda57bd9ad617342586d5769121a4fef4eab1,"@@ -1,8 +1,9 @@
     public void transition(ContainerImpl container, ContainerEvent event) {
+      container.setIsPaused(false);
       // Pause the process/process-grp if it is supported by the container
       container.dispatcher.getEventHandler().handle(
           new ContainersLauncherEvent(container,
               ContainersLauncherEventType.RESUME_CONTAINER));
       ContainerResumeEvent resumeEvent = (ContainerResumeEvent) event;
       container.addDiagnostics(resumeEvent.getDiagnostic() + ""\n"");
     }",NotBuggy,"YARN-9428. Add metrics for paused containers in NodeManager. Contributed by Abhishek Modi.
",Buggy,"The diff modifies the `transition` method within a `ContainerImpl` class. It adds `container.setIsPaused(false);` at the beginning of the method. This indicates a change in the container's state management, specifically related to pausing and resuming containers. While the commit message mentions adding metrics, this specific code change focuses on setting the paused state to false when resuming a container. This suggests a potential bug fix related to the container's paused state not being correctly reset upon resumption, or an enhancement to ensure the paused state is explicitly set. The answer is **Buggy**."
hadoop,64072.json,dab00da19f25619ccc71c7f803a235b21766bf1e,"@@ -1,12 +1,5 @@
-      public void close() throws IOException {
-        try {
-          super.close();
-        } finally {
-          if (!closed.getAndSet(true)) {
-            if (!bufferFile.delete()) {
-              LOG.warn(""delete({}) returned false"",
-                  bufferFile.getAbsoluteFile());
-            }
-          }
-        }
-      }

+        public synchronized void close() {
+          LOG.debug(""ByteBufferInputStream.close() for {}"",
+              ByteBufferBlock.super.toString());
+          byteBuffer = null;
+        }",NotBuggy,"HADOOP-14028. S3A BlockOutputStreams doesn't delete temporary files in multipart uploads or handle part upload failures.
Contributed by Steve Loughran.

(cherry picked from commit 29fe5af017b945d8750c074ca39031b5b777eddd)
",Buggy,"The commit message indicates a fix for S3A BlockOutputStreams related to temporary file deletion and part upload failures. The provided diff shows a change in the `close()` method of a `ByteBufferInputStream`. The original `close()` method attempted to delete a `bufferFile` and logged a warning if the deletion failed. The modified `close()` method simply sets the `byteBuffer` to null. This change suggests that the original implementation had issues with resource management or cleanup, which could lead to temporary files not being deleted as intended, aligning with the commit message's description of the bug. Therefore, the answer is **Buggy**."
hadoop,79999.json,2b452b4e6063072b2bec491edd3f412eb7ac21f3,"@@ -1,16 +1,13 @@
   void init() {
     if (used.get() < 0) {
       used.set(0);
+      if (!shouldFirstRefresh) {
+        // Skip initial refresh operation, so we need to do first refresh
+        // operation immediately in refresh thread.
+        initRefeshThread(true);
+        return;
+      }
       refresh();
     }
-
-    if (refreshInterval > 0) {
-      refreshUsed = new Thread(new RefreshThread(this),
-          ""refreshUsed-"" + dirPath);
-      refreshUsed.setDaemon(true);
-      refreshUsed.start();
-    } else {
-      running.set(false);
-      refreshUsed = null;
-    }
+    initRefeshThread(false);
   }",NotBuggy,"HDFS-14986. ReplicaCachingGetSpaceUsed throws ConcurrentModificationException. Contributed by Aiphago.
",Buggy,"The commit message indicates a fix for a `ConcurrentModificationException` in `ReplicaCachingGetSpaceUsed`. The diff modifies the `init` method, specifically how the refresh thread is initialized. The original code initializes the refresh thread only if `refreshInterval > 0`. The modified code introduces a new method `initRefeshThread` and a flag `shouldFirstRefresh`. The refresh thread initialization logic is moved into `initRefeshThread`. The original code initializes the `used` field and calls `refresh()` before potentially starting the refresh thread. The modified code initializes the `used` field and calls `refresh()` only if `shouldFirstRefresh` is true. The purpose of this change is to ensure that the first refresh operation is performed immediately in the refresh thread if it was skipped initially. This change likely addresses a race condition or timing issue that could lead to the `ConcurrentModificationException`. The changes suggest a bug fix related to thread synchronization or data consistency. The answer is **Buggy**.
"
hadoop,9493.json,1594b472bb9df7537dbc001411c99058cc11ba41,"@@ -1,3 +1,3 @@
-  public float getVolumeDataDensity() {
+  public double getVolumeDataDensity() {
     return volumeDataDensity;
   }",NotBuggy,"HDFS-9543. DiskBalancer: Add Data mover. Contributed by Anu Engineer.
",NotBuggy,"The commit message indicates a new feature addition (""Add Data mover""). The code change involves modifying the return type of the `getVolumeDataDensity()` method from `float` to `double`. This change is likely related to the new feature and doesn't inherently suggest a bug fix. The change in return type could be for increased precision or to align with other parts of the new data mover functionality. The answer is **NotBuggy**."
hadoop,37365.json,9875325d5c63f343809907d06bf48a298035a611,"@@ -1,28 +1,28 @@
   public static ReservationAllocationStateProto buildStateProto(
       ReservationAllocation allocation) {
     ReservationAllocationStateProto.Builder builder =
         ReservationAllocationStateProto.newBuilder();
 
-    builder.setAcceptanceTimestamp(allocation.getAcceptanceTime());
+    builder.setAcceptanceTime(allocation.getAcceptanceTime());
     builder.setContainsGangs(allocation.containsGangs());
     builder.setStartTime(allocation.getStartTime());
     builder.setEndTime(allocation.getEndTime());
     builder.setUser(allocation.getUser());
     ReservationDefinitionProto definitionProto = convertToProtoFormat(
         allocation.getReservationDefinition());
     builder.setReservationDefinition(definitionProto);
 
     for (Map.Entry<ReservationInterval, Resource> entry :
         allocation.getAllocationRequests().entrySet()) {
       ResourceAllocationRequestProto p =
           ResourceAllocationRequestProto.newBuilder()
           .setStartTime(entry.getKey().getStartTime())
           .setEndTime(entry.getKey().getEndTime())
           .setResource(convertToProtoFormat(entry.getValue()))
           .build();
       builder.addAllocationRequests(p);
     }
 
     ReservationAllocationStateProto allocationProto = builder.build();
     return allocationProto;
   }",NotBuggy,"YARN-4340. Add list API to reservation system. (Sean Po via wangda)
",Buggy,The commit message indicates the addition of a list API to the reservation system. The code changes involve modifying the `buildStateProto` method to correctly set the `AcceptanceTime` in the `ReservationAllocationStateProto` builder. It seems like a typo fix where `AcceptanceTimestamp` was used instead of `AcceptanceTime`. This is likely a bug fix. The answer is **Buggy**.
hadoop,10353.json,d1d4e16690cc85f7f22fbead9cf596260819b561,"@@ -1,3 +1,3 @@
   boolean sendImmediately() {
-    return readyToSend;
+    return readyToSend && monotonicNow() - ibrInterval >= lastIBR;
   }",NotBuggy,"HDFS-9710. DN can be configured to send block receipt IBRs in batches.
",Buggy,"The commit message indicates a change in how DataNodes (DN) send Inter-Block Reports (IBRs), specifically introducing batching. The diff modifies the `sendImmediately()` method, which now includes a check to ensure that enough time (`ibrInterval`) has passed since the last IBR (`lastIBR`) before sending a new one. This change directly relates to the commit message's description of sending IBRs in batches, as it prevents immediate sending and enforces a time-based condition. This change is likely to fix a bug where IBRs were being sent too frequently, causing performance issues. The answer is **Buggy**.
"
hadoop,60643.json,a81144daa012e13590725f67f53e35ef84a6f1ec,"@@ -1,26 +1,23 @@
-          public Iterator<ApplicationTimeoutMapProto> iterator() {
-            return new Iterator<ApplicationTimeoutMapProto>() {
-              private Iterator<ApplicationTimeoutType> iterator =
-                  applicationTimeouts.keySet().iterator();
+      public Iterator<StringStringMapProto> iterator() {
+        return new Iterator<StringStringMapProto>() {
+          private Iterator<String> iterator = schedulingProperties.keySet()
+              .iterator();
 
-              @Override
-              public boolean hasNext() {
-                return iterator.hasNext();
-              }
+          @Override
+          public boolean hasNext() {
+            return iterator.hasNext();
+          }
 
-              @Override
-              public ApplicationTimeoutMapProto next() {
-                ApplicationTimeoutType key = iterator.next();
-                return ApplicationTimeoutMapProto.newBuilder()
-                    .setTimeout(applicationTimeouts.get(key))
-                    .setApplicationTimeoutType(
-                        ProtoUtils.convertToProtoFormat(key))
-                    .build();
-              }
+          @Override
+          public StringStringMapProto next() {
+            String key = iterator.next();
+            return StringStringMapProto.newBuilder()
+                .setValue(schedulingProperties.get(key)).setKey(key).build();
+          }
 
-              @Override
-              public void remove() {
-                throw new UnsupportedOperationException();
-              }
-            };
-          }

+          @Override
+          public void remove() {
+            throw new UnsupportedOperationException();
+          }
+        };
+      }",NotBuggy,"YARN-7666. Introduce scheduler specific environment variable support in ApplicationSubmissionContext for better scheduling placement configurations. (Sunil G via wangda)

Change-Id: I0fd826490f5160d47d42af2a9ac0bd8ec4e959dc
",NotBuggy,"The commit message ""YARN-7666. Introduce scheduler specific environment variable support in ApplicationSubmissionContext for better scheduling placement configurations"" indicates a new feature is being added. The code diff shows a change in the iterator implementation, specifically changing from `ApplicationTimeoutMapProto` to `StringStringMapProto` and using `schedulingProperties` instead of `applicationTimeouts`. This change aligns with the commit message as it introduces support for scheduler-specific environment variables, likely stored as key-value pairs in `schedulingProperties`. This doesn't seem to be fixing a bug, but rather adding new functionality. The answer is **NotBuggy**."
hadoop,14879.json,1b5451bf054c335188e4cd66f7b4a1d80013e86d,"@@ -1,8 +1,7 @@
     public String getLongUsage() {
-      TableListing listing = AdminHelper.getOptionDescriptionListing();
-      listing.addRow(""<path>"", ""The path of the directory ""
-          + ""from which the erasure coding policy will be unset."");
-      return getShortUsage() + ""\n""
-          + ""Unset the erasure coding policy for a directory.\n\n""
-          + listing.toString();
+      return getShortUsage() + ""\n"" +
+          ""Get the list of supported erasure coding codecs and coders.\n"" +
+          ""A coder is an implementation of a codec. A codec can have "" +
+          ""different implementations, thus different coders.\n"" +
+          ""The coders for a codec are listed in a fall back order.\n"";
     }",NotBuggy,"HDFS-11794. Add ec sub command -listCodec to show currently supported ec codecs. Contributed by SammiChen.
",NotBuggy,"The commit message indicates a new feature addition (""Add ec sub command -listCodec""). The diff provides the long usage description for the new command, explaining its purpose and functionality. There is no indication of a bug fix in either the commit message or the code changes. The answer is **NotBuggy**.
"
RxJava,6339.json,a97d871ee7161fc9f4684d95cae3e94340cd0ccf,"@@ -1,105 +1,109 @@
         void drain() {
             if (getAndIncrement() != 0) {
                 return;
             }
 
             int missed = 1;
 
             final Subscriber<? super R> downstream = this.downstream;
             final SimpleQueue<T> queue = this.queue;
             final AtomicThrowable error = this.error;
             Iterator<? extends R> iterator = this.currentIterator;
             long requested = this.requested.get();
             long emitted = this.emitted;
             final int limit = prefetch - (prefetch >> 2);
             boolean canRequest = sourceMode != QueueFuseable.SYNC;
 
             for (;;) {
                 if (cancelled) {
                     queue.clear();
                     clearCurrentSuppressCloseError();
                 } else {
                     boolean isDone = upstreamDone;
                     if (error.get() != null) {
                         downstream.onError(error.get());
                         cancelled = true;
                         continue;
                     }
 
                     if (iterator == null) {
                         T t;
 
                         try {
                             t = queue.poll();
                         } catch (Throwable ex) {
+                            Exceptions.throwIfFatal(ex);
                             trySignalError(downstream, ex);
                             continue;
                         }
 
                         boolean isEmpty = t == null;
 
                         if (isDone && isEmpty) {
                             downstream.onComplete();
                             cancelled = true;
                         }
                         else if (!isEmpty) {
                             if (canRequest && ++consumed == limit) {
                                 consumed = 0;
                                 upstream.request(limit);
                             }
 
                             Stream<? extends R> stream;
                             try {
                                 stream = Objects.requireNonNull(mapper.apply(t), ""The mapper returned a null Stream"");
                                 iterator = stream.iterator();
 
                                 if (iterator.hasNext()) {
                                     currentIterator = iterator;
                                     currentCloseable = stream;
                                 } else {
                                     iterator = null;
                                 }
                             } catch (Throwable ex) {
+                                Exceptions.throwIfFatal(ex);
                                 trySignalError(downstream, ex);
                             }
                             continue;
                         }
                     }
                     if (iterator != null && emitted != requested) {
                         R item;
 
                         try {
                             item = Objects.requireNonNull(iterator.next(), ""The Stream.Iterator returned a null value"");
                         } catch (Throwable ex) {
+                            Exceptions.throwIfFatal(ex);
                             trySignalError(downstream, ex);
                             continue;
                         }
 
                         if (!cancelled) {
                             downstream.onNext(item);
                             emitted++;
 
                             if (!cancelled) {
                                 try {
                                     if (!iterator.hasNext()) {
                                         iterator = null;
                                         clearCurrentRethrowCloseError();
                                     }
                                 } catch (Throwable ex) {
+                                    Exceptions.throwIfFatal(ex);
                                     trySignalError(downstream, ex);
                                 }
                             }
                         }
 
                         continue;
                     }
                 }
 
                 this.emitted = emitted;
                 missed = addAndGet(-missed);
                 if (missed == 0) {
                     break;
                 }
                 requested = this.requested.get();
             }
         }",NotBuggy,"3.x: Add missing throwIfFatal calls (#6801)

",Buggy,"The commit message indicates the addition of `throwIfFatal` calls. The diff shows that `Exceptions.throwIfFatal(ex)` is added in multiple catch blocks within the `drain` method. This addition is crucial for preventing fatal exceptions from being caught and potentially masked, which could lead to unexpected behavior or application crashes. By explicitly rethrowing fatal exceptions, the code ensures that these critical errors are properly handled and propagated, indicating a bug fix related to error handling. The answer is **Buggy**."
RxJava,6463.json,6ba932c9a3752c6d85d65bd2292c3a47cab64fd6,"@@ -1,13 +1,13 @@
         public T poll() {
             if (iterator == null) {
                 return null;
             }
             if (!once) {
                 once = true;
             } else {
                 if (!iterator.hasNext()) {
                     return null;
                 }
             }
-            return ObjectHelper.requireNonNull(iterator.next(), ""Iterator.next() returned a null value"");
+            return Objects.requireNonNull(iterator.next(), ""Iterator.next() returned a null value"");
         }",NotBuggy,"Replace ObjectHelper.requireNonNull with Objects.requireNonNull (#6771)

",NotBuggy,"The commit message indicates a replacement of `ObjectHelper.requireNonNull` with `Objects.requireNonNull`. The diff confirms this replacement in the `poll()` method. This change doesn't inherently indicate a bug fix, but rather a change in the utility method used for null checks. The functionality remains the same: ensuring that the iterator doesn't return null values. Therefore, it's a refactoring change, not a bug fix. The answer is **NotBuggy**."
RxJava,6464.json,af17c6e268d0e2fe4675feb46c19a29227cb135b,"@@ -1,3 +1,10 @@
         public boolean isEmpty() {
-            return iterator == null || !iterator.hasNext();
+            Iterator<T> it = iterator;
+            if (it != null) {
+                if (!once || it.hasNext()) {
+                    return false;
+                }
+                clear();
+            }
+            return true;
         }",NotBuggy,"3.x: [Java 8] Add Observable operators + cleanup (#6797)

",Buggy,"The commit message indicates the addition of Observable operators and cleanup. The diff modifies the `isEmpty()` method. The original implementation directly checked if the iterator was null or had a next element. The modified version introduces a local variable `it` to store the iterator, checks if `once` is false or if the iterator has a next element, and clears the iterator if it's empty. This change likely addresses a potential issue where the iterator's state wasn't being properly managed, possibly leading to incorrect `isEmpty()` results or resource leaks. The introduction of the `once` variable suggests a specific scenario where the iterator's emptiness needs to be evaluated only once. This looks like a bug fix. The answer is **Buggy**."
RxJava,6454.json,a249f4f1241f1c0ebe8e03dc80bdd16e29962c53,"@@ -1,23 +1,3 @@
     protected void subscribeActual(Subscriber<? super T> s) {
-        Iterator<T> iterator;
-        try {
-            iterator = stream.iterator();
-
-            if (!iterator.hasNext()) {
-                EmptySubscription.complete(s);
-                closeSafely(stream);
-                return;
-            }
-        } catch (Throwable ex) {
-            Exceptions.throwIfFatal(ex);
-            EmptySubscription.error(ex, s);
-            closeSafely(stream);
-            return;
-        }
-
-        if (s instanceof ConditionalSubscriber) {
-            s.onSubscribe(new StreamConditionalSubscription<T>((ConditionalSubscriber<? super T>)s, iterator, stream));
-        } else {
-            s.onSubscribe(new StreamSubscription<>(s, iterator, stream));
-        }
+        subscribeStream(s, stream);
     }",NotBuggy,"3.x: [Java 8] Add blockingStream & flatMapStream to Flowable (#6779)

",Buggy,"The commit message indicates the addition of `blockingStream` and `flatMapStream` to Flowable, targeting Java 8. The diff shows a change in the `subscribeActual` method. The original code had a try-catch block to handle potential exceptions when obtaining the iterator from the stream. It also checked if the iterator had any elements before proceeding. If an exception occurred or the iterator was empty, it would complete or error the subscriber and close the stream. The modified code replaces this entire block with a call to `subscribeStream(s, stream)`. This suggests that the exception handling and empty stream checks have been moved into the `subscribeStream` method. While the commit message describes a feature addition, the diff itself shows a refactoring of the subscription logic, potentially to consolidate error handling and stream management. It's possible that the original code had issues with exception handling or resource management, which this change addresses. Therefore, the change indicates a bug fix. The answer is **Buggy**."
RxJava,5791.json,a97d871ee7161fc9f4684d95cae3e94340cd0ccf,"@@ -1,14 +1,15 @@
     static int getIntProperty(boolean enabled, String key, int defaultNotFound, int defaultNotEnabled, Function<String, String> propertyAccessor) {
         if (enabled) {
             try {
                 String value = propertyAccessor.apply(key);
                 if (value == null) {
                     return defaultNotFound;
                 }
                 return Integer.parseInt(value);
             } catch (Throwable ex) {
+                Exceptions.throwIfFatal(ex);
                 return defaultNotFound;
             }
         }
         return defaultNotEnabled;
     }",NotBuggy,"3.x: Add missing throwIfFatal calls (#6801)

",Buggy,"The commit message indicates the addition of `throwIfFatal` calls. The diff shows a `try-catch` block where a `Throwable` is caught. Inside the `catch` block, `Exceptions.throwIfFatal(ex)` is called. This change is related to error handling, specifically addressing fatal exceptions that should not be caught and handled like regular exceptions. This addition suggests a previous oversight in handling fatal exceptions, which could be considered a bug. The answer is **Buggy**."
RxJava,6341.json,a97d871ee7161fc9f4684d95cae3e94340cd0ccf,"@@ -1,7 +1,8 @@
         void clearCurrentSuppressCloseError() {
             try {
                 clearCurrentRethrowCloseError();
             } catch (Throwable ex) {
+                Exceptions.throwIfFatal(ex);
                 RxJavaPlugins.onError(ex);
             }
         }",NotBuggy,"3.x: Add missing throwIfFatal calls (#6801)

",Buggy,"The commit message indicates the addition of `throwIfFatal` calls. The diff shows that a `throwIfFatal(ex)` call has been added within a catch block. This addition is likely a bug fix, as it addresses the potential for fatal errors to be swallowed and unhandled, which could lead to unexpected program behavior. The answer is **Buggy**.
"
RxJava,6378.json,a97d871ee7161fc9f4684d95cae3e94340cd0ccf,"@@ -1,23 +1,24 @@
     protected void subscribeActual(Observer<? super R> observer) {
         if (source instanceof Supplier) {
             Stream<? extends R> stream = null;
             try {
                 @SuppressWarnings(""unchecked"")
                 T t = ((Supplier<T>)source).get();
                 if (t != null) {
                     stream = Objects.requireNonNull(mapper.apply(t), ""The mapper returned a null Stream"");
                 }
             } catch (Throwable ex) {
+                Exceptions.throwIfFatal(ex);
                 EmptyDisposable.error(ex, observer);
                 return;
             }
 
             if (stream != null) {
                 ObservableFromStream.subscribeStream(observer, stream);
             } else {
                 EmptyDisposable.complete(observer);
             }
         } else {
             source.subscribe(new FlatMapStreamObserver<>(observer, mapper));
         }
     }",NotBuggy,"3.x: Add missing throwIfFatal calls (#6801)

",Buggy,"The commit message indicates the addition of missing `throwIfFatal` calls. The diff shows a `try-catch` block where a `Throwable` is caught. Inside the `catch` block, `Exceptions.throwIfFatal(ex)` is added before propagating the error to the observer. This addition is crucial for preventing fatal exceptions from being silently caught and potentially causing unexpected behavior or application crashes. The added call ensures that fatal exceptions are properly re-thrown, adhering to the expected error handling practices. The answer is **Buggy**.
"
RxJava,5792.json,a97d871ee7161fc9f4684d95cae3e94340cd0ccf,"@@ -1,14 +1,15 @@
     static boolean getBooleanProperty(boolean enabled, String key, boolean defaultNotFound, boolean defaultNotEnabled, Function<String, String> propertyAccessor) {
         if (enabled) {
             try {
                 String value = propertyAccessor.apply(key);
                 if (value == null) {
                     return defaultNotFound;
                 }
                 return ""true"".equals(value);
             } catch (Throwable ex) {
+                Exceptions.throwIfFatal(ex);
                 return defaultNotFound;
             }
         }
         return defaultNotEnabled;
     }",NotBuggy,"3.x: Add missing throwIfFatal calls (#6801)

",Buggy,"The commit message indicates the addition of `throwIfFatal` calls. The diff shows that a `throwIfFatal(ex)` call has been added to the catch block of the `getBooleanProperty` method. This addition is intended to ensure that fatal exceptions are re-thrown, preventing the application from continuing in an undefined state. This is a bug fix because it addresses the potential issue of fatal exceptions being silently caught and ignored, which could lead to unexpected behavior or application crashes. The answer is **Buggy**."
RxJava,6466.json,6ba932c9a3752c6d85d65bd2292c3a47cab64fd6,"@@ -1,56 +1,56 @@
         public void run(long n) {
             long emitted = 0L;
             Iterator<T> iterator = this.iterator;
             ConditionalSubscriber<? super T> downstream = this.downstream;
 
             for (;;) {
 
                 if (cancelled) {
                     clear();
                     break;
                 } else {
                     T next;
                     try {
-                        next = ObjectHelper.requireNonNull(iterator.next(), ""The Stream's Iterator returned a null value"");
+                        next = Objects.requireNonNull(iterator.next(), ""The Stream's Iterator returned a null value"");
                     } catch (Throwable ex) {
                         Exceptions.throwIfFatal(ex);
                         downstream.onError(ex);
                         cancelled = true;
                         continue;
                     }
 
                     if (downstream.tryOnNext(next)) {
                         emitted++;
                     }
 
                     if (cancelled) {
                         continue;
                     }
 
                     try {
                         if (!iterator.hasNext()) {
                             downstream.onComplete();
                             cancelled = true;
                             continue;
                         }
                     } catch (Throwable ex) {
                         Exceptions.throwIfFatal(ex);
                         downstream.onError(ex);
                         cancelled = true;
                         continue;
                     }
 
                     if (emitted != n) {
                         continue;
                     }
                 }
 
                 n = get();
                 if (emitted == n) {
                     if (compareAndSet(n, 0L)) {
                         break;
                     }
                     n = get();
                 }
             }
         }",NotBuggy,"Replace ObjectHelper.requireNonNull with Objects.requireNonNull (#6771)

",NotBuggy,"The commit message indicates a replacement of `ObjectHelper.requireNonNull` with `Objects.requireNonNull`. The diff confirms this change. While this might be part of a larger bug fix or refactoring effort, the change itself doesn't inherently indicate a bug fix. It's more likely a code modernization or dependency update. The answer is **NotBuggy**."
RxJava,7468.json,1ede60786a1a8f00f1fb3968d0701a323ed86f2f,"@@ -1,3 +1,3 @@
     public static <T> Flowable<T> mergeArray(SingleSource<? extends T>... sources) {
-        return Flowable.fromArray(sources).flatMapSingle(Functions.identity(), false, sources.length);
+        return Flowable.fromArray(sources).flatMapSingle(Functions.identity(), false, Math.max(1, sources.length));
     }",NotBuggy,"3.x: Use more appropriate operators when delegating to Flowable ops (#6888)

* 3.x: Use more appropriate operators when delegating to Flowable ops

* Remove now-unused classes
",Buggy,"The commit message indicates the use of more appropriate operators when delegating to Flowable operations. The diff shows a change in the `mergeArray` method where `sources.length` is replaced with `Math.max(1, sources.length)` as the `maxConcurrency` argument to `flatMapSingle`. This change ensures that `maxConcurrency` is at least 1, even when the `sources` array is empty. This prevents potential issues or unexpected behavior when dealing with empty arrays. This can be considered a bug fix, as it addresses a potential edge case where the original code might not have behaved as expected. The answer is **Buggy**.
"
RxJava,8113.json,6ba932c9a3752c6d85d65bd2292c3a47cab64fd6,"@@ -1,4 +1,4 @@
     public static <T> Flowable<@NonNull T> fromStream(@NonNull Stream<T> stream) {
-        ObjectHelper.requireNonNull(stream, ""stream is null"");
+        Objects.requireNonNull(stream, ""stream is null"");
         return RxJavaPlugins.onAssembly(new FlowableFromStream<>(stream));
     }",NotBuggy,"Replace ObjectHelper.requireNonNull with Objects.requireNonNull (#6771)

",NotBuggy,"The commit message indicates a replacement of `ObjectHelper.requireNonNull` with `Objects.requireNonNull`. The diff confirms this change. While this might be part of a larger refactoring or dependency update, it doesn't inherently indicate a bug fix. The functionality remains the same: checking for null and throwing a `NullPointerException` if the stream is null.
The answer is **NotBuggy**."
RxJava,8410.json,f1a795da0d4cdfb3133206ad8e3887bdb8ba948b,"@@ -1,3 +1,3 @@
     public final <R> Maybe<R> concatMapSingle(@NonNull Function<? super T, ? extends SingleSource<? extends R>> mapper) {
-        return flatMapSingleElement(mapper);
+        return flatMapSingle(mapper);
     }",NotBuggy,"3.x: Swap Maybe.flatMapSingle and Maybe.flatMapSingleElement (#6891)

* Deleted Maybe.flatMapSingle

And replaced tests with flatMapSingle().toSingle()

* Renamed Maybe.flatMapSingleElement to flatMapSingle

* Deleted unused MaybeFlatMapSingle operator

* Renamed operator MaybeFlatMapSingleElement to MaybeFlatMapSingle

* Add Ignore to XFlatMapTest.maybeSingle
",NotBuggy,"The commit message indicates a renaming and removal of methods related to `Maybe.flatMapSingle` and `Maybe.flatMapSingleElement`. The diff shows that `concatMapSingle` now calls `flatMapSingle` instead of `flatMapSingleElement`. This change suggests a refactoring and simplification of the API, rather than a bug fix. The renaming and removal of methods do not inherently indicate a bug being resolved. The answer is **NotBuggy**."
RxJava,8112.json,6ba932c9a3752c6d85d65bd2292c3a47cab64fd6,"@@ -1,4 +1,4 @@
     public static <T> Flowable<@NonNull T> fromCompletionStage(@NonNull CompletionStage<T> stage) {
-        ObjectHelper.requireNonNull(stage, ""stage is null"");
+        Objects.requireNonNull(stage, ""stage is null"");
         return RxJavaPlugins.onAssembly(new FlowableFromCompletionStage<>(stage));
     }",NotBuggy,"Replace ObjectHelper.requireNonNull with Objects.requireNonNull (#6771)

",NotBuggy,"The commit message indicates a replacement of `ObjectHelper.requireNonNull` with `Objects.requireNonNull`. The diff confirms this change. While `requireNonNull` is used for null checks, replacing one implementation with another doesn't inherently indicate a bug fix. It's more likely a refactoring or standardization effort. The answer is **NotBuggy**."
RxJava,7469.json,1ede60786a1a8f00f1fb3968d0701a323ed86f2f,"@@ -1,3 +1,3 @@
     public static <T> Flowable<T> mergeArrayDelayError(@NonNull SingleSource<? extends T>... sources) {
-        return Flowable.fromArray(sources).flatMapSingle(Functions.identity(), true, sources.length);
+        return Flowable.fromArray(sources).flatMapSingle(Functions.identity(), true, Math.max(1, sources.length));
     }",NotBuggy,"3.x: Use more appropriate operators when delegating to Flowable ops (#6888)

* 3.x: Use more appropriate operators when delegating to Flowable ops

* Remove now-unused classes
",Buggy,"The commit message indicates the use of more appropriate operators when delegating to Flowable operations. The diff shows a change in the `mergeArrayDelayError` method where `sources.length` is replaced with `Math.max(1, sources.length)` in the `flatMapSingle` operator. This change likely addresses a potential issue where `sources.length` could be zero, which might lead to unexpected behavior or errors in the `flatMapSingle` operator. By using `Math.max(1, sources.length)`, the code ensures that the minimum concurrency hint passed to `flatMapSingle` is 1, preventing potential issues when the input array is empty. This adjustment suggests a correction to a potential bug or edge case. The answer is **Buggy**.
"
RxJava,6762.json,ecfb6b741d411de9d060b1c4b1fcd90ff7530854,"@@ -1,3 +1,4 @@
     static AutoCloseable toAutoCloseable(@NonNull Disposable disposable) {
+        Objects.requireNonNull(disposable, ""disposable is null"");
         return disposable::dispose;
     }",NotBuggy,"3.x: Improve Javadocs of Completable and some others (#6809)

",Buggy,"The commit message indicates improvements to Javadocs and other minor changes. The diff adds a null check for the `disposable` parameter in the `toAutoCloseable` method. This addition prevents a potential `NullPointerException` if a null `Disposable` is passed to the method. This is a defensive programming measure that addresses a potential bug. The answer is **Buggy**.
"
junit4,587.json,83a25d440d118b7d10779a884d35f31045949e27,"@@ -1,3 +1,3 @@
             public int compare(T o1, T o2) {
-                return fSorter.compare(describeChild(o1), describeChild(o2));
+                return sorter.compare(describeChild(o1), describeChild(o2));
             }",NotBuggy,"Treatments for parallel execution
",NotBuggy,"The commit message ""Treatments for parallel execution"" is vague. The code change replaces `fSorter` with `sorter`. Without more context, it's difficult to determine if this change is related to a bug fix or an enhancement for parallel execution. It could be a simple renaming or a more significant change to ensure thread safety or correct behavior in a parallel environment. Without further information, it's impossible to definitively say if this is a bug fix.

**NotBuggy**
"
junit4,522.json,627b85abfd9c1f6c69b14f9d33fb292868fe6826,"@@ -1,3 +1,4 @@
-    public static void assertThrows(Class<? extends Throwable> expectedThrowable, ThrowingRunnable runnable) {
-        expectThrows(expectedThrowable, runnable);
+    public static <T extends Throwable> T assertThrows(Class<T> expectedThrowable,
+            ThrowingRunnable runnable) {
+        return assertThrows(null, expectedThrowable, runnable);
     }",NotBuggy,"rename expectThrows() to assertThrows()
",NotBuggy,"The commit message indicates a renaming of the `expectThrows()` method to `assertThrows()`. The diff shows that the original `assertThrows()` method now calls a new overloaded `assertThrows()` method. This change suggests a refactoring or renaming of a method, which doesn't necessarily indicate a bug fix. The core functionality remains the same, but the method name has been updated. The answer is **NotBuggy**."
junit4,313.json,5501ad5d546427b228b208da15cec997e0b13cc8,"@@ -1,3 +1,3 @@
-	public RememberingListener listener() {
+	public RunListener listener() {
 		return new RememberingListener();
 	}",NotBuggy,"Finished Max javadoc
",NotBuggy,"The commit message ""Finished Max javadoc"" suggests documentation updates, but the diff shows a change in the return type of the `listener()` method from `RememberingListener` to `RunListener`. This indicates a change in the API, which is not related to documentation. Therefore, this is not a bug fix. The answer is **NotBuggy**.
"
junit4,738.json,86b9346e4c8fa263810dbd55a2775081194b1d6f,"@@ -1,3 +1,3 @@
     public int hashCode() {
-        return uniqueId.hashCode();
+        return fUniqueId.hashCode();
     }",NotBuggy,"Ensure serialization compatibility where possible

Not possible for org.junit.runner.Result
",Buggy,"The commit message indicates a change related to serialization compatibility. The diff shows a change from `uniqueId.hashCode()` to `fUniqueId.hashCode()`. This suggests that the field name `uniqueId` was changed to `fUniqueId`, and this change ensures that the `hashCode()` method continues to work correctly after serialization/deserialization. This is likely a bug fix related to serialization. The answer is **Buggy**."
junit4,271.json,df00d5eced3a7737b88de0f6f9e3673f0cf88f88,"@@ -1,4 +1,4 @@
     private String missingExceptionMessage() {
-        String expectation= StringDescription.toString(fMatcherBuilder.build());
+        String expectation= StringDescription.toString(matcherBuilder.build());
         return format(missingExceptionMessage, expectation);
     }",NotBuggy,"remove 'f' prefix from field names, add LEGACY_CODING_STYLE, change CODING_STYLE
",NotBuggy,"The commit message indicates a change in field names by removing the 'f' prefix and introduces a new coding style. The diff shows the removal of the 'f' prefix from `fMatcherBuilder` to `matcherBuilder`. This change aligns with the commit message's intention to refactor field names and adjust the coding style. There is no error handling, logical correction, or exception handling improvement. The answer is **NotBuggy**."
junit4,657.json,fbb1efed56f2b495d68a598d5345cbc95b118d35,"@@ -1,3 +1,3 @@
     public boolean isStatic() {
-        return Modifier.isStatic(fMethod.getModifiers());
+        return Modifier.isStatic(getModifiers());
     }",NotBuggy,"Remove duplicate code.
",NotBuggy,"The commit message indicates the removal of duplicate code. The diff shows a change in the `isStatic()` method, where `fMethod.getModifiers()` is replaced with `getModifiers()`. It is highly likely that `fMethod` was an instance variable that this class was using to get the modifiers, and now the class is directly calling `getModifiers()` which is likely a method inherited from a superclass or interface. This eliminates the need to access the modifiers through the instance variable `fMethod`, thus removing duplicate code. The answer is **NotBuggy**.
"
junit4,498.json,9ac4cc842d70dc7ef266f4d677d90d9b177e4be6,"@@ -1,5 +1,5 @@
     static public void assertEquals(String message, long expected, long actual) {
         if (expected != actual) {
-            failNotEquals(message, expected, actual);
+            failNotEquals(message, Long.valueOf(expected), Long.valueOf(actual));
         }
     }",NotBuggy,"Use explicit auto-boxing.
",NotBuggy,"The commit message ""Use explicit auto-boxing"" suggests a change in how primitive types are converted to their corresponding wrapper objects. The diff shows that `failNotEquals` is now called with `Long.valueOf(expected)` and `Long.valueOf(actual)` instead of `expected` and `actual`. This change explicitly converts the `long` values `expected` and `actual` to `Long` objects before passing them to the `failNotEquals` method. This change doesn't fix a bug, but rather improves code clarity or avoids potential issues with implicit autoboxing. The answer is **NotBuggy**."
junit4,163.json,df00d5eced3a7737b88de0f6f9e3673f0cf88f88,"@@ -1,4 +1,4 @@
     protected void describeMismatchSafely(T item, Description description) {
         description.appendText(""message "");
-        fMatcher.describeMismatch(item.getMessage(), description);
+        matcher.describeMismatch(item.getMessage(), description);
     }",NotBuggy,"remove 'f' prefix from field names, add LEGACY_CODING_STYLE, change CODING_STYLE
",NotBuggy,"The commit message indicates renaming a field from `fMatcher` to `matcher` and introducing a constant `LEGACY_CODING_STYLE`. The diff shows the field `fMatcher` being changed to `matcher`. This suggests a refactoring or code cleanup rather than a bug fix.
The answer is **NotBuggy**."
junit4,267.json,df00d5eced3a7737b88de0f6f9e3673f0cf88f88,"@@ -1,3 +1,3 @@
     private boolean isAnyExceptionExpected() {
-        return fMatcherBuilder.expectsThrowable();
+        return matcherBuilder.expectsThrowable();
     }",NotBuggy,"remove 'f' prefix from field names, add LEGACY_CODING_STYLE, change CODING_STYLE
",NotBuggy,"The commit message indicates a renaming of fields by removing the 'f' prefix and the introduction of a new coding style. The diff shows a change from `fMatcherBuilder` to `matcherBuilder`, which aligns with the commit message's intention to remove the 'f' prefix from field names. This change is likely a refactoring or code style update rather than a bug fix. The answer is **NotBuggy**.
"
junit4,663.json,02c328028b4d32c15bbf0becc9838e54ecbafcbf,"@@ -1,7 +1,11 @@
     public Runner safeRunnerForClass(Class<?> testClass) {
         try {
-            return runnerForClass(testClass);
+            Runner runner = runnerForClass(testClass);
+            if (runner != null) {
+                configureRunner(runner);
+            }
+            return runner;
         } catch (Throwable e) {
             return new ErrorReportingRunner(testClass, e);
         }
     }",NotBuggy,"Add Ordering, Orderable and @OrderWith (#1130)

* Add Ordering, Orderable and @OrderWith.

These APIs allow arbitrary ordering of tests, including randomization.
",NotBuggy,"The commit message introduces new features related to test ordering. The diff modifies the `safeRunnerForClass` method to configure the runner after it's created, potentially applying the new ordering features. This change doesn't seem to fix a bug but rather adds functionality. The answer is **NotBuggy**."
junit4,165.json,df00d5eced3a7737b88de0f6f9e3673f0cf88f88,"@@ -1,4 +1,4 @@
     public void describeTo(Description description) {
         description.appendText(""exception with cause "");
-        description.appendDescriptionOf(fMatcher);
+        description.appendDescriptionOf(matcher);
     }",NotBuggy,"remove 'f' prefix from field names, add LEGACY_CODING_STYLE, change CODING_STYLE
",NotBuggy,"The commit message indicates a renaming of fields by removing the 'f' prefix. The diff confirms this change, replacing `fMatcher` with `matcher`. This is a refactoring change to improve code style and readability. There's no indication of a bug fix in the description or the code changes. The answer is **NotBuggy**.
"
junit4,788.json,b01900fddca5ff54258e621025ccdbbc75f880f1,"@@ -1,8 +1,8 @@
-    FilterFactory createFilterFactory(Class<? extends FilterFactory> filterFactoryClass)
+    static FilterFactory createFilterFactory(Class<? extends FilterFactory> filterFactoryClass)
             throws FilterFactoryNotCreatedException {
         try {
             return filterFactoryClass.getConstructor().newInstance();
         } catch (Exception e) {
             throw new FilterFactoryNotCreatedException(e);
         }
     }",NotBuggy,"FilterFactoryFactory renamed to FilterFactories and its methods made static.
ClassUtil renamed to Classes for consistency.
",NotBuggy,"The commit message indicates a renaming and a change to static methods. The diff shows the method `createFilterFactory` being made static. This change doesn't appear to be related to fixing a bug, but rather a refactoring or restructuring of the code. The functionality should remain the same. The answer is **NotBuggy**.
"
junit4,702.json,95bd62b2ffd80394d1a17e9078a4d79478423030,"@@ -1,3 +1,3 @@
 			protected void notifyListener(RunListener each) throws Exception {
-				each.testIgnoredReason(description, e);
+				each.testAssumptionFailed(description, e);
 			};",NotBuggy,"Start separating the idea of failed assumptions from ignored tests
",NotBuggy,"The commit message indicates a change in how test results are categorized, specifically distinguishing between ""failed assumptions"" and ""ignored tests."" The diff replaces a call to `testIgnoredReason` with `testAssumptionFailed`. This suggests a refinement in the test reporting mechanism, where tests that fail due to unmet assumptions are now reported differently from tests that are simply ignored. This change doesn't inherently indicate a bug fix, but rather a change in test reporting. The answer is **NotBuggy**.
"
junit4,640.json,df00d5eced3a7737b88de0f6f9e3673f0cf88f88,"@@ -1,3 +1,3 @@
     public Class<?> getDeclaringClass() {
-        return fMethod.getDeclaringClass();
+        return method.getDeclaringClass();
     }",NotBuggy,"remove 'f' prefix from field names, add LEGACY_CODING_STYLE, change CODING_STYLE
",NotBuggy,"The commit message indicates a change in field naming convention and the introduction of a new coding style constant, along with a modification to the existing coding style. The diff shows a field name change from `fMethod` to `method`. This suggests a refactoring to adhere to the new coding style, specifically removing the 'f' prefix from field names. This change doesn't directly indicate a bug fix, but rather an improvement in code style and readability. The answer is **NotBuggy**.
"
junit4,348.json,a72b0dbef4b01e8ad0b832d9a579dd7fabd5a071,"@@ -1,3 +1,3 @@
-	public static Executioner methods() {
-		return new ParallelExecutioner(false, true);
+	public static Computer methods() {
+		return new ParallelComputer(false, true);
 	}",NotBuggy,"Clean up parallel execution naming.
",NotBuggy,"The commit message indicates a cleanup of parallel execution naming. The diff replaces `Executioner` with `Computer` in the `methods()` method. This change suggests a renaming or refactoring of classes related to parallel execution. It doesn't directly indicate a bug fix, but rather an improvement in naming consistency or code organization. The answer is **NotBuggy**."
facebook-android-sdk,3514.json,752ea7b77a1779ed8b784dbd55a886cc00aeb8a6,"@@ -1,5 +1,5 @@
-    public static final Set<LoggingBehavior> getLoggingBehaviors() {
+    public static Set<LoggingBehavior> getLoggingBehaviors() {
         synchronized (loggingBehaviors) {
             return Collections.unmodifiableSet(new HashSet<LoggingBehavior>(loggingBehaviors));
         }
     }",NotBuggy,"Facebook Android SDK 4.0
",NotBuggy,"The commit message indicates an update to the Facebook Android SDK to version 4.0. The code diff changes the return type of the `getLoggingBehaviors()` method from `public static final Set<LoggingBehavior>` to `public static Set<LoggingBehavior>`. The `final` keyword is removed from the method signature. This change doesn't inherently indicate a bug fix. It might be part of a broader API update or refactoring within the SDK. There's no evidence of error handling, logical corrections, or exception handling improvements. The answer is **NotBuggy**."
facebook-android-sdk,1334.json,bc17a04007ed19a340cfb2753c07f927e514b5ad,"@@ -1,7 +1,7 @@
     private void logAuthorizationMethodStart(String method) {
         Bundle bundle = newAuthorizationLoggingBundle(pendingRequest.getAuthId());
         bundle.putLong(EVENT_PARAM_TIMESTAMP, System.currentTimeMillis());
         bundle.putString(EVENT_PARAM_METHOD, method);
 
-        appEventsLogger.logSdkEvent(EVENT_NAME_LOGIN_METHOD_START, null, bundle);
+        getAppEventsLogger().logSdkEvent(EVENT_NAME_LOGIN_METHOD_START, null, bundle);
     }",NotBuggy,"Facebook Android SDK 3.5.1 (see https://developers.facebook.com/docs/android/change-log-3.x/)

Summary: Facebook Android SDK 3.5.1 (see https://developers.facebook.com/docs/android/change-log-3.x/)

Test Plan: - Built ZIP

Reviewers: mingfli

Reviewed By: mingfli

CC: subodh, platform-diffs@lists

Differential Revision: https://phabricator.fb.com/D986742
",Buggy,"The change replaces `appEventsLogger` with `getAppEventsLogger()`. This suggests that `appEventsLogger` might have been directly accessed, and now it's being accessed through a getter. This could be related to encapsulation or thread-safety issues, indicating a potential bug fix related to how the logger is accessed. The answer is **Buggy**.
"
facebook-android-sdk,2354.json,2921bd7e2a268d0e08d10c13f90f8381d27c24ca,"@@ -1,8 +1,11 @@
     public void add(Activity activity) {
+        if (InternalSettings.isUnityApp()) {
+            return;
+        }
         if (Thread.currentThread() != Looper.getMainLooper().getThread()) {
             throw new FacebookException(""Can't add activity to CodelessMatcher on non-UI thread"");
         }
         this.activitiesSet.add(activity);
         delegateMap.clear();
         startTracking();
     }",NotBuggy,"Delegate event matching to unity sdk

Summary:
Codeless event matching should be handled in Unity SDK due to the special tree hierarchy in Unity apps. This diff does 2 things:
1. Send event mappin to Unity on parsing the app settings.
2. Skip event matching for unity apps.

Differential Revision: D10215680

fbshipit-source-id: 28b2128619aa220cd5cf178d7024464d5e126bd3
",Buggy,"The commit message indicates a change in how event matching is handled for Unity apps. The code diff shows that the `add` method now includes a check for whether the app is a Unity app using `InternalSettings.isUnityApp()`. If it is a Unity app, the method returns immediately, effectively skipping the event matching logic in the `CodelessMatcher` for Unity apps. This change is made because event matching for Unity apps is now delegated to the Unity SDK. This delegation suggests that the previous event matching logic was either incorrect or insufficient for Unity apps, indicating a bug or limitation in the original implementation. The code changes are directly related to addressing this issue by skipping the flawed logic. The answer is **Buggy**.
"
facebook-android-sdk,1430.json,8d168dd203acf2e068a6b6f6029e9baaea992538,"@@ -1,4 +1,3 @@
                     public void onBackgroundProcessingStarted() {
-                        view.findViewById(R.id.com_facebook_login_activity_progress_bar)
-                                .setVisibility(View.VISIBLE);
+                        progressBar.setVisibility(View.VISIBLE);
                     }",NotBuggy,"Facebook Android SDK 4.28
",NotBuggy,"The commit message indicates an update to the Facebook Android SDK. The diff shows a change in how the progress bar's visibility is handled. Instead of directly accessing the view using `findViewById`, it now uses a `progressBar` variable. This suggests a refactoring or simplification of the code, potentially to improve readability or maintainability. It doesn't inherently indicate a bug fix. The answer is **NotBuggy**.
"
facebook-android-sdk,2780.json,9dd41692e4bc962ab7bc56c5a17679be80ab50bf,"@@ -1,4 +1,7 @@
     private static String getUserAgent() {
-        // TODO port: construct user agent string with version
-        return ""FBAndroidSDK"";
+        if (userAgent == null) {
+            userAgent = String.format(""%s.%s"", USER_AGENT_BASE, FacebookSdkVersion.BUILD);
+        }
+
+        return userAgent;
     }",NotBuggy,"Added UserAgent, Migration Bundle

Summary: Adding version string in UserAgent and current sdk Migration Bundle.

Test Plan:
- Unit tests

Revert Plan:
- ok

Tags:

Reviewers: mingfli, clang, jacl

Reviewed By: mingfli

CC: msdkexp@

Differential Revision: https://phabricator.fb.com/D583727

Task ID: 994912
",NotBuggy,"The commit message indicates the addition of a UserAgent string that includes the SDK version. The diff shows that the `getUserAgent` method is modified to construct a user agent string using `USER_AGENT_BASE` and `FacebookSdkVersion.BUILD`. This change introduces version information into the UserAgent, which was previously a simple static string. This doesn't seem to be fixing a bug, but rather adding a new feature or improving existing functionality. The answer is **NotBuggy**.
"
facebook-android-sdk,2061.json,752ea7b77a1779ed8b784dbd55a886cc00aeb8a6,"@@ -1,3 +1,3 @@
-    public List<String> getDeclinedPermissions() {
+    public Set<String> getDeclinedPermissions() {
         return this.declinedPermissions;
     }",NotBuggy,"Facebook Android SDK 4.0
",NotBuggy,"The commit message indicates a version update to Facebook Android SDK 4.0. The diff changes the return type of `getDeclinedPermissions()` from `List<String>` to `Set<String>`. This change suggests an update to the API, possibly to ensure uniqueness of declined permissions or to improve performance. It doesn't inherently indicate a bug fix, but rather a change in the API's structure or behavior. The answer is **NotBuggy**.
"
facebook-android-sdk,775.json,752ea7b77a1779ed8b784dbd55a886cc00aeb8a6,"@@ -1,3 +1,3 @@
-        public int describeContents() {
-            return 0;
-        }

+    public int describeContents() {
+        return 0;
+    }",NotBuggy,"Facebook Android SDK 4.0
",NotBuggy,"The change involves only formatting, specifically the removal of an empty line before a method definition. This doesn't indicate any bug fix or functional change. The answer is **NotBuggy**.
"
facebook-android-sdk,477.json,0086a52b843d60753942a20573fcdde4c9a3e948,"@@ -1,4 +1,4 @@
-        public Builder setObjectType(final LikeView.ObjectType objectType) {
-            this.objectType = objectType == null ? LikeView.ObjectType.UNKNOWN : objectType;
+        public Builder setObjectType(final String objectType) {
+            this.objectType = objectType;
             return this;
         }",NotBuggy,"Facebook Android SDK 4.1
",NotBuggy,"The commit message ""Facebook Android SDK 4.1"" suggests a general SDK update rather than a specific bug fix. The diff shows a change in the `setObjectType` method's parameter type from `LikeView.ObjectType` to `String`. This indicates a modification to the API, likely for flexibility or to accommodate different object types. The change doesn't appear to be related to correcting any specific error or faulty behavior. Therefore, it's not a bug fix. The answer is **NotBuggy**.
"
facebook-android-sdk,818.json,845ba874cc5552f4deccf7d6262d351e4bbd72e4,"@@ -1,4 +1,5 @@
-        public void writeToParcel(Parcel parcel, int flags) {
-            parcel.writeString(actionText);
-            parcel.writeLong(publishDate.getTime());
+        public void writeToParcel(Parcel parcel, int i) {
+            parcel.writeString(callId.toString());
+            parcel.writeParcelable(requestIntent, 0);
+            parcel.writeInt(requestCode);
         }",NotBuggy,"Facebook Android SDK 3.5 (see https://developers.facebook.com/android/change-log-3.x/)

Summary: Facebook Android SDK 3.5

Test Plan: - Release testing

Reviewers: mingfli

Reviewed By: mingfli

CC: platform-diffs@lists, subodh, dreiss

Differential Revision: https://phabricator.fb.com/D935263
",Buggy,"The commit message indicates a release of the Facebook Android SDK 3.5, pointing to a change log for details. The diff shows changes to the `writeToParcel` method, specifically altering the data being written to the parcel. The original code wrote `actionText` and `publishDate`, while the modified code writes `callId`, `requestIntent`, and `requestCode`. This change suggests a modification in how data is serialized for inter-process communication or storage. Without more context, it's difficult to determine if this is a bug fix. However, the change in data being written suggests a potential update to the data structure or serialization logic, which could be related to fixing a bug in how data was previously handled. The answer is **Buggy**."
facebook-android-sdk,2702.json,d4a9d0c92085e732231e5bbb09f8e1f1fe292f6a,"@@ -1,3 +1,11 @@
-    public static Request newMyFriendsRequest(Session session, Callback callback) {
-        return new Request(session, MY_FRIENDS, null, null, callback);
+    public static Request newMyFriendsRequest(Session session, final GraphUserListCallback callback) {
+        Callback wrapper = new Callback() {
+            @Override
+            public void onCompleted(Response response) {
+                if (callback != null) {
+                    callback.onCompleted(typedListFromResponse(response, GraphUser.class), response);
+                }
+            }
+        };
+        return new Request(session, MY_FRIENDS, null, null, wrapper);
     }",NotBuggy,"Minor Request API changes

Summary:
- Added execute variants to Request static helpers
- Made callbacks strongly typed for Request statics
- Renamed execute -> executeAndWait
- Added instance executeAndWait/executeAsync to Request and RequestBatch

Test Plan:
- added unit tests

Revert Plan:
- ok

Tags:

Reviewers: mingfli, clang

Reviewed By: mingfli

Differential Revision: https://phabricator.fb.com/D600294

Task ID: 1801701
",NotBuggy,"The commit message indicates API changes, including making callbacks strongly typed. The diff shows a change in the `newMyFriendsRequest` method where the `Callback` is wrapped to provide a `GraphUserListCallback` with a typed list of `GraphUser` objects. This change ensures that the callback receives a strongly typed list, which aligns with the commit message's intention to improve type safety. There is no indication of a bug fix. The answer is **NotBuggy**.
"
facebook-android-sdk,2571.json,1ebea4d8e02aff0bc96cad34c959ab680421f8eb,"@@ -1,7 +1,9 @@
     private static void cancelCurrentTask() {
-        if (currentFuture != null) {
-            currentFuture.cancel(false);
-        }
+        synchronized (currentFutureLock) {
+            if (currentFuture != null) {
+                currentFuture.cancel(false);
+            }
 
-        currentFuture = null;
+            currentFuture = null;
+        }
     }",NotBuggy,"Facebook Android SDK 4.27
",Buggy,"The commit message indicates an update to the Facebook Android SDK. The diff introduces a synchronization block around the cancellation and nullification of `currentFuture`. This change likely addresses a concurrency issue where multiple threads might try to access or modify `currentFuture` simultaneously, potentially leading to race conditions or unexpected behavior. The addition of `synchronized (currentFutureLock)` ensures that only one thread can access and modify `currentFuture` at a time, preventing these issues. This is a bug fix. The answer is **Buggy**.
"
facebook-android-sdk,2275.json,2edb0d361bce159d052355395d6f3dd443c29a6b,"@@ -1,38 +1,38 @@
     private static synchronized void initialize() {
         try {
             FetchedAppSettings settings = FetchedAppSettingsManager.queryAppSettings(
                     FacebookSdk.getApplicationId(), false);
             if (settings == null) {
                 return;
             }
             String eventFilterResponse = settings.getRestrictiveDataSetting();
             if (!eventFilterResponse.isEmpty()) {
                 JSONObject jsonObject = new JSONObject(eventFilterResponse);
 
-                deprecatedParams.clear();
+                deprecatedParamFilters.clear();
 
                 Iterator<String> keys = jsonObject.keys();
                 while (keys.hasNext()) {
                     String key = keys.next();
                     JSONObject json = jsonObject.getJSONObject(key);
                     if (json != null) {
                         if (json.optBoolean(""is_deprecated_event"")) {
                             deprecatedEvents.add(key);
                         } else {
                             JSONArray deprecatedParamJsonArray = json
                                     .optJSONArray(""deprecated_param"");
-                            DeprecatedParam deprecatedParam = new DeprecatedParam(key,
+                            DeprecatedParamFilter deprecatedParamFilter = new DeprecatedParamFilter(key,
                                     new ArrayList<String>());
                             if (deprecatedParamJsonArray != null) {
-                                deprecatedParam.deprecateParams = Utility
+                                deprecatedParamFilter.deprecateParams = Utility
                                         .convertJSONArrayToList(deprecatedParamJsonArray);
                             }
-                            deprecatedParams.add(deprecatedParam);
+                            deprecatedParamFilters.add(deprecatedParamFilter);
                         }
                     }
                 }
             }
         } catch (Exception e) {
             /* swallow */
         }
     }",NotBuggy,"Refactor: better naming and several small changes

Reviewed By: ZebingZong

Differential Revision: D19523661

fbshipit-source-id: 664f47c474172136f9ef6e2f7f157f428cd8f5f6
",NotBuggy,"The commit message indicates a refactoring with better naming and small changes. The diff shows renaming `deprecatedParams` to `deprecatedParamFilters` and `DeprecatedParam` to `DeprecatedParamFilter`. The logic appears to remain the same. The catch block swallowing the exception suggests defensive programming, but it doesn't necessarily indicate a bug fix. Overall, the changes seem to be focused on improving code clarity and organization rather than addressing a specific bug. The answer is **NotBuggy**.
"
facebook-android-sdk,2617.json,426267f3dcc9daadcf383ec1896bffa1d7d46852,"@@ -1,9 +1,7 @@
         public String toKey() {
             switch (this) {
-                case ADDRESS_DETECTION:
-                case APP_EVENT_PREDICTION: return ""fc3"";
                 case MTML_ADDRESS_DETECTION: return ""address_detect"";
                 case MTML_APP_EVENT_PREDICTION: return ""app_event_pred"";
             }
             return ""Unknown"";
         }",NotBuggy,"Remove Non-MTML model

Summary: Same as the title

Reviewed By: jiangcici

Differential Revision: D20863645

fbshipit-source-id: e34e7b2a6a3aaddbf82b543c46eda834339da7ae
",NotBuggy,"The commit message indicates the removal of a ""Non-MTML model"". The code diff shows the removal of two cases, `ADDRESS_DETECTION` and `APP_EVENT_PREDICTION`, from a switch statement. These cases likely represent the ""Non-MTML model"" mentioned in the commit message. This removal suggests the code is being cleaned up or refactored to remove obsolete or deprecated functionality. There is no indication of a bug fix in the traditional sense, such as correcting a calculation error or handling an exception. The change seems to be about removing code, not fixing a flaw in existing code. The answer is **NotBuggy**.
"
facebook-android-sdk,304.json,b2fa42b8fca13e5f44025fb76b270ef8b9733eb8,"@@ -1,20 +1,26 @@
     private static void validate(ShareContent content, Validator validator)
             throws FacebookException {
         if (content == null) {
             throw new FacebookException(""Must provide non-null content to share"");
         }
 
         if (content instanceof ShareLinkContent) {
             validator.validate((ShareLinkContent) content);
         } else if (content instanceof SharePhotoContent) {
             validator.validate((SharePhotoContent) content);
         } else if (content instanceof ShareVideoContent) {
             validator.validate((ShareVideoContent) content);
         } else if (content instanceof ShareOpenGraphContent) {
             validator.validate((ShareOpenGraphContent) content);
         } else if (content instanceof ShareMediaContent) {
             validator.validate((ShareMediaContent) content);
         } else if (content instanceof ShareCameraEffectContent) {
             validator.validate((ShareCameraEffectContent) content);
+        } else if (content instanceof ShareMessengerOpenGraphMusicTemplateContent) {
+            validator.validate((ShareMessengerOpenGraphMusicTemplateContent) content);
+        } else if (content instanceof ShareMessengerMediaTemplateContent) {
+            validator.validate((ShareMessengerMediaTemplateContent) content);
+        } else if (content instanceof ShareMessengerGenericTemplateContent) {
+            validator.validate((ShareMessengerGenericTemplateContent) content);
         }
     }",NotBuggy,"Facebook Android SDK 4.29
",NotBuggy,"The commit message indicates an update to the Facebook Android SDK. The diff adds validation for three new content types: `ShareMessengerOpenGraphMusicTemplateContent`, `ShareMessengerMediaTemplateContent`, and `ShareMessengerGenericTemplateContent`. This change extends the functionality of the SDK to support these new content types. There is no indication of a bug fix in the commit message or the code changes. The answer is **NotBuggy**."
titan,4308.json,53441fa4ba940ba38ebbe2710ce9041819722020,"@@ -1,3 +1,3 @@
-    public boolean isValidDataType(Class<?> clazz) {
-        return true;
+    public boolean isQNF() {
+        return false;
     }",NotBuggy,"Added the notion of Query-Normal-Form (QNF) to property constraints and queries in Titan. Now, all queries inside titan have to conform to QNF which makes it easier to deal with conditions because one can precondition on QNF and then process accordingly. NOT YET TESTED.
",NotBuggy,"The commit message introduces ""Query-Normal-Form (QNF)"" and the diff changes a method `isValidDataType` to `isQNF` and makes it return `false`. This change seems to be related to the introduction of QNF, and the commit message states that it is ""NOT YET TESTED"". The change itself doesn't seem to fix a bug, but rather to introduce a new feature and related constraints. The original method `isValidDataType` always returned true, which might have been a simplification or a default behavior. The new method `isQNF` always returns false, indicating that the QNF is not yet implemented or enabled. This is not a bug fix. The answer is **NotBuggy**.
"
titan,1229.json,ba36d65be185a738d418b86746aad625361bb71b,"@@ -1,3 +1,3 @@
     private static int getLimit(long limitAndValuePos) {
-        return (int)(limitAndValuePos>>>32);
+        return (int)(limitAndValuePos>>>32l);
     }",NotBuggy,"Bugifixing of backend change
",Buggy,"The commit message indicates a bug fix. The code change involves a subtle modification to a bitwise right shift operation. Specifically, the shift amount `32` is changed to `32l`. This suggests that the original code might have been treating `32` as an integer, potentially leading to unexpected behavior or incorrect results when used with a `long` value. By changing it to `32l`, it ensures that the shift operation is performed correctly on the `long` value. The answer is **Buggy**.
"
titan,4809.json,ae15683f30b52b053cb7c62c6bd9162e374f62a4,"@@ -1,3 +1,3 @@
-	public int getNumCompactions() {
-		return numberOfCompactions;
-	}

+    public int getNumCompactions() {
+        return numberOfCompactions;
+    }",NotBuggy,"big fatty refactor -- tests do not pass.
",Buggy,"The commit message indicates a ""big fatty refactor"" and explicitly states that the tests do not pass, suggesting potential issues or bugs introduced during the refactoring process. The code change itself is minimal, only adjusting the indentation of the code. However, given the commit message, it is likely that this change is part of a larger refactoring effort that has introduced bugs. The answer is **Buggy**."
titan,4181.json,9737d31b23256e071d7e761a53819feff896b472,"@@ -1,6 +1,9 @@
     public QueryProfiler addNested(String groupName) {
+        //Flatten out AND/OR nesting
+        if (groupName.equals(AND_QUERY) || groupName.equals(OR_QUERY)) return this;
+
         int nextId = (subMetricCounter++);
         MutableMetrics nested = new MutableMetrics(metrics.getId()+"".""+groupName+""_""+nextId,groupName);
         metrics.addNested(nested);
         return new TP3ProfileWrapper(nested);
     }",NotBuggy,"Flattening profile metrics
",Buggy,"The commit message indicates a change related to ""Flattening profile metrics"". The code diff shows that when the `groupName` is either ""AND_QUERY"" or ""OR_QUERY"", the method returns `this` without creating a new nested metric. This suggests that the intention is to avoid creating nested metrics for AND/OR queries, effectively flattening the profile metrics. This change likely addresses a bug or undesired behavior where nested AND/OR queries were creating overly complex or incorrect metrics. The answer is **Buggy**."
titan,816.json,eab917b78fa300d8e6d81a543d4f1738f945b5b2,"@@ -1,3 +1,3 @@
-    public ByteBuffer getSliceEnd() {
+    public StaticBuffer getSliceEnd() {
         return sliceEnd;
     }",NotBuggy,"Refactored titan-core to use custom Buffer interfaces for better abstraction and thread safety instead of ByteBuffer.

INTERMEDIATE COMMIT - DOES NOT BUILD
",NotBuggy,"The commit message indicates a refactoring of titan-core to use custom Buffer interfaces instead of ByteBuffer for better abstraction and thread safety. The diff shows a change in the return type of the `getSliceEnd()` method from `ByteBuffer` to `StaticBuffer`. This change aligns with the commit message's intention to replace `ByteBuffer` with custom buffer interfaces. This refactoring aims to improve abstraction and thread safety, but it doesn't directly indicate a bug fix. The answer is **NotBuggy**.
"
titan,3913.json,ae15683f30b52b053cb7c62c6bd9162e374f62a4,"@@ -1,3 +1,3 @@
         public boolean hasNext() {
-            return nextEdge!=null;
+            return nextEdge != null;
         }",NotBuggy,"big fatty refactor -- tests do not pass.
",NotBuggy,"The commit message indicates a refactoring, and the tests are not passing. The diff itself is a simple change in spacing. While the tests not passing could indicate a bug, the diff itself doesn't show any bug fix. The answer is **NotBuggy**."
titan,2659.json,d5662482340f61d370cde27fece75d7ecb6b16e4,"@@ -1,3 +1,3 @@
-                public boolean apply(@Nullable InternalRelation internalRelation) {
-                    return internalRelation.getType().equals(key);
-                }

+                    public boolean apply(@Nullable TitanVertex vertex) {
+                        return query.matches(vertex);
+                    }",NotBuggy,"Intermediate commit on indexing and complete refactor
",NotBuggy,"The commit message indicates a refactoring and changes related to indexing. The code diff shows a change in the `apply` method's functionality. It seems the original code filtered `InternalRelation` objects based on their type, while the modified code filters `TitanVertex` objects based on a `query.matches` condition. This suggests a shift in the filtering logic, likely related to the refactoring and indexing changes mentioned in the commit message. While it's a significant change, it doesn't inherently indicate a bug fix. The change seems to be part of a larger refactoring effort. The answer is **NotBuggy**."
titan,526.json,d4056cb5a0ec5764ba5d0bd7454c68062d576ba1,"@@ -1,4 +1,4 @@
-        public Builder supportedMappings(Mapping... maps) {
+        public Builder supportedStringMappings(Mapping... maps) {
             supportedMappings.addAll(Arrays.asList(maps));
             return this;
         }",NotBuggy,"Finised streamlining test cases and query semantics across the different indexing backends. Added additional test coverage for Lucene.
SOLR does not yet work.
",NotBuggy,"The commit message indicates streamlining test cases and query semantics across different indexing backends, along with added test coverage for Lucene. The diff shows a method name change from `supportedMappings` to `supportedStringMappings`. This change doesn't seem to be directly related to bug fixing but rather to refactoring or clarifying the purpose of the method. The change in name suggests a more specific type of mapping being supported. Therefore, it's unlikely that this change is a bug fix. The answer is **NotBuggy**.
"
titan,5594.json,31b68d5c462b46d42f4ca7d3b3f63923a6d5a1c7,"@@ -1,12 +1,12 @@
     public static ScanMetrics hbaseRemove(String titanPropertiesPath, String indexName, String relationType)
             throws InterruptedException, IOException, ClassNotFoundException {
         Properties p = new Properties();
         FileInputStream fis = null;
         try {
             fis = new FileInputStream(titanPropertiesPath);
             p.load(fis);
             return hbaseRemove(p, indexName, relationType);
         } finally {
-            IOUtils.close(fis);
+            IOUtils.closeQuietly(fis);
         }
     }",NotBuggy,"Remove elasticsearch-hadoop-mr dependency

titan-hadoop has depended on this artifact since
042d04148489f4650e232682388f24ace709a95b, but it was only actually
used for its IOUtils.close static helper method.  Titan already has a
bunch of alternative providers of static helper methods with
equivalent functionality.
",NotBuggy,"The commit message indicates the removal of a dependency due to the limited usage of a single utility method (`IOUtils.close`). The diff replaces `IOUtils.close(fis)` with `IOUtils.closeQuietly(fis)`. While this change relates to closing a stream, the commit message suggests the primary motivation is dependency removal rather than fixing a bug in stream closing. The change from `close` to `closeQuietly` suggests a change in error handling, but it's more related to the dependency removal. The answer is **NotBuggy**.
"
titan,2713.json,51075f047f33c09c17c33f7e61fcf859c815ed52,"@@ -1,5 +1,6 @@
-    protected void verifyAccess() {
-        if (isRemoved()) {
-            throw InvalidElementException.removedException(this);
-        }
+    protected final void verifyAccess() {
+        return;
+//        if (isRemoved()) {
+//            throw InvalidElementException.removedException(this);
+//        }
     }",NotBuggy,"Updated to new TP3
",Buggy,"The commit message ""Updated to new TP3"" indicates a general update, likely to a new version of a library or framework called TP3. The code diff shows that the `verifyAccess` method is being modified. The original code had a check to see if the element was removed, and if so, it would throw an `InvalidElementException`. The updated code now has an empty method body with a commented-out version of the original code. This change effectively disables the check for removed elements. It's difficult to say if this is a bug fix without more context. It is possible that the check was causing issues in the new version of TP3, or that the check is now handled elsewhere. However, given the information, it appears that the original check was intentionally removed. It's not immediately clear if this resolves a bug or introduces one. Since the original check was removed, it is possible that this change is related to a bug fix.

The answer is **Buggy**."
titan,2620.json,aead96abd7152d9843e25578fd640dbbdaa8b765,"@@ -1,3 +1,3 @@
     public Iterable<Edge> getEdges() {
-        return new VertexCentricEdgeIterable(getVertices());
+        return new VertexCentricEdgeIterable(getInternalVertices(),RelationCategory.EDGE);
     }",NotBuggy,"Intermediate big bang commit
",NotBuggy,"The commit message ""Intermediate big bang commit"" is vague and doesn't provide specific information about the purpose of the change. The code modifies the `getEdges()` method to use `getInternalVertices()` and `RelationCategory.EDGE`. Without more context, it's difficult to determine if this change is related to a bug fix or a new feature. It could be an internal refactoring or part of a larger architectural change. Therefore, based on the available information, it is not possible to determine if the change is related to a bug fix. The answer is **NotBuggy**."
titan,4872.json,ae15683f30b52b053cb7c62c6bd9162e374f62a4,"@@ -1,12 +1,12 @@
-	public static final long[] arrayDifference(long[] arr, long[] subset) {
-		long[] res = new long[arr.length-subset.length];
-		int pos = 0;
-		for (int i=0;i<arr.length;i++) {
-			if (!Longs.contains(subset, arr[i])) {
-				res[pos]=arr[i];
-				pos++;
-			}
-		}
-		assert pos==res.length;
-		return res;
-	}

+    public static final long[] arrayDifference(long[] arr, long[] subset) {
+        long[] res = new long[arr.length - subset.length];
+        int pos = 0;
+        for (int i = 0; i < arr.length; i++) {
+            if (!Longs.contains(subset, arr[i])) {
+                res[pos] = arr[i];
+                pos++;
+            }
+        }
+        assert pos == res.length;
+        return res;
+    }",NotBuggy,"big fatty refactor -- tests do not pass.
",NotBuggy,"The commit message indicates a refactor, and the provided diff shows a change in indentation. There are no functional changes, error handling updates, logical corrections, or exception-handling improvements. Therefore, this is not a bug fix. The answer is **NotBuggy**."
titan,4164.json,7f94323100a007d1f346b163485b14a43a53dd4c,"@@ -1,3 +1,3 @@
     public int getLimit() {
-        return getBranchFactor();
+        return this.limit;
     }",NotBuggy,"Updated optimizers to include PropertyStep. Reworked current logic. Tests to be added.
",Buggy,"The commit message indicates an update to optimizers and a rework of current logic. The diff shows a change in the `getLimit()` method, where the return value is switched from `getBranchFactor()` to `this.limit`. This suggests a modification in how the limit is obtained, potentially correcting a previous incorrect implementation or aligning it with the reworked logic. Without more context, it's difficult to definitively say if this is a bug fix, but the change in logic suggests a potential correction. The answer is **Buggy**."
titan,2594.json,e1ac9ae34518829d69f60234525b2133b31a88e2,"@@ -1,4 +1,4 @@
     public InternalVertex getOtherPartitionVertex(TitanVertex partitionedVertex, long otherPartition) {
         Preconditions.checkArgument(isPartitionedVertex(partitionedVertex));
-        return getExistingVertex(idManager.getPartitionedVertexId(partitionedVertex.getID(), otherPartition));
+        return getExistingVertex(idManager.getPartitionedVertexId(partitionedVertex.getLongId(), otherPartition));
     }",NotBuggy,"Partial Faunus refactoring
",Buggy,"The commit message indicates a refactoring. The code change replaces `partitionedVertex.getID()` with `partitionedVertex.getLongId()`. This suggests that the original code might have been using the wrong method to retrieve the vertex ID, potentially leading to incorrect behavior. This correction indicates a bug fix. The answer is **Buggy**.
"
