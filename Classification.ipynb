{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%autosave 120\n",
    "import sys\n",
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from modules.utilities import are_bugs_from_tangled, has_bug, is_same_history, parse_hash_delimited_string\n",
    "from sklearn.metrics import (accuracy_score, f1_score, matthews_corrcoef, mean_absolute_error, r2_score, mean_absolute_percentage_error, mean_squared_error, roc_auc_score, root_mean_squared_error, precision_score, recall_score)\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, RocCurveDisplay\n",
    "from tabulate import tabulate\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.calibration import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import average_precision_score\n",
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cdf(vals, labels, title):\n",
    "    plt.figure(figsize=(20, 6))\n",
    "    markers = [\"o\", \"^\", \"D\"]\n",
    "    for values, marker, label in zip(vals,markers[:len(vals)], labels):\n",
    "        values = np.array(values)\n",
    "        sorted_values = np.sort(values)\n",
    "\n",
    "        cdf_R = np.arange(1, len(sorted_values) + 1) / len(sorted_values)\n",
    "        \n",
    "        plt.plot(sorted_values, cdf_R, marker=marker, label=label)\n",
    "        \n",
    "    plt.yticks(np.arange(0,1.1,0.1))\n",
    "    plt.xticks(np.arange(0,1.1,0.1))\n",
    "    plt.xlabel(\"Score\")\n",
    "    plt.ylabel(\"CDF\")\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_label(x):\n",
    "    if x == \"NotBuggy\":\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "columns = ['SLOCStandard', 'Readability', 'SimpleReadability', 'NVAR',\n",
    "       'NCOMP', 'Mcclure', 'McCabe', 'IndentSTD', 'MaximumBlockDepth',\n",
    "       'totalFanOut', 'Length', 'MaintainabilityIndex', 'SATD', 'Parameters',\n",
    "       'LocalVariables', \"Decision\", \"Detection\"]   \n",
    "x_dfs = []\n",
    "y_dfs = []\n",
    "y_cleaned_dfs = []\n",
    "names = []    \n",
    "\n",
    "csv_files = glob.glob(\"./data/Cleaned/*.csv\")\n",
    "for csv_file in tqdm(csv_files):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    df = df[columns]\n",
    "\n",
    "    df[\"Decision\"] = df[\"Decision\"].apply(to_label)\n",
    "    df[\"Detection\"] = df[\"Detection\"].apply(to_label)\n",
    "\n",
    "    y_dfs.append(df[\"Decision\"])\n",
    "    y_cleaned_dfs.append(df[\"Detection\"])\n",
    "\n",
    "    X = df.drop([\"Decision\",\"Detection\"], axis=1)\n",
    "    X = X.map(lambda x: x.split(\"#\")[0])\n",
    "    X = X.astype(float)\n",
    "    x_dfs.append(X)\n",
    "\n",
    "    names.append(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = []\n",
    "precisions= []\n",
    "recalls = []\n",
    "fscores = []\n",
    "\n",
    "precision_micro = []\n",
    "recall_micro = []\n",
    "fscore_micro = []\n",
    "\n",
    "precision_macro = []\n",
    "recall_macro = []\n",
    "fscore_macro = []\n",
    "\n",
    "precision_weighted = []\n",
    "recall_weighted = []\n",
    "fscore_weighted = []\n",
    "\n",
    "mcc = []\n",
    "roc = []\n",
    "\n",
    "cleaned_acc = []\n",
    "cleaned_precisions= []\n",
    "cleaned_recalls = []\n",
    "cleaned_fscores = []\n",
    "\n",
    "cleaned_precision_micro = []\n",
    "cleaned_recall_micro = []\n",
    "cleaned_fscore_micro = []\n",
    "\n",
    "cleaned_precision_macro = []\n",
    "cleaned_recall_macro = []\n",
    "cleaned_fscore_macro = []\n",
    "\n",
    "cleaned_precision_weighted = []\n",
    "cleaned_recall_weighted = []\n",
    "cleaned_fscore_weighted = []\n",
    "\n",
    "cleaned_roc = []\n",
    "cleaned_mcc = []\n",
    "\n",
    "sampling = True\n",
    "sampler = RandomOverSampler(random_state=random_seed)\n",
    "\n",
    "cv = LeaveOneOut()\n",
    "for train, test in tqdm(cv.split(x_dfs)):\n",
    "    x_train = [x_dfs[i] for i in train]\n",
    "    x_train = pd.concat(x_train,ignore_index=True)\n",
    "\n",
    "    x_test = x_dfs[test[0]]\n",
    "\n",
    "    # Original\n",
    "\n",
    "    y_train = [y_dfs[i] for i in train]\n",
    "    y_train = pd.concat(y_train,ignore_index=True)\n",
    "\n",
    "    y_test = y_dfs[test[0]]\n",
    "\n",
    "\n",
    "    if not sampling:\n",
    "        x_train_res, y_train_res = x_train, y_train\n",
    "    else:\n",
    "        x_train_res, y_train_res = sampler.fit_resample(x_train, y_train)\n",
    "\n",
    "    model = RandomForestClassifier(random_state=random_seed)\n",
    "    model.fit(x_train_res, y_train_res)\n",
    "\n",
    "    y_pred = model.predict(x_test)\n",
    "    \n",
    "    acc.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "    precisions.append(precision_score(y_test, y_pred, pos_label=1))\n",
    "    recalls.append(recall_score(y_test, y_pred, pos_label=1))\n",
    "    fscores.append(f1_score(y_test, y_pred, pos_label=1))\n",
    "\n",
    "    precision_micro.append(precision_score(y_test, y_pred, average=\"micro\", pos_label=1))\n",
    "    recall_micro.append(recall_score(y_test, y_pred, average=\"micro\", pos_label=1))\n",
    "\n",
    "    precision_macro.append(precision_score(y_test, y_pred, average=\"macro\", pos_label=1))\n",
    "    recall_macro.append(recall_score(y_test, y_pred, average=\"macro\", pos_label=1))\n",
    "\n",
    "    precision_weighted.append(precision_score(y_test, y_pred, average=\"weighted\", pos_label=1))\n",
    "    recall_weighted.append(recall_score(y_test, y_pred, average=\"weighted\", pos_label=1))\n",
    "\n",
    "    roc.append(roc_auc_score(y_test, y_pred))\n",
    "    mcc.append(matthews_corrcoef(y_test, y_pred))\n",
    "\n",
    "    # Cleaned\n",
    "\n",
    "    y_cleaned_train = [y_cleaned_dfs[i] for i in train]\n",
    "    y_cleaned_train = pd.concat(y_cleaned_train, ignore_index=True)\n",
    "\n",
    "    y_cleaned_test = y_cleaned_dfs[test[0]]\n",
    "\n",
    "    if not sampling:\n",
    "        x_train_res, y_cleaned_train_res = x_train, y_cleaned_train\n",
    "    else:\n",
    "        x_train_res, y_cleaned_train_res = sampler.fit_resample(x_train, y_cleaned_train)\n",
    "\n",
    "    model = RandomForestClassifier(random_state=random_seed)\n",
    "    model.fit(x_train_res, y_cleaned_train_res)\n",
    "\n",
    "    y_pred = model.predict(x_test)\n",
    "    \n",
    "    cleaned_acc.append(accuracy_score(y_cleaned_test, y_pred))\n",
    "\n",
    "    cleaned_precisions.append(precision_score(y_cleaned_test, y_pred, pos_label=1))\n",
    "    cleaned_recalls.append(recall_score(y_cleaned_test, y_pred, pos_label=1))\n",
    "    cleaned_fscores.append(f1_score(y_cleaned_test, y_pred, pos_label=1))\n",
    "\n",
    "    cleaned_precision_micro.append(precision_score(y_cleaned_test, y_pred, average=\"micro\", pos_label=1))\n",
    "    cleaned_recall_micro.append(recall_score(y_cleaned_test, y_pred, average=\"micro\", pos_label=1))\n",
    "\n",
    "    cleaned_precision_macro.append(precision_score(y_cleaned_test, y_pred, average=\"macro\", pos_label=1))\n",
    "    cleaned_recall_macro.append(recall_score(y_cleaned_test, y_pred, average=\"macro\", pos_label=1))\n",
    "\n",
    "    cleaned_precision_weighted.append(precision_score(y_cleaned_test, y_pred, average=\"weighted\", pos_label=1))\n",
    "    cleaned_recall_weighted.append(recall_score(y_cleaned_test, y_pred, average=\"weighted\", pos_label=1))\n",
    "\n",
    "    cleaned_roc.append(roc_auc_score(y_cleaned_test, y_pred))\n",
    "    mcc.append(matthews_corrcoef(y_cleaned_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result():\n",
    "    data = {\n",
    "        \"Metric\": [\n",
    "            \"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"Micro Precision\", \"Micro Recall\", \"Macro Precision\", \"Macro Recall\", \"Weighted Precision\", \"Weighted Recall\", \"ROC\", \"MCC\"\n",
    "        ],\n",
    "        \"With Noise (Mean)\": [\n",
    "            np.mean(acc), np.mean(precisions), np.mean(recalls), np.mean(fscores), np.mean(precision_micro), np.mean(recall_micro), \n",
    "            np.mean(precision_macro), np.mean(recall_macro), np.mean(precision_weighted), \n",
    "            np.mean(recall_weighted), np.mean(roc), np.mean(mcc)\n",
    "        ],\n",
    "        \"With Noise (Std)\": [\n",
    "            np.std(acc), np.std(precisions), np.std(recalls), np.std(fscores), np.std(precision_micro), np.std(recall_micro), \n",
    "            np.std(precision_macro), np.std(recall_macro), np.std(precision_weighted), \n",
    "            np.std(recall_weighted), np.std(roc), np.std(mcc)\n",
    "        ],\n",
    "        \"Without Noise (Mean)\": [\n",
    "            np.mean(cleaned_acc), np.mean(cleaned_precisions), np.mean(cleaned_recalls), np.mean(cleaned_fscores), np.mean(cleaned_precision_micro), np.mean(cleaned_recall_micro), \n",
    "            np.mean(cleaned_precision_macro), np.mean(cleaned_recall_macro), np.mean(cleaned_precision_weighted), \n",
    "            np.mean(cleaned_recall_weighted), np.mean(cleaned_roc), np.mean(cleaned_mcc)\n",
    "        ],\n",
    "        \"Without Noise (Std)\": [\n",
    "            np.std(cleaned_acc), np.std(cleaned_precisions), np.std(cleaned_recalls), np.std(cleaned_fscores), np.std(cleaned_precision_micro), np.std(cleaned_recall_micro), \n",
    "            np.std(cleaned_precision_macro), np.std(cleaned_recall_macro), np.std(cleaned_precision_weighted), \n",
    "            np.std(cleaned_recall_weighted), np.std(cleaned_roc), np.std(cleaned_mcc)\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cdf([precisions, cleaned_precisions], [\"With Noise\", \"Without Noise\"], \"Precision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cdf([recalls, cleaned_recalls], [\"With Noise\", \"Without Noise\"], \"Recall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cdf([fscores, cleaned_fscores], [\"With Noise\", \"Without Noise\"], \"F1-score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_result()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
